Title: From Commercial to Open Source Cloud Foundry in the Enterprise
Publication date: 2018-10-11
Playlist: Cloud Foundry Summit Europe 2018
Description: 
	From Commercial to Open Source Cloud Foundry in the Enterprise - Rachael Wonnacott, Fidelity
International

Fidelityâ€™s PaaS supports business critical applications such as retail websites and trading systems, while offering a six-9s SLA across both the UK and Asia. A key driver in their cloud strategy has always been to deliver features to developers in a cost effective way. The transition to using open source Cloud Foundry has afforded Fidelity the autonomy to tailor releases to meet the rising demand from developers. Fidelity would like to share their experience focusing on the deployment and migration decisions (running two CFs in parallel, migrating applications with zero down-time, and the benefits of a single deployment mechanism). In addition, to explore the ongoing implications for the globally distributed PaaS team who operate it (increased responsibility, utilisation of the open-source community, and the change in workload) and ask - 'Is the platform sustainable?'

https://cfseu18.sched.com/event/FRzD/from-commercial-to-open-source-cloud-foundry-in-the-enterprise-rachael-wonnacott-fidelity-international
Captions: 
	00:00:00,030 --> 00:00:03,300
good evening everyone my name is Rachel

00:00:02,669 --> 00:00:05,549
wanna cots

00:00:03,300 --> 00:00:07,080
I'm from fidelity international and it's

00:00:05,549 --> 00:00:08,880
really my pleasure today to tell you

00:00:07,080 --> 00:00:10,950
about our journey from commercial to

00:00:08,880 --> 00:00:14,750
open source cloud foundry and the

00:00:10,950 --> 00:00:14,750
enterprise I hope some of you can catch

00:00:15,830 --> 00:00:27,359
if you want the front engineers can

00:00:23,699 --> 00:00:29,369
throw okay so before we get started

00:00:27,359 --> 00:00:31,260
today I thought it was personal I set

00:00:29,369 --> 00:00:33,210
the scene and for those of you who

00:00:31,260 --> 00:00:35,010
perhaps haven't seen us at summit before

00:00:33,210 --> 00:00:37,020
tell you a little bit more about

00:00:35,010 --> 00:00:40,520
fidelity international we are a

00:00:37,020 --> 00:00:43,110
privately owned company founded in 1969

00:00:40,520 --> 00:00:45,660
offering both investment solutions that

00:00:43,110 --> 00:00:47,820
are our own and those of others we have

00:00:45,660 --> 00:00:50,850
about three hundred and fifteen point

00:00:47,820 --> 00:00:53,160
six billion pounds under assets for

00:00:50,850 --> 00:00:56,850
clients ranging from sovereign wealth

00:00:53,160 --> 00:00:58,620
funds pension funds insurance private

00:00:56,850 --> 00:01:01,170
corporate banks all over the world in

00:00:58,620 --> 00:01:04,530
reasons such as Asia Pacific Europe

00:01:01,170 --> 00:01:08,040
South America and even Africa that's a

00:01:04,530 --> 00:01:09,810
bit about us we started our cloud

00:01:08,040 --> 00:01:12,390
foundry journey about four years ago

00:01:09,810 --> 00:01:14,549
when we were a technology driven

00:01:12,390 --> 00:01:16,500
organization that was looking to reduce

00:01:14,549 --> 00:01:19,170
our time to market it was really as

00:01:16,500 --> 00:01:21,090
simple as that and the appeal of cloud

00:01:19,170 --> 00:01:23,430
foundry lied in that it lets the app

00:01:21,090 --> 00:01:25,409
developers only care about the

00:01:23,430 --> 00:01:27,570
applications and the data without caring

00:01:25,409 --> 00:01:29,670
about the underlying infrastructure but

00:01:27,570 --> 00:01:31,950
additionally we really like that cloud

00:01:29,670 --> 00:01:33,930
foundry is governed by a foundation and

00:01:31,950 --> 00:01:34,740
would give us the opportunity to go

00:01:33,930 --> 00:01:36,780
open-source

00:01:34,740 --> 00:01:40,020
at a later date should we choose to do

00:01:36,780 --> 00:01:41,790
so now in general as a team and as a

00:01:40,020 --> 00:01:43,950
company we were really excited about the

00:01:41,790 --> 00:01:46,740
idea of going open source to share ideas

00:01:43,950 --> 00:01:51,450
to share code but we didn't have any

00:01:46,740 --> 00:01:53,280
experience of managing app as so in

00:01:51,450 --> 00:01:54,680
those four years we've come a really

00:01:53,280 --> 00:01:57,960
long way

00:01:54,680 --> 00:02:00,149
last year the disruptive principles that

00:01:57,960 --> 00:02:01,799
underpin our path service were nominated

00:02:00,149 --> 00:02:04,710
for an award at the financial services

00:02:01,799 --> 00:02:06,509
technology Awards we presented at summit

00:02:04,710 --> 00:02:08,849
two years ago with a keynote on how we

00:02:06,509 --> 00:02:11,940
put our product team together so we're a

00:02:08,849 --> 00:02:12,870
DevOps team and last year are really

00:02:11,940 --> 00:02:14,790
wonderful product

00:02:12,870 --> 00:02:17,970
mohamad who sadly isn't with us today

00:02:14,790 --> 00:02:19,980
was awarded the clouds now women in

00:02:17,970 --> 00:02:21,840
cloud Innovation Award 2017

00:02:19,980 --> 00:02:23,340
a little bit of a mouthful but we're

00:02:21,840 --> 00:02:25,080
really proud of what we've achieved and

00:02:23,340 --> 00:02:27,330
I'm personally really proud to be here

00:02:25,080 --> 00:02:30,299
today to represent my team and tell you

00:02:27,330 --> 00:02:32,849
all about our journey so in the

00:02:30,299 --> 00:02:34,980
beginning it was really difficult for us

00:02:32,849 --> 00:02:37,349
to know what the future of the platform

00:02:34,980 --> 00:02:40,019
would be what would the adoption of the

00:02:37,349 --> 00:02:42,269
platform be like we were introducing a

00:02:40,019 --> 00:02:45,000
very new concept into what I would argue

00:02:42,269 --> 00:02:46,620
was a very traditional landscape and it

00:02:45,000 --> 00:02:48,480
was difficult to estimate how many

00:02:46,620 --> 00:02:51,090
applications would want to migrate and

00:02:48,480 --> 00:02:52,109
of those that did at what rates would

00:02:51,090 --> 00:02:54,390
they want to migrate

00:02:52,109 --> 00:02:57,599
what would the traditional landscape

00:02:54,390 --> 00:02:59,609
mean for us how would we transform from

00:02:57,599 --> 00:03:02,640
traditional deployment methodologies to

00:02:59,609 --> 00:03:04,829
cloud native ones ultimately our entire

00:03:02,640 --> 00:03:07,430
journey has been underpinned by one

00:03:04,829 --> 00:03:10,290
theme and that theme here is capability

00:03:07,430 --> 00:03:12,959
so when I say capability what do I mean

00:03:10,290 --> 00:03:15,269
I mean two things I mean the capability

00:03:12,959 --> 00:03:18,269
of the platform team and I mean the

00:03:15,269 --> 00:03:19,590
capability of the developers and as I

00:03:18,269 --> 00:03:21,810
said in the beginning it was difficult

00:03:19,590 --> 00:03:23,790
to confirm what it would mean for us to

00:03:21,810 --> 00:03:25,160
run a cloud foundry platform so we

00:03:23,790 --> 00:03:27,660
wanted to ask ourselves some questions

00:03:25,160 --> 00:03:30,209
so how highly available

00:03:27,660 --> 00:03:32,700
would this platform be in practice what

00:03:30,209 --> 00:03:34,470
monitoring should we put in place how

00:03:32,700 --> 00:03:37,019
would we manage the dependencies between

00:03:34,470 --> 00:03:40,079
services how would we upgrade the

00:03:37,019 --> 00:03:42,569
platform how would we assure and secure

00:03:40,079 --> 00:03:44,430
reliable new services and perhaps most

00:03:42,569 --> 00:03:46,680
importantly this one at the bottom

00:03:44,430 --> 00:03:49,230
how efficient would our app devs be in

00:03:46,680 --> 00:03:50,970
housekeeping they're instances now all

00:03:49,230 --> 00:03:53,579
of these questions are valid questions

00:03:50,970 --> 00:03:55,139
for any cloud platform but what brings

00:03:53,579 --> 00:03:56,940
them all together in a common theme is

00:03:55,139 --> 00:04:01,290
that they're things that a vendor would

00:03:56,940 --> 00:04:03,510
usually assure for you so without any

00:04:01,290 --> 00:04:05,190
concrete estimates for the answers to

00:04:03,510 --> 00:04:07,319
these questions we also could not

00:04:05,190 --> 00:04:09,660
predict the peak load on the platform

00:04:07,319 --> 00:04:11,280
and at this point in our journey the

00:04:09,660 --> 00:04:13,079
maturity of the platform team was also

00:04:11,280 --> 00:04:15,239
quite low we were brand new to the

00:04:13,079 --> 00:04:17,130
technology we didn't have an experience

00:04:15,239 --> 00:04:19,530
and the result of both of these factors

00:04:17,130 --> 00:04:22,079
was that our risk appetite was very low

00:04:19,530 --> 00:04:23,940
and so a supported commercial offering

00:04:22,079 --> 00:04:25,940
was simply the most comfortable option

00:04:23,940 --> 00:04:28,970
for us

00:04:25,940 --> 00:04:30,760
over the first two years it became very

00:04:28,970 --> 00:04:33,110
apparent that a path was very popular

00:04:30,760 --> 00:04:34,700
many existing business critical

00:04:33,110 --> 00:04:37,280
applications have migrated to the

00:04:34,700 --> 00:04:39,470
platform and it was a really popular

00:04:37,280 --> 00:04:41,750
choice also for greenfield projects so

00:04:39,470 --> 00:04:42,920
we had a bit of a challenge and this

00:04:41,750 --> 00:04:44,330
challenge is something that I will

00:04:42,920 --> 00:04:47,630
affectionately call the bloat

00:04:44,330 --> 00:04:50,390
hence the pufferfish and by the bloats I

00:04:47,630 --> 00:04:53,390
mean a swell of platform utilization

00:04:50,390 --> 00:04:56,210
when our legacy apps first began to move

00:04:53,390 --> 00:04:57,710
to the cloud they were not optimized to

00:04:56,210 --> 00:04:58,160
be run in the cloud for a couple of

00:04:57,710 --> 00:05:01,190
reasons

00:04:58,160 --> 00:05:04,430
we had monolithic architectures we were

00:05:01,190 --> 00:05:06,860
utilizing memory intensive languages we

00:05:04,430 --> 00:05:08,690
had long release cycles we even had

00:05:06,860 --> 00:05:11,630
separate development and testing teams

00:05:08,690 --> 00:05:14,630
and people were quite uncomfortable with

00:05:11,630 --> 00:05:17,330
the concept of spinning up and up and

00:05:14,630 --> 00:05:20,030
down on demand both their applications

00:05:17,330 --> 00:05:21,410
and also their dev environments and this

00:05:20,030 --> 00:05:24,190
kind of meant that we had this really

00:05:21,410 --> 00:05:27,050
big peak load this swell of the platform

00:05:24,190 --> 00:05:28,820
over time with experience and education

00:05:27,050 --> 00:05:30,920
people started to split out their

00:05:28,820 --> 00:05:31,670
applications into micro services which

00:05:30,920 --> 00:05:34,160
was fantastic

00:05:31,670 --> 00:05:36,200
however with this came a demand who

00:05:34,160 --> 00:05:38,680
split out these larger orgs that had

00:05:36,200 --> 00:05:42,500
previously been representative of apps

00:05:38,680 --> 00:05:44,480
into smaller org spaces that were

00:05:42,500 --> 00:05:45,950
representative of shared micro services

00:05:44,480 --> 00:05:48,230
that could be consumed by multiple

00:05:45,950 --> 00:05:50,150
applications which is great but the

00:05:48,230 --> 00:05:53,090
problem with our original licensing

00:05:50,150 --> 00:05:54,710
model this was very expensive so how

00:05:53,090 --> 00:05:56,540
could we encourage good behavioral

00:05:54,710 --> 00:06:01,040
change when it came at a price for the

00:05:56,540 --> 00:06:02,180
app developers so over those first two

00:06:01,040 --> 00:06:04,790
years that I've just been talking about

00:06:02,180 --> 00:06:07,160
we'd built a great confidence in

00:06:04,790 --> 00:06:09,440
ourselves as a platform team and our

00:06:07,160 --> 00:06:10,880
ability to manage the platform and the

00:06:09,440 --> 00:06:13,370
demand for the pars was showing

00:06:10,880 --> 00:06:14,990
absolutely no signs of slowing down so

00:06:13,370 --> 00:06:17,540
we needed a solution that would scale

00:06:14,990 --> 00:06:19,490
more favorably with our organization so

00:06:17,540 --> 00:06:21,530
we started to investigate what it would

00:06:19,490 --> 00:06:28,550
mean to fully adopt open source cloud

00:06:21,530 --> 00:06:30,950
foundry how did we do it well our team

00:06:28,550 --> 00:06:32,180
likes to eat biscuits but unfortunately

00:06:30,950 --> 00:06:36,500
this problem was a little bit bigger

00:06:32,180 --> 00:06:37,580
than a few gingerbreads so really how do

00:06:36,500 --> 00:06:40,460
we do it

00:06:37,580 --> 00:06:42,919
a move to open-source was a huge risk

00:06:40,460 --> 00:06:46,610
and this risk needed to be evaluated and

00:06:42,919 --> 00:06:48,530
from an enterprise perspective as an FCA

00:06:46,610 --> 00:06:50,659
regulated company with very little

00:06:48,530 --> 00:06:52,669
appetite for risk we were traditionally

00:06:50,659 --> 00:06:54,979
accustomed to using vendors and making

00:06:52,669 --> 00:06:57,620
use of support contracts so open source

00:06:54,979 --> 00:07:00,229
felt very uncomfortable the two main

00:06:57,620 --> 00:07:02,180
concerns that we had were one how will

00:07:00,229 --> 00:07:04,669
we take code from the internet and

00:07:02,180 --> 00:07:06,530
package it for internal use and how

00:07:04,669 --> 00:07:09,740
would we support this platform as a

00:07:06,530 --> 00:07:12,080
standalone team our platform hosts

00:07:09,740 --> 00:07:14,030
Triple A plus rated applications and

00:07:12,080 --> 00:07:15,500
therefore we really have to ask the

00:07:14,030 --> 00:07:18,259
questions of security for what it would

00:07:15,500 --> 00:07:20,060
mean to code in the open and that

00:07:18,259 --> 00:07:21,919
involved conversations with both legal

00:07:20,060 --> 00:07:23,750
and security teams and they gave us a

00:07:21,919 --> 00:07:25,639
list of things that we would have to be

00:07:23,750 --> 00:07:27,830
able to do so we could have no

00:07:25,639 --> 00:07:29,990
degradation in the service no

00:07:27,830 --> 00:07:32,210
introduction of risk we needed to be

00:07:29,990 --> 00:07:33,710
able to maintain our security assurance

00:07:32,210 --> 00:07:36,409
there could be no change in the

00:07:33,710 --> 00:07:39,620
headcount and no downtime during the

00:07:36,409 --> 00:07:40,819
transition so how do we persuade the

00:07:39,620 --> 00:07:42,469
company that we could do all of those

00:07:40,819 --> 00:07:42,979
things with a team of about six to eight

00:07:42,469 --> 00:07:44,900
people

00:07:42,979 --> 00:07:47,539
well the persuader came with the

00:07:44,900 --> 00:07:48,949
benefits of open-source themselves when

00:07:47,539 --> 00:07:50,569
we first went open-source we were a

00:07:48,949 --> 00:07:52,370
little bit worried that it was going to

00:07:50,569 --> 00:07:54,229
be leaving the comfort of the commercial

00:07:52,370 --> 00:07:56,000
cruise liner and getting into a small

00:07:54,229 --> 00:07:58,550
speed boat and driving off by ourselves

00:07:56,000 --> 00:08:00,500
but actually being part of the

00:07:58,550 --> 00:08:02,150
open-source community is yes you might

00:08:00,500 --> 00:08:03,589
be a small team in a small boat but

00:08:02,150 --> 00:08:05,659
there are lots of other teams and lots

00:08:03,589 --> 00:08:09,830
for the boats and you're all supporting

00:08:05,659 --> 00:08:11,839
each other open-source code hopefully

00:08:09,830 --> 00:08:14,169
you agree is robust due to the volume

00:08:11,839 --> 00:08:17,389
and the enthusiasm of the contributors

00:08:14,169 --> 00:08:19,190
projects are all consumer driven and we

00:08:17,389 --> 00:08:21,169
can add features that meet customer

00:08:19,190 --> 00:08:22,940
demand so by being a member of the

00:08:21,169 --> 00:08:24,889
open-source community we also had the

00:08:22,940 --> 00:08:28,190
opportunity to drive focus which was

00:08:24,889 --> 00:08:30,469
exciting additionally as a team and as a

00:08:28,190 --> 00:08:33,229
company we were keen to give back to the

00:08:30,469 --> 00:08:34,789
community we are silver members and in

00:08:33,229 --> 00:08:36,050
the early days when we were looking at

00:08:34,789 --> 00:08:38,329
open source we were making

00:08:36,050 --> 00:08:39,769
customizations on our local that we

00:08:38,329 --> 00:08:41,899
weren't able to share with the community

00:08:39,769 --> 00:08:43,159
by making upstream pool requests so this

00:08:41,899 --> 00:08:44,720
is something that we're doing now so if

00:08:43,159 --> 00:08:49,670
you want to check us out on github by

00:08:44,720 --> 00:08:51,430
all means please do right so it was more

00:08:49,670 --> 00:08:54,040
than just the open source community

00:08:51,430 --> 00:08:56,470
it was about trust and we the platform

00:08:54,040 --> 00:08:57,940
team trusted the Cloud Foundry code so

00:08:56,470 --> 00:08:59,620
if you consider this from an

00:08:57,940 --> 00:09:01,450
architectural perspective the

00:08:59,620 --> 00:09:04,060
open-source platform would really be no

00:09:01,450 --> 00:09:05,290
different to the commercial one the only

00:09:04,060 --> 00:09:06,940
difference we could see was that the

00:09:05,290 --> 00:09:09,420
responsibility to package and release

00:09:06,940 --> 00:09:11,230
the platform would now be ours and

00:09:09,420 --> 00:09:12,970
initially people were quite nervous

00:09:11,230 --> 00:09:14,800
about this as they thought that it would

00:09:12,970 --> 00:09:16,510
bring change to the platform but

00:09:14,800 --> 00:09:17,890
actually from the perspective the

00:09:16,510 --> 00:09:20,910
developers they should have seen no

00:09:17,890 --> 00:09:23,529
change whatsoever so what about security

00:09:20,910 --> 00:09:25,690
how would we secure taking the code from

00:09:23,529 --> 00:09:27,310
the internet a lot of people have asked

00:09:25,690 --> 00:09:29,380
us about this today so this is a point I

00:09:27,310 --> 00:09:31,180
want to highlight actually I would argue

00:09:29,380 --> 00:09:32,680
that our approach to security is now

00:09:31,180 --> 00:09:35,410
infinitely more flexible that we are

00:09:32,680 --> 00:09:37,240
with open-source this new responsibility

00:09:35,410 --> 00:09:38,620
to package our own releases makes it

00:09:37,240 --> 00:09:41,589
possible for us to choose to upgrade

00:09:38,620 --> 00:09:43,540
individual components therefore if we

00:09:41,589 --> 00:09:45,760
are to you hear about a security or

00:09:43,540 --> 00:09:47,740
notice security fix for vulnerability in

00:09:45,760 --> 00:09:49,420
a component upstream we are able to

00:09:47,740 --> 00:09:51,279
upgrade just that component I don't have

00:09:49,420 --> 00:09:54,339
to wait we don't have to wait as a team

00:09:51,279 --> 00:09:55,839
for a vendor to package a release we can

00:09:54,339 --> 00:09:57,520
update just that component and this

00:09:55,839 --> 00:10:00,700
reduces the time it takes us to secure

00:09:57,520 --> 00:10:02,650
our platform equally we can make the

00:10:00,700 --> 00:10:04,720
executive decision on what components to

00:10:02,650 --> 00:10:06,459
upgrade and when do we want to do a

00:10:04,720 --> 00:10:08,770
large release and upgrade all components

00:10:06,459 --> 00:10:10,690
in one go or do we prioritize individual

00:10:08,770 --> 00:10:12,550
components that have known defects or

00:10:10,690 --> 00:10:14,410
vulnerabilities equally if there's a

00:10:12,550 --> 00:10:16,000
customer demand or a development demands

00:10:14,410 --> 00:10:18,580
for a particular component we can also

00:10:16,000 --> 00:10:20,350
upgrade that to meet their needs we are

00:10:18,580 --> 00:10:24,550
now autonomous in our ability to

00:10:20,350 --> 00:10:26,980
prioritize and schedule so hopefully

00:10:24,550 --> 00:10:28,720
some of you will recognize this so in

00:10:26,980 --> 00:10:30,430
addition to demonstrating that there

00:10:28,720 --> 00:10:31,870
were no architectural differences

00:10:30,430 --> 00:10:34,180
between the commercial and the

00:10:31,870 --> 00:10:35,800
open-source platform we needed to be

00:10:34,180 --> 00:10:38,579
able to demonstrate that we could deploy

00:10:35,800 --> 00:10:41,260
and operate the platform ourselves a

00:10:38,579 --> 00:10:43,510
commercial Cloud Foundry will have an

00:10:41,260 --> 00:10:46,209
assured upgrade path so we need to be

00:10:43,510 --> 00:10:48,970
able to replicate this ie we needed to

00:10:46,209 --> 00:10:52,209
be able to reliably deploy monitor and

00:10:48,970 --> 00:10:54,100
upgrade the platform so at this point we

00:10:52,209 --> 00:10:56,110
were using another open-source tool

00:10:54,100 --> 00:10:58,950
called concourse to run automated

00:10:56,110 --> 00:11:01,959
monitoring jobs against our platform and

00:10:58,950 --> 00:11:04,000
we could already see the benefits at

00:11:01,959 --> 00:11:05,740
this stage of concourse as a tool that

00:11:04,000 --> 00:11:07,750
it's of automation and we could see the

00:11:05,740 --> 00:11:10,530
pent potential to use it for what it's

00:11:07,750 --> 00:11:12,910
really intended which is C ICD pipelines

00:11:10,530 --> 00:11:15,490
and when we started writing our

00:11:12,910 --> 00:11:18,250
pipelines from scratch we had a look and

00:11:15,490 --> 00:11:21,970
assess our current environment so back

00:11:18,250 --> 00:11:24,010
in 2015 in the Frankfort summit we had a

00:11:21,970 --> 00:11:27,040
presentation about how we deploy Cloud

00:11:24,010 --> 00:11:29,290
Foundry to two datacenters and have that

00:11:27,040 --> 00:11:31,090
highly available across Multi datacenter

00:11:29,290 --> 00:11:32,860
we're now actually deploying to six

00:11:31,090 --> 00:11:35,830
different datacenters internationally

00:11:32,860 --> 00:11:38,620
and because we do this we are able to

00:11:35,830 --> 00:11:40,300
temporarily offline a data center so

00:11:38,620 --> 00:11:42,940
that we can make changes to it be that

00:11:40,300 --> 00:11:44,920
maintenance or upgrades and reroute all

00:11:42,940 --> 00:11:46,780
out traffic to the other side so there's

00:11:44,920 --> 00:11:51,370
no interruption to the service for our

00:11:46,780 --> 00:11:54,280
customers right this is my favorite

00:11:51,370 --> 00:11:56,170
slide so there are only two things in

00:11:54,280 --> 00:11:59,560
our world and those two things are

00:11:56,170 --> 00:12:01,770
configuration and code and both of these

00:11:59,560 --> 00:12:05,040
things should be version controlled

00:12:01,770 --> 00:12:07,300
peer-reviewed and stored in get

00:12:05,040 --> 00:12:08,500
everything we do as a team follows on

00:12:07,300 --> 00:12:09,730
from those principles if you take

00:12:08,500 --> 00:12:13,120
anything away from my talk please

00:12:09,730 --> 00:12:15,400
remember this slide so our deployment

00:12:13,120 --> 00:12:18,160
pipeline is the core unit of everything

00:12:15,400 --> 00:12:20,650
that we do day-to-day this pipeline is

00:12:18,160 --> 00:12:22,900
using Concours which is a CI CD system

00:12:20,650 --> 00:12:25,510
with the pipeline as declarative

00:12:22,900 --> 00:12:28,750
configuration we practice pair

00:12:25,510 --> 00:12:31,030
programming so any change is viewed and

00:12:28,750 --> 00:12:33,250
re reviewed by multiple engineers a pair

00:12:31,030 --> 00:12:35,260
to write the codes a pair to review the

00:12:33,250 --> 00:12:38,200
pool request so in theory that's four

00:12:35,260 --> 00:12:40,060
sets of eyes and by storing everything

00:12:38,200 --> 00:12:43,540
in get we're provided with a full audit

00:12:40,060 --> 00:12:45,790
trail so what was merged when was it

00:12:43,540 --> 00:12:47,680
merged and by whom so we are both

00:12:45,790 --> 00:12:49,630
confident that any change we've made has

00:12:47,680 --> 00:12:51,310
gone through multiple checks but also

00:12:49,630 --> 00:12:56,170
that if we needed to we can recover to

00:12:51,310 --> 00:12:57,670
any known state at any time okay so

00:12:56,170 --> 00:12:59,740
there are several prerequisites that you

00:12:57,670 --> 00:13:01,780
need to meet to deploy a Cloud Foundry

00:12:59,740 --> 00:13:04,660
platform some of them are here so there

00:13:01,780 --> 00:13:05,470
are IP ranges VM folders storage

00:13:04,660 --> 00:13:08,380
assignments

00:13:05,470 --> 00:13:10,000
perhaps C sed system and when we were

00:13:08,380 --> 00:13:11,970
running a commercial platform we were

00:13:10,000 --> 00:13:13,839
meeting these prerequisites manually

00:13:11,970 --> 00:13:15,550
mostly using a GUI

00:13:13,839 --> 00:13:16,910
which unfortunately left us open to

00:13:15,550 --> 00:13:18,530
making mistakes and it

00:13:16,910 --> 00:13:21,170
also did not provide us with that audit

00:13:18,530 --> 00:13:22,850
trail so we really wants to find a way

00:13:21,170 --> 00:13:27,350
to automate this and make everything

00:13:22,850 --> 00:13:29,000
either configuration or code so we did

00:13:27,350 --> 00:13:31,520
that by building our own automation

00:13:29,000 --> 00:13:34,310
tooling and then using Concours and you

00:13:31,520 --> 00:13:35,600
can see some boards here as a CI CD

00:13:34,310 --> 00:13:37,580
mechanism to deploy it

00:13:35,600 --> 00:13:38,990
this means we now have a set of

00:13:37,580 --> 00:13:41,300
configuration which provides us with

00:13:38,990 --> 00:13:43,280
both a higher level of deployment

00:13:41,300 --> 00:13:45,410
confidence but also that previously

00:13:43,280 --> 00:13:46,790
mentioned audit trail and as an FCA

00:13:45,410 --> 00:13:48,170
regulated company this is something

00:13:46,790 --> 00:13:51,710
that's really crucial from us for us

00:13:48,170 --> 00:13:53,600
from a security perspective so besides

00:13:51,710 --> 00:13:55,670
the obvious benefits of end-to-end

00:13:53,600 --> 00:13:57,830
automation in terms of freeing up time

00:13:55,670 --> 00:14:01,100
for your engineers to pursue other

00:13:57,830 --> 00:14:03,020
pursuits hopefully more creative ones it

00:14:01,100 --> 00:14:07,670
also gives us predictability and

00:14:03,020 --> 00:14:08,650
assurance we have something I hope

00:14:07,670 --> 00:14:11,570
you're familiar with which is

00:14:08,650 --> 00:14:13,550
idempotency so the concept that multiple

00:14:11,570 --> 00:14:16,220
identical requests have the same effect

00:14:13,550 --> 00:14:18,950
as making a single request so anytime we

00:14:16,220 --> 00:14:20,510
want we run this pipeline we know that

00:14:18,950 --> 00:14:23,120
it's following exactly the same process

00:14:20,510 --> 00:14:25,400
it's executing exactly the same tests

00:14:23,120 --> 00:14:28,730
and therefore producing exactly the same

00:14:25,400 --> 00:14:30,620
platform so this and by writing our

00:14:28,730 --> 00:14:33,410
pipeline as a series of asynchronous

00:14:30,620 --> 00:14:35,090
jobs we can rerun any individual step

00:14:33,410 --> 00:14:37,610
without affecting the end state of the

00:14:35,090 --> 00:14:38,960
platform so to go back to my previous

00:14:37,610 --> 00:14:40,910
slide about the things we had to meet to

00:14:38,960 --> 00:14:44,060
run open source we're not introducing

00:14:40,910 --> 00:14:46,160
any unexpected change and thereby we re

00:14:44,060 --> 00:14:49,730
running a job there's no introduced risk

00:14:46,160 --> 00:14:52,270
and this is a sort of a inbuilt error

00:14:49,730 --> 00:14:54,950
recovery method so with any system

00:14:52,270 --> 00:14:57,020
something might fail and we must

00:14:54,950 --> 00:14:59,320
engineer for said failure and we can be

00:14:57,020 --> 00:15:01,700
confident that we can recover quickly so

00:14:59,320 --> 00:15:04,010
idempotency can bind with this ability

00:15:01,700 --> 00:15:08,540
to rerun individual steps means we have

00:15:04,010 --> 00:15:11,510
effective inbuilt error recovery okay so

00:15:08,540 --> 00:15:13,940
this is our deployment pipeline it's

00:15:11,510 --> 00:15:16,970
very big so you probably can't read

00:15:13,940 --> 00:15:18,170
anything that's partly deliberate but I

00:15:16,970 --> 00:15:20,810
haven't going to go through what's half

00:15:18,170 --> 00:15:23,180
a day on screen so the very first step

00:15:20,810 --> 00:15:25,550
of our pipeline is a lock and that means

00:15:23,180 --> 00:15:27,920
that only a single execution can happen

00:15:25,550 --> 00:15:29,910
at any one time and that's helping us to

00:15:27,920 --> 00:15:32,340
ensure that idempotency I've just meant

00:15:29,910 --> 00:15:35,010
so only one set of configuration will be

00:15:32,340 --> 00:15:36,930
running through at any one time so if we

00:15:35,010 --> 00:15:38,820
do rerun any individual job we're not

00:15:36,930 --> 00:15:43,200
introducing any new state or any new

00:15:38,820 --> 00:15:45,570
risk but we then have a step that

00:15:43,200 --> 00:15:46,800
checking for configuration change for

00:15:45,570 --> 00:15:49,530
concourse that's all happening

00:15:46,800 --> 00:15:51,630
automatically and we're going to send

00:15:49,530 --> 00:15:53,400
notifications to our users to alert them

00:15:51,630 --> 00:15:55,500
of any maintenance or outbreak that

00:15:53,400 --> 00:15:58,500
happens via email that's the third step

00:15:55,500 --> 00:15:59,760
along our datacenter is then marked

00:15:58,500 --> 00:16:01,140
offline in the load balancer

00:15:59,760 --> 00:16:02,490
configuration so that's going back to

00:16:01,140 --> 00:16:04,890
the point I mentioned about operating

00:16:02,490 --> 00:16:06,420
out of two days sensors and we now need

00:16:04,890 --> 00:16:09,180
to deploy bosh because of course we're

00:16:06,420 --> 00:16:10,800
using bosh to deploy cloud foundry we've

00:16:09,180 --> 00:16:12,630
also recently started making user cred

00:16:10,800 --> 00:16:15,330
hub to dynamically generate our

00:16:12,630 --> 00:16:17,190
credentials and then we provide Bosch

00:16:15,330 --> 00:16:20,010
the cloud configuration to generate the

00:16:17,190 --> 00:16:21,240
manifests for the CF databases now by

00:16:20,010 --> 00:16:22,680
listing all of these things what I'm

00:16:21,240 --> 00:16:24,270
hoping you're taking away is a lot of

00:16:22,680 --> 00:16:26,040
stuff has to happen before we can even

00:16:24,270 --> 00:16:27,810
consider deploying cloud foundry so by

00:16:26,040 --> 00:16:30,330
automating this we've made our jobs a

00:16:27,810 --> 00:16:32,520
lot easier and at this point a lot of

00:16:30,330 --> 00:16:34,530
stuff starts to happen in parallel which

00:16:32,520 --> 00:16:36,540
speeds up a lot of time and that

00:16:34,530 --> 00:16:38,070
includes but is not limited to a myriad

00:16:36,540 --> 00:16:41,040
of tests and that's what I'd like to

00:16:38,070 --> 00:16:43,560
focus on now okay

00:16:41,040 --> 00:16:45,480
so when we were using a commercial cloud

00:16:43,560 --> 00:16:47,580
foundry we were relying on their

00:16:45,480 --> 00:16:50,550
acceptance tests of which we didn't have

00:16:47,580 --> 00:16:52,350
any visibility whereas now that we're

00:16:50,550 --> 00:16:54,120
using open source and the open source

00:16:52,350 --> 00:16:55,950
acceptance tests we actually have gained

00:16:54,120 --> 00:16:58,980
a greater understanding of the platform

00:16:55,950 --> 00:17:00,960
so for any failing test I can look at

00:16:58,980 --> 00:17:03,480
what failed the error message and

00:17:00,960 --> 00:17:04,740
thereby gain a greater understanding of

00:17:03,480 --> 00:17:07,320
the behavior of that particular

00:17:04,740 --> 00:17:09,660
component so for each service on our

00:17:07,320 --> 00:17:11,760
platform we have three synchronous steps

00:17:09,660 --> 00:17:15,570
which I have summarized here so that's

00:17:11,760 --> 00:17:18,420
interpolate deploy and then test we have

00:17:15,570 --> 00:17:20,400
known state at all times in the form of

00:17:18,420 --> 00:17:22,770
these controlled configuration files

00:17:20,400 --> 00:17:25,260
hopefully you're sensing a theme and the

00:17:22,770 --> 00:17:27,120
interpolate step reads this config so

00:17:25,260 --> 00:17:29,130
that it to Porsche can deploy it in a

00:17:27,120 --> 00:17:31,080
required state which can then be

00:17:29,130 --> 00:17:33,930
redeployed to a known state should it be

00:17:31,080 --> 00:17:36,690
required to do so so if for some point

00:17:33,930 --> 00:17:38,400
at some point so the internet wasn't

00:17:36,690 --> 00:17:41,250
working we could just rerun the job and

00:17:38,400 --> 00:17:43,110
everything would be fine for confidence

00:17:41,250 --> 00:17:44,370
in our deployment we

00:17:43,110 --> 00:17:46,470
actually pushed lightweight test

00:17:44,370 --> 00:17:48,330
applications to perform simple

00:17:46,470 --> 00:17:50,640
integration tests that we can validate

00:17:48,330 --> 00:17:53,549
that we are able to still push an app

00:17:50,640 --> 00:17:55,049
and bind said service to the app I know

00:17:53,549 --> 00:17:57,690
it sounds really basic but you're never

00:17:55,049 --> 00:17:59,700
too good for the basics any example apps

00:17:57,690 --> 00:18:00,900
and services are torn down afterwards to

00:17:59,700 --> 00:18:04,110
make sure that we're not introducing

00:18:00,900 --> 00:18:06,210
extra load to the platform additionally

00:18:04,110 --> 00:18:08,190
we have our own custom acceptance tests

00:18:06,210 --> 00:18:10,320
which check the behavior of components

00:18:08,190 --> 00:18:12,420
such as Doppler in the logo gate system

00:18:10,320 --> 00:18:14,450
and also test components which are

00:18:12,420 --> 00:18:17,070
unique to our business requirements

00:18:14,450 --> 00:18:19,860
we're currently offering operating a

00:18:17,070 --> 00:18:21,360
shared logging solution and while it is

00:18:19,860 --> 00:18:22,770
known they're not all logs are

00:18:21,360 --> 00:18:24,480
guaranteed to make it to their

00:18:22,770 --> 00:18:27,090
destination it is important that a

00:18:24,480 --> 00:18:29,190
certain threshold do so we are

00:18:27,090 --> 00:18:32,730
monitoring our local gate system and to

00:18:29,190 --> 00:18:34,530
end by deploying an application that

00:18:32,730 --> 00:18:37,080
emits no logs and then we check to see

00:18:34,530 --> 00:18:39,299
how many reach their side additionally

00:18:37,080 --> 00:18:41,250
it's worth noting that Doppler resources

00:18:39,299 --> 00:18:43,200
require scaling to accommodate overall

00:18:41,250 --> 00:18:45,299
log volume so it's actually crucial that

00:18:43,200 --> 00:18:47,490
as our platform is expanding we know

00:18:45,299 --> 00:18:51,780
whether we need to scale our components

00:18:47,490 --> 00:18:53,010
alongside it I'm not going to spend too

00:18:51,780 --> 00:18:54,059
much time talking about it but it is

00:18:53,010 --> 00:18:56,520
worth mentioning that we're also

00:18:54,059 --> 00:18:59,520
utilizing Prometheus to capture our

00:18:56,520 --> 00:19:02,910
platform metrics so how can we safely

00:18:59,520 --> 00:19:05,100
make changes to our tests well all our

00:19:02,910 --> 00:19:06,630
configuration is managed by git so we

00:19:05,100 --> 00:19:08,940
can do all of our testing on branches

00:19:06,630 --> 00:19:10,950
and this means that we can utilize that

00:19:08,940 --> 00:19:12,419
same deployment pipeline for testing our

00:19:10,950 --> 00:19:13,799
code before we even raise any pull

00:19:12,419 --> 00:19:16,320
requests and we can check the behavior

00:19:13,799 --> 00:19:17,910
so we are already very highly confident

00:19:16,320 --> 00:19:23,760
before we worries a pull request that

00:19:17,910 --> 00:19:25,679
things are going to work ok so when we

00:19:23,760 --> 00:19:28,590
are happy with these code changes that

00:19:25,679 --> 00:19:30,450
we have tested in one of our three dev

00:19:28,590 --> 00:19:32,370
environments and it's made its way

00:19:30,450 --> 00:19:34,679
through your PR review which is

00:19:32,370 --> 00:19:38,190
typically done in a different dev

00:19:34,679 --> 00:19:40,470
environment the the merge change is

00:19:38,190 --> 00:19:42,809
automatically picked up by our CI

00:19:40,470 --> 00:19:45,150
environment and if it makes it through

00:19:42,809 --> 00:19:47,100
the CI environment successfully that's

00:19:45,150 --> 00:19:49,590
automatically picked up by staging one

00:19:47,100 --> 00:19:51,419
and so on if it's successful in staging

00:19:49,590 --> 00:19:54,419
one it's automatically picked up by

00:19:51,419 --> 00:19:57,309
staging 2 at each stage here our

00:19:54,419 --> 00:20:02,830
deployment is tagged to say that it was

00:19:57,309 --> 00:20:04,929
successful the entire commit sorry the

00:20:02,830 --> 00:20:06,940
entire release including all of its

00:20:04,929 --> 00:20:09,429
commit references for every single

00:20:06,940 --> 00:20:11,710
component that has been deployed as

00:20:09,429 --> 00:20:13,539
tagged and the men could be fed into

00:20:11,710 --> 00:20:15,549
subsequent pipelines and that way we can

00:20:13,539 --> 00:20:18,600
ensure consistency from pipeline to

00:20:15,549 --> 00:20:22,210
pipeline if everything makes it through

00:20:18,600 --> 00:20:24,129
ci staging one staging to we're then

00:20:22,210 --> 00:20:26,350
highly confident for it to pass into non

00:20:24,129 --> 00:20:27,899
production which is part of our runtime

00:20:26,350 --> 00:20:31,480
environments represented just here

00:20:27,899 --> 00:20:32,919
however to minimize disruption we do

00:20:31,480 --> 00:20:34,450
often choose to make releases by

00:20:32,919 --> 00:20:36,100
grouping a set of changes but again

00:20:34,450 --> 00:20:38,590
because we run the platform we can

00:20:36,100 --> 00:20:40,749
choose how and when we do that something

00:20:38,590 --> 00:20:43,240
I would like to call out is this GPG

00:20:40,749 --> 00:20:45,129
signing up here so we're using GPG

00:20:43,240 --> 00:20:46,809
commit signing to indicate a release

00:20:45,129 --> 00:20:49,149
version for deployment into the runtime

00:20:46,809 --> 00:20:51,820
environments which is essentially our

00:20:49,149 --> 00:20:53,499
product owner seal of approval and as an

00:20:51,820 --> 00:20:54,999
extra level of security only certain

00:20:53,499 --> 00:20:59,529
members of the team have the correct

00:20:54,999 --> 00:21:02,820
admin privileges to be able to do so so

00:20:59,529 --> 00:21:05,320
how do we tag successful deployments so

00:21:02,820 --> 00:21:07,419
the reason that we're doing that is that

00:21:05,320 --> 00:21:10,210
we want to know that the code has been

00:21:07,419 --> 00:21:12,429
successful and the code that we deploy

00:21:10,210 --> 00:21:14,679
into higher level environments ie non

00:21:12,429 --> 00:21:16,269
prods and prod we need to be completely

00:21:14,679 --> 00:21:18,279
confident they've been testing that they

00:21:16,269 --> 00:21:19,539
work so nothing that goes into a higher

00:21:18,279 --> 00:21:22,690
level environment hasn't already

00:21:19,539 --> 00:21:24,399
succeeded in a lower one and tagging

00:21:22,690 --> 00:21:25,929
also provides a complete audit trail I

00:21:24,399 --> 00:21:27,820
know that I've mentioned order a few

00:21:25,929 --> 00:21:30,580
times but it's very important so if

00:21:27,820 --> 00:21:33,309
anything were to go wrong we can go back

00:21:30,580 --> 00:21:35,919
and see exactly what code was running at

00:21:33,309 --> 00:21:40,659
that time so we can replicate whatever

00:21:35,919 --> 00:21:43,049
was happening we can also see oh don't

00:21:40,659 --> 00:21:53,309
done that we can also see who merged it

00:21:43,049 --> 00:21:53,309
no bear with me technical difficulties

00:21:54,179 --> 00:21:58,889
there we go we can also see who merged

00:21:56,789 --> 00:22:00,749
the change so if we did have to hold

00:21:58,889 --> 00:22:05,159
anybody accountable for anything that is

00:22:00,749 --> 00:22:07,350
possible okay so what you saw on the

00:22:05,159 --> 00:22:08,879
other slide isn't quite accurate things

00:22:07,350 --> 00:22:11,399
actually look a little bit more like

00:22:08,879 --> 00:22:13,950
this if you're good at counting you'll

00:22:11,399 --> 00:22:17,580
notice 24 CF foundations we actually

00:22:13,950 --> 00:22:20,220
have 26 so we've got all of our dev M's

00:22:17,580 --> 00:22:21,659
on the left we have that CI staging one

00:22:20,220 --> 00:22:23,309
the staging two scenario that I've just

00:22:21,659 --> 00:22:24,960
walked you through and then you've got a

00:22:23,309 --> 00:22:26,999
bit more of an accurate picture of our

00:22:24,960 --> 00:22:29,460
runtime environment on the right the

00:22:26,999 --> 00:22:31,740
clocks represent a two day wait which is

00:22:29,460 --> 00:22:32,999
just an extra level of security to make

00:22:31,740 --> 00:22:35,539
sure that we're happy that what happened

00:22:32,999 --> 00:22:38,549
in non prods can then go into production

00:22:35,539 --> 00:22:41,070
these 26 Cloud Foundry foundations are

00:22:38,549 --> 00:22:42,659
spread across the UK and Asia and

00:22:41,070 --> 00:22:44,850
they're all deployed using that same

00:22:42,659 --> 00:22:47,129
deployment pipeline we are making use of

00:22:44,850 --> 00:22:49,909
the stopover resource from our friends

00:22:47,129 --> 00:22:52,350
engineer better he's on his phone so

00:22:49,909 --> 00:22:55,139
automatically pin our conquers pipelines

00:22:52,350 --> 00:22:57,299
to specific versions of resources and

00:22:55,139 --> 00:22:58,980
this allows us to reuse that same

00:22:57,299 --> 00:23:00,960
pipeline yeah more for all our different

00:22:58,980 --> 00:23:03,749
environments we don't like to add code

00:23:00,960 --> 00:23:05,429
we always want to minimize code and this

00:23:03,749 --> 00:23:09,929
way we can basically ensure consistency

00:23:05,429 --> 00:23:11,190
across to everything okay

00:23:09,929 --> 00:23:12,570
so you've heard about how we're doing

00:23:11,190 --> 00:23:16,649
the deployment but how do we make the

00:23:12,570 --> 00:23:18,840
transition okay so we chose to run to

00:23:16,649 --> 00:23:20,490
cloud boundaries in parallel the

00:23:18,840 --> 00:23:22,019
commercial cloud and the cloud foundry

00:23:20,490 --> 00:23:24,210
one well the open source Cloud Foundry

00:23:22,019 --> 00:23:26,970
one and this of course meant an

00:23:24,210 --> 00:23:28,049
increased cost of running and we also

00:23:26,970 --> 00:23:30,330
needed to invest in some new

00:23:28,049 --> 00:23:32,269
infrastructure since we run our CF on

00:23:30,330 --> 00:23:35,340
virtualized infrastructure internally

00:23:32,269 --> 00:23:37,289
despite this overhead deploying to cloud

00:23:35,340 --> 00:23:39,059
foundries allowed us to directly compare

00:23:37,289 --> 00:23:40,919
the behavior of both of the platforms

00:23:39,059 --> 00:23:42,539
and to ensure confidence that there

00:23:40,919 --> 00:23:45,539
would be no degradation of service or

00:23:42,539 --> 00:23:46,769
introduction of risk and by running them

00:23:45,539 --> 00:23:49,559
both at the same time it basically gave

00:23:46,769 --> 00:23:50,940
us the time to do that how do we ensure

00:23:49,559 --> 00:23:52,919
that we were capable of doing this

00:23:50,940 --> 00:23:55,499
direct comparison well when we built our

00:23:52,919 --> 00:23:56,940
open-source platform we chose components

00:23:55,499 --> 00:23:58,320
that match the build of the current

00:23:56,940 --> 00:24:00,659
version of the commercial one where

00:23:58,320 --> 00:24:03,359
possible we ran exactly the same test

00:24:00,659 --> 00:24:05,220
suite against both platforms and we also

00:24:03,359 --> 00:24:06,899
utilize the open source Cloud Foundry

00:24:05,220 --> 00:24:07,559
acceptance test to have even more

00:24:06,899 --> 00:24:08,730
coverage

00:24:07,559 --> 00:24:11,490
to what we originally had of the

00:24:08,730 --> 00:24:13,559
commercial platform and additionally we

00:24:11,490 --> 00:24:15,210
asked our lovely app developers to push

00:24:13,559 --> 00:24:17,100
to both platforms during a migration

00:24:15,210 --> 00:24:18,629
period so that we could do a direct

00:24:17,100 --> 00:24:19,740
comparison of performance so that all

00:24:18,629 --> 00:24:21,090
the developers were happy that there

00:24:19,740 --> 00:24:23,220
would be no change for their experience

00:24:21,090 --> 00:24:26,580
or the customer experience when using

00:24:23,220 --> 00:24:29,580
their applications this is meant to be

00:24:26,580 --> 00:24:31,830
the migration period so due to a hot

00:24:29,580 --> 00:24:34,019
architecture already being set up across

00:24:31,830 --> 00:24:35,700
two days centers apps were actually

00:24:34,019 --> 00:24:37,740
already written to be deployed to

00:24:35,700 --> 00:24:39,749
separate Cloud Foundry foundations so

00:24:37,740 --> 00:24:42,269
that was really useful for us the

00:24:39,749 --> 00:24:44,039
network routing that was already there

00:24:42,269 --> 00:24:46,529
actually enabled us to be able to push

00:24:44,039 --> 00:24:48,019
twice to two different platforms so we

00:24:46,529 --> 00:24:51,029
didn't need to engineer anything fresh

00:24:48,019 --> 00:24:53,070
it was also therefore reasonable for us

00:24:51,029 --> 00:24:54,570
to ask our developers to push to another

00:24:53,070 --> 00:24:56,999
platform in addition to where they were

00:24:54,570 --> 00:24:58,769
already pushing and we were also quite

00:24:56,999 --> 00:25:00,929
prepared for moving our security groups

00:24:58,769 --> 00:25:02,909
as we had an existing plugin plugin that

00:25:00,929 --> 00:25:05,009
means that our developers can directly

00:25:02,909 --> 00:25:07,110
push their security group configuration

00:25:05,009 --> 00:25:08,399
alongside their apps so what I really

00:25:07,110 --> 00:25:09,929
want to highlight here is that we didn't

00:25:08,399 --> 00:25:12,990
do a lot of engineering here for the

00:25:09,929 --> 00:25:16,230
migration our app teams were mostly

00:25:12,990 --> 00:25:18,360
using some form of CI or CD workflows to

00:25:16,230 --> 00:25:20,639
deploy their apps we're not prescriptive

00:25:18,360 --> 00:25:22,350
and what they choose to use so in

00:25:20,639 --> 00:25:23,999
general it was as simple as asking them

00:25:22,350 --> 00:25:26,639
to add the new foundation to their

00:25:23,999 --> 00:25:28,649
workflows the key here for the migration

00:25:26,639 --> 00:25:31,289
was actually communication not

00:25:28,649 --> 00:25:33,720
technology so we need to ensure that our

00:25:31,289 --> 00:25:35,519
developers knew well in advance about

00:25:33,720 --> 00:25:37,980
what they would need to do to deploy

00:25:35,519 --> 00:25:39,360
their apps and to educate them on the

00:25:37,980 --> 00:25:41,519
use of open source there was a lot of

00:25:39,360 --> 00:25:42,509
nervous energy and we needed to make

00:25:41,519 --> 00:25:45,659
sure that everyone was happy with the

00:25:42,509 --> 00:25:47,820
move we have numerous different product

00:25:45,659 --> 00:25:49,470
teams and it's quite hard to get them to

00:25:47,820 --> 00:25:52,919
agree to do everything all at the same

00:25:49,470 --> 00:25:54,690
time so in general coordinating a large

00:25:52,919 --> 00:25:56,009
number of people is not easy especially

00:25:54,690 --> 00:25:58,139
when they're spread across the globe

00:25:56,009 --> 00:26:00,360
operating two different schedules and in

00:25:58,139 --> 00:26:02,759
different time zones so what we wanted

00:26:00,360 --> 00:26:04,409
to have was a mechanism whereby we would

00:26:02,759 --> 00:26:08,299
still be able to complete the switchover

00:26:04,409 --> 00:26:11,220
even if one product team wasn't ready so

00:26:08,299 --> 00:26:14,119
we had this really really simple golang

00:26:11,220 --> 00:26:16,740
up aptly named the redirector up and

00:26:14,119 --> 00:26:18,629
this would basically send any traffic to

00:26:16,740 --> 00:26:20,700
the right place even if that hadn't been

00:26:18,629 --> 00:26:23,530
migrated

00:26:20,700 --> 00:26:25,930
okay so how do we migrate security and

00:26:23,530 --> 00:26:28,300
service settings that hasn't rendered

00:26:25,930 --> 00:26:30,130
very nicely I apologize it would be

00:26:28,300 --> 00:26:32,470
unreasonable customer experience to ask

00:26:30,130 --> 00:26:34,810
developers to make requests for all of

00:26:32,470 --> 00:26:36,610
their services again not to mention a

00:26:34,810 --> 00:26:38,680
large volume of work for us to recreate

00:26:36,610 --> 00:26:40,750
them and thankfully we were able to

00:26:38,680 --> 00:26:42,580
automate this and it was quite simple we

00:26:40,750 --> 00:26:45,550
simply wrote scripts that queried the

00:26:42,580 --> 00:26:48,190
UAA database and then repopulated all of

00:26:45,550 --> 00:26:49,720
that in the open source platform all the

00:26:48,190 --> 00:26:51,520
previous configuration had an audit

00:26:49,720 --> 00:26:53,860
trail and had product owner approval so

00:26:51,520 --> 00:26:57,100
it was simply a copy and paste exercise

00:26:53,860 --> 00:26:58,960
really and the only real complexity was

00:26:57,100 --> 00:27:02,260
making sure that we could automate it by

00:26:58,960 --> 00:27:03,910
writing the plug-in as a way of ensuring

00:27:02,260 --> 00:27:05,050
that everything had been copied we

00:27:03,910 --> 00:27:06,580
checked that all the commands went

00:27:05,050 --> 00:27:07,930
through successfully and anything that

00:27:06,580 --> 00:27:09,910
didn't go through was recorded in a

00:27:07,930 --> 00:27:13,210
retrials file which was just that

00:27:09,910 --> 00:27:15,400
retried and as a very final pass over we

00:27:13,210 --> 00:27:16,600
checked a few things manually but for

00:27:15,400 --> 00:27:20,050
the most part everything was automated

00:27:16,600 --> 00:27:21,940
end to end okay I imagine the question

00:27:20,050 --> 00:27:24,790
that you're all asking is how long did

00:27:21,940 --> 00:27:27,850
this take us well not actually that long

00:27:24,790 --> 00:27:30,190
so our inception period start is at the

00:27:27,850 --> 00:27:31,780
end of January early February and by the

00:27:30,190 --> 00:27:34,360
end of October we completely switched

00:27:31,780 --> 00:27:35,890
off our commercial platform so the go

00:27:34,360 --> 00:27:37,960
live for the platform was really at the

00:27:35,890 --> 00:27:39,490
end of July and between July and

00:27:37,960 --> 00:27:42,190
September we are encouraging all of our

00:27:39,490 --> 00:27:44,380
app developers to migrate to both sides

00:27:42,190 --> 00:27:46,090
and then at the beginning of September

00:27:44,380 --> 00:27:47,440
we were expecting all of our app

00:27:46,090 --> 00:27:49,840
developers to have moved their apps

00:27:47,440 --> 00:27:51,280
across we left that month period there

00:27:49,840 --> 00:27:52,750
essentially to predict that there would

00:27:51,280 --> 00:27:53,830
be some stragglers I think in any

00:27:52,750 --> 00:27:55,240
organization you're going to have people

00:27:53,830 --> 00:27:57,400
running late so we had that there as a

00:27:55,240 --> 00:27:59,860
fail-safe and any of those people were

00:27:57,400 --> 00:28:02,650
caught by the redirector app so a long

00:27:59,860 --> 00:28:03,400
story short I'd want to highlight that

00:28:02,650 --> 00:28:05,200
if you're thinking about going

00:28:03,400 --> 00:28:07,180
open-source and you're currently using a

00:28:05,200 --> 00:28:08,350
commercial platform we were already in a

00:28:07,180 --> 00:28:10,390
really good place we were very

00:28:08,350 --> 00:28:11,910
comfortable with running the platform we

00:28:10,390 --> 00:28:14,650
were very comfortable with the software

00:28:11,910 --> 00:28:16,690
there was a lot of trust trust in the

00:28:14,650 --> 00:28:18,850
Cloud Foundry code trust in ourselves to

00:28:16,690 --> 00:28:20,370
run the platform and we were very open

00:28:18,850 --> 00:28:22,450
that it was a big risk we weren't

00:28:20,370 --> 00:28:25,390
deluding ourselves any way shape or form

00:28:22,450 --> 00:28:26,890
but we could see those huge benefits and

00:28:25,390 --> 00:28:28,330
the fact that I'm stood here today to

00:28:26,890 --> 00:28:29,910
tell you about them means that it paid

00:28:28,330 --> 00:28:33,040
off and I'm really grateful to be here

00:28:29,910 --> 00:28:34,179
we now have a total of 26 foundations

00:28:33,040 --> 00:28:37,749
across three diff

00:28:34,179 --> 00:28:39,639
and geographical locations thank you so

00:28:37,749 --> 00:28:41,980
much for listening if you want to hear

00:28:39,639 --> 00:28:44,049
anything more about what we've done we

00:28:41,980 --> 00:28:45,639
do have a booth in the sponsor arena too

00:28:44,049 --> 00:28:48,009
lovely team members down at the front

00:28:45,639 --> 00:28:50,700
supporting me and come talk to us I'd

00:28:48,009 --> 00:28:50,700
love to hear from you

00:28:51,150 --> 00:28:59,680

YouTube URL: https://www.youtube.com/watch?v=vwVVQi_OkyU


