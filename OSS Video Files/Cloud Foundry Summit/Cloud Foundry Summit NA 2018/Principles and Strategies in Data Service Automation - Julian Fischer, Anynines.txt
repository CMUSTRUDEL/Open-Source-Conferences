Title: Principles and Strategies in Data Service Automation - Julian Fischer, Anynines
Publication date: 2018-04-21
Playlist: Cloud Foundry Summit NA 2018
Description: 
	Principles and Strategies in Data Service Automation - Julian Fischer, Anynines

In this talk, Julian Fischer, an automation enthusiast will talk about the exciting challenges you will encounter when automating a large set of data services across several platforms, infrastructures, and operating systems.

You will be guided through the impact a full automation mission has on existing operational models. Learn how the separation of duties, delegation, and automation need to work alongside to accomplish the mission.

See the impact and limitations of the Open Service Broker API and get a sense for false friends and challenging problems to solve when automating a multitude of data services with a small team.
At the end of the talk, you will have a clear picture of how to approach creating a set of production grade, highly scalable new data service for modern platforms including Cloud Foundry and Kubernetes.

About Julian Fischer
Julian Fischer is CEO of anynines a well established platform consultancy with focus on Cloud Foundry. Julian and his anynines team love operating Cloud Foundry environments but their true passion is automating the a9s Data Services.

As a true Cloud Foundry enthusiast, Julian has visited all CF Summits and spoke at most of them.
Captions: 
	00:00:00,000 --> 00:00:04,440
all right so welcome to this talk about

00:00:02,090 --> 00:00:08,670
principles and strategies of data

00:00:04,440 --> 00:00:11,040
service automation so few words about

00:00:08,670 --> 00:00:14,820
myself my name is Julian Fisher I'm the

00:00:11,040 --> 00:00:19,490
dirty guy on the right side CEO of a

00:00:14,820 --> 00:00:22,710
company called any nines and we've been

00:00:19,490 --> 00:00:25,140
we are a cloud foundry consultancy with

00:00:22,710 --> 00:00:27,750
focus on cloud foundry and kubernetes

00:00:25,140 --> 00:00:31,080
operation as well as data service

00:00:27,750 --> 00:00:33,570
automation so I'm trying to collect some

00:00:31,080 --> 00:00:36,780
of the experience we've collected in in

00:00:33,570 --> 00:00:39,450
the past four or five years automating

00:00:36,780 --> 00:00:41,809
the lifecycle of around half a dozen

00:00:39,450 --> 00:00:45,239
little bit more than that data services

00:00:41,809 --> 00:00:50,520
and presenting some of the strategies

00:00:45,239 --> 00:00:52,230
here with you the first thing I would

00:00:50,520 --> 00:00:55,440
like to point out before we come to the

00:00:52,230 --> 00:00:58,500
principles themselves is that it is very

00:00:55,440 --> 00:01:00,949
important that when you approach the

00:00:58,500 --> 00:01:04,890
automation of a data service a database

00:01:00,949 --> 00:01:08,070
message queue or whatever that you

00:01:04,890 --> 00:01:09,510
should make yourself a mission because

00:01:08,070 --> 00:01:11,400
there are so many things you can

00:01:09,510 --> 00:01:15,659
automate in so many ways and how you can

00:01:11,400 --> 00:01:17,549
do that so I've seen that a couple of

00:01:15,659 --> 00:01:19,970
times when customers approach that topic

00:01:17,549 --> 00:01:23,250
that they ended up with a very big chaos

00:01:19,970 --> 00:01:25,650
because there were different teams that

00:01:23,250 --> 00:01:28,590
held the responsibility for a particular

00:01:25,650 --> 00:01:31,170
data service and in the end every data

00:01:28,590 --> 00:01:34,680
service was a little different with a

00:01:31,170 --> 00:01:37,560
different operational model so that in

00:01:34,680 --> 00:01:41,700
the end created a lot of effort to

00:01:37,560 --> 00:01:43,320
maintain and operate them so mission is

00:01:41,700 --> 00:01:45,390
actually meant to give you motivation

00:01:43,320 --> 00:01:48,689
and strength strength to survive

00:01:45,390 --> 00:01:52,530
setbacks and also provide a accomplice

00:01:48,689 --> 00:01:55,320
effect enabling autonomous decisions

00:01:52,530 --> 00:01:57,750
within your team so I'm just giving you

00:01:55,320 --> 00:02:01,500
one of the examples your mission

00:01:57,750 --> 00:02:05,100
statement could look differently for any

00:02:01,500 --> 00:02:07,469
nines we decided to strive for the four

00:02:05,100 --> 00:02:10,080
fully automating the entire lifecycle of

00:02:07,469 --> 00:02:12,060
a wide range of data services to run on

00:02:10,080 --> 00:02:13,770
cloud native platforms across

00:02:12,060 --> 00:02:16,530
infrastructures at scale

00:02:13,770 --> 00:02:19,200
so there are already a few constraints

00:02:16,530 --> 00:02:21,060
mentioned here that will change the way

00:02:19,200 --> 00:02:25,250
we approach the automation in the end

00:02:21,060 --> 00:02:29,130
and that that'll give us directions so

00:02:25,250 --> 00:02:31,800
it its main purpose is to narrow down

00:02:29,130 --> 00:02:35,580
those endless possibilities and provide

00:02:31,800 --> 00:02:38,840
us and navigational means to design

00:02:35,580 --> 00:02:40,920
decisions so when we now look at those

00:02:38,840 --> 00:02:42,090
principles you will see that if you

00:02:40,920 --> 00:02:45,570
change the mission statement

00:02:42,090 --> 00:02:49,500
significantly it may have impact on on

00:02:45,570 --> 00:02:51,990
the principles as well most of them

00:02:49,500 --> 00:02:55,680
however quite generic so you should be

00:02:51,990 --> 00:03:00,000
able to translate them to other missions

00:02:55,680 --> 00:03:03,090
of data service automations as well so

00:03:00,000 --> 00:03:06,060
the first and most important principle

00:03:03,090 --> 00:03:09,480
it also is reflected in the title of the

00:03:06,060 --> 00:03:14,730
talk is that we want to have a mantra

00:03:09,480 --> 00:03:17,130
which is automate automate automate so

00:03:14,730 --> 00:03:17,880
why is automation of data service is so

00:03:17,130 --> 00:03:20,820
important

00:03:17,880 --> 00:03:24,830
mmm I think from the experience of

00:03:20,820 --> 00:03:27,840
consulting many customers on the digital

00:03:24,830 --> 00:03:30,120
journey of digital transformation we've

00:03:27,840 --> 00:03:34,110
seen that the adoption of application

00:03:30,120 --> 00:03:36,480
platforms usually comes with ignoring

00:03:34,110 --> 00:03:40,320
the data service topic in in the first

00:03:36,480 --> 00:03:42,900
phase of adopting to the platform there

00:03:40,320 --> 00:03:44,460
are there are data stores such as Oracle

00:03:42,900 --> 00:03:47,700
databases somewhere in the enterprise

00:03:44,460 --> 00:03:49,950
and and in the beginning they they

00:03:47,700 --> 00:03:52,290
people are very busy with learning how

00:03:49,950 --> 00:03:55,170
to do agile development how to do micro

00:03:52,290 --> 00:03:57,420
servers architectures and so on but over

00:03:55,170 --> 00:03:59,370
time when you realize that such an

00:03:57,420 --> 00:04:01,500
application platform only works when

00:03:59,370 --> 00:04:03,600
there is a counterpart on the data

00:04:01,500 --> 00:04:05,910
service side that will provide you with

00:04:03,600 --> 00:04:08,820
the same user experience just for data

00:04:05,910 --> 00:04:11,160
services what platform like cloud

00:04:08,820 --> 00:04:13,800
foundry provides for applications you

00:04:11,160 --> 00:04:16,500
will see that automation is very

00:04:13,800 --> 00:04:18,270
important and data services need to

00:04:16,500 --> 00:04:21,480
experience the same automation as

00:04:18,270 --> 00:04:23,640
applications so in modern application

00:04:21,480 --> 00:04:25,919
development you're looking at micro

00:04:23,640 --> 00:04:27,389
service architectures where monolithic

00:04:25,919 --> 00:04:29,879
applications have been split in

00:04:27,389 --> 00:04:32,639
to application systems more applications

00:04:29,879 --> 00:04:35,669
exist and subsequently more data

00:04:32,639 --> 00:04:38,930
services exist as the local autonomy of

00:04:35,669 --> 00:04:41,490
those teams will make decisions more

00:04:38,930 --> 00:04:43,590
specific to a particular service so one

00:04:41,490 --> 00:04:45,689
service could have a relational database

00:04:43,590 --> 00:04:49,680
the other could have a document database

00:04:45,689 --> 00:04:51,509
and so on so this leads to the contact

00:04:49,680 --> 00:04:54,210
consequence that nowadays and the

00:04:51,509 --> 00:04:58,110
applications have numerous more

00:04:54,210 --> 00:05:00,300
applications than before and these

00:04:58,110 --> 00:05:03,840
applications have more data services and

00:05:00,300 --> 00:05:07,460
more data service instances so as we

00:05:03,840 --> 00:05:10,500
still aim for improving the overall

00:05:07,460 --> 00:05:13,139
innovation the speed of innovation we

00:05:10,500 --> 00:05:15,180
have to reduce the operational friction

00:05:13,139 --> 00:05:18,360
that which comes with that increasing

00:05:15,180 --> 00:05:21,089
number of applications and data services

00:05:18,360 --> 00:05:24,539
therefore automation becomes a

00:05:21,089 --> 00:05:28,469
competitive advantage among platforms

00:05:24,539 --> 00:05:32,610
and therefore the automation of data

00:05:28,469 --> 00:05:34,710
services also important referring back

00:05:32,610 --> 00:05:36,539
to the mission statement there was the

00:05:34,710 --> 00:05:41,069
statement fully automate the entire

00:05:36,539 --> 00:05:42,990
lifecycle and this is what the full

00:05:41,069 --> 00:05:46,460
lifecycle management actually refers to

00:05:42,990 --> 00:05:48,659
it actually comprises the challenge of

00:05:46,460 --> 00:05:52,080
automating everything a database

00:05:48,659 --> 00:05:54,300
administrator would do and the reason

00:05:52,080 --> 00:05:56,300
why this has to be pointed out is

00:05:54,300 --> 00:05:59,779
because there are a lot of automation

00:05:56,300 --> 00:06:02,610
solutions out there that claim to be

00:05:59,779 --> 00:06:05,610
data service automation solutions but

00:06:02,610 --> 00:06:07,560
they ignore second day operations so

00:06:05,610 --> 00:06:09,300
it's one thing to have what's for

00:06:07,560 --> 00:06:11,669
example in my sequel database up and

00:06:09,300 --> 00:06:15,169
running we have done that with chef like

00:06:11,669 --> 00:06:18,149
seven years ago but in contrast to

00:06:15,169 --> 00:06:20,639
application containers your databases

00:06:18,149 --> 00:06:23,939
have state and the lifecycle of a

00:06:20,639 --> 00:06:26,310
database usually is years so you have to

00:06:23,939 --> 00:06:28,819
guide that database through very

00:06:26,310 --> 00:06:32,759
different scenarios such as a failing

00:06:28,819 --> 00:06:36,979
host underneath or patch level releases

00:06:32,759 --> 00:06:39,779
minor releases and even major releases

00:06:36,979 --> 00:06:40,870
so if you look into the topic what is

00:06:39,779 --> 00:06:43,060
actually the lie

00:06:40,870 --> 00:06:44,980
cycle of a data service the first

00:06:43,060 --> 00:06:47,740
thought is about what is the lifecycle

00:06:44,980 --> 00:06:50,530
of a database data service instance in

00:06:47,740 --> 00:06:54,130
the terminology of the service broker

00:06:50,530 --> 00:06:55,870
API it is called a service instance at

00:06:54,130 --> 00:06:59,710
some point you will create such a

00:06:55,870 --> 00:07:04,030
service instance and this diagram is not

00:06:59,710 --> 00:07:06,160
a not exact but it it intends to express

00:07:04,030 --> 00:07:09,250
that the engineering effort of creating

00:07:06,160 --> 00:07:11,320
a service instance is not necessarily

00:07:09,250 --> 00:07:12,910
all you have to think about because

00:07:11,320 --> 00:07:16,210
there are other lifecycle operations

00:07:12,910 --> 00:07:18,400
such as various possibilities to changes

00:07:16,210 --> 00:07:20,110
in an existing service instance that

00:07:18,400 --> 00:07:25,540
will eat up a lot of your engineering

00:07:20,110 --> 00:07:28,660
efforts as well so if you start automate

00:07:25,540 --> 00:07:31,540
a data service it's wise to iterally

00:07:28,660 --> 00:07:33,160
increase the depth of the automation to

00:07:31,540 --> 00:07:35,620
start with low-hanging fruit

00:07:33,160 --> 00:07:38,790
those lifecycle operations that are most

00:07:35,620 --> 00:07:41,980
important and most frequently requested

00:07:38,790 --> 00:07:45,190
so after a while this may look like this

00:07:41,980 --> 00:07:48,070
you have covered the life cycle of a

00:07:45,190 --> 00:07:51,850
data service instance creating a data

00:07:48,070 --> 00:07:54,850
service may be clustered may be single

00:07:51,850 --> 00:07:57,280
versions of it for example with Postgres

00:07:54,850 --> 00:07:59,320
you could have a single VM Postgres you

00:07:57,280 --> 00:08:02,130
could have a plan with cluster Postgres

00:07:59,320 --> 00:08:04,860
both of different sizes you could have

00:08:02,130 --> 00:08:07,590
means to go through version updates

00:08:04,860 --> 00:08:10,930
activate or deactivate Postgres plug-ins

00:08:07,590 --> 00:08:14,050
create backups and restore them so this

00:08:10,930 --> 00:08:18,810
as the lifecycle automation of a service

00:08:14,050 --> 00:08:21,520
instance it also goes it also covers

00:08:18,810 --> 00:08:24,310
very essential parts of what we are

00:08:21,520 --> 00:08:26,200
talking about here but I definitely have

00:08:24,310 --> 00:08:28,420
to point out that this is not everything

00:08:26,200 --> 00:08:31,510
a few years back and I've been giving

00:08:28,420 --> 00:08:34,060
talks about that topic for for four

00:08:31,510 --> 00:08:34,420
years now I would have said we are done

00:08:34,060 --> 00:08:38,020
here

00:08:34,420 --> 00:08:40,090
but we've been taught otherwise because

00:08:38,020 --> 00:08:42,340
if you look in the into the total

00:08:40,090 --> 00:08:44,530
efforts you have to spend on the topic

00:08:42,340 --> 00:08:47,110
you will see that there are other

00:08:44,530 --> 00:08:49,600
aspects that will consume significant

00:08:47,110 --> 00:08:52,540
time which is the general release

00:08:49,600 --> 00:08:53,690
management the delivering of the

00:08:52,540 --> 00:08:55,910
automation releases

00:08:53,690 --> 00:08:57,980
into the platform environments and then

00:08:55,910 --> 00:09:02,000
in the end the automation of the

00:08:57,980 --> 00:09:03,800
lifecycle itself so if you look into a

00:09:02,000 --> 00:09:06,800
value chain like this here we're using

00:09:03,800 --> 00:09:09,470
Postgres as an example you will see that

00:09:06,800 --> 00:09:11,360
on the left side there's an open-source

00:09:09,470 --> 00:09:14,810
database such as Postgres could be

00:09:11,360 --> 00:09:17,270
MongoDB Redis or RabbitMQ and whenever

00:09:14,810 --> 00:09:19,520
there is an upstream change then we

00:09:17,270 --> 00:09:20,900
would like to have a automation release

00:09:19,520 --> 00:09:24,680
shortly after that

00:09:20,900 --> 00:09:27,160
so with that automation release you

00:09:24,680 --> 00:09:30,830
still need to ship it into the target

00:09:27,160 --> 00:09:32,720
environments and from there users

00:09:30,830 --> 00:09:35,750
platform users can use it to create

00:09:32,720 --> 00:09:38,480
service instances and lifecycle

00:09:35,750 --> 00:09:40,880
management to guide that whole system

00:09:38,480 --> 00:09:43,760
that hold it delivery chain through the

00:09:40,880 --> 00:09:47,440
lifecycle of that particular data

00:09:43,760 --> 00:09:52,970
services but we come to that a bit later

00:09:47,440 --> 00:09:55,130
back to that a bit later if you if you

00:09:52,970 --> 00:09:57,230
start automating it's it's kind of

00:09:55,130 --> 00:10:00,020
obvious that depending on the choice of

00:09:57,230 --> 00:10:02,960
your data service the the efforts of

00:10:00,020 --> 00:10:04,520
automation will differ as some of the

00:10:02,960 --> 00:10:06,620
data services have been created with

00:10:04,520 --> 00:10:10,790
manual operations in mind

00:10:06,620 --> 00:10:12,710
Postgres for example is such an is such

00:10:10,790 --> 00:10:16,610
a data service that has a lot of legacy

00:10:12,710 --> 00:10:20,240
in a stay in a because it emerged in in

00:10:16,610 --> 00:10:22,640
a time where automated data service

00:10:20,240 --> 00:10:24,380
operation was not a thing so

00:10:22,640 --> 00:10:27,020
subsequently it's a little bit more

00:10:24,380 --> 00:10:28,550
complicated to automate them services

00:10:27,020 --> 00:10:33,590
such as elasticsearch which are

00:10:28,550 --> 00:10:36,820
inherently a bit more modern and maybe

00:10:33,590 --> 00:10:40,180
also because of its nature easier to

00:10:36,820 --> 00:10:43,100
operate and therefore easier to automate

00:10:40,180 --> 00:10:45,170
so let's say that there are decision

00:10:43,100 --> 00:10:49,430
factors you should be investigating when

00:10:45,170 --> 00:10:52,880
collecting candidates for your platform

00:10:49,430 --> 00:10:55,160
and they are numerous

00:10:52,880 --> 00:10:57,230
this list is far from being complete but

00:10:55,160 --> 00:11:00,770
it demonstrates the complexity of such a

00:10:57,230 --> 00:11:02,690
decision and in most of the cases you

00:11:00,770 --> 00:11:05,330
will also look at legacy use case this

00:11:02,690 --> 00:11:08,030
application that will demand that you

00:11:05,330 --> 00:11:10,070
automate for legacy services

00:11:08,030 --> 00:11:11,420
Postgres is a good example as every

00:11:10,070 --> 00:11:13,730
platform needs a relational database

00:11:11,420 --> 00:11:16,820
management system you're looking into

00:11:13,730 --> 00:11:19,070
Postgres and my sequel for sure you will

00:11:16,820 --> 00:11:20,840
have and this leads us to the datasource

00:11:19,070 --> 00:11:23,390
categories you'll have to cover the

00:11:20,840 --> 00:11:27,170
document database the caches key value

00:11:23,390 --> 00:11:29,960
store message queues and so on so in in

00:11:27,170 --> 00:11:31,940
all these services we'll defer on how

00:11:29,960 --> 00:11:34,340
robust they are whether they are cluster

00:11:31,940 --> 00:11:36,250
Abell or how they how they handle

00:11:34,340 --> 00:11:40,240
replication and so on

00:11:36,250 --> 00:11:43,250
so once you've picked your data services

00:11:40,240 --> 00:11:46,070
it's it's somehow determines how much

00:11:43,250 --> 00:11:47,780
effort you will spend on automation form

00:11:46,070 --> 00:11:49,580
experience I can tell you that there's a

00:11:47,780 --> 00:11:53,840
vast difference between one and the

00:11:49,580 --> 00:11:55,850
other end of the spectrum I also believe

00:11:53,840 --> 00:11:58,640
that over time data service will be

00:11:55,850 --> 00:12:01,130
easier to automate as this will also

00:11:58,640 --> 00:12:03,890
have a certain impact on the design of

00:12:01,130 --> 00:12:05,560
data stores in the long run you can see

00:12:03,890 --> 00:12:09,050
that with Postgres which has recently

00:12:05,560 --> 00:12:11,990
introduced client-side failover a big

00:12:09,050 --> 00:12:15,980
step makes making automation easier for

00:12:11,990 --> 00:12:18,080
example another principle when

00:12:15,980 --> 00:12:20,030
automating data service is that you

00:12:18,080 --> 00:12:22,910
should design for scale and the reason

00:12:20,030 --> 00:12:25,610
for that is because platforms such as

00:12:22,910 --> 00:12:29,360
Cloud Foundry they require a certain

00:12:25,610 --> 00:12:33,620
scale to be economically viable so when

00:12:29,360 --> 00:12:35,630
doing platform consulting we've seen

00:12:33,620 --> 00:12:36,980
that most of the organizations we've

00:12:35,630 --> 00:12:38,780
been talking to if they've made

00:12:36,980 --> 00:12:40,790
multi-million investments into your

00:12:38,780 --> 00:12:43,100
building such a platform because it's

00:12:40,790 --> 00:12:45,350
just a very much effort to move a group

00:12:43,100 --> 00:12:47,750
of people to adapt to such a new thing

00:12:45,350 --> 00:12:49,820
and change the way they develop software

00:12:47,750 --> 00:12:52,580
and so on so in an organization it's

00:12:49,820 --> 00:12:54,740
very expensive to do that so after a

00:12:52,580 --> 00:12:57,320
while a platform that's economic a

00:12:54,740 --> 00:12:59,330
viable has hundreds if not thousands or

00:12:57,320 --> 00:13:01,970
ten thousands of applications otherwise

00:12:59,330 --> 00:13:05,810
it's just a very very expensive hobby

00:13:01,970 --> 00:13:08,600
and for that reason the data service

00:13:05,810 --> 00:13:10,190
automation should keep up that standard

00:13:08,600 --> 00:13:11,870
so whatever there is

00:13:10,190 --> 00:13:14,120
whatever Cloud Foundry is for

00:13:11,870 --> 00:13:18,530
applications the solution we are looking

00:13:14,120 --> 00:13:20,360
for is for data services now it's a fact

00:13:18,530 --> 00:13:21,649
that if you want to do something at

00:13:20,360 --> 00:13:24,470
scale that chain

00:13:21,649 --> 00:13:27,350
just the optimal technical solution for

00:13:24,470 --> 00:13:29,959
it and as I've been doing software

00:13:27,350 --> 00:13:33,170
development for quite a while and while

00:13:29,959 --> 00:13:34,490
this seems to be obvious people tend to

00:13:33,170 --> 00:13:37,699
ignore that this is the case for

00:13:34,490 --> 00:13:40,399
software too so I prepare that that

00:13:37,699 --> 00:13:42,259
little funny comparison so we have three

00:13:40,399 --> 00:13:45,170
different technical solutions to do one

00:13:42,259 --> 00:13:49,129
thing which is barbeque sausages so it's

00:13:45,170 --> 00:13:51,679
just the scale that the first see the

00:13:49,129 --> 00:13:54,470
left and the right one they they just

00:13:51,679 --> 00:13:56,689
are different in the scale they do the

00:13:54,470 --> 00:13:59,660
same thing and it's pretty much the same

00:13:56,689 --> 00:14:03,610
for your data service automation so when

00:13:59,660 --> 00:14:06,649
I talk about the scale of a platform

00:14:03,610 --> 00:14:08,779
then we are talking about thousands or

00:14:06,649 --> 00:14:14,449
ten thousands of data service instances

00:14:08,779 --> 00:14:16,749
so if your automation would not allow

00:14:14,449 --> 00:14:19,490
you to do that then you should maybe

00:14:16,749 --> 00:14:21,470
rethink your mission statement because

00:14:19,490 --> 00:14:24,230
this is basically what a platform is

00:14:21,470 --> 00:14:26,990
designed to do for applications so most

00:14:24,230 --> 00:14:31,220
likely you will need that for data

00:14:26,990 --> 00:14:33,679
service as well but the number of

00:14:31,220 --> 00:14:36,980
service instances is a particular bad

00:14:33,679 --> 00:14:39,920
metric to describe the scale of a

00:14:36,980 --> 00:14:42,410
solution because if you think back into

00:14:39,920 --> 00:14:44,959
that value chain we've just seen there

00:14:42,410 --> 00:14:47,689
are many influencing factors such as the

00:14:44,959 --> 00:14:50,870
frequency application writes and reads

00:14:47,689 --> 00:14:52,370
from a data service instance the amount

00:14:50,870 --> 00:14:54,860
of data written to it

00:14:52,370 --> 00:14:57,230
the number of service instances

00:14:54,860 --> 00:15:00,230
coexisting the data service types being

00:14:57,230 --> 00:15:03,529
available the number of environments

00:15:00,230 --> 00:15:05,329
you're deploying to and the number of

00:15:03,529 --> 00:15:08,629
infrastructures you're deploying to so

00:15:05,329 --> 00:15:10,759
we can simplify that and say well there

00:15:08,629 --> 00:15:13,279
are certain categories of scale and the

00:15:10,759 --> 00:15:15,499
most important one ones are the ones

00:15:13,279 --> 00:15:17,929
that describe the service instance

00:15:15,499 --> 00:15:19,879
itself the one that describes the

00:15:17,929 --> 00:15:22,369
service program in its automation as

00:15:19,879 --> 00:15:25,939
well as the release management and

00:15:22,369 --> 00:15:29,420
delivery so if you look back into into

00:15:25,939 --> 00:15:34,370
that diagram where the entire value

00:15:29,420 --> 00:15:35,370
chain is shown that one of the aspects

00:15:34,370 --> 00:15:38,730
is

00:15:35,370 --> 00:15:40,320
in a platformer you at any time a user

00:15:38,730 --> 00:15:42,780
should have the possibility to

00:15:40,320 --> 00:15:44,370
undermount self-service so if you wake

00:15:42,780 --> 00:15:46,470
up at 3 o'clock in the morning and you

00:15:44,370 --> 00:15:51,180
want to create a Postgres database there

00:15:46,470 --> 00:15:53,730
you go the dominant pattern to do that

00:15:51,180 --> 00:15:56,760
is today the on-demand provisioning of

00:15:53,730 --> 00:15:59,160
dedicated service instances where such a

00:15:56,760 --> 00:16:02,130
service instance would be represented by

00:15:59,160 --> 00:16:03,950
a virtual machine a container or a set

00:16:02,130 --> 00:16:07,760
of virtual machines or containers as

00:16:03,950 --> 00:16:11,220
this guarantees a horizontal scalability

00:16:07,760 --> 00:16:13,020
in contrast to shared clusters where you

00:16:11,220 --> 00:16:15,180
have a fixed set of virtual machines

00:16:13,020 --> 00:16:16,920
which you will then slice up into

00:16:15,180 --> 00:16:20,850
service instances the on-demand

00:16:16,920 --> 00:16:22,800
provisioning scales until the boundaries

00:16:20,850 --> 00:16:25,080
of your infrastructure but it's not

00:16:22,800 --> 00:16:28,920
architectural II limited to the number

00:16:25,080 --> 00:16:32,340
of service instances obviously when

00:16:28,920 --> 00:16:36,000
talking about cloud solutions you also

00:16:32,340 --> 00:16:38,850
want the vertical scalability so you can

00:16:36,000 --> 00:16:41,190
stop an application and just create a

00:16:38,850 --> 00:16:43,140
bigger instance of it and the same you

00:16:41,190 --> 00:16:45,540
want to do with your data of the

00:16:43,140 --> 00:16:49,530
database but with the difference that

00:16:45,540 --> 00:16:50,970
then you'll have to keep ensure yet that

00:16:49,530 --> 00:16:52,500
you have to ensure that the data is

00:16:50,970 --> 00:16:54,870
still there after you're scaling this up

00:16:52,500 --> 00:16:56,910
and you want to minimize the downtime so

00:16:54,870 --> 00:17:00,780
this is about service instant

00:16:56,910 --> 00:17:03,630
scalability now if you provide a

00:17:00,780 --> 00:17:05,580
platform to your user and you've seen

00:17:03,630 --> 00:17:07,890
the impact of the no sequel movement

00:17:05,580 --> 00:17:10,380
over the year is that more database

00:17:07,890 --> 00:17:12,690
types become more popular and while

00:17:10,380 --> 00:17:15,300
splitting down the application from a

00:17:12,690 --> 00:17:17,820
monolith to more micro service based

00:17:15,300 --> 00:17:21,120
architectures more applications will

00:17:17,820 --> 00:17:23,400
have or will put the choice of

00:17:21,120 --> 00:17:26,880
particular specialized data services

00:17:23,400 --> 00:17:29,310
upon local teams so that it is likely

00:17:26,880 --> 00:17:32,040
that your users will demand more and

00:17:29,310 --> 00:17:34,860
more data services over time as they may

00:17:32,040 --> 00:17:38,520
specifically solve a certain kind of

00:17:34,860 --> 00:17:41,220
problems very well so this also requires

00:17:38,520 --> 00:17:43,500
that when you automate data services you

00:17:41,220 --> 00:17:45,630
have to ensure that you can automate a

00:17:43,500 --> 00:17:47,940
larger set of data service efficiently

00:17:45,630 --> 00:17:48,480
and once you got those automation

00:17:47,940 --> 00:17:50,250
release

00:17:48,480 --> 00:17:53,669
you have to ensure that you ship those

00:17:50,250 --> 00:17:56,700
into the customer environment and just

00:17:53,669 --> 00:17:59,280
to give you a understanding for example

00:17:56,700 --> 00:18:01,710
we deliver data services in too many

00:17:59,280 --> 00:18:04,620
different organizations each customer

00:18:01,710 --> 00:18:07,950
has it's very own 300 page handbook on

00:18:04,620 --> 00:18:10,799
on security so this delivery pipeline is

00:18:07,950 --> 00:18:13,910
highly customer specific and includes

00:18:10,799 --> 00:18:16,110
pen test and security tests

00:18:13,910 --> 00:18:18,809
vulnerability scans and so on that are

00:18:16,110 --> 00:18:20,850
highly customer specific so while this

00:18:18,809 --> 00:18:22,530
has to be part of your delivery pipeline

00:18:20,850 --> 00:18:24,750
so a customer expects us to give him

00:18:22,530 --> 00:18:27,270
something you still have to be very

00:18:24,750 --> 00:18:30,510
quick in adjusting that to the customer

00:18:27,270 --> 00:18:32,640
needs and keep up the pace so that this

00:18:30,510 --> 00:18:34,200
delivery pipeline does not slow down the

00:18:32,640 --> 00:18:40,110
delivery cadence of your overall

00:18:34,200 --> 00:18:41,970
solution so that's what I'm saying is

00:18:40,110 --> 00:18:45,120
you have to get your release management

00:18:41,970 --> 00:18:48,299
right because delivering fast is very

00:18:45,120 --> 00:18:51,000
important so look at that scenario in

00:18:48,299 --> 00:18:53,220
the past there was a DBA who has access

00:18:51,000 --> 00:18:55,320
to the upstream source code of for

00:18:53,220 --> 00:19:00,090
example Postgres and will be able to

00:18:55,320 --> 00:19:02,910
apply a patch to a database now we

00:19:00,090 --> 00:19:05,070
create post rest automation release and

00:19:02,910 --> 00:19:07,559
ship it to the customer environment and

00:19:05,070 --> 00:19:10,710
own and then it is up to the platform

00:19:07,559 --> 00:19:13,350
user to update that service instance so

00:19:10,710 --> 00:19:15,690
we have three things to do the first of

00:19:13,350 --> 00:19:17,429
all is the automation release should be

00:19:15,690 --> 00:19:20,580
available shortly after the upstream

00:19:17,429 --> 00:19:23,790
change and in our case for example this

00:19:20,580 --> 00:19:26,190
is done entirely automatic automatically

00:19:23,790 --> 00:19:29,280
that means that for example if Postgres

00:19:26,190 --> 00:19:32,520
nine point four point one nine point

00:19:29,280 --> 00:19:34,919
four point two is released a few minutes

00:19:32,520 --> 00:19:36,600
later we have an automation release that

00:19:34,919 --> 00:19:38,940
will trigger the execution of a

00:19:36,600 --> 00:19:40,650
comprehensive test suite to ensure that

00:19:38,940 --> 00:19:42,480
the contract between the data service

00:19:40,650 --> 00:19:45,929
and our automation is still valid in

00:19:42,480 --> 00:19:48,690
intact so then you have to ship that to

00:19:45,929 --> 00:19:51,480
the customer as fast as you can

00:19:48,690 --> 00:19:54,120
minimizing that time requires to have a

00:19:51,480 --> 00:19:56,640
framework here a template for such a

00:19:54,120 --> 00:19:59,610
delivery pipeline but then specify and

00:19:56,640 --> 00:20:01,350
somehow customize it with the specifics

00:19:59,610 --> 00:20:04,440
of a certain customer

00:20:01,350 --> 00:20:06,120
and even one customer usually has half a

00:20:04,440 --> 00:20:09,570
dozen or more of those Cloud Foundry

00:20:06,120 --> 00:20:12,929
environments for example so that this is

00:20:09,570 --> 00:20:15,210
already a matter of scalability so you

00:20:12,929 --> 00:20:19,440
have to be quick with that and obviously

00:20:15,210 --> 00:20:21,779
now the platform users are have to

00:20:19,440 --> 00:20:24,889
update their service instances so once

00:20:21,779 --> 00:20:27,870
you have installed the new version of

00:20:24,889 --> 00:20:30,840
Postgres with your new automation into

00:20:27,870 --> 00:20:32,429
the cloud environment then you still

00:20:30,840 --> 00:20:34,950
have hundreds of those database

00:20:32,429 --> 00:20:36,809
instances being locally held by

00:20:34,950 --> 00:20:39,750
different maybe customers of the

00:20:36,809 --> 00:20:41,759
customer and and need to be encouraged

00:20:39,750 --> 00:20:44,250
to update their service instances and to

00:20:41,759 --> 00:20:48,269
do that fast because you want to deliver

00:20:44,250 --> 00:20:50,370
security patches fast right and to

00:20:48,269 --> 00:20:53,220
minimize that time you have to provide

00:20:50,370 --> 00:20:56,279
your users means to do that easily so

00:20:53,220 --> 00:20:58,669
for example just perform a CF update

00:20:56,279 --> 00:21:05,039
service and the update will take care

00:20:58,669 --> 00:21:08,370
will take place automatically now one of

00:21:05,039 --> 00:21:09,960
the other strategies is it's basically

00:21:08,370 --> 00:21:12,330
something that's very essential to any

00:21:09,960 --> 00:21:16,139
cloud operations is the approach to

00:21:12,330 --> 00:21:18,269
rebuild things instead of fixing it this

00:21:16,139 --> 00:21:20,759
is not new for example the Linux kernel

00:21:18,269 --> 00:21:23,639
in the early days had a lot of code

00:21:20,759 --> 00:21:25,559
about failure recovery and some

00:21:23,639 --> 00:21:28,110
developers said why why don't we just

00:21:25,559 --> 00:21:30,539
take out that the entire code which

00:21:28,110 --> 00:21:32,610
saves us about 50% of the codebase and

00:21:30,539 --> 00:21:35,309
just throw kearney back a panic and

00:21:32,610 --> 00:21:37,919
shout down the hole that somebody needs

00:21:35,309 --> 00:21:39,809
to restart that server and if you're

00:21:37,919 --> 00:21:41,639
running Linux today he will see kernel

00:21:39,809 --> 00:21:45,269
panic so it's kind of a strategy that

00:21:41,639 --> 00:21:47,029
works today as well coming back to data

00:21:45,269 --> 00:21:49,830
service it's not as easy

00:21:47,029 --> 00:21:51,389
so while Cloud Foundry will just restart

00:21:49,830 --> 00:21:54,299
your application somewhere in the

00:21:51,389 --> 00:21:57,629
cluster when an instance dies that's not

00:21:54,299 --> 00:21:59,549
as easy with your data service because

00:21:57,629 --> 00:22:01,649
you have state and that state needs to

00:21:59,549 --> 00:22:05,460
be stored somewhere so one of the most

00:22:01,649 --> 00:22:08,549
the more essential strategies in in in

00:22:05,460 --> 00:22:10,830
cloud operation today is to separate the

00:22:08,549 --> 00:22:12,539
life cycle of the ephemeral virtual

00:22:10,830 --> 00:22:14,010
machine where your database is running

00:22:12,539 --> 00:22:16,560
from the persist

00:22:14,010 --> 00:22:19,770
disk where the database stores the data

00:22:16,560 --> 00:22:21,630
and that is meaningful because the

00:22:19,770 --> 00:22:24,180
machine where your database is being

00:22:21,630 --> 00:22:26,580
executed is a cheap off-the-shelf server

00:22:24,180 --> 00:22:29,490
or at least it's not a high-end server

00:22:26,580 --> 00:22:32,640
anymore where the storage server is very

00:22:29,490 --> 00:22:34,920
specific Shirley has hardware

00:22:32,640 --> 00:22:37,880
redundancies to a certain degree and has

00:22:34,920 --> 00:22:40,680
a very different service level than the

00:22:37,880 --> 00:22:43,620
unavailability ask the host machine

00:22:40,680 --> 00:22:45,600
where your database runs so if you look

00:22:43,620 --> 00:22:48,930
into that virtual machine that contains

00:22:45,600 --> 00:22:51,090
your database and now the virtual

00:22:48,930 --> 00:22:53,610
machine goes away your persistent disk

00:22:51,090 --> 00:22:56,010
prevails it can be a gracefully

00:22:53,610 --> 00:22:57,720
unmounted and a new virtual machine can

00:22:56,010 --> 00:23:00,240
be created where you remount that

00:22:57,720 --> 00:23:03,450
persistent disk and there you go this is

00:23:00,240 --> 00:23:05,310
something that happens we are using Bosh

00:23:03,450 --> 00:23:07,140
to automate the creation of service

00:23:05,310 --> 00:23:10,380
instances and this is something that

00:23:07,140 --> 00:23:13,980
happens within minutes if you're using

00:23:10,380 --> 00:23:16,200
kubernetes the restart of containers

00:23:13,980 --> 00:23:19,650
even faster so this can happen within

00:23:16,200 --> 00:23:22,170
seconds be aware you shouldn't you

00:23:19,650 --> 00:23:25,410
shouldn't place your workloads of your

00:23:22,170 --> 00:23:28,500
data services on a kubernetes cluster we

00:23:25,410 --> 00:23:30,840
are currently investigating deploying

00:23:28,500 --> 00:23:33,300
data services to kubernetes but we

00:23:30,840 --> 00:23:36,780
wouldn't recommend using that until

00:23:33,300 --> 00:23:39,000
kubernetes has solved the i/o isolation

00:23:36,780 --> 00:23:41,460
issue that it currently has so you

00:23:39,000 --> 00:23:43,680
couldn't ensure that two database nodes

00:23:41,460 --> 00:23:46,770
be a two database instances being

00:23:43,680 --> 00:23:48,510
co-located on the same kubernetes node

00:23:46,770 --> 00:23:50,010
wouldn't affect each other's i/o

00:23:48,510 --> 00:23:55,680
performance which is something that is

00:23:50,010 --> 00:23:57,990
very relevant for a database so now all

00:23:55,680 --> 00:23:59,520
the automation is a very interesting and

00:23:57,990 --> 00:24:01,800
it's very nice so we've been through

00:23:59,520 --> 00:24:04,260
that creation of the database and so on

00:24:01,800 --> 00:24:06,270
but look now you've created they have

00:24:04,260 --> 00:24:08,610
the possibility to create a thousand

00:24:06,270 --> 00:24:12,120
Postgres clusters MongoDB clusters

00:24:08,610 --> 00:24:14,340
rabbitmq clusters just like that

00:24:12,120 --> 00:24:17,130
imagine you have hundreds of customers

00:24:14,340 --> 00:24:20,130
who do who are using that in and your

00:24:17,130 --> 00:24:22,320
infrastructure has a hiccup and or for

00:24:20,130 --> 00:24:25,920
some other reasons a larger number of

00:24:22,320 --> 00:24:28,020
clusters fail simultaneously you still

00:24:25,920 --> 00:24:31,530
need to ensure this

00:24:28,020 --> 00:24:35,100
on-demand self-service so how can you do

00:24:31,530 --> 00:24:38,220
that for example rabbitmq if you if you

00:24:35,100 --> 00:24:40,650
use RabbitMQ wrongly you can easily

00:24:38,220 --> 00:24:41,780
easily kill such a cluster so how would

00:24:40,650 --> 00:24:44,730
you recover

00:24:41,780 --> 00:24:47,580
depending on the data service it will

00:24:44,730 --> 00:24:51,720
also always be important that you'll

00:24:47,580 --> 00:24:54,510
have such a recovery and as a last

00:24:51,720 --> 00:24:56,040
resort so if you want to automate data

00:24:54,510 --> 00:24:58,890
service you have to get your backup

00:24:56,040 --> 00:25:00,450
strategy right and going back to the

00:24:58,890 --> 00:25:02,910
mission statement where you want to

00:25:00,450 --> 00:25:05,400
create the automation of a larger sum of

00:25:02,910 --> 00:25:08,340
data services you have to ensure that

00:25:05,400 --> 00:25:10,860
you'll find a way to deal with the

00:25:08,340 --> 00:25:13,020
heterogeneity as the backup procedure of

00:25:10,860 --> 00:25:16,830
different data services can be vastly

00:25:13,020 --> 00:25:19,710
different so by providing a unified

00:25:16,830 --> 00:25:21,690
backup API you can actually abstract

00:25:19,710 --> 00:25:25,800
from the SATA regina t at some point and

00:25:21,690 --> 00:25:28,170
then settle on more generic resolution

00:25:25,800 --> 00:25:30,780
logic for example recovering data

00:25:28,170 --> 00:25:33,420
service instances which makes the

00:25:30,780 --> 00:25:36,270
existence of such a backup framework for

00:25:33,420 --> 00:25:39,380
example necessary where you have data

00:25:36,270 --> 00:25:41,760
service specific plugins and also

00:25:39,380 --> 00:25:43,860
storage specific products on the right

00:25:41,760 --> 00:25:47,400
side and in between a filter chain that

00:25:43,860 --> 00:25:49,410
does compression and encryption now it's

00:25:47,400 --> 00:25:51,360
a pretty interesting time because we

00:25:49,410 --> 00:25:55,470
have Cloud Foundry on the one hand that

00:25:51,360 --> 00:25:57,540
allows us to create applications we have

00:25:55,470 --> 00:25:59,730
underneath Bosh that can also be used to

00:25:57,540 --> 00:26:01,950
provision data services and at the same

00:25:59,730 --> 00:26:04,860
time we see kubernetes becoming more and

00:26:01,950 --> 00:26:06,810
more popular and I would make a bet that

00:26:04,860 --> 00:26:09,300
at some point that isolation problem

00:26:06,810 --> 00:26:12,510
will go away and kou Benitez will be a

00:26:09,300 --> 00:26:15,780
fair choice to use for data service

00:26:12,510 --> 00:26:18,470
automation so we should be prepared to

00:26:15,780 --> 00:26:21,420
ship in two different automation formats

00:26:18,470 --> 00:26:24,230
including boss releases PCF tiles and

00:26:21,420 --> 00:26:27,360
kubernetes helm charts for example and

00:26:24,230 --> 00:26:29,430
this is more rhetorical question because

00:26:27,360 --> 00:26:31,560
at this point you have to somehow

00:26:29,430 --> 00:26:33,990
organize the answer to that question

00:26:31,560 --> 00:26:36,330
what is actually shared across the

00:26:33,990 --> 00:26:38,700
automation of PCF tiles and kubernetes

00:26:36,330 --> 00:26:40,860
ham charts and the answer is your

00:26:38,700 --> 00:26:43,290
operational model because

00:26:40,860 --> 00:26:45,210
once you solved the magic around

00:26:43,290 --> 00:26:46,799
organizing a Postgres cluster its

00:26:45,210 --> 00:26:49,440
cluster management its failover

00:26:46,799 --> 00:26:51,720
its leader election and and the

00:26:49,440 --> 00:26:54,510
propagation of a new master node this

00:26:51,720 --> 00:26:56,760
logic will basically be the same because

00:26:54,510 --> 00:26:58,670
the underlying principles are the same

00:26:56,760 --> 00:27:02,429
so whether you are restarting a

00:26:58,670 --> 00:27:04,710
container or your VM will be pretty much

00:27:02,429 --> 00:27:06,990
the same thing and I bet you could use

00:27:04,710 --> 00:27:12,090
the same test suite as long as you

00:27:06,990 --> 00:27:13,980
abstract the actual deployment so yeah

00:27:12,090 --> 00:27:17,400
this should be done whenever you

00:27:13,980 --> 00:27:19,559
automate that you should well we use

00:27:17,400 --> 00:27:20,970
open source databases and because we

00:27:19,559 --> 00:27:25,049
believe that this is an open standard

00:27:20,970 --> 00:27:26,640
between the application end and the NT

00:27:25,049 --> 00:27:30,210
database and you can get it from

00:27:26,640 --> 00:27:32,490
everywhere so preventing a login the

00:27:30,210 --> 00:27:34,679
automation language so the language you

00:27:32,490 --> 00:27:37,169
as a platform user talk to trigger the

00:27:34,679 --> 00:27:39,750
creation of a database should be an open

00:27:37,169 --> 00:27:41,900
standard as well and the open service

00:27:39,750 --> 00:27:45,059
broker API does a really good job here

00:27:41,900 --> 00:27:46,650
so this avoids a bender login as long as

00:27:45,059 --> 00:27:48,780
you have a post-race on the left side

00:27:46,650 --> 00:27:50,549
and the open service broke on the right

00:27:48,780 --> 00:27:56,910
side you can't exchange the automation

00:27:50,549 --> 00:27:58,590
in the middle looking at this very at

00:27:56,910 --> 00:28:00,419
this mission of having a broader set of

00:27:58,590 --> 00:28:04,200
data services one of the things that

00:28:00,419 --> 00:28:06,510
ensures fast release cadence is to never

00:28:04,200 --> 00:28:08,580
touch the upstream source code with the

00:28:06,510 --> 00:28:10,679
exception of temporary hot fixes that

00:28:08,580 --> 00:28:13,919
are that have to be monitored as long as

00:28:10,679 --> 00:28:15,900
they applied you wouldn't do that in the

00:28:13,919 --> 00:28:18,780
long term because then the release

00:28:15,900 --> 00:28:20,960
cadence becomes slower because you still

00:28:18,780 --> 00:28:23,040
have to maintain the manual

00:28:20,960 --> 00:28:26,460
harmonization between your patch and

00:28:23,040 --> 00:28:28,860
upstream changes so that's why the only

00:28:26,460 --> 00:28:31,080
true way on on changing the source code

00:28:28,860 --> 00:28:35,460
would be to have a pull request against

00:28:31,080 --> 00:28:37,919
the upstream source code another

00:28:35,460 --> 00:28:39,390
strategy and this is a major one is that

00:28:37,919 --> 00:28:41,460
you should solve any issue on the

00:28:39,390 --> 00:28:43,200
framework level if possible you

00:28:41,460 --> 00:28:44,970
shouldn't approach the automation of a

00:28:43,200 --> 00:28:47,250
broader set of data service without

00:28:44,970 --> 00:28:50,669
having a framework that contains

00:28:47,250 --> 00:28:52,440
solutions to most common issues so for

00:28:50,669 --> 00:28:54,010
example on-demand provisioning of

00:28:52,440 --> 00:28:56,020
dedicated instances some

00:28:54,010 --> 00:28:58,900
that we've put into a framework so

00:28:56,020 --> 00:29:00,790
whenever we automate a new service we

00:28:58,900 --> 00:29:02,530
can rely on that functionality and all

00:29:00,790 --> 00:29:04,420
we have to do is create a bas-reliefs

00:29:02,530 --> 00:29:06,490
and a component that manages credentials

00:29:04,420 --> 00:29:08,620
as well as a backup and restore plugin

00:29:06,490 --> 00:29:10,590
that boils down the creation of new

00:29:08,620 --> 00:29:14,350
services to around four to eight weeks

00:29:10,590 --> 00:29:16,750
which is which is a pretty nice thing to

00:29:14,350 --> 00:29:18,460
have and the reason why this is possible

00:29:16,750 --> 00:29:20,260
because we always think about any

00:29:18,460 --> 00:29:22,420
problem we have is is this a framework

00:29:20,260 --> 00:29:25,960
level problem also or data service

00:29:22,420 --> 00:29:28,030
specific problem so only if that cannot

00:29:25,960 --> 00:29:30,730
be solved on us on a framework level or

00:29:28,030 --> 00:29:33,360
we have great benefit to solve a data

00:29:30,730 --> 00:29:35,740
service specifically this is what we do

00:29:33,360 --> 00:29:37,930
so one of the problems one of the issues

00:29:35,740 --> 00:29:40,510
that you could for example solve at the

00:29:37,930 --> 00:29:43,450
the framework level is that you should

00:29:40,510 --> 00:29:45,400
you should protect your data service

00:29:43,450 --> 00:29:47,620
instances against such a trivial thing

00:29:45,400 --> 00:29:49,600
like like the disk is full you would be

00:29:47,620 --> 00:29:52,420
surprised how many data servers respond

00:29:49,600 --> 00:29:55,270
to that event with horriffic failure and

00:29:52,420 --> 00:29:57,670
data loss so a parachute mechanism for

00:29:55,270 --> 00:29:59,920
example that prevents that is one of the

00:29:57,670 --> 00:30:04,810
things you can easily build for all data

00:29:59,920 --> 00:30:06,760
services and then reapply so obviously

00:30:04,810 --> 00:30:08,710
we've seen that the release cadence

00:30:06,760 --> 00:30:12,160
among the entire value chain is very

00:30:08,710 --> 00:30:14,110
important to us so without release tests

00:30:12,160 --> 00:30:16,570
you how would you deal with the

00:30:14,110 --> 00:30:18,820
uncertainties of those upstream changes

00:30:16,570 --> 00:30:20,710
how would you detect that the contract

00:30:18,820 --> 00:30:23,500
is broken between the Postgres version

00:30:20,710 --> 00:30:25,960
you just recently had the new one and

00:30:23,500 --> 00:30:29,140
your automation release for example as

00:30:25,960 --> 00:30:31,420
trivial things as a configuration

00:30:29,140 --> 00:30:33,670
parameter that has changed or a default

00:30:31,420 --> 00:30:36,600
value that has been introduced that will

00:30:33,670 --> 00:30:38,710
change something about your assumption

00:30:36,600 --> 00:30:41,050
especially when dealing with clusters

00:30:38,710 --> 00:30:42,940
there's a lot of complexity so you

00:30:41,050 --> 00:30:44,890
should be able to test those things and

00:30:42,940 --> 00:30:47,380
should provide a comprehensive test

00:30:44,890 --> 00:30:49,480
suite walking through each release

00:30:47,380 --> 00:30:52,570
through the lifecycle of such a database

00:30:49,480 --> 00:30:54,580
so for example we excessively test the

00:30:52,570 --> 00:30:56,400
reintegration of a failed master in

00:30:54,580 --> 00:30:59,170
Postgres just to give you one example

00:30:56,400 --> 00:31:01,570
this gives you the confidence that a new

00:30:59,170 --> 00:31:06,100
up stream automation release that has

00:31:01,570 --> 00:31:08,320
been created by you will be working when

00:31:06,100 --> 00:31:10,600
you deliver it more than that

00:31:08,320 --> 00:31:12,429
you should also take whatever you have

00:31:10,600 --> 00:31:15,730
been riding to the run books in the past

00:31:12,429 --> 00:31:21,090
years and an edit to your test suite so

00:31:15,730 --> 00:31:23,860
that a failure never occurs twice so

00:31:21,090 --> 00:31:26,049
this talk has been at least three times

00:31:23,860 --> 00:31:29,710
as long so I had to narrow it down

00:31:26,049 --> 00:31:31,630
I will also release a plot post series

00:31:29,710 --> 00:31:33,519
around that topic with a longer video

00:31:31,630 --> 00:31:37,299
version of that talk so if you want to

00:31:33,519 --> 00:31:38,919
dig deeper into that topic just go by

00:31:37,299 --> 00:31:42,960
the any ninth block and you will find

00:31:38,919 --> 00:31:45,759
more to that to just reiterate I

00:31:42,960 --> 00:31:47,350
strongly believe that if you change that

00:31:45,759 --> 00:31:50,200
mission statement for example and

00:31:47,350 --> 00:31:52,029
replace that idea of having a lot of

00:31:50,200 --> 00:31:54,730
data service to be automated with just

00:31:52,029 --> 00:31:58,179
one that also has a lot of impact on the

00:31:54,730 --> 00:32:01,029
principles so setting your mission in in

00:31:58,179 --> 00:32:03,250
conjunction to your platform strategy is

00:32:01,029 --> 00:32:05,110
very important it leads to some

00:32:03,250 --> 00:32:07,659
principles that should be known to all

00:32:05,110 --> 00:32:09,940
your team members and enables local

00:32:07,659 --> 00:32:13,090
autonomous decisions if you split your

00:32:09,940 --> 00:32:14,799
automation team into smaller groups the

00:32:13,090 --> 00:32:16,960
automation is very important and should

00:32:14,799 --> 00:32:19,240
rely on a framework that has a lot of

00:32:16,960 --> 00:32:22,830
those common issues across data service

00:32:19,240 --> 00:32:25,210
work into it and as you see there are

00:32:22,830 --> 00:32:27,909
technological and strategic shifts that

00:32:25,210 --> 00:32:31,029
make agnosticism important where you try

00:32:27,909 --> 00:32:33,220
to abstract though so for example by

00:32:31,029 --> 00:32:36,610
using technology such as boss you can

00:32:33,220 --> 00:32:38,679
protect against or you can be flexible

00:32:36,610 --> 00:32:41,110
and deploy to different infrastructures

00:32:38,679 --> 00:32:43,960
with the open service broker API you can

00:32:41,110 --> 00:32:48,039
support multiple platforms and so on so

00:32:43,960 --> 00:32:52,389
you try to keep your investment safe by

00:32:48,039 --> 00:32:53,980
committing to the right things so I hope

00:32:52,389 --> 00:32:56,139
you gained something for your particular

00:32:53,980 --> 00:32:58,179
data service mission if you have any

00:32:56,139 --> 00:33:02,440
questions feel free to reach out we have

00:32:58,179 --> 00:33:04,960
a booth shoot any questions if come by

00:33:02,440 --> 00:33:07,149
and ask me any question or just tweet me

00:33:04,960 --> 00:33:09,370
or send me a mail there are any

00:33:07,149 --> 00:33:13,259
questions left just feel free to ask I

00:33:09,370 --> 00:33:13,259
think we have a few minutes left

00:33:17,900 --> 00:33:26,450
no wait then thank you very much

00:33:21,540 --> 00:33:26,450

YouTube URL: https://www.youtube.com/watch?v=4S9YKyeqyqY


