Title: Your Data Center will Implode in 30 days, a Success Story - Brian Seguin & Bill Chapman, Stark & Way
Publication date: 2019-04-11
Playlist: Cloud Foundry Summit NA 2019 - Philadelphia
Description: 
	Your Data Center will Implode in 30 days, a Success Story - Brian Seguin & Bill Chapman, Stark & Wayne

Due to forces beyond your control, you have only 30 days until you lose access to your data center which is housing hundreds of core business applications. How can you evaluate where, when, and how to migrate in such a short window? If you can get help, what do you look for in how your solution will be delivered? How can you front load the migration to ensure no loss of data. What are some acceptable workarounds if you cannot access your new environment in a timely manner? This talk will cover a real world use case and provide answers to these questions.

About Bill Chapman
Bill is VP of Engineering at Stark & Wayne where he focuses on helping his teams deliver world-class solutions to cloud-native problems.
Prior to joining Stark & Wayne, Bill specialized in helping to start software companies along with helping established companies greenfield brand- new software efforts.
Bill was fortunate to start his first business during the Y2K era, when they were giving technical work to anyone who knew what a computer was. Fortunately, he's learned a lot since then

About Brian Seguin
Brian Seguin is the Director of Operations at Stark & Wayne, where he brings success to the companyâ€™s day-to-day business operations, contracting, and partnerships. His experience managing controls teams for multi-billion dollar trading desks is where he forged a blend of high-tech and business skills. As an Eagle Scout, Knight of Columbus, and former Volunteer Fire Captain, Brian brings a passion for giving back to the community. That passion extends to the Cloud Foundry Community, which he believes can facilitate very real and very positive changes for the high-tech and business worlds.

When Brian is not laboring at his computer, he can be found working on his farm, experiencing the trials and tribulations of tractor repair, hard soil, and predators.

https://www.cloudfoundry.org/
Captions: 
	00:00:00,030 --> 00:00:05,850
all right so this is a user story your

00:00:03,840 --> 00:00:08,820
data center will implode in 30 days it

00:00:05,850 --> 00:00:11,190
is based on a true story but the names

00:00:08,820 --> 00:00:14,000
and data center locations are have been

00:00:11,190 --> 00:00:17,760
changed to protect the innocent

00:00:14,000 --> 00:00:19,410
so we are Stark and Wayne we are a

00:00:17,760 --> 00:00:21,750
founding silver member of the club

00:00:19,410 --> 00:00:24,240
foundry foundation you know we've been

00:00:21,750 --> 00:00:26,130
going to these summits for a very long

00:00:24,240 --> 00:00:27,359
time I think since the inception we've

00:00:26,130 --> 00:00:31,920
been everyone we've been every one of

00:00:27,359 --> 00:00:33,510
them so we're here to help where as I

00:00:31,920 --> 00:00:35,309
like to say were the leading compound

00:00:33,510 --> 00:00:39,329
rain cloud native technology consultancy

00:00:35,309 --> 00:00:41,280
and myself I'm Brian segment I'm the CEO

00:00:39,329 --> 00:00:47,850
of Stark and Wayne and with me is Bill

00:00:41,280 --> 00:00:48,930
Chapman he's our VP of engineering so we

00:00:47,850 --> 00:00:53,930
wanted to have some fun with this

00:00:48,930 --> 00:00:56,879
because it's kind of a fun story and fun

00:00:53,930 --> 00:00:57,800
so what we what we're doing here is we

00:00:56,879 --> 00:01:00,210
actually have an act

00:00:57,800 --> 00:01:04,979
we've kind of orchestrated the play here

00:01:00,210 --> 00:01:07,049
for for this and so I will be playing

00:01:04,979 --> 00:01:09,360
the you know the VP in charge of the

00:01:07,049 --> 00:01:12,240
cloud platform for the for this custom

00:01:09,360 --> 00:01:13,950
for this company and this is a platform

00:01:12,240 --> 00:01:16,619
of a multi-billion dollar company that

00:01:13,950 --> 00:01:18,960
has 300 business critical applications

00:01:16,619 --> 00:01:22,680
on it with 2008 eyes and a whole bunch

00:01:18,960 --> 00:01:24,869
of data services things like that so I

00:01:22,680 --> 00:01:27,060
had thought as this VP in charge of this

00:01:24,869 --> 00:01:29,970
platform that my director of engineering

00:01:27,060 --> 00:01:31,079
was taking care of the migration efforts

00:01:29,970 --> 00:01:33,060
because we knew this was a long time

00:01:31,079 --> 00:01:36,630
coming we had our data centers contract

00:01:33,060 --> 00:01:38,970
was coming to a close so I thought

00:01:36,630 --> 00:01:41,310
everything was being handled and I

00:01:38,970 --> 00:01:42,990
approached him one day it were we're

00:01:41,310 --> 00:01:46,409
just a little bit about 30 just a little

00:01:42,990 --> 00:01:47,640
more than 30 days out from this the

00:01:46,409 --> 00:01:50,430
Stata Center migration that we would

00:01:47,640 --> 00:01:52,049
have and I talked to him I said all

00:01:50,430 --> 00:01:54,229
right so what's the plans what's the

00:01:52,049 --> 00:01:57,899
progress you know okay I have a report

00:01:54,229 --> 00:01:59,759
and he says to me you know I don't have

00:01:57,899 --> 00:02:05,310
anything and by the way here's my notice

00:01:59,759 --> 00:02:08,580
okay so so at this point I'm sitting

00:02:05,310 --> 00:02:10,979
here thinking all right well my my my

00:02:08,580 --> 00:02:12,010
director of engineering is left I really

00:02:10,979 --> 00:02:20,470
only have a few option

00:02:12,010 --> 00:02:22,599
I could quit I could beg for an

00:02:20,470 --> 00:02:24,849
extension from our data center people

00:02:22,599 --> 00:02:26,980
but apparently they had already sold

00:02:24,849 --> 00:02:30,459
this space and they're forcing the

00:02:26,980 --> 00:02:32,620
contract out okay or I could find

00:02:30,459 --> 00:02:35,069
someone dumb enough who thinks they can

00:02:32,620 --> 00:02:39,329
actually do this all in 30 days

00:02:35,069 --> 00:02:42,220
so I hire a new director of engineering

00:02:39,329 --> 00:02:43,750
so please welcome bill Chapman who is

00:02:42,220 --> 00:02:49,720
our new director of engineering that

00:02:43,750 --> 00:02:51,940
says he thinks he can do this we find

00:02:49,720 --> 00:02:53,859
our fearful c-level executive pacing in

00:02:51,940 --> 00:02:59,799
their office they've just got the bad

00:02:53,859 --> 00:03:05,489
news bill what are our options here

00:02:59,799 --> 00:03:07,840
well I think our primary focus should be

00:03:05,489 --> 00:03:10,690
standing up a new platform in the new

00:03:07,840 --> 00:03:14,260
data center and we will have the

00:03:10,690 --> 00:03:15,819
application developers each have a

00:03:14,260 --> 00:03:17,260
maintenance window where they will

00:03:15,819 --> 00:03:22,599
deploy their applications in the new

00:03:17,260 --> 00:03:25,989
data center we have 300 applications

00:03:22,599 --> 00:03:28,450
here and there's 300 applications teams

00:03:25,989 --> 00:03:30,280
and about like three times the size of

00:03:28,450 --> 00:03:32,560
that in people do you think you can

00:03:30,280 --> 00:03:34,060
orchestrate all of those teams to get in

00:03:32,560 --> 00:03:36,220
sync and do all of this and not in this

00:03:34,060 --> 00:03:39,609
small window maybe we should go to the

00:03:36,220 --> 00:03:41,980
whiteboard so what you're saying is

00:03:39,609 --> 00:03:45,760
we've got our Cloud Foundry and it's

00:03:41,980 --> 00:03:49,500
Platt individual platforms we've got our

00:03:45,760 --> 00:03:53,500
data services we've got 300 applications

00:03:49,500 --> 00:03:55,150
2,000 application instances one thing we

00:03:53,500 --> 00:03:57,299
haven't mentioned we've got six

00:03:55,150 --> 00:03:59,560
brokerage service teams so that means

00:03:57,299 --> 00:04:01,750
those are all a separate issue those

00:03:59,560 --> 00:04:03,459
need to be migrated as well and we have

00:04:01,750 --> 00:04:07,569
various user-defined services that we

00:04:03,459 --> 00:04:09,370
don't have here it might make more sense

00:04:07,569 --> 00:04:11,799
to deal with the six brokered service

00:04:09,370 --> 00:04:13,870
services teams and then treat the

00:04:11,799 --> 00:04:17,650
platform as a data service as its sell

00:04:13,870 --> 00:04:19,849
on its self migrate it the same way let

00:04:17,650 --> 00:04:28,069
me

00:04:19,849 --> 00:04:31,650
well let's I mean I like the idea that

00:04:28,069 --> 00:04:34,469
six teams versus 300 I mean that's a lot

00:04:31,650 --> 00:04:35,909
easier you know I mean so how would you

00:04:34,469 --> 00:04:37,560
do that migrate like how would this work

00:04:35,909 --> 00:04:40,250
like one of the migration let me talk to

00:04:37,560 --> 00:04:48,990
the team about that we have some options

00:04:40,250 --> 00:04:51,000
okay they've all slept on it calls have

00:04:48,990 --> 00:04:55,860
been made teams have been alerted no one

00:04:51,000 --> 00:04:58,949
is happy no one is happy but plans are

00:04:55,860 --> 00:05:02,419
in action hey boss good news I think we

00:04:58,949 --> 00:05:02,419
have a plan that we can work with okay

00:05:03,500 --> 00:05:08,580
so we've got a staging platform we've

00:05:06,930 --> 00:05:11,490
got a pre-production platform we've got

00:05:08,580 --> 00:05:15,479
a production platform we've got a data

00:05:11,490 --> 00:05:18,469
services tenant in Poughkeepsie

00:05:15,479 --> 00:05:21,539
we need to migrate all of the databases

00:05:18,469 --> 00:05:24,240
we need to migrate all of the blobs

00:05:21,539 --> 00:05:26,310
these blob stores could be large we

00:05:24,240 --> 00:05:28,139
don't know how fast the pipe is gonna be

00:05:26,310 --> 00:05:29,580
between the data centers you know the

00:05:28,139 --> 00:05:34,219
second data center is in Anchorage

00:05:29,580 --> 00:05:38,159
Alaska so we may need to fly it there

00:05:34,219 --> 00:05:41,849
that is a possibility well fly it there

00:05:38,159 --> 00:05:43,889
I mean I guess I could talk to the CEO

00:05:41,849 --> 00:05:45,990
about a corporate jet but I don't think

00:05:43,889 --> 00:05:48,029
he's gonna be I mean there's a lot of

00:05:45,990 --> 00:05:50,310
risk associated to that it's it's not

00:05:48,029 --> 00:05:52,409
likely we need to do some investigation

00:05:50,310 --> 00:05:54,569
I think the pipe will be fine we should

00:05:52,409 --> 00:05:58,490
be able to sync everything over I'm just

00:05:54,569 --> 00:05:58,490
keeping you aware okay

00:06:03,570 --> 00:06:11,350
so so with this plan I mean is there

00:06:08,740 --> 00:06:14,160
gonna be any like freezes to develop man

00:06:11,350 --> 00:06:18,660
like what's art what's her you know

00:06:14,160 --> 00:06:22,480
what's our downtime gonna look like well

00:06:18,660 --> 00:06:25,570
most of the risk is gonna fall on the

00:06:22,480 --> 00:06:31,720
service bound teams so our database back

00:06:25,570 --> 00:06:33,910
services our MySQL team our rabbitmq

00:06:31,720 --> 00:06:36,820
team those teams are going to need to

00:06:33,910 --> 00:06:37,720
migrate their data services individually

00:06:36,820 --> 00:06:39,370
those are going to have to be

00:06:37,720 --> 00:06:44,380
coordinated with all six of those teams

00:06:39,370 --> 00:06:45,550
the platform itself we won't have to

00:06:44,380 --> 00:06:47,350
worry too much about freezing

00:06:45,550 --> 00:06:50,200
development but we will have to freeze

00:06:47,350 --> 00:06:52,390
deployment so the developers will not be

00:06:50,200 --> 00:06:54,730
able to deploy to the platform while

00:06:52,390 --> 00:06:56,200
it's in active migration the data

00:06:54,730 --> 00:06:57,700
service teams on the other hand that's a

00:06:56,200 --> 00:07:01,540
different story the data service teams

00:06:57,700 --> 00:07:03,220
may have to have extended periods where

00:07:01,540 --> 00:07:04,720
there may be outages that affect some of

00:07:03,220 --> 00:07:07,300
the applications we're hoping to

00:07:04,720 --> 00:07:09,160
mitigate that what might be ideal what

00:07:07,300 --> 00:07:11,550
might work best is if we deploy those

00:07:09,160 --> 00:07:15,070
data services to a separate location

00:07:11,550 --> 00:07:16,450
proxy to that location the new the apps

00:07:15,070 --> 00:07:18,100
won't know the difference there would

00:07:16,450 --> 00:07:19,870
just be a small cut over but we're not

00:07:18,100 --> 00:07:22,510
sure yet we're still in day one here

00:07:19,870 --> 00:07:25,330
well so what's our data loss risk here

00:07:22,510 --> 00:07:28,240
data loss risk is minimal if you

00:07:25,330 --> 00:07:32,580
remember data migration is really just

00:07:28,240 --> 00:07:35,770
disaster recovery that you get to plan

00:07:32,580 --> 00:07:35,920
so what happens if it doesn't work all

00:07:35,770 --> 00:07:37,810
right

00:07:35,920 --> 00:07:39,430
am I asking the CEO for the corporate

00:07:37,810 --> 00:07:42,030
chat or are we playing Ice Road Truckers

00:07:39,430 --> 00:07:44,710
and trying to drive this to Alaska well

00:07:42,030 --> 00:07:46,240
once the new platform is deployed we can

00:07:44,710 --> 00:07:49,200
always fall back to what I said on day

00:07:46,240 --> 00:07:51,520
one the application developers can just

00:07:49,200 --> 00:07:53,220
individually reach their applications

00:07:51,520 --> 00:07:57,970
and it will converge on the new state

00:07:53,220 --> 00:08:00,420
okay so I mean how many people do we do

00:07:57,970 --> 00:08:03,040
you actually need to get this done I

00:08:00,420 --> 00:08:05,440
would say we need at least one member of

00:08:03,040 --> 00:08:07,450
each data service team that knows that

00:08:05,440 --> 00:08:10,390
data service and they can plan out their

00:08:07,450 --> 00:08:13,480
migrations individually since I run the

00:08:10,390 --> 00:08:15,680
platform team I will focus on that and I

00:08:13,480 --> 00:08:17,540
think I'm going to need two engineers

00:08:15,680 --> 00:08:19,280
thirty days is gonna be cutting it kind

00:08:17,540 --> 00:08:21,110
of close but they're gonna need to be

00:08:19,280 --> 00:08:23,479
focused on the price yeah I mean how

00:08:21,110 --> 00:08:26,660
much time is that is this day they're

00:08:23,479 --> 00:08:28,040
gonna take the sink like do we even have

00:08:26,660 --> 00:08:33,440
enough time to sync the data with our

00:08:28,040 --> 00:08:36,140
30-day window if we use the jet but but

00:08:33,440 --> 00:08:38,479
seriously no I think we the blobstore is

00:08:36,140 --> 00:08:41,510
our biggest concern those are those are

00:08:38,479 --> 00:08:42,919
the bits of source code and stuff that's

00:08:41,510 --> 00:08:45,080
compiled and sitting up there those can

00:08:42,919 --> 00:08:47,779
be very large the last time I had to

00:08:45,080 --> 00:08:50,300
sync when it took about 10 hours but

00:08:47,779 --> 00:08:53,540
we'll sync it ahead of time so by the

00:08:50,300 --> 00:08:57,290
time we get to the actual migration date

00:08:53,540 --> 00:08:59,000
we're only syncing the difference so

00:08:57,290 --> 00:09:01,070
you're gonna have whatever has changed

00:08:59,000 --> 00:09:03,710
since the last time if we sync it daily

00:09:01,070 --> 00:09:08,270
it's only gonna be 24 hours worth of

00:09:03,710 --> 00:09:10,460
change coupled that with the deployment

00:09:08,270 --> 00:09:11,960
freeze so that the developers can't

00:09:10,460 --> 00:09:13,850
actually push anything difference in

00:09:11,960 --> 00:09:16,250
that last 24 hours might have nothing

00:09:13,850 --> 00:09:17,779
there might be no difference so we'll

00:09:16,250 --> 00:09:24,440
already know the platform is ready when

00:09:17,779 --> 00:09:26,390
we actually do the final switchover it's

00:09:24,440 --> 00:09:28,910
been a rough two weeks some things went

00:09:26,390 --> 00:09:31,070
well some things went very poorly Jayne

00:09:28,910 --> 00:09:32,750
the platform team lead came in Friday

00:09:31,070 --> 00:09:39,440
and said don't forget I'm on vacation

00:09:32,750 --> 00:09:41,959
next week so we've managed to migrate

00:09:39,440 --> 00:09:44,270
staging and the data services that are

00:09:41,959 --> 00:09:46,100
needed for the applications in stated

00:09:44,270 --> 00:09:50,690
staging were handled by the individual

00:09:46,100 --> 00:09:52,250
application earth data service teams pre

00:09:50,690 --> 00:09:56,690
product production are still not

00:09:52,250 --> 00:10:00,589
provisioned but we have automation to

00:09:56,690 --> 00:10:04,820
run once those are available so I spoke

00:10:00,589 --> 00:10:07,760
to Tom why'd it take you guys week to

00:10:04,820 --> 00:10:11,120
get all the networking stuff that you

00:10:07,760 --> 00:10:12,560
needed turns out the networking team was

00:10:11,120 --> 00:10:15,470
complaining that there isn't enough IP

00:10:12,560 --> 00:10:17,150
space available for what we're asking

00:10:15,470 --> 00:10:20,209
for the new platform and remember that

00:10:17,150 --> 00:10:23,270
since we're doing a direct migration of

00:10:20,209 --> 00:10:24,890
the platform we have to have the same IP

00:10:23,270 --> 00:10:26,970
space available in the new data center

00:10:24,890 --> 00:10:28,769
so this is

00:10:26,970 --> 00:10:31,350
a common problem we have with with

00:10:28,769 --> 00:10:33,990
networking issues with we want too many

00:10:31,350 --> 00:10:35,639
IP addresses it's a large class

00:10:33,990 --> 00:10:38,759
what other blockers do we need to worry

00:10:35,639 --> 00:10:40,350
about at this point since we've done the

00:10:38,759 --> 00:10:43,589
migration to staging we've proven out

00:10:40,350 --> 00:10:47,879
the idea we have gotten that test in

00:10:43,589 --> 00:10:49,379
place the plan seems to work is expected

00:10:47,879 --> 00:10:52,050
but we're really pushing it for time

00:10:49,379 --> 00:10:56,490
also two of the data service teams were

00:10:52,050 --> 00:10:58,889
not yet able to migrate okay so it took

00:10:56,490 --> 00:11:00,750
you two weeks to get what I'm looking at

00:10:58,889 --> 00:11:01,980
it's one environment so like my math I'm

00:11:00,750 --> 00:11:03,540
sitting here thinking all right it's

00:11:01,980 --> 00:11:05,639
gonna take us two weeks for the next

00:11:03,540 --> 00:11:07,410
burden to the next two environments so

00:11:05,639 --> 00:11:11,610
that's four weeks out we only have two

00:11:07,410 --> 00:11:13,319
weeks left I mean well I understand but

00:11:11,610 --> 00:11:14,790
the results in staging have been

00:11:13,319 --> 00:11:17,089
encouraging and the reason it took so

00:11:14,790 --> 00:11:22,589
long is because we did a large upfront

00:11:17,089 --> 00:11:23,610
proof-of-concept we have the excuse me I

00:11:22,589 --> 00:11:25,079
thought you said this was gonna work

00:11:23,610 --> 00:11:27,079
well why do you need to do a proof of

00:11:25,079 --> 00:11:32,279
concept if you've done this before I

00:11:27,079 --> 00:11:38,759
might not have done this before but I

00:11:32,279 --> 00:11:41,579
promise you it works now also by the way

00:11:38,759 --> 00:11:43,379
the Stark and Wayne team has a really

00:11:41,579 --> 00:11:50,990
cool product called shield that helped

00:11:43,379 --> 00:11:50,990
us out a lot along the way anyways

00:12:04,289 --> 00:12:09,579
the migration is complete things have

00:12:07,359 --> 00:12:11,679
gone pretty well some developers are

00:12:09,579 --> 00:12:15,999
grumpy but developers are always grumpy

00:12:11,679 --> 00:12:21,939
the team is reflecting on the month so

00:12:15,999 --> 00:12:24,519
although this was successful and I still

00:12:21,939 --> 00:12:25,839
yet to see seem ungrateful to our

00:12:24,519 --> 00:12:30,789
director of engineering who has actually

00:12:25,839 --> 00:12:32,909
pulled this off so bill can you actually

00:12:30,789 --> 00:12:35,739
confirm that we did not lose any data

00:12:32,909 --> 00:12:37,869
well the data services teams have

00:12:35,739 --> 00:12:41,879
informed me that their migrations were

00:12:37,869 --> 00:12:41,879
successful I'm going to trust them

00:12:41,970 --> 00:12:49,289
honestly it really is just a expansion

00:12:46,509 --> 00:12:51,849
of the application migration paradigm

00:12:49,289 --> 00:12:54,489
when you when you push an application

00:12:51,849 --> 00:12:56,349
you are going to move the data move the

00:12:54,489 --> 00:12:58,209
application cutover dns this is the

00:12:56,349 --> 00:13:01,539
exact same thing we've just done it for

00:12:58,209 --> 00:13:03,369
300 applications I'm fairly confident

00:13:01,539 --> 00:13:05,529
that it went pretty well I have talked

00:13:03,369 --> 00:13:08,649
to the leads of every data service team

00:13:05,529 --> 00:13:10,539
they have confirmed that each data

00:13:08,649 --> 00:13:15,699
service is intact and running as expect

00:13:10,539 --> 00:13:18,189
expected and we did have some grumpiness

00:13:15,699 --> 00:13:21,099
from some of the developers when they

00:13:18,189 --> 00:13:22,479
couldn't deploy but they were all happy

00:13:21,099 --> 00:13:25,089
to find that their data was where it

00:13:22,479 --> 00:13:28,629
needed to be when they were able to get

00:13:25,089 --> 00:13:31,059
back to work so you know when we kind of

00:13:28,629 --> 00:13:32,559
spoke a couple weeks ago you said we

00:13:31,059 --> 00:13:34,419
might actually have like minutes of

00:13:32,559 --> 00:13:38,919
actual application downtime and only

00:13:34,419 --> 00:13:40,059
like a 10 hour window of development

00:13:38,919 --> 00:13:43,089
downtime where they couldn't push our

00:13:40,059 --> 00:13:44,019
applications what was it down time we

00:13:43,089 --> 00:13:50,169
actually incurred

00:13:44,019 --> 00:13:52,839
so we had a 24 hour deployment freeze

00:13:50,169 --> 00:13:55,659
for our application developers so we had

00:13:52,839 --> 00:13:59,279
300 developers who were not able to do

00:13:55,659 --> 00:14:01,749
you know their weekly deployments but

00:13:59,279 --> 00:14:03,220
some of the missed deadlines we didn't

00:14:01,749 --> 00:14:06,519
do a very good job communicating that to

00:14:03,220 --> 00:14:09,419
them but with respect to actual downtime

00:14:06,519 --> 00:14:11,799
it was measured in minutes Oh

00:14:09,419 --> 00:14:14,729
data services were migrated ahead of

00:14:11,799 --> 00:14:17,600
time things were proxied accordingly

00:14:14,729 --> 00:14:21,050
when the platform came up

00:14:17,600 --> 00:14:22,850
converged on the proper state and there

00:14:21,050 --> 00:14:25,940
were only about five applications that

00:14:22,850 --> 00:14:28,160
weren't running as expected one of them

00:14:25,940 --> 00:14:30,380
is still being trouble shot it turns out

00:14:28,160 --> 00:14:33,319
it wasn't running as expected before the

00:14:30,380 --> 00:14:37,819
migration so you're the scapegoat yes

00:14:33,319 --> 00:14:40,160
okay so you know and all honestly I'm

00:14:37,819 --> 00:14:41,600
looking at the projects of the team

00:14:40,160 --> 00:14:43,550
still working at and I still see that

00:14:41,600 --> 00:14:45,470
there's migration efforts going on and

00:14:43,550 --> 00:14:47,240
you know everything's been up and

00:14:45,470 --> 00:14:49,579
running for a few days now so why are

00:14:47,240 --> 00:14:50,839
people still working on this there's

00:14:49,579 --> 00:14:52,639
still a lot of questions about the

00:14:50,839 --> 00:14:54,230
migration there are still developers who

00:14:52,639 --> 00:14:56,779
had problems before the migrations that

00:14:54,230 --> 00:14:58,759
weren't noticed they followed his us to

00:14:56,779 --> 00:15:00,680
the other platform when we did our due

00:14:58,759 --> 00:15:02,329
diligence up front we knew what state

00:15:00,680 --> 00:15:03,740
every application was in but it turned

00:15:02,329 --> 00:15:05,209
out some of the developers didn't know

00:15:03,740 --> 00:15:07,069
what state their own applications were

00:15:05,209 --> 00:15:08,959
in so we're still fighting through that

00:15:07,069 --> 00:15:11,240
you've told me our mission statement is

00:15:08,959 --> 00:15:14,509
to be helpful so that's what we're doing

00:15:11,240 --> 00:15:17,269
okay so how much longer will they need

00:15:14,509 --> 00:15:20,000
to be on this project it's mostly done

00:15:17,269 --> 00:15:22,819
we're gonna need somebody partially

00:15:20,000 --> 00:15:24,500
engaged for both PR for the platform

00:15:22,819 --> 00:15:26,569
team so that they understand what

00:15:24,500 --> 00:15:29,630
happened and why this was a tremendous

00:15:26,569 --> 00:15:31,790
effort and success on our end but we're

00:15:29,630 --> 00:15:33,769
also going to want to help developers so

00:15:31,790 --> 00:15:34,970
that they understand that there really

00:15:33,769 --> 00:15:37,069
aren't any differences in the new

00:15:34,970 --> 00:15:39,139
platform most of them should have been

00:15:37,069 --> 00:15:41,360
unaware if it wasn't for the 24 hour

00:15:39,139 --> 00:15:42,680
freeze on deploying most of the

00:15:41,360 --> 00:15:44,269
developers would have been unaware that

00:15:42,680 --> 00:15:46,209
we moved the platform six thousand miles

00:15:44,269 --> 00:15:51,410
away

00:15:46,209 --> 00:15:53,630
awesome well I mean good job and thank

00:15:51,410 --> 00:15:55,660
you for your efforts and as a token of

00:15:53,630 --> 00:16:02,889
our appreciation please accept this pen

00:15:55,660 --> 00:16:02,889
this is my pen oh thank you

00:16:05,850 --> 00:16:17,500
are there any questions so you know this

00:16:14,410 --> 00:16:20,290
is actually you know we kind of took

00:16:17,500 --> 00:16:22,589
some Liberty of what actually happened

00:16:20,290 --> 00:16:25,509
from the from the comedic standpoint but

00:16:22,589 --> 00:16:26,740
this is actually what happened for a

00:16:25,509 --> 00:16:31,779
migration effort that Stark and Wayne

00:16:26,740 --> 00:16:33,639
was involved with not last year and it

00:16:31,779 --> 00:16:35,439
was it was crazy you know they we have a

00:16:33,639 --> 00:16:39,100
customer that comes to us and they you

00:16:35,439 --> 00:16:42,430
know they say well we only have 30 days

00:16:39,100 --> 00:16:44,069
or otherwise you know our the Stata

00:16:42,430 --> 00:16:46,480
Center is gonna delete all our data and

00:16:44,069 --> 00:16:49,209
we said well can't you talk to the data

00:16:46,480 --> 00:16:51,430
center people and can't you do this

00:16:49,209 --> 00:16:52,899
can't you do that and they said well we

00:16:51,430 --> 00:16:55,389
had all these conversations and the

00:16:52,899 --> 00:16:57,579
answer is no do you think we can do it

00:16:55,389 --> 00:17:00,160
and and the funny thing is is they were

00:16:57,579 --> 00:17:02,829
going off of you know a timeline in

00:17:00,160 --> 00:17:05,230
quotes that we had given them like three

00:17:02,829 --> 00:17:07,030
to six months previous so immediately

00:17:05,230 --> 00:17:09,970
they come knock on our door and they say

00:17:07,030 --> 00:17:11,740
hey we have this emergency can you still

00:17:09,970 --> 00:17:13,990
commit to the the timeline that you said

00:17:11,740 --> 00:17:20,020
was a best-case scenario and we kind of

00:17:13,990 --> 00:17:23,020
said yes and no and we actually had a

00:17:20,020 --> 00:17:24,789
lot of really big discussions on it and

00:17:23,020 --> 00:17:26,620
we did do some proof of concepts before

00:17:24,789 --> 00:17:28,000
saying yes and the other fascinating

00:17:26,620 --> 00:17:29,590
part about it was something that was

00:17:28,000 --> 00:17:31,480
kind of implicit in this but it was I

00:17:29,590 --> 00:17:33,130
mean there's a couple ways you can do

00:17:31,480 --> 00:17:35,440
this migrations a lot of ways but the

00:17:33,130 --> 00:17:36,880
easiest way is to just deploy a new

00:17:35,440 --> 00:17:38,980
platform and to get all of your

00:17:36,880 --> 00:17:41,230
developers on the same page and allowed

00:17:38,980 --> 00:17:44,110
them to CF push on the other side

00:17:41,230 --> 00:17:48,370
they're literally worth 300 development

00:17:44,110 --> 00:17:50,799
teams that nobody had control over so at

00:17:48,370 --> 00:17:53,320
the end of the day they wanted to leave

00:17:50,799 --> 00:17:59,169
them out of the situation so what we had

00:17:53,320 --> 00:18:01,659
to do is deploy the platform move all of

00:17:59,169 --> 00:18:03,190
the data and the blobs and the and the

00:18:01,659 --> 00:18:05,559
bits that matter over to the new

00:18:03,190 --> 00:18:08,679
platform and then just hope everything

00:18:05,559 --> 00:18:11,500
converged on the same exact state that

00:18:08,679 --> 00:18:13,720
it was in in the other data center there

00:18:11,500 --> 00:18:16,980
is a technical talk on this concept

00:18:13,720 --> 00:18:19,450
are on this coming today at five o'clock

00:18:16,980 --> 00:18:21,149
5:15 and the actual engineers who did

00:18:19,450 --> 00:18:25,269
the migration will be giving you the

00:18:21,149 --> 00:18:27,580
detail about it but we just wanted to

00:18:25,269 --> 00:18:34,139
kind of talk through kind of the

00:18:27,580 --> 00:18:34,139
high-level here yes

00:18:43,410 --> 00:18:49,560
I believe that if we thought we could

00:18:47,700 --> 00:18:51,840
have coordinated that many different

00:18:49,560 --> 00:18:53,670
teams and given them a minimal downtime

00:18:51,840 --> 00:18:55,770
window because that was the real problem

00:18:53,670 --> 00:18:58,200
the problem was lights out in this I

00:18:55,770 --> 00:19:00,450
mean we didn't have to go lights out

00:18:58,200 --> 00:19:01,830
here lights on here because what they

00:19:00,450 --> 00:19:05,910
did is they proxied all the data

00:19:01,830 --> 00:19:07,440
services from a third location but it in

00:19:05,910 --> 00:19:08,640
the beginning we didn't know that we

00:19:07,440 --> 00:19:10,200
didn't know how that was gonna happen we

00:19:08,640 --> 00:19:11,790
were a week and a half for two weeks

00:19:10,200 --> 00:19:13,200
into the process before we knew how they

00:19:11,790 --> 00:19:15,870
were going to handle the data services

00:19:13,200 --> 00:19:18,390
so that was the concern there was that

00:19:15,870 --> 00:19:20,430
if all the data moves then all 300 teams

00:19:18,390 --> 00:19:22,050
have to be on board right away and

00:19:20,430 --> 00:19:23,610
all-cif pushed their app so they're

00:19:22,050 --> 00:19:28,950
gonna get downtime until they actually

00:19:23,610 --> 00:19:30,480
see f.push so we figured moving it and

00:19:28,950 --> 00:19:32,310
letting it converge on that state and

00:19:30,480 --> 00:19:34,290
not even really caring if the develop

00:19:32,310 --> 00:19:36,090
caring is the wrong term but not

00:19:34,290 --> 00:19:38,730
worrying about whether the development

00:19:36,090 --> 00:19:40,080
teams knew even because from our

00:19:38,730 --> 00:19:41,910
perspective we weren't the ones

00:19:40,080 --> 00:19:43,050
notifying the Vallon teams were under

00:19:41,910 --> 00:19:44,910
the impression that a lot of them

00:19:43,050 --> 00:19:48,510
weren't even told this was happening

00:19:44,910 --> 00:19:49,920
because once DNS was managed and data

00:19:48,510 --> 00:19:51,950
was moved

00:19:49,920 --> 00:19:54,840
they wouldn't know any difference

00:19:51,950 --> 00:19:58,490
because all all the endpoints were

00:19:54,840 --> 00:19:58,490
identical in the new data center

00:20:28,580 --> 00:20:33,090
honestly in the beginning we had we had

00:20:31,200 --> 00:20:35,430
many heated discussions about the best

00:20:33,090 --> 00:20:37,920
way to do it we've done it before but

00:20:35,430 --> 00:20:40,800
this that the developers might not know

00:20:37,920 --> 00:20:43,020
about it was the real clincher here

00:20:40,800 --> 00:20:44,670
because we had to make sure that they

00:20:43,020 --> 00:20:47,310
didn't experience any significant

00:20:44,670 --> 00:20:49,470
downtime and even when we got to the

00:20:47,310 --> 00:20:51,900
other end we were pretty pretty happy

00:20:49,470 --> 00:20:53,580
that it worked I mean but now we know it

00:20:51,900 --> 00:20:56,940
works it's a viable strategy because

00:20:53,580 --> 00:21:00,260
we've proven it out but we had some

00:20:56,940 --> 00:21:00,260
concerns all the way through the process

00:21:04,100 --> 00:21:09,900
we all we had access to was the CF

00:21:07,560 --> 00:21:12,840
endpoints so we could profile the system

00:21:09,900 --> 00:21:16,200
we could figure out what's there but as

00:21:12,840 --> 00:21:17,880
I joked in the talk we didn't know if

00:21:16,200 --> 00:21:19,470
all of those developers even knew what

00:21:17,880 --> 00:21:22,230
state their apps were supposed to be in

00:21:19,470 --> 00:21:23,820
in any given or you know any given part

00:21:22,230 --> 00:21:25,320
or some of those apps didn't even have

00:21:23,820 --> 00:21:28,410
teams assigned to them they were just

00:21:25,320 --> 00:21:31,050
there and some some automation somewhere

00:21:28,410 --> 00:21:32,850
was pushing it right yeah and what was

00:21:31,050 --> 00:21:35,610
neat about this is any teams that were

00:21:32,850 --> 00:21:37,500
modern and being proactive about their

00:21:35,610 --> 00:21:39,330
development and had pipelines that were

00:21:37,500 --> 00:21:41,730
working on things all of that automation

00:21:39,330 --> 00:21:43,020
should have just worked because all the

00:21:41,730 --> 00:21:45,240
endpoints stayed the same and all the

00:21:43,020 --> 00:21:48,120
access was proxied appropriately so it

00:21:45,240 --> 00:21:50,100
was it was pretty neat to see happen mkb

00:21:48,120 --> 00:21:52,590
right here is who accomplished this feat

00:21:50,100 --> 00:21:54,240
and and we just scratched on the surface

00:21:52,590 --> 00:21:55,350
but his effort was Herculean it was

00:21:54,240 --> 00:21:58,800
actually impressive

00:21:55,350 --> 00:22:02,670
very impressive thank you

00:21:58,800 --> 00:22:05,190
I do want to say that the the satire

00:22:02,670 --> 00:22:06,900
about actually flying there or driving

00:22:05,190 --> 00:22:15,870
it there was a legitimate conversation

00:22:06,900 --> 00:22:17,040
that we had right and and and we were we

00:22:15,870 --> 00:22:19,200
need we're like well how big are the

00:22:17,040 --> 00:22:20,820
blobs and are we gonna have to sneak her

00:22:19,200 --> 00:22:23,310
net it to you know to the other

00:22:20,820 --> 00:22:25,530
statement and we couldn't believe we

00:22:23,310 --> 00:22:26,670
were having that conversation in 28th

00:22:25,530 --> 00:22:28,020
00:22:26,670 --> 00:22:34,560
we can't believe we're having the

00:22:28,020 --> 00:22:36,120
conversation and it was had we did we

00:22:34,560 --> 00:22:44,690
solve some time if we have more

00:22:36,120 --> 00:22:48,789
questions thanks everyone thank you

00:22:44,690 --> 00:22:48,789

YouTube URL: https://www.youtube.com/watch?v=53-9EIykrNw


