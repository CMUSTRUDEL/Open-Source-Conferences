Title: Lossless Upgrade of BOSH Deployment - Shashank Mohan Jain & Saurav Mondal, SAP
Publication date: 2019-04-11
Playlist: Cloud Foundry Summit NA 2019 - Philadelphia
Description: 
	Lossless Upgrade of BOSH Deployment - Shashank Mohan Jain & Saurav Mondal, SAP

A BOSH deployment update involves the recreation of VMs while preserving the data on the persistence disk. But what about the in-memory data which was very much part of the deployment and was being used by the application. For example, the shared memory buffers used by databases such as PostgreSQL or transient messages used by messaging engines like RabbitMQ or the TCP connections for an application all of which are stored in memory. For a PaaS provider it becomes important to consider the in-memory data for an update use case so as to abide by SLAs and to avoid any performance impact on the customer used service instances.

In this talk, Ashish and Shashank will explain the problems we have observed during updates for services such as PostgreSQL, RabbitMQ and how we could mitigate them by preserving the in-memory state. The proposed solution can be easily plugged into BOSH drain scripts and provides a seamless experience in a live migration whereby preserving the end-user experience pre and post update. This talk will be followed by a short demo on a sample use case.

About Saurav Mondal
Saurav Mondal is working as developer and part of Service Fabrik in SAP, an incubation project of Cloud Foundry. His Interest mainly around cloud computing, distributed computing and machine learning etc.

https://www.cloudfoundry.org/
Captions: 
	00:00:00,030 --> 00:00:13,139
Thank You Angela hi everyone hi so

00:00:10,800 --> 00:00:16,199
welcome to my presentation which I

00:00:13,139 --> 00:00:19,859
titled as lossless upgrade of Bosch

00:00:16,199 --> 00:00:24,949
deployment I also call it stateful

00:00:19,859 --> 00:00:29,010
upgrade of Bosch deployment yeah

00:00:24,949 --> 00:00:33,440
so the problem what we we are going to

00:00:29,010 --> 00:00:36,480
talk about today is so basically I

00:00:33,440 --> 00:00:38,460
working back in Services team so we

00:00:36,480 --> 00:00:44,960
basically deal with different data

00:00:38,460 --> 00:00:48,480
services so which also consistently

00:00:44,960 --> 00:00:51,090
expected to have a stateful and these

00:00:48,480 --> 00:00:54,000
are wrapped up in Bosch deployments so

00:00:51,090 --> 00:00:58,260
few examples you know from our product

00:00:54,000 --> 00:01:02,699
offerings track is possibly sequel then

00:00:58,260 --> 00:01:05,700
readies this MongoDB then RabbitMQ so

00:01:02,699 --> 00:01:09,030
apart from data offering for the

00:01:05,700 --> 00:01:12,659
customers it is also important the other

00:01:09,030 --> 00:01:14,970
states of the deployment right so when

00:01:12,659 --> 00:01:18,180
Bosch deployment gets updated or

00:01:14,970 --> 00:01:21,540
upgraded right so then States needs to

00:01:18,180 --> 00:01:25,439
be preserved so by state I don't mean

00:01:21,540 --> 00:01:29,009
that the state of the customers data but

00:01:25,439 --> 00:01:31,590
it's more of transient state so that

00:01:29,009 --> 00:01:34,409
transient States includes the memory

00:01:31,590 --> 00:01:36,479
pages then open files open files

00:01:34,409 --> 00:01:41,130
includes normal pipes then domain

00:01:36,479 --> 00:01:44,790
sockets CP connection etc so upgrade or

00:01:41,130 --> 00:01:47,640
migration we categorize in two types

00:01:44,790 --> 00:01:50,899
maybe in your production system you want

00:01:47,640 --> 00:01:54,329
to upgrade your deployment you want to

00:01:50,899 --> 00:01:57,600
upgrade the stem cell of your deployment

00:01:54,329 --> 00:01:59,729
or maybe you want to move your

00:01:57,600 --> 00:02:03,960
deployment from one bosh to another

00:01:59,729 --> 00:02:06,649
Bosch so in both the cases here what it

00:02:03,960 --> 00:02:09,160
happens so you're creating a new VM and

00:02:06,649 --> 00:02:12,070
also in case

00:02:09,160 --> 00:02:14,980
say you're updating the services let's

00:02:12,070 --> 00:02:17,260
say political version 9.4 210 you're

00:02:14,980 --> 00:02:20,290
upgrading that time your process gets

00:02:17,260 --> 00:02:24,280
rebooted and that exactly the problem

00:02:20,290 --> 00:02:27,580
starts so app the consumer of the data

00:02:24,280 --> 00:02:28,720
services what they expect so they expect

00:02:27,580 --> 00:02:30,730
high availability

00:02:28,720 --> 00:02:33,610
maybe that is that is that can be taken

00:02:30,730 --> 00:02:36,070
care by - - constantly running processes

00:02:33,610 --> 00:02:38,050
yeah that's fine but when the second

00:02:36,070 --> 00:02:40,990
process comes up or maybe when the

00:02:38,050 --> 00:02:42,730
failover happens right so how to take

00:02:40,990 --> 00:02:44,500
care of the states which are already

00:02:42,730 --> 00:02:46,960
there which are already connected with

00:02:44,500 --> 00:02:49,410
the app how about TCP connections have a

00:02:46,960 --> 00:02:53,200
good memory memory state how about them

00:02:49,410 --> 00:02:54,640
so our problem is exactly there so we

00:02:53,200 --> 00:02:57,660
are talking about that so take one

00:02:54,640 --> 00:03:02,470
real-life example so let's say you're

00:02:57,660 --> 00:03:04,990
booking one ticket service and you're

00:03:02,470 --> 00:03:08,080
booking some tickets from online app and

00:03:04,990 --> 00:03:10,180
in the behind the scene your app is

00:03:08,080 --> 00:03:12,480
consuming one back-end database service

00:03:10,180 --> 00:03:15,130
let us say take example of positives so

00:03:12,480 --> 00:03:17,530
when you are querying constantly right

00:03:15,130 --> 00:03:21,100
so the data after certain point of time

00:03:17,530 --> 00:03:23,709
it will be served from the cache not

00:03:21,100 --> 00:03:25,810
from the disk do in order to have the

00:03:23,709 --> 00:03:27,760
better performance but if the process

00:03:25,810 --> 00:03:30,550
restarts after aggradation

00:03:27,760 --> 00:03:33,940
all the cache are lost so how to save

00:03:30,550 --> 00:03:37,209
the state and when it comes back how to

00:03:33,940 --> 00:03:39,970
get it back so that the app doesn't face

00:03:37,209 --> 00:03:42,850
any performance issue after coming back

00:03:39,970 --> 00:03:44,739
right of the services so from their

00:03:42,850 --> 00:03:47,410
perspective it should be smooth enough

00:03:44,739 --> 00:03:52,120
while you are upgrading your system in

00:03:47,410 --> 00:03:54,880
back-end right so as I talking about few

00:03:52,120 --> 00:03:57,030
backing services so the banking services

00:03:54,880 --> 00:04:01,090
which are primarily memory intensive

00:03:57,030 --> 00:04:03,850
let's take example of a rabbitmq so

00:04:01,090 --> 00:04:07,150
which comprises of transient messages

00:04:03,850 --> 00:04:09,280
and queues then also possibly sequel you

00:04:07,150 --> 00:04:10,780
might think it's not very memory

00:04:09,280 --> 00:04:13,780
intensive but behind the scene it

00:04:10,780 --> 00:04:16,180
heavily depends on the primary memory so

00:04:13,780 --> 00:04:19,210
which part of this is shared buffer

00:04:16,180 --> 00:04:20,979
cache so the most of the queries are

00:04:19,210 --> 00:04:22,490
salt-free current queries are sort from

00:04:20,979 --> 00:04:25,970
the shared buffer cache

00:04:22,490 --> 00:04:30,470
also same as for Aries and other

00:04:25,970 --> 00:04:34,099
stateful services so how do you provide

00:04:30,470 --> 00:04:37,509
a solution so we need one stateful

00:04:34,099 --> 00:04:42,620
migration of the boss deployment or

00:04:37,509 --> 00:04:46,130
backing services right so the tool we

00:04:42,620 --> 00:04:49,220
are using for this prototype is Q so Q

00:04:46,130 --> 00:04:51,319
is basically responsible for check

00:04:49,220 --> 00:04:54,169
pointing and restore of the process

00:04:51,319 --> 00:04:57,849
state or memory state of the deployment

00:04:54,169 --> 00:05:00,949
or of their services right so that app

00:04:57,849 --> 00:05:07,699
experiences our smoothness migration in

00:05:00,949 --> 00:05:11,199
the down on maintenance activity so this

00:05:07,699 --> 00:05:14,630
slide is I think not new to you so it's

00:05:11,199 --> 00:05:18,320
very popular so here I am talking about

00:05:14,630 --> 00:05:18,830
the life cycle of particular CF service

00:05:18,320 --> 00:05:21,909
instance

00:05:18,830 --> 00:05:25,940
so when manage service instance offers

00:05:21,909 --> 00:05:30,470
the services so it comprises of four

00:05:25,940 --> 00:05:32,690
major parts one is provision bind unbind

00:05:30,470 --> 00:05:36,919
deprivation so what is the most

00:05:32,690 --> 00:05:39,620
important part of it so upper bind app

00:05:36,919 --> 00:05:41,570
starts consuming the services so that is

00:05:39,620 --> 00:05:45,009
the whole point of offering services

00:05:41,570 --> 00:05:47,659
right so the point is app should

00:05:45,009 --> 00:05:52,599
experience a smoothness migration when

00:05:47,659 --> 00:05:58,340
you're updating your deployment so here

00:05:52,599 --> 00:06:00,770
I'll bit of talk about Q so Q stands for

00:05:58,340 --> 00:06:04,159
a check point and restore in user space

00:06:00,770 --> 00:06:06,380
so it runs in either space so the

00:06:04,159 --> 00:06:08,180
tournelle module is not involved it does

00:06:06,380 --> 00:06:11,300
not run in kernel model it runs only in

00:06:08,180 --> 00:06:14,180
user space and it also heavily depends

00:06:11,300 --> 00:06:16,430
on the petrous system call so it's Linux

00:06:14,180 --> 00:06:19,250
based tool and pts is a system call

00:06:16,430 --> 00:06:25,070
which basically helps you to who can

00:06:19,250 --> 00:06:28,460
hook into another processes context or

00:06:25,070 --> 00:06:28,880
address space so at high level how it

00:06:28,460 --> 00:06:32,930
works

00:06:28,880 --> 00:06:34,010
so if our process is running so cRIO

00:06:32,930 --> 00:06:36,830
will do

00:06:34,010 --> 00:06:39,710
dump of the all the state files of the

00:06:36,830 --> 00:06:42,410
process and it will store as an image

00:06:39,710 --> 00:06:45,350
binary format and when the process again

00:06:42,410 --> 00:06:50,240
comes back after up gradation so then

00:06:45,350 --> 00:06:52,820
from the image it restores the state so

00:06:50,240 --> 00:06:55,130
more about clear how it works

00:06:52,820 --> 00:06:57,380
so in our namespace there's error

00:06:55,130 --> 00:06:59,630
process tree is there and one root

00:06:57,380 --> 00:07:03,590
process and all the subsequent processes

00:06:59,630 --> 00:07:06,890
so recursively queue then takes the

00:07:03,590 --> 00:07:10,550
state of the all kernel level objects by

00:07:06,890 --> 00:07:12,290
sockets files sockets may be domain

00:07:10,550 --> 00:07:15,650
socket or may be TCP connections also

00:07:12,290 --> 00:07:19,700
and then pipes and then stores as image

00:07:15,650 --> 00:07:23,270
files by in binary format see here in

00:07:19,700 --> 00:07:27,380
detail how check point and restore is

00:07:23,270 --> 00:07:29,720
done by crew so in check point so first

00:07:27,380 --> 00:07:31,580
it collects the process tree the route

00:07:29,720 --> 00:07:34,250
process and all these subsequent

00:07:31,580 --> 00:07:38,510
recursively child processes it collects

00:07:34,250 --> 00:07:41,030
and freezes it then it collects that

00:07:38,510 --> 00:07:44,120
tasks resources associated with those

00:07:41,030 --> 00:07:47,600
processes so tasks tasks resources

00:07:44,120 --> 00:07:51,190
comprise comprises of file descriptors

00:07:47,600 --> 00:07:55,850
it also depends on the proc interface

00:07:51,190 --> 00:07:58,880
Linux proc interface filesystem FDS

00:07:55,850 --> 00:08:04,660
FB infos then also memory maps then also

00:07:58,880 --> 00:08:07,790
it takes care of the CPU registers so

00:08:04,660 --> 00:08:10,760
here I'll probably a bit of talk about

00:08:07,790 --> 00:08:14,510
the petrous was what I was talking so

00:08:10,760 --> 00:08:19,190
petrus is a system call so it helps to

00:08:14,510 --> 00:08:22,340
inject a parasite code into the context

00:08:19,190 --> 00:08:24,500
of another process so that that code can

00:08:22,340 --> 00:08:28,070
collect the all the process state or

00:08:24,500 --> 00:08:30,080
memory from that tracy process so queue

00:08:28,070 --> 00:08:31,970
injects the parasite code and makes a

00:08:30,080 --> 00:08:35,479
domain circuit connection back with the

00:08:31,970 --> 00:08:38,000
crew and that code collects all the all

00:08:35,479 --> 00:08:41,150
the memory state from the tracy process

00:08:38,000 --> 00:08:43,580
and sends it back to q and q saves it in

00:08:41,150 --> 00:08:45,380
the image for image format so at the

00:08:43,580 --> 00:08:47,000
last step it cleans up also using the

00:08:45,380 --> 00:08:49,630
same petrus paul after

00:08:47,000 --> 00:08:54,170
collecting all the state then it also

00:08:49,630 --> 00:09:01,610
discards the parasite code which which

00:08:54,170 --> 00:09:06,250
was taking the backup so at restore what

00:09:01,610 --> 00:09:10,280
it happens so eat first queue comes off

00:09:06,250 --> 00:09:12,320
comes up as a process and then it forks

00:09:10,280 --> 00:09:14,480
the route process and all subsequent

00:09:12,320 --> 00:09:17,930
child process the entire process tree

00:09:14,480 --> 00:09:20,110
and attaches it to itself so the crew is

00:09:17,930 --> 00:09:23,960
now parent of this whole process tree

00:09:20,110 --> 00:09:26,030
right so then it restores the all the

00:09:23,960 --> 00:09:28,310
task resources associated with the

00:09:26,030 --> 00:09:30,710
process tree it restores all the memory

00:09:28,310 --> 00:09:34,580
pages then TCP connections everything it

00:09:30,710 --> 00:09:37,370
restores then eat the den crew detaches

00:09:34,580 --> 00:09:39,290
it the the root process the whole entire

00:09:37,370 --> 00:09:41,360
process tree from itself and attaches

00:09:39,290 --> 00:09:47,660
back to unit process so that it can

00:09:41,360 --> 00:09:50,810
smoothly run so here is a summary so how

00:09:47,660 --> 00:09:54,050
crew works under under the hood so as I

00:09:50,810 --> 00:09:56,330
was mentioning tree you uses petrus

00:09:54,050 --> 00:09:59,150
system called to inject the parasite

00:09:56,330 --> 00:10:01,640
code into the traci process and in it

00:09:59,150 --> 00:10:04,930
takes out all the memory state from it

00:10:01,640 --> 00:10:09,260
and saves in an image file and it also

00:10:04,930 --> 00:10:11,030
depends on the proc interface to take an

00:10:09,260 --> 00:10:13,270
account of all the empties file

00:10:11,030 --> 00:10:16,160
descriptors and also any I note

00:10:13,270 --> 00:10:21,260
associated with the files for the

00:10:16,160 --> 00:10:24,020
specific files and then it also relies

00:10:21,260 --> 00:10:28,339
on K CMP system call so which is

00:10:24,020 --> 00:10:30,380
basically responsible for taking out the

00:10:28,339 --> 00:10:31,130
state between any inter process

00:10:30,380 --> 00:10:34,700
communication

00:10:31,130 --> 00:10:37,460
basically pipes so till here all the

00:10:34,700 --> 00:10:39,680
memory states are preserved so now now

00:10:37,460 --> 00:10:43,339
let's talk a bit to talk about the TCP

00:10:39,680 --> 00:10:46,400
connections so crew also takes care of

00:10:43,339 --> 00:10:49,820
the TCP connections so how it does so

00:10:46,400 --> 00:10:53,420
all the open TCP connections which are

00:10:49,820 --> 00:10:57,350
established right so it moves it back to

00:10:53,420 --> 00:10:59,960
TCP repair mode then what it does so

00:10:57,350 --> 00:11:02,840
then the socket buffer

00:10:59,960 --> 00:11:05,480
RTX are excused it captures or fetches

00:11:02,840 --> 00:11:07,660
all the data from there then it also

00:11:05,480 --> 00:11:11,030
takes care the packet sequence number

00:11:07,660 --> 00:11:14,270
pertaining to the TCP connections then

00:11:11,030 --> 00:11:18,590
it also few TCP handle our data it also

00:11:14,270 --> 00:11:21,650
saves at a time of restore so if from

00:11:18,590 --> 00:11:24,830
the state file image file so it restores

00:11:21,650 --> 00:11:26,540
back the state then also for the TCP

00:11:24,830 --> 00:11:29,720
connections the socket buffers it

00:11:26,540 --> 00:11:32,000
restores back and at the end all the

00:11:29,720 --> 00:11:36,470
packet packet sequence numbers so this

00:11:32,000 --> 00:11:38,420
is how it restores so here we will talk

00:11:36,470 --> 00:11:41,530
about how the stateful migration happens

00:11:38,420 --> 00:11:44,990
in a boss deployment so I will talk how

00:11:41,530 --> 00:11:47,360
for a rabbitmq from a source vm to

00:11:44,990 --> 00:11:52,460
destination vm how the state migration

00:11:47,360 --> 00:11:56,180
happens and smoothly how it happens so

00:11:52,460 --> 00:11:58,880
in the first source vm say one rabbitmq

00:11:56,180 --> 00:12:01,130
process is running crew is running the

00:11:58,880 --> 00:12:02,990
migration tool is running and then we

00:12:01,130 --> 00:12:05,540
take help of boss events so this is a

00:12:02,990 --> 00:12:07,430
this is a prototype so this borscht runs

00:12:05,540 --> 00:12:10,250
what it does the the corresponding

00:12:07,430 --> 00:12:12,680
scripts can be also injected into into

00:12:10,250 --> 00:12:15,320
that drain script so that when you do

00:12:12,680 --> 00:12:18,290
boss stock or any controlled update

00:12:15,320 --> 00:12:20,960
before the process is killing right so

00:12:18,290 --> 00:12:22,940
you can take capture all the states so

00:12:20,960 --> 00:12:24,590
we simulate exact the same v ever using

00:12:22,940 --> 00:12:26,930
the borscht event so it's nothing but

00:12:24,590 --> 00:12:29,030
running a script so that script can be

00:12:26,930 --> 00:12:31,100
injected into the drain script so that

00:12:29,030 --> 00:12:33,230
it collects all the step so when it

00:12:31,100 --> 00:12:39,260
comes back again after starting you can

00:12:33,230 --> 00:12:42,440
give give them back so the app is

00:12:39,260 --> 00:12:45,860
connected to the VM so the service is up

00:12:42,440 --> 00:12:48,800
and running when the control update

00:12:45,860 --> 00:12:51,020
happens when the backup happens so we

00:12:48,800 --> 00:12:53,450
dump the all the process states as I

00:12:51,020 --> 00:12:55,370
mentioned earlier so into a into any

00:12:53,450 --> 00:12:57,830
distributed storage it can be any

00:12:55,370 --> 00:12:59,630
persistent disk or it can be bla storage

00:12:57,830 --> 00:13:02,720
anything any distributed persistent

00:12:59,630 --> 00:13:07,280
storage right so as part of this what we

00:13:02,720 --> 00:13:10,040
do we run one boss event say Q dump

00:13:07,280 --> 00:13:12,380
so first what it does so it stops the

00:13:10,040 --> 00:13:13,139
app communication to the service so that

00:13:12,380 --> 00:13:16,980
app can

00:13:13,139 --> 00:13:19,949
right the data now onwards yeah so after

00:13:16,980 --> 00:13:21,179
that it does the checkpointing and that

00:13:19,949 --> 00:13:22,799
checkpointing how it is done

00:13:21,179 --> 00:13:27,149
so into the image fencer details I

00:13:22,799 --> 00:13:36,989
talked so this is slightly dim down

00:13:27,149 --> 00:13:39,720
version so then so this source VM let's

00:13:36,989 --> 00:13:41,489
say you are upgrading one stem cell

00:13:39,720 --> 00:13:43,290
update you are upgrading your boss

00:13:41,489 --> 00:13:44,910
deployment using stem cell updates so

00:13:43,290 --> 00:13:46,679
the new of course the new VM will be

00:13:44,910 --> 00:13:50,069
created because because the stem cell

00:13:46,679 --> 00:13:51,689
version is new or you're migrating VM

00:13:50,069 --> 00:13:53,790
the production system may be one

00:13:51,689 --> 00:13:55,199
hypervisor to another hypervisor so in a

00:13:53,790 --> 00:13:57,660
control to a you are you will be

00:13:55,199 --> 00:14:01,410
creating another new VM so in the new VM

00:13:57,660 --> 00:14:03,809
a whole fresh set of RabbitMQ cream and

00:14:01,410 --> 00:14:05,730
wash errands and those processes will

00:14:03,809 --> 00:14:09,959
come up but how about the old state

00:14:05,730 --> 00:14:12,319
which app is expecting right so we store

00:14:09,959 --> 00:14:14,999
from the from the distributed storage

00:14:12,319 --> 00:14:24,660
the process states and all the tcp

00:14:14,999 --> 00:14:27,540
connections and and we give the

00:14:24,660 --> 00:14:30,029
communication back right so here also we

00:14:27,540 --> 00:14:32,819
run one boss share and comment say Cree

00:14:30,029 --> 00:14:36,029
you restore so it destroys the all the

00:14:32,819 --> 00:14:38,279
domed state and it also gives back the

00:14:36,029 --> 00:14:42,769
IP rules so that app gets connectivity

00:14:38,279 --> 00:14:42,769
back to the new VM to the new services

00:14:43,579 --> 00:14:49,829
so in it's demo time

00:14:45,480 --> 00:14:52,410
so in demo what I'll show so in demo it

00:14:49,829 --> 00:14:54,720
only takes care about the memory state

00:14:52,410 --> 00:14:56,639
not the TCP connection so that can be

00:14:54,720 --> 00:14:59,399
easily integrated but as part of this

00:14:56,639 --> 00:15:04,189
demo only memory State we'll see how

00:14:59,399 --> 00:15:04,189
here's backing up and then restoring

00:15:07,870 --> 00:15:15,340
I'm not sure from the back whether it is

00:15:10,640 --> 00:15:15,340
visible maybe you can come with front

00:15:18,970 --> 00:15:23,870
yeah so we have created already one

00:15:21,860 --> 00:15:26,290
service instance rabbitmq service

00:15:23,870 --> 00:15:26,290
instance

00:15:33,360 --> 00:15:41,759
so it comprises of five beams 2h epoxy

00:15:36,989 --> 00:15:43,949
and three rabbitmq vm's so in its inside

00:15:41,759 --> 00:15:49,679
of the each VM the RabbitMQ process is

00:15:43,949 --> 00:15:54,619
running also the Cree running so here

00:15:49,679 --> 00:15:57,480
showing the rabbitmq Q's so app

00:15:54,619 --> 00:16:01,559
constantly pushing up some messages but

00:15:57,480 --> 00:16:04,339
using one API will be hitting will be

00:16:01,559 --> 00:16:07,619
hitting 10,000 messages into the Q and

00:16:04,339 --> 00:16:10,199
after after that we'll be stopping the

00:16:07,619 --> 00:16:11,970
process and after stopping the process

00:16:10,199 --> 00:16:22,379
those messages should come back around

00:16:11,970 --> 00:16:24,779
10,000 messages so here it's showing how

00:16:22,379 --> 00:16:27,689
many how many how many ready to consumed

00:16:24,779 --> 00:16:33,569
how many are already already in queue

00:16:27,689 --> 00:16:36,329
and all so yes so $9,000 9700 out of

00:16:33,569 --> 00:16:41,160
10,000 already it is ready to consume

00:16:36,329 --> 00:16:44,939
and by the consumer it's gradually

00:16:41,160 --> 00:16:50,009
receiving so here we are running the

00:16:44,939 --> 00:16:53,629
boss around preview dump to take the

00:16:50,009 --> 00:16:53,629
state of the all messages

00:17:03,949 --> 00:17:11,640
and also as part of that script we also

00:17:08,579 --> 00:17:13,860
stopped freezes though all the IP rules

00:17:11,640 --> 00:17:18,569
so that app doesn't get any connectivity

00:17:13,860 --> 00:17:25,559
to the service and yeah it's consuming

00:17:18,569 --> 00:17:29,059
one by one now we will now we'll do one

00:17:25,559 --> 00:17:33,570
yeah now we will do a hard stop of the

00:17:29,059 --> 00:17:35,039
VMS so that it will be recreated it I

00:17:33,570 --> 00:17:37,409
mean basically here we are simulating

00:17:35,039 --> 00:17:39,799
the stem-cell update behavior where the

00:17:37,409 --> 00:17:44,039
VM gets recreated

00:17:39,799 --> 00:17:50,190
so stop harder the hard of RabbitMQ so

00:17:44,039 --> 00:17:52,679
all the three gems are deleted yeah only

00:17:50,190 --> 00:17:57,780
those 2h a proxy gems are there earlier

00:17:52,679 --> 00:17:59,940
you have seen five iums then you are

00:17:57,780 --> 00:18:02,309
starting the RabbitMQ VMS so it will

00:17:59,940 --> 00:18:05,870
then recreate all the PM's and the

00:18:02,309 --> 00:18:05,870
processes inside that

00:18:24,090 --> 00:18:29,840
yeah you see there is no no no message

00:18:27,210 --> 00:18:29,840
in the queues

00:18:35,880 --> 00:18:39,050
just a second

00:18:45,400 --> 00:18:51,760
so here which also fired a comment

00:18:49,450 --> 00:18:55,830
boss share and comment for query store

00:18:51,760 --> 00:18:55,830
so that all the messages will come back

00:18:57,270 --> 00:19:03,520
here also we open the TCP connections

00:19:00,670 --> 00:19:06,190
from the app so that from the app it

00:19:03,520 --> 00:19:09,930
gets connectivity so now the messages

00:19:06,190 --> 00:19:09,930
starting starting up

00:19:23,270 --> 00:19:28,320
you see now the total number of received

00:19:26,490 --> 00:19:31,020
messages are increasing one by one and

00:19:28,320 --> 00:19:33,210
that should go till around thousand at

00:19:31,020 --> 00:19:42,289
the same time also the app also pushing

00:19:33,210 --> 00:19:42,289
some messages you are now eight thousand

00:19:46,280 --> 00:19:52,220
yeah it's decreasing so from the cube

00:19:49,020 --> 00:20:02,850
from the recipient it's going on yeah

00:19:52,220 --> 00:20:05,850
9600 9800 yeah around ten thousand

00:20:02,850 --> 00:20:07,950
thirteen so app also constantly pushing

00:20:05,850 --> 00:20:11,100
some messages so ten thousand messages

00:20:07,950 --> 00:20:15,210
are back so we restarted we restarted

00:20:11,100 --> 00:20:18,330
the process but states all are back so

00:20:15,210 --> 00:20:20,309
this is example of RabbitMQ so it also

00:20:18,330 --> 00:20:23,340
similarly applies for any any memory

00:20:20,309 --> 00:20:25,559
intensive services so for that matter

00:20:23,340 --> 00:20:27,809
I'll call opposed B's sequel is also a

00:20:25,559 --> 00:20:30,570
memory intensive services because it

00:20:27,809 --> 00:20:33,059
serves query and the queries are cached

00:20:30,570 --> 00:20:35,220
so those are salt from cache so those

00:20:33,059 --> 00:20:37,500
are part of transient State not the

00:20:35,220 --> 00:20:40,309
persistent State so it's very important

00:20:37,500 --> 00:20:44,820
for an app to have this smoothness

00:20:40,309 --> 00:21:03,960
migration activity you have any

00:20:44,820 --> 00:21:07,860
questions is yeah sure q does encryption

00:21:03,960 --> 00:21:10,000
on those files yes yeah yeah those are

00:21:07,860 --> 00:21:20,100
safe yes

00:21:10,000 --> 00:21:20,100
yes when you're doing the first yes

00:21:21,270 --> 00:21:42,550
there is no downtime yes yes that stop

00:21:40,900 --> 00:21:44,710
all traffic so for the maintenance

00:21:42,550 --> 00:21:47,320
window I mean app will experience up

00:21:44,710 --> 00:21:49,840
little bit of downtime so which is also

00:21:47,320 --> 00:22:08,200
true for any upgrade right but states

00:21:49,840 --> 00:22:10,750
are back yeah yes yes so in real world

00:22:08,200 --> 00:22:12,970
how it will happen so they are say there

00:22:10,750 --> 00:22:14,830
are two nodes or three nodes so from the

00:22:12,970 --> 00:22:17,440
one node which is master so traffic will

00:22:14,830 --> 00:22:19,900
start coming there so then when stopped

00:22:17,440 --> 00:22:23,020
and it will move the traffic there and

00:22:19,900 --> 00:22:25,920
we take state of this and immediately

00:22:23,020 --> 00:22:29,350
after taking the state I was talking I

00:22:25,920 --> 00:22:32,230
restored that state back to a new VM but

00:22:29,350 --> 00:22:35,650
which is already running to that process

00:22:32,230 --> 00:22:37,480
also we can transfer these states so in

00:22:35,650 --> 00:22:39,460
the backend I mean it doesn't lose all

00:22:37,480 --> 00:22:45,970
those messages if you take example of

00:22:39,460 --> 00:22:51,700
RabbitMQ process here yeah yes yes it

00:22:45,970 --> 00:22:53,700
also exactly it also depends on how how

00:22:51,700 --> 00:22:56,740
you are handling the high availability

00:22:53,700 --> 00:22:57,220
say yeah how you're managing the

00:22:56,740 --> 00:23:00,700
failover

00:22:57,220 --> 00:23:02,770
yeah so this is borscht arrant so as I

00:23:00,700 --> 00:23:06,040
told it can be injected in 10 script of

00:23:02,770 --> 00:23:08,580
that particular process yeah yeah oh

00:23:06,040 --> 00:23:08,580
yeah sure

00:23:14,870 --> 00:23:30,600
code upgrade yes yes yeah yes so exactly

00:23:28,830 --> 00:23:34,049
same I was telling so when it does

00:23:30,600 --> 00:23:37,169
upgrade right so yeah it also depends on

00:23:34,049 --> 00:23:40,289
the system files so basically we

00:23:37,169 --> 00:23:42,750
measured you are right that maybe in 9.4

00:23:40,289 --> 00:23:44,700
what is the system file structure or

00:23:42,750 --> 00:23:47,039
directory structure might not be hold

00:23:44,700 --> 00:23:50,039
true for the other structure but from

00:23:47,039 --> 00:23:52,799
the data perspective if you take a take

00:23:50,039 --> 00:23:55,200
example of possibly Sheik cql shared

00:23:52,799 --> 00:23:58,260
buffer so it should be consistent across

00:23:55,200 --> 00:24:00,059
the versions so we are talking about we

00:23:58,260 --> 00:24:07,679
are talking about preserving those

00:24:00,059 --> 00:24:10,320
states yes so that is why I mean purely

00:24:07,679 --> 00:24:12,649
OS I mean OS related so we are here

00:24:10,320 --> 00:24:17,539
taking state of the particular process

00:24:12,649 --> 00:24:17,539
specifically Linux memory processes yeah

00:24:29,930 --> 00:24:41,970
yes so yeah exactly so say I mean this

00:24:39,000 --> 00:24:44,160
is also happened I mean this restore

00:24:41,970 --> 00:24:46,830
backing up restore and also your

00:24:44,160 --> 00:24:49,170
failover process so these two this two

00:24:46,830 --> 00:24:51,270
should work together so it also depends

00:24:49,170 --> 00:24:53,220
on how you're handling that fell over so

00:24:51,270 --> 00:24:55,410
let's say your failover is taking ten

00:24:53,220 --> 00:24:57,090
seconds or 15 seconds of downtime so

00:24:55,410 --> 00:24:58,560
then this state should be back

00:24:57,090 --> 00:25:00,240
immediately after fifteen seconds of

00:24:58,560 --> 00:25:05,900
time so these two needs to work together

00:25:00,240 --> 00:25:31,650
yeah yeah yeah exactly

00:25:05,900 --> 00:25:34,370
sure yeah yes yeah exactly

00:25:31,650 --> 00:25:34,370
yeah yeah

00:25:45,470 --> 00:25:52,710
yeah I think I've seen my creativity

00:25:49,890 --> 00:25:54,779
used for like live migration exactly

00:25:52,710 --> 00:25:58,080
exactly so pew is heavily used for the

00:25:54,779 --> 00:26:00,000
container live migration yeah so I mean

00:25:58,080 --> 00:26:02,039
container and continuous means have

00:26:00,000 --> 00:26:04,440
happen in a few seconds but for the

00:26:02,039 --> 00:26:06,769
Bosch I mean for to achieve high

00:26:04,440 --> 00:26:08,940
availability you need at least two VMs

00:26:06,769 --> 00:26:12,210
based on your availability zones also

00:26:08,940 --> 00:26:15,090
sometimes see VMs so for having the high

00:26:12,210 --> 00:26:17,279
availability so as I told so from one VM

00:26:15,090 --> 00:26:19,799
you collect the state and another VM

00:26:17,279 --> 00:26:21,990
when you're moving the traffic back to

00:26:19,799 --> 00:26:28,470
the or existing VM so you should also

00:26:21,990 --> 00:26:32,960
transfer this state to there any other

00:26:28,470 --> 00:26:32,960
questions please yeah

00:26:53,500 --> 00:27:13,030
I mean what is the question exactly yes

00:27:03,520 --> 00:27:16,360
yes yes yeah yeah that would be great

00:27:13,030 --> 00:27:19,059
for super level up optimization so what

00:27:16,360 --> 00:27:22,900
it happens if you take an example of

00:27:19,059 --> 00:27:24,340
database services so all the data so

00:27:22,900 --> 00:27:26,290
those are a synchronous asynchronous

00:27:24,340 --> 00:27:30,850
they also backed up or moved to the

00:27:26,290 --> 00:27:34,480
secondary VM but this processes state it

00:27:30,850 --> 00:27:37,780
also can be incremental II moved but I

00:27:34,480 --> 00:27:40,030
would prefer to move at last when you

00:27:37,780 --> 00:27:42,010
have all the states because it doesn't

00:27:40,030 --> 00:27:44,890
take so much time at least that

00:27:42,010 --> 00:27:47,260
transient state back in backing part so

00:27:44,890 --> 00:27:49,990
incremental you'd be challenging I am

00:27:47,260 --> 00:27:52,179
Not sure because I mean after two

00:27:49,990 --> 00:27:54,910
seconds of time what the state will be I

00:27:52,179 --> 00:27:58,809
don't know right so so it's snapshotting

00:27:54,910 --> 00:28:01,780
of the state I am Not sure but at the

00:27:58,809 --> 00:28:03,160
end you can definitely go back so when

00:28:01,780 --> 00:28:05,920
you're moving the traffic back to the

00:28:03,160 --> 00:28:08,410
another VM so you can definitely yeah

00:28:05,920 --> 00:28:14,830
move their states also from the previous

00:28:08,410 --> 00:28:17,280
master or PPS serving a process yeah

00:28:14,830 --> 00:28:17,280
yeah sure

00:28:19,400 --> 00:28:32,400
yes yeah exactly so yeah that that

00:28:29,610 --> 00:28:34,380
matters how you deal with the app

00:28:32,400 --> 00:28:38,130
connectivity so that is the binding

00:28:34,380 --> 00:28:40,380
parameters so you're behind one virtual

00:28:38,130 --> 00:28:42,470
IP say so you have to also move the

00:28:40,380 --> 00:28:45,570
virtual IP back to the new VM

00:28:42,470 --> 00:28:48,150
what shall I pure se URL ne so you have

00:28:45,570 --> 00:28:52,580
to move back the URL so that app gets

00:28:48,150 --> 00:28:52,580
connectivity yeah

00:29:02,540 --> 00:29:12,890
find evening in the sense Kelly can you

00:29:04,400 --> 00:29:15,770
elaborate okay so yeah so it's I think

00:29:12,890 --> 00:29:18,350
independent of any services so it's deal

00:29:15,770 --> 00:29:20,150
with only the Linux process yeah so

00:29:18,350 --> 00:29:22,160
that's it I mean it's you can apply to

00:29:20,150 --> 00:29:25,130
any any process for that matter so for

00:29:22,160 --> 00:29:27,590
that matter in any app or any process

00:29:25,130 --> 00:29:30,100
you can apply this it doesn't depend on

00:29:27,590 --> 00:29:30,100
the service yeah

00:29:47,490 --> 00:29:51,659
with one experience with this

00:29:52,890 --> 00:29:57,960
okay yeah yeah it's a prototype so there

00:29:55,710 --> 00:30:02,429
are other other few options so cream is

00:29:57,960 --> 00:30:05,040
not the only tool openvz then BL CR is

00:30:02,429 --> 00:30:09,299
also a tool so there are few other tools

00:30:05,040 --> 00:30:14,220
so you can check that out but Q has I

00:30:09,299 --> 00:30:18,059
mean it's many other advantages over all

00:30:14,220 --> 00:30:20,190
other tools but if we want to move into

00:30:18,059 --> 00:30:22,770
the production so then we need really

00:30:20,190 --> 00:30:24,690
need to think I mean whether Q is the

00:30:22,770 --> 00:30:31,309
only option or we should use some other

00:30:24,690 --> 00:30:31,309
tools for a fine-tuning perspective yeah

00:30:35,540 --> 00:30:38,540
okay

00:30:41,679 --> 00:30:54,919
okay yeah so if you move to the

00:30:52,460 --> 00:30:57,280
different kernels so again as I sold

00:30:54,919 --> 00:31:01,250
proc interface so those should work

00:30:57,280 --> 00:31:03,980
similarly say proc then PID FD is right

00:31:01,250 --> 00:31:05,780
so those file structure should be also

00:31:03,980 --> 00:31:08,049
same into the new system otherwise it

00:31:05,780 --> 00:31:08,049
doesn't work

00:31:13,000 --> 00:31:28,400
yes exactly yeah any other questions

00:31:24,290 --> 00:31:36,440
okay in that case thanks everybody

00:31:28,400 --> 00:31:38,510
thanks for your time so if you want to

00:31:36,440 --> 00:31:41,059
contact me so this is my email address

00:31:38,510 --> 00:31:43,220
and also I am part of service fabric so

00:31:41,059 --> 00:31:45,200
you can check it out it's an Cloud

00:31:43,220 --> 00:31:47,059
Foundry incubation project you can try

00:31:45,200 --> 00:31:49,460
out in your locale and you can

00:31:47,059 --> 00:31:52,190
contribute you can create issues so

00:31:49,460 --> 00:31:55,780
we'll happy to help you and also we have

00:31:52,190 --> 00:31:59,830
a slack channel service fabric yeah

00:31:55,780 --> 00:31:59,830
thank you everybody thanks for your time

00:32:00,940 --> 00:32:03,940
Thanks

00:32:06,600 --> 00:32:08,660

YouTube URL: https://www.youtube.com/watch?v=4eKlXJoW_yA


