Title: Testing Production Environments and Verifying Open Service Broker API Compliance - Oliver Wolf
Publication date: 2019-04-11
Playlist: Cloud Foundry Summit NA 2019 - Philadelphia
Description: 
	Testing Production Environments and Verifying Open Service Broker API Compliance - Oliver Wolf, anynines

You’ve developed a Service Broker and want to test your broker’s implementation? Maybe you're considering writing a new Service Broker and want some kind of test suite to validate against.

anynines makes use of two different kind of tests suites when it comes to automating deployments with the Open Service Broker API. The first ones are the “BOSH releases tests” and are used to verify that service instances behave like expected during failure scenarios. While these kind of tests are used in the CI/CD pipeline of the anynines Data Services, they don’t verify compliance against the OSBAPI or whether a concrete customer production setup is working correctly.

This talk focuses on the latest type of tests and shows how a generic test suite that verifies production platform environments and OSBAPI compliance can be used for different kind of data services.

About Oliver Wolf
Oliver works with Cloud Foundry and BOSH for over three years now. Started with CF V1 his responsibilities where operating the public anynines platform as well developing additional solution around Cloud Foundry. At the moment he is part of the anynines service development team and takes care that the architecture meets the customer requirements. Frequently he is involved in consulting and training projects regarding Cloud Foundry, BOSH and cloud ready app development.

https://www.cloudfoundry.org/
Captions: 
	00:00:00,030 --> 00:00:05,790
so hello everybody I'm Oliver I work for

00:00:03,389 --> 00:00:08,040
any nights and today I want to talk

00:00:05,790 --> 00:00:10,320
about the open service broker API and

00:00:08,040 --> 00:00:12,719
especially how we at any nines build the

00:00:10,320 --> 00:00:15,599
service broker how we test the service

00:00:12,719 --> 00:00:17,220
broker and also how we ensure that once

00:00:15,599 --> 00:00:19,740
we deploy the service broker into

00:00:17,220 --> 00:00:21,930
production environment it works and

00:00:19,740 --> 00:00:24,510
function within that environment so that

00:00:21,930 --> 00:00:26,760
means that once we've deploy a new

00:00:24,510 --> 00:00:29,310
release of a service broker into let's

00:00:26,760 --> 00:00:31,230
say our internal systems or even at

00:00:29,310 --> 00:00:34,950
systems that are running on premise on

00:00:31,230 --> 00:00:36,780
customer side that yeah that we have a

00:00:34,950 --> 00:00:38,879
way to ensure that this service broker

00:00:36,780 --> 00:00:41,340
is is running even after the deployment

00:00:38,879 --> 00:00:43,350
so we have some kind of test suite that

00:00:41,340 --> 00:00:45,809
run before the deployment before the

00:00:43,350 --> 00:00:50,670
release phase but also test Suites that

00:00:45,809 --> 00:00:53,280
run after the deployment but before we

00:00:50,670 --> 00:00:55,710
start into how we tested I want to short

00:00:53,280 --> 00:00:58,109
to describe how our journey with Cloud

00:00:55,710 --> 00:01:00,930
Foundry at any 9 started and how we came

00:00:58,109 --> 00:01:03,030
to developing data services and

00:01:00,930 --> 00:01:05,700
implementing the open service broker API

00:01:03,030 --> 00:01:09,150
so actually our Cloud Foundry journey

00:01:05,700 --> 00:01:11,280
started roughly about six years ago when

00:01:09,150 --> 00:01:14,640
we started to build a public platform

00:01:11,280 --> 00:01:17,700
for for end-users who wanted to deploy

00:01:14,640 --> 00:01:19,619
their application to platform and then

00:01:17,700 --> 00:01:21,000
make it available in the internet so it

00:01:19,619 --> 00:01:22,890
was a shared platform that means

00:01:21,000 --> 00:01:24,509
different parties could deploy their

00:01:22,890 --> 00:01:26,939
application to the same platform and

00:01:24,509 --> 00:01:30,360
they shared the resources but also the

00:01:26,939 --> 00:01:32,880
costs and if you have a look at the open

00:01:30,360 --> 00:01:35,009
source cloud foundry which is a major

00:01:32,880 --> 00:01:36,600
component of our public platform so we

00:01:35,009 --> 00:01:39,900
are based on the open source cloud

00:01:36,600 --> 00:01:41,549
foundry then you know that this open

00:01:39,900 --> 00:01:44,579
source cloud foundry is very good at

00:01:41,549 --> 00:01:47,130
running and stateless application so you

00:01:44,579 --> 00:01:49,320
can deploy those applications very easy

00:01:47,130 --> 00:01:51,810
to Cloud Foundry you can scale them you

00:01:49,320 --> 00:01:54,090
can update them and if something breaks

00:01:51,810 --> 00:01:56,579
cloud form we will take care to restart

00:01:54,090 --> 00:01:58,560
those applications so platform is very

00:01:56,579 --> 00:02:02,610
good at running those stateless and web

00:01:58,560 --> 00:02:07,380
applications but if you deploy the CF

00:02:02,610 --> 00:02:11,230
deployment then you see that you need

00:02:07,380 --> 00:02:13,450
more than just runtime to to run your

00:02:11,230 --> 00:02:15,610
web applications probably your web

00:02:13,450 --> 00:02:17,620
application needs a database like a

00:02:15,610 --> 00:02:19,810
Postgres and my sequel or maybe a

00:02:17,620 --> 00:02:22,090
MongoDB or maybe a search server like

00:02:19,810 --> 00:02:24,879
elasticsearch or message queue like

00:02:22,090 --> 00:02:27,489
rabbitmq for that purpose Cloud Foundry

00:02:24,879 --> 00:02:29,290
provides a marketplace so the developers

00:02:27,489 --> 00:02:31,569
who deploy their application to Cloud

00:02:29,290 --> 00:02:34,300
Foundry can look up this marketplace and

00:02:31,569 --> 00:02:37,030
see what or which of those Data Services

00:02:34,300 --> 00:02:39,190
databases are offered in that Cloud

00:02:37,030 --> 00:02:41,230
Foundry environment and then they pick a

00:02:39,190 --> 00:02:44,019
data service like for example they need

00:02:41,230 --> 00:02:46,450
a my sequel service they say I need the

00:02:44,019 --> 00:02:49,480
my sequel service of that size with that

00:02:46,450 --> 00:02:51,310
amount of nodes and a few minutes later

00:02:49,480 --> 00:02:53,530
they have that cluster up and running

00:02:51,310 --> 00:02:54,849
then they can take that cluster and bind

00:02:53,530 --> 00:02:57,840
the to application and then the

00:02:54,849 --> 00:03:00,040
application will be able to find

00:02:57,840 --> 00:03:02,829
credentials to that service instance

00:03:00,040 --> 00:03:05,110
within the environment variables but the

00:03:02,829 --> 00:03:06,910
thing is when you set up the CF

00:03:05,110 --> 00:03:10,209
deployment you will notice that this

00:03:06,910 --> 00:03:12,280
marketplace is empty to get something

00:03:10,209 --> 00:03:15,010
into that marketplace you as a platform

00:03:12,280 --> 00:03:16,450
administrator or platform provider has

00:03:15,010 --> 00:03:18,819
to build something that is called the

00:03:16,450 --> 00:03:20,769
service broker and the service broker is

00:03:18,819 --> 00:03:22,989
the component that actually takes

00:03:20,769 --> 00:03:25,900
requests from the platform and takes

00:03:22,989 --> 00:03:30,280
care that you yet takes care that the

00:03:25,900 --> 00:03:32,380
user becomes the database setup the to

00:03:30,280 --> 00:03:34,660
write the service broker and to plug it

00:03:32,380 --> 00:03:36,280
into your cloud from the environment you

00:03:34,660 --> 00:03:39,069
have to fulfill a specific API

00:03:36,280 --> 00:03:42,010
specification and it's called the open

00:03:39,069 --> 00:03:44,230
service broker API specification the

00:03:42,010 --> 00:03:45,850
open service broker API specification is

00:03:44,230 --> 00:03:47,920
quite lean so there's a couple of

00:03:45,850 --> 00:03:51,220
methods you have to implement like for

00:03:47,920 --> 00:03:53,079
example create a database or create the

00:03:51,220 --> 00:03:56,049
service whatever server to the service

00:03:53,079 --> 00:03:57,940
it is update the service to a bigger

00:03:56,049 --> 00:04:02,049
plan so that you can provide more

00:03:57,940 --> 00:04:04,299
capacity to your end users or create a

00:04:02,049 --> 00:04:06,250
credential set so that the application

00:04:04,299 --> 00:04:08,709
can use that credential set to access

00:04:06,250 --> 00:04:11,109
the database so it's a quite lean

00:04:08,709 --> 00:04:16,720
interface maybe five methods you have to

00:04:11,109 --> 00:04:18,459
implement and it has been yeah it has

00:04:16,720 --> 00:04:20,620
been born in context of the Cloud

00:04:18,459 --> 00:04:22,810
Foundry community so Cloud Foundry was

00:04:20,620 --> 00:04:24,550
the first project who came up with that

00:04:22,810 --> 00:04:28,990
interface whoo

00:04:24,550 --> 00:04:32,259
who implemented that interface but today

00:04:28,990 --> 00:04:34,389
there are other technologies who adapted

00:04:32,259 --> 00:04:36,520
that specification so for example if you

00:04:34,389 --> 00:04:38,740
have a open service broker api's

00:04:36,520 --> 00:04:42,039
compliant service broker you can also

00:04:38,740 --> 00:04:44,889
adapt or yet connect those service

00:04:42,039 --> 00:04:47,319
brokers to Cuba native environment or to

00:04:44,889 --> 00:04:50,039
a Cuban it is similar like similar

00:04:47,319 --> 00:04:52,539
environment like and Retta openshift

00:04:50,039 --> 00:04:54,430
we even have some customer who don't

00:04:52,539 --> 00:04:55,840
have a platform at all or who don't

00:04:54,430 --> 00:04:58,539
consume the service broker from a

00:04:55,840 --> 00:05:00,879
platform at all instead they came up

00:04:58,539 --> 00:05:03,270
with their own customer panel and

00:05:00,879 --> 00:05:07,719
directly talk to the service broker API

00:05:03,270 --> 00:05:09,190
and that that how it looks today so the

00:05:07,719 --> 00:05:10,990
context in which the service broker

00:05:09,190 --> 00:05:13,449
operate and in which the service broker

00:05:10,990 --> 00:05:14,889
has to fulfill its function it's getting

00:05:13,449 --> 00:05:18,099
more diverse because more platforms

00:05:14,889 --> 00:05:21,069
adopting those those service brokers

00:05:18,099 --> 00:05:22,810
standards and that what also makes it

00:05:21,069 --> 00:05:26,949
quite hard to test those service broker

00:05:22,810 --> 00:05:28,719
implementations you have a look how the

00:05:26,949 --> 00:05:31,840
marketplace looks like most probably you

00:05:28,719 --> 00:05:33,969
already know but for those who don't saw

00:05:31,840 --> 00:05:35,949
a cloud from the marketplace yet that is

00:05:33,969 --> 00:05:38,529
how it looks like so you say CF

00:05:35,949 --> 00:05:41,050
marketplace and you get all the services

00:05:38,529 --> 00:05:43,630
that are provided across all the brokers

00:05:41,050 --> 00:05:46,180
that are registered in that cloud from

00:05:43,630 --> 00:05:48,039
the endpoint and you see there are a

00:05:46,180 --> 00:05:49,840
couple of services and with each service

00:05:48,039 --> 00:05:52,690
there are also a couple of service

00:05:49,840 --> 00:05:55,930
plants which define how big the service

00:05:52,690 --> 00:05:58,150
instance will be that you book and also

00:05:55,930 --> 00:06:00,159
what topology the service instance would

00:05:58,150 --> 00:06:02,650
have like for example whether you have a

00:06:00,159 --> 00:06:04,900
single node my secret cluster order you

00:06:02,650 --> 00:06:07,330
have a three node cluster that they just

00:06:04,900 --> 00:06:09,580
have some kind of replication that when

00:06:07,330 --> 00:06:14,860
one node fails the other nodes take over

00:06:09,580 --> 00:06:16,779
the workload so if you see this open

00:06:14,860 --> 00:06:19,479
service broker API specification you

00:06:16,779 --> 00:06:21,940
might think oh that looks easy I go and

00:06:19,479 --> 00:06:24,639
implement the service broker now but

00:06:21,940 --> 00:06:26,500
when we started that we thought the same

00:06:24,639 --> 00:06:28,960
thing but then we learned a few lessons

00:06:26,500 --> 00:06:31,419
especially our enterprise customers

00:06:28,960 --> 00:06:33,820
teach to us that they're a bit more to

00:06:31,419 --> 00:06:36,580
take care of them just provisioning a

00:06:33,820 --> 00:06:38,380
database we learned that this day to

00:06:36,580 --> 00:06:40,030
operation is very hard

00:06:38,380 --> 00:06:41,680
that also these day two operations are

00:06:40,030 --> 00:06:45,490
very hard to ensure that it's working

00:06:41,680 --> 00:06:48,310
when we update our service offerings so

00:06:45,490 --> 00:06:51,400
what what are typical operations that

00:06:48,310 --> 00:06:53,410
might not be in your mind when you're

00:06:51,400 --> 00:06:56,800
starting implementing the open service

00:06:53,410 --> 00:06:59,020
broker API so what most of our customer

00:06:56,800 --> 00:07:00,430
demanded from us was that these services

00:06:59,020 --> 00:07:03,070
are running on dedicated virtual

00:07:00,430 --> 00:07:05,970
machines so even containers gain more

00:07:03,070 --> 00:07:08,770
and more traction now and still

00:07:05,970 --> 00:07:10,840
enterprises feel better in saver if they

00:07:08,770 --> 00:07:13,480
know that this kind of stateful work

00:07:10,840 --> 00:07:16,150
load is running in a dedicated virtual

00:07:13,480 --> 00:07:19,030
machine because it's still the better

00:07:16,150 --> 00:07:23,050
isolation provides still the better bail

00:07:19,030 --> 00:07:24,640
Noisia hood protection so yeah but the

00:07:23,050 --> 00:07:26,860
problem is we've dedicated virtual

00:07:24,640 --> 00:07:29,080
machine they're quite costly so that

00:07:26,860 --> 00:07:31,540
means that you also have to provision

00:07:29,080 --> 00:07:34,840
them on demand so you don't want to spin

00:07:31,540 --> 00:07:36,760
up 100 post-crash clusters and wait till

00:07:34,840 --> 00:07:39,040
you sold them you want to spin them up

00:07:36,760 --> 00:07:41,230
whenever a new user or new developer

00:07:39,040 --> 00:07:43,390
books that post press cluster then you

00:07:41,230 --> 00:07:45,610
want those virtual machines be created

00:07:43,390 --> 00:07:47,620
on demand although this makes testing

00:07:45,610 --> 00:07:50,530
quite hard because creating virtual

00:07:47,620 --> 00:07:52,270
machines takes up to five minutes and if

00:07:50,530 --> 00:07:56,620
you have a cluster you have to create

00:07:52,270 --> 00:07:58,750
more than one virtual machine also what

00:07:56,620 --> 00:08:00,670
is important for us that this the

00:07:58,750 --> 00:08:03,220
service brokers run in on-premise

00:08:00,670 --> 00:08:04,810
environments so that customer can

00:08:03,220 --> 00:08:06,220
install it in their own environment

00:08:04,810 --> 00:08:09,940
that's not only running on our public

00:08:06,220 --> 00:08:12,070
pass so when we think of on-premise

00:08:09,940 --> 00:08:15,160
environment this also plays a role when

00:08:12,070 --> 00:08:18,430
testing the thing because we don't know

00:08:15,160 --> 00:08:23,440
at what the circumstances are in which

00:08:18,430 --> 00:08:25,330
those brokers will run if you know PCF

00:08:23,440 --> 00:08:28,120
the commercial pill cloud from the

00:08:25,330 --> 00:08:31,030
offering from pivotal you maybe also

00:08:28,120 --> 00:08:33,280
know the pivotal ops manager and our

00:08:31,030 --> 00:08:37,000
goal is to provide the installation

00:08:33,280 --> 00:08:40,240
method for PCF and for open source cloud

00:08:37,000 --> 00:08:42,370
from the Orabrush directly also this

00:08:40,240 --> 00:08:44,470
creates more diversity in the

00:08:42,370 --> 00:08:49,240
environments which we want our brokers

00:08:44,470 --> 00:08:50,920
to run we want this broker to be or the

00:08:49,240 --> 00:08:51,329
automation to be infrastructure in the

00:08:50,920 --> 00:08:53,189
pen

00:08:51,329 --> 00:08:56,579
that should be run on OpenStack should

00:08:53,189 --> 00:08:59,160
be deployable on this fear should be

00:08:56,579 --> 00:09:01,439
platform independence independent so it

00:08:59,160 --> 00:09:05,100
means should be consumable from from

00:09:01,439 --> 00:09:07,290
Cloud Foundry or from kubernetes the

00:09:05,100 --> 00:09:09,299
whole the old platform should be highly

00:09:07,290 --> 00:09:11,459
available so that means the provisioning

00:09:09,299 --> 00:09:13,439
API itself the service broker API itself

00:09:11,459 --> 00:09:16,619
should be highly available but also the

00:09:13,439 --> 00:09:18,749
cluster which gets created and also we

00:09:16,619 --> 00:09:21,480
want to have backups of our service

00:09:18,749 --> 00:09:23,670
instances and backups are quite crucial

00:09:21,480 --> 00:09:27,540
and the backup logic should be tested

00:09:23,670 --> 00:09:30,029
carefully we also want capacity updates

00:09:27,540 --> 00:09:31,799
so that is actually an thing that is

00:09:30,029 --> 00:09:35,339
covered in the open service broker API

00:09:31,799 --> 00:09:37,499
in the yeah and the API terminology that

00:09:35,339 --> 00:09:40,649
is called plan updates so if you observe

00:09:37,499 --> 00:09:42,029
that your disk is running out of space

00:09:40,649 --> 00:09:45,749
you want to upgrade your service

00:09:42,029 --> 00:09:49,529
instance so that you have more space

00:09:45,749 --> 00:09:51,899
available before it actually crashes and

00:09:49,529 --> 00:09:53,549
we are the most important we one of the

00:09:51,899 --> 00:09:57,059
most important things we take very

00:09:53,549 --> 00:09:59,489
really care of is that when we want or

00:09:57,059 --> 00:10:01,259
when we want to implement or bring a new

00:09:59,489 --> 00:10:03,419
data service into a cloud from the

00:10:01,259 --> 00:10:06,480
market place like for example we decide

00:10:03,419 --> 00:10:10,019
tomorrow that we want a couch TB or and

00:10:06,480 --> 00:10:12,089
calf come with within our cloud from

00:10:10,019 --> 00:10:13,799
your market place we should be able to

00:10:12,089 --> 00:10:16,559
provide the automation of the data

00:10:13,799 --> 00:10:18,239
service very fast so that means we came

00:10:16,559 --> 00:10:21,149
up with a framework that allow us to

00:10:18,239 --> 00:10:22,679
bring such new kind of automations for

00:10:21,149 --> 00:10:25,379
new data services into the cloud from

00:10:22,679 --> 00:10:28,259
the marketplace but we also came up with

00:10:25,379 --> 00:10:31,079
testing framework so we have a framework

00:10:28,259 --> 00:10:35,309
to build those automations but we also

00:10:31,079 --> 00:10:38,279
have a framework to to come up with test

00:10:35,309 --> 00:10:42,029
suite for for those automations which

00:10:38,279 --> 00:10:46,319
makes reuse of the existing logic as

00:10:42,029 --> 00:10:49,379
much as possible let's have a look how

00:10:46,319 --> 00:10:50,669
we fulfilled those goals we we have

00:10:49,379 --> 00:10:53,879
implemented the micro service

00:10:50,669 --> 00:10:56,459
architecture which plays together to

00:10:53,879 --> 00:10:58,829
provision virtual machines and

00:10:56,459 --> 00:11:02,910
installing Postgres and so on it's it's

00:10:58,829 --> 00:11:05,330
bei actually based on Bosch but on top

00:11:02,910 --> 00:11:08,000
of Bosch there if you

00:11:05,330 --> 00:11:10,060
have a look from the top top view based

00:11:08,000 --> 00:11:13,010
on Bosch there's still ten more

00:11:10,060 --> 00:11:15,680
microservices which play together to

00:11:13,010 --> 00:11:18,200
fulfill those requirements and if you

00:11:15,680 --> 00:11:20,540
zoom in you see that it's not only about

00:11:18,200 --> 00:11:22,160
these ten micro-services but there are

00:11:20,540 --> 00:11:24,410
more than 40 components playing together

00:11:22,160 --> 00:11:29,150
to manage the lifecycle of those

00:11:24,410 --> 00:11:30,800
databases and when it comes to this

00:11:29,150 --> 00:11:33,440
micro service architecture and micro

00:11:30,800 --> 00:11:35,680
services are a quite new thing so we had

00:11:33,440 --> 00:11:38,840
to think about how we test those on

00:11:35,680 --> 00:11:41,090
those solutions those service brokers

00:11:38,840 --> 00:11:44,120
and if we have a look at the test

00:11:41,090 --> 00:11:46,250
pyramid and we try to the beginning to

00:11:44,120 --> 00:11:48,260
be comply to that test pyramid to test

00:11:46,250 --> 00:11:52,010
pyramid says that you should have as

00:11:48,260 --> 00:11:53,720
much logic tested by unit tests then you

00:11:52,010 --> 00:11:55,580
have some kind of integration tests

00:11:53,720 --> 00:11:58,040
ensuring that components play together

00:11:55,580 --> 00:12:00,770
quite good and then you have end-to-end

00:11:58,040 --> 00:12:02,600
tests or manual tests which are tests

00:12:00,770 --> 00:12:05,480
from the end users perspective so you're

00:12:02,600 --> 00:12:10,370
testing your whole stack like a user

00:12:05,480 --> 00:12:12,740
would click through your product and we

00:12:10,370 --> 00:12:16,040
learned quite fast that in a system like

00:12:12,740 --> 00:12:19,490
that it's hard to ensure a certain

00:12:16,040 --> 00:12:22,730
quality by just providing unit tests we

00:12:19,490 --> 00:12:25,640
did that and we we ended up in a lot of

00:12:22,730 --> 00:12:31,100
manual testing because we lost trust in

00:12:25,640 --> 00:12:33,140
our test suite so we we focused on those

00:12:31,100 --> 00:12:34,820
end-to-end tests because in a system

00:12:33,140 --> 00:12:37,460
like that we are so many parts played

00:12:34,820 --> 00:12:40,190
together for us it seemed to be obvious

00:12:37,460 --> 00:12:44,060
that we need more end-to-end tests and

00:12:40,190 --> 00:12:46,700
that's why we came up with with a

00:12:44,060 --> 00:12:48,500
framework that we use to write

00:12:46,700 --> 00:12:50,510
end-to-end tests for new service

00:12:48,500 --> 00:12:52,490
automation is quite easy and that is

00:12:50,510 --> 00:12:55,010
what this talk is actually about I want

00:12:52,490 --> 00:12:59,510
to show you how our end-to-end test

00:12:55,010 --> 00:13:02,090
framework looks like the goal of this

00:12:59,510 --> 00:13:05,300
end-to-end test suite is that we want to

00:13:02,090 --> 00:13:07,940
reuse existing test logic as much as

00:13:05,300 --> 00:13:10,070
possible so when we for example decide

00:13:07,940 --> 00:13:12,530
tomorrow that we want the couchy be

00:13:10,070 --> 00:13:14,120
within our cloud from the marketplace we

00:13:12,530 --> 00:13:16,250
want this couch should be to be very

00:13:14,120 --> 00:13:18,950
fast in that marketplace that the whole

00:13:16,250 --> 00:13:21,230
lifecycle is automated quite fast

00:13:18,950 --> 00:13:24,530
and also that we have a test three that

00:13:21,230 --> 00:13:26,540
we can execute against the production

00:13:24,530 --> 00:13:28,850
production environment and then that

00:13:26,540 --> 00:13:36,890
test suite will tell us whether things

00:13:28,850 --> 00:13:39,530
are working or not and for example if we

00:13:36,890 --> 00:13:41,540
have the test suite and we now don't add

00:13:39,530 --> 00:13:43,340
the new data service tool to our product

00:13:41,540 --> 00:13:45,770
but we add a new test case like for

00:13:43,340 --> 00:13:47,990
example we figured out that the

00:13:45,770 --> 00:13:50,720
particular scenario must be tested

00:13:47,990 --> 00:13:53,720
because it fails quite often so we add a

00:13:50,720 --> 00:13:56,090
new test case to that test framework and

00:13:53,720 --> 00:13:59,150
the ideas that then that test case

00:13:56,090 --> 00:14:02,030
becomes available to all all the data

00:13:59,150 --> 00:14:03,800
service implementations and also those

00:14:02,030 --> 00:14:07,250
tests which should be executable by the

00:14:03,800 --> 00:14:09,950
customer so in customer environments

00:14:07,250 --> 00:14:11,780
they usually differ so some customer

00:14:09,950 --> 00:14:13,550
environments are running on that version

00:14:11,780 --> 00:14:15,440
of our automation some customer

00:14:13,550 --> 00:14:17,930
environments running on a nother version

00:14:15,440 --> 00:14:20,480
and so the ideas that the customer by

00:14:17,930 --> 00:14:21,800
himself can verify for example that when

00:14:20,480 --> 00:14:23,330
he updates from that version to that

00:14:21,800 --> 00:14:26,210
version that everything is working and

00:14:23,330 --> 00:14:28,490
that the update will succeed so another

00:14:26,210 --> 00:14:30,620
goal is that those those test Suites can

00:14:28,490 --> 00:14:32,150
be executed by the customer as as a

00:14:30,620 --> 00:14:35,510
feature of the product and that the

00:14:32,150 --> 00:14:38,150
customer can verify that that's works if

00:14:35,510 --> 00:14:40,610
you're familiar with Bosch that is a

00:14:38,150 --> 00:14:43,610
piece of a deployment Bosch deployment

00:14:40,610 --> 00:14:45,470
manifest we are using to specify and

00:14:43,610 --> 00:14:47,390
configure our smoke tests so we are

00:14:45,470 --> 00:14:50,240
currently recalling this test smoke

00:14:47,390 --> 00:14:52,720
tests so you can actually configure

00:14:50,240 --> 00:14:55,220
which services you want to test and

00:14:52,720 --> 00:14:59,120
which plants of those services you want

00:14:55,220 --> 00:15:01,520
to test and you also see that this test

00:14:59,120 --> 00:15:03,740
suite has some feature flags so that you

00:15:01,520 --> 00:15:07,490
can activate and deactivate some of

00:15:03,740 --> 00:15:09,800
those tests manually so the customer

00:15:07,490 --> 00:15:12,770
itself can decide which test cases you

00:15:09,800 --> 00:15:15,050
want to execute on on its platform you

00:15:12,770 --> 00:15:17,510
also see that you can specify a cloud

00:15:15,050 --> 00:15:20,090
from the endpoint and so that means the

00:15:17,510 --> 00:15:22,340
test suite will actually perform the

00:15:20,090 --> 00:15:25,070
test cases like a user dot this does

00:15:22,340 --> 00:15:27,650
he's using this the test suite is using

00:15:25,070 --> 00:15:29,660
VCF CLI and actually creating a service

00:15:27,650 --> 00:15:32,010
waiting until the service is ready and

00:15:29,660 --> 00:15:36,000
then performing those tests

00:15:32,010 --> 00:15:38,220
a nice thing is because yeah the problem

00:15:36,000 --> 00:15:41,370
is the problem is that those tests take

00:15:38,220 --> 00:15:43,140
quite long so it waits until the server

00:15:41,370 --> 00:15:45,330
is ready and because it's provisioned on

00:15:43,140 --> 00:15:46,980
demand and can it can take up to five

00:15:45,330 --> 00:15:49,200
minutes until the service instance is

00:15:46,980 --> 00:15:51,750
running and if we want to test a couple

00:15:49,200 --> 00:15:55,320
of service plants its thumbs up and and

00:15:51,750 --> 00:15:57,030
it's quite long-running so that's why we

00:15:55,320 --> 00:16:00,120
decided that it should be possible to

00:15:57,030 --> 00:16:03,870
somehow parallel ice those test runs and

00:16:00,120 --> 00:16:05,730
that what you can configure here at the

00:16:03,870 --> 00:16:08,310
top that you can configure I want to run

00:16:05,730 --> 00:16:11,490
five tests and parallel so let's have a

00:16:08,310 --> 00:16:15,150
look which test cases we want to test

00:16:11,490 --> 00:16:16,890
and after that we have a look which with

00:16:15,150 --> 00:16:20,100
which interface we came up to

00:16:16,890 --> 00:16:22,260
encapsulate the service specific things

00:16:20,100 --> 00:16:24,990
and that we can come up with a generic

00:16:22,260 --> 00:16:26,990
test suite that just uses an interface

00:16:24,990 --> 00:16:30,090
to test all the service implementation

00:16:26,990 --> 00:16:32,640
so the test cases we currently test is

00:16:30,090 --> 00:16:35,580
we are testing that a service can be

00:16:32,640 --> 00:16:37,410
created we test that a service can be

00:16:35,580 --> 00:16:39,330
bound to an application and that the

00:16:37,410 --> 00:16:41,730
credentials are actionable actually

00:16:39,330 --> 00:16:43,620
usable so that means that the

00:16:41,730 --> 00:16:45,270
application can take the credentials

00:16:43,620 --> 00:16:50,760
connect to the database and then write

00:16:45,270 --> 00:16:53,510
some data in it yeah that's meant by X

00:16:50,760 --> 00:16:56,730
can apps can access the service binding

00:16:53,510 --> 00:16:58,860
we check that once an application

00:16:56,730 --> 00:17:01,890
doesn't need the service binding anymore

00:16:58,860 --> 00:17:03,660
because we want to delete it we ensure

00:17:01,890 --> 00:17:05,400
that the credentials are not usable

00:17:03,660 --> 00:17:09,090
anymore after the binding has been

00:17:05,400 --> 00:17:11,490
deleted for some services we check the

00:17:09,090 --> 00:17:13,620
arbitrary parameters for example the

00:17:11,490 --> 00:17:15,780
user can say as you have create service

00:17:13,620 --> 00:17:18,270
and I he can specify some custom user

00:17:15,780 --> 00:17:20,220
parameters like for example and what

00:17:18,270 --> 00:17:22,620
Postgres blocking should be installed in

00:17:20,220 --> 00:17:24,300
the Postgres instance there are some

00:17:22,620 --> 00:17:26,820
tests that are quite service specific

00:17:24,300 --> 00:17:29,280
but should also be tested then we test

00:17:26,820 --> 00:17:31,920
plan updates and we want in thus these

00:17:29,280 --> 00:17:33,840
tests we ensure that after plan update

00:17:31,920 --> 00:17:36,480
the data of the database is still there

00:17:33,840 --> 00:17:39,870
and still accessible also we test

00:17:36,480 --> 00:17:43,650
backups and restores so let's have a

00:17:39,870 --> 00:17:45,420
look how we try to come up with with a

00:17:43,650 --> 00:17:49,740
generic framework

00:17:45,420 --> 00:17:52,800
that makes those tests yeah as generic

00:17:49,740 --> 00:17:54,420
as possible and the first attempt was we

00:17:52,800 --> 00:17:56,790
wrote an application which we called

00:17:54,420 --> 00:17:59,490
service binding checker and it was

00:17:56,790 --> 00:18:02,280
actually our first attempt it reached

00:17:59,490 --> 00:18:05,580
credential from recap services and then

00:18:02,280 --> 00:18:07,230
it makes sure's or it tries to make use

00:18:05,580 --> 00:18:11,220
of those credentials and access the

00:18:07,230 --> 00:18:14,060
service and make use of the service this

00:18:11,220 --> 00:18:17,760
application was written in Ruby and

00:18:14,060 --> 00:18:20,490
because we had this requirement to run a

00:18:17,760 --> 00:18:22,830
lot of tests in parallel we also had to

00:18:20,490 --> 00:18:24,690
deploy a lot of those service binding

00:18:22,830 --> 00:18:27,540
checker apps into a cloud from your

00:18:24,690 --> 00:18:29,970
runtime so it ended up to be very memory

00:18:27,540 --> 00:18:32,280
consuming so that's why we decided to

00:18:29,970 --> 00:18:34,320
rewrite that application and go now we

00:18:32,280 --> 00:18:37,410
call it pin dingo like binding checker

00:18:34,320 --> 00:18:41,190
up and go and that is actually the

00:18:37,410 --> 00:18:43,440
interface of that bin dingo so whenever

00:18:41,190 --> 00:18:45,750
we add the new service like couch chibi

00:18:43,440 --> 00:18:50,000
or Kafka we have to implement that

00:18:45,750 --> 00:18:52,290
interface for this particular service

00:18:50,000 --> 00:18:54,150
but let's have a look at that interface

00:18:52,290 --> 00:18:58,290
what does it mean so the first end point

00:18:54,150 --> 00:19:01,680
actually says that when someone calls

00:18:58,290 --> 00:19:04,200
this end point the the app should return

00:19:01,680 --> 00:19:07,020
whether the the service instance is

00:19:04,200 --> 00:19:09,420
functional so for example when we talk

00:19:07,020 --> 00:19:12,690
about the post quest and what happens if

00:19:09,420 --> 00:19:15,510
someone calls slash status and this app

00:19:12,690 --> 00:19:17,430
is bound to a post crisis that the app

00:19:15,510 --> 00:19:19,530
creates a table in that Postgres

00:19:17,430 --> 00:19:21,540
instance it inserts a record within that

00:19:19,530 --> 00:19:24,600
table and then it's delete the record

00:19:21,540 --> 00:19:28,320
deletes a table again and then it

00:19:24,600 --> 00:19:31,080
returns steps HTTP status 200 if

00:19:28,320 --> 00:19:33,090
everything worked out for rabbit and

00:19:31,080 --> 00:19:36,840
queue it looks a bit different so for

00:19:33,090 --> 00:19:39,060
rabbitmq we create an EQ we insert a

00:19:36,840 --> 00:19:41,130
message we consume that message and

00:19:39,060 --> 00:19:43,440
we'll be delete the queue and once that

00:19:41,130 --> 00:19:45,300
worked we will return status code 200

00:19:43,440 --> 00:19:52,050
that everything is fine and that the

00:19:45,300 --> 00:19:54,300
rabbitmq seems to be functional the only

00:19:52,050 --> 00:19:56,430
difference between let's say a RabbitMQ

00:19:54,300 --> 00:19:58,790
and the post quest is in the URL you

00:19:56,430 --> 00:20:03,380
specify which kind of service

00:19:58,790 --> 00:20:05,780
it is beside of that it's your calling

00:20:03,380 --> 00:20:08,420
the status endpoint for each four

00:20:05,780 --> 00:20:11,330
different kinds of services and then the

00:20:08,420 --> 00:20:14,300
upper or yet the logic the test logic

00:20:11,330 --> 00:20:17,090
which relies on that interface is quite

00:20:14,300 --> 00:20:18,590
generic let's have a look at the other

00:20:17,090 --> 00:20:20,150
three endpoints because they are quite

00:20:18,590 --> 00:20:24,560
important when it comes to testing

00:20:20,150 --> 00:20:27,050
updates and plan updates or backups so

00:20:24,560 --> 00:20:29,630
the next next end point is that we want

00:20:27,050 --> 00:20:31,850
to insert a record into the database or

00:20:29,630 --> 00:20:35,990
into the data service so it's the put

00:20:31,850 --> 00:20:38,300
data service ID and this is about at

00:20:35,990 --> 00:20:41,030
that point we are going to insert a data

00:20:38,300 --> 00:20:42,860
into our Postgres for example so we are

00:20:41,030 --> 00:20:44,900
going to create the table again and then

00:20:42,860 --> 00:20:48,470
we insert the record but we don't delete

00:20:44,900 --> 00:20:50,630
it now for that purpose we have a second

00:20:48,470 --> 00:20:52,910
or third end point where we say delete

00:20:50,630 --> 00:20:55,070
that record and then we specify the ID

00:20:52,910 --> 00:20:57,650
we have chosen above when we created it

00:20:55,070 --> 00:21:00,320
and then the record is gone also we have

00:20:57,650 --> 00:21:02,480
a method to check whether a data record

00:21:00,320 --> 00:21:07,790
that we created with the put endpoint is

00:21:02,480 --> 00:21:09,680
still in the database for everton q it's

00:21:07,790 --> 00:21:12,140
like that so you create a queue and you

00:21:09,680 --> 00:21:13,870
insert the message into that queue and

00:21:12,140 --> 00:21:16,520
then whenever you check whether the

00:21:13,870 --> 00:21:18,380
queue is still in the or the message is

00:21:16,520 --> 00:21:20,870
still in the queue you go and check

00:21:18,380 --> 00:21:26,000
estimate is the message still in the

00:21:20,870 --> 00:21:28,270
queue so let's see which how the test

00:21:26,000 --> 00:21:31,520
cases looks like with that for method

00:21:28,270 --> 00:21:35,090
method that encapsulated all the service

00:21:31,520 --> 00:21:37,400
specific things so if you for example

00:21:35,090 --> 00:21:39,620
you want to test or implement a new

00:21:37,400 --> 00:21:41,510
Kafka service for your cloud from the

00:21:39,620 --> 00:21:42,920
marketplace or platform marketplace you

00:21:41,510 --> 00:21:45,800
would implement this method and then you

00:21:42,920 --> 00:21:49,250
can reuse the test suite and perform a

00:21:45,800 --> 00:21:52,610
couple of test cases without any further

00:21:49,250 --> 00:21:54,470
effort so let's see how the test cases

00:21:52,610 --> 00:21:57,320
work and how the test cases make use of

00:21:54,470 --> 00:22:00,140
this this interface so the first test

00:21:57,320 --> 00:22:03,650
case is that an app can access the

00:22:00,140 --> 00:22:05,300
service so the generic algorithm generic

00:22:03,650 --> 00:22:08,120
means it doesn't matter which kind of

00:22:05,300 --> 00:22:11,750
services like whether it's the Postgres

00:22:08,120 --> 00:22:12,650
or elastic search or Redis is the test

00:22:11,750 --> 00:22:15,460
suite

00:22:12,650 --> 00:22:18,230
to service instance with create service

00:22:15,460 --> 00:22:21,140
after that it pushes the pin bingo

00:22:18,230 --> 00:22:23,330
application to Cloud Foundry it does

00:22:21,140 --> 00:22:27,170
this in parallel because both operation

00:22:23,330 --> 00:22:30,380
takes some time and then it waits until

00:22:27,170 --> 00:22:33,140
the service is ready and then it waits

00:22:30,380 --> 00:22:35,330
until the app is ready it binds the app

00:22:33,140 --> 00:22:37,670
to the service instance it restarts the

00:22:35,330 --> 00:22:39,800
app and then it checks the status

00:22:37,670 --> 00:22:41,420
endpoint and then the status endpoint

00:22:39,800 --> 00:22:44,540
will insert some data into the database

00:22:41,420 --> 00:22:47,240
or into the message queue and then once

00:22:44,540 --> 00:22:50,260
this endpoint returns status code 200 we

00:22:47,240 --> 00:22:53,630
are sure the database seems to be ok

00:22:50,260 --> 00:22:55,850
another thing another test case is we

00:22:53,630 --> 00:22:57,920
want to test that each service has

00:22:55,850 --> 00:23:00,080
dedicated credentials or that each apps

00:22:57,920 --> 00:23:05,090
app that this bount or service becomes

00:23:00,080 --> 00:23:09,230
gets its cred dedicated credentials how

00:23:05,090 --> 00:23:11,900
that works is we create the service

00:23:09,230 --> 00:23:15,080
again we buy we push one app we push a

00:23:11,900 --> 00:23:16,850
second app then we wait every until

00:23:15,080 --> 00:23:18,710
everything is ready so service instance

00:23:16,850 --> 00:23:22,460
should be ready and ready and both apps

00:23:18,710 --> 00:23:27,050
should be ready then we bind both apps

00:23:22,460 --> 00:23:31,370
to the service instance we restart both

00:23:27,050 --> 00:23:33,200
apps and we check that both apps are

00:23:31,370 --> 00:23:37,730
working so it means that both

00:23:33,200 --> 00:23:40,850
credentials are usable then we unbind

00:23:37,730 --> 00:23:44,750
the first app we restart the first app

00:23:40,850 --> 00:23:46,940
and then we check both app and we expect

00:23:44,750 --> 00:23:49,490
the first app or the first call to fail

00:23:46,940 --> 00:23:52,910
and the second call to succeed so that

00:23:49,490 --> 00:23:54,440
we can somehow ensure that both

00:23:52,910 --> 00:23:56,300
application have a different set of

00:23:54,440 --> 00:23:57,860
credentials and at once we unbind the

00:23:56,300 --> 00:23:59,600
app the credential is not you

00:23:57,860 --> 00:24:03,440
credentials that is not usable anymore

00:23:59,600 --> 00:24:07,160
and the last thing we do in that test

00:24:03,440 --> 00:24:10,540
case is we unbind the second app and we

00:24:07,160 --> 00:24:13,130
expect both both calls to fail now

00:24:10,540 --> 00:24:17,030
another interesting test case is how we

00:24:13,130 --> 00:24:19,670
test backups and restore the generic

00:24:17,030 --> 00:24:21,440
logic for that test case would be like

00:24:19,670 --> 00:24:23,780
that so instead of creating a new

00:24:21,440 --> 00:24:26,179
instance we reusing an instance we weak

00:24:23,780 --> 00:24:30,320
provision in the last test case

00:24:26,179 --> 00:24:32,480
then we bind an app to the instance we

00:24:30,320 --> 00:24:34,429
check that everything is okay and now we

00:24:32,480 --> 00:24:36,529
use the put end point and we insert some

00:24:34,429 --> 00:24:38,360
test data and again it doesn't matter

00:24:36,529 --> 00:24:42,580
whether it's a post Chris or RabbitMQ

00:24:38,360 --> 00:24:44,720
the algorithm is the same we ensure that

00:24:42,580 --> 00:24:46,429
that the record has been inserted

00:24:44,720 --> 00:24:48,529
successfully by just checking whether

00:24:46,429 --> 00:24:51,139
it's in the database or not and then we

00:24:48,529 --> 00:24:53,090
trigger a backup so actually do we have

00:24:51,139 --> 00:24:55,309
a backup API we trigger that backup and

00:24:53,090 --> 00:25:02,029
then we wait until the backup has been

00:24:55,309 --> 00:25:04,279
created successfully and then we delete

00:25:02,029 --> 00:25:06,799
the test value and we ensure that it's

00:25:04,279 --> 00:25:08,899
actually deleted by just looking it up

00:25:06,799 --> 00:25:09,440
and saying okay it's not done here

00:25:08,899 --> 00:25:12,409
anymore

00:25:09,440 --> 00:25:14,240
and then we restore the backup and we

00:25:12,409 --> 00:25:16,220
wait until the restore is finished and

00:25:14,240 --> 00:25:19,549
then we check whether the value is in

00:25:16,220 --> 00:25:21,679
the database again and here we expect

00:25:19,549 --> 00:25:25,940
that the value should be here again and

00:25:21,679 --> 00:25:28,519
we expect the status code 200 it's a

00:25:25,940 --> 00:25:31,070
quite similar scenario we can use the

00:25:28,519 --> 00:25:32,840
same method to test plan updates the

00:25:31,070 --> 00:25:34,789
point with the plan update is that we

00:25:32,840 --> 00:25:36,470
want to ensure that after the plan

00:25:34,789 --> 00:25:39,529
update the data is still in the database

00:25:36,470 --> 00:25:41,480
so we use we don't create a new new

00:25:39,529 --> 00:25:43,970
service instance we reuse the existing

00:25:41,480 --> 00:25:45,470
service instance again we bind the bingo

00:25:43,970 --> 00:25:49,009
app we check that everything is working

00:25:45,470 --> 00:25:51,639
we write some records in it we trigger a

00:25:49,009 --> 00:25:54,740
backup we wait until it's it's finished

00:25:51,639 --> 00:25:56,299
even it's a plan update we trigger a

00:25:54,740 --> 00:25:58,850
backup because we want to ensure that

00:25:56,299 --> 00:26:01,009
after the plan update we can restore the

00:25:58,850 --> 00:26:06,440
old backup which has been created by the

00:26:01,009 --> 00:26:08,330
old plan we update to the bigger plan we

00:26:06,440 --> 00:26:11,600
wait until the update is finished and

00:26:08,330 --> 00:26:15,230
then we check that database is working

00:26:11,600 --> 00:26:19,129
after the update and we check whether

00:26:15,230 --> 00:26:21,259
the data is still in the database and so

00:26:19,129 --> 00:26:23,539
we expect the data to be in the database

00:26:21,259 --> 00:26:26,570
even after the plan update then we

00:26:23,539 --> 00:26:28,190
delete the record which ensure record

00:26:26,570 --> 00:26:29,629
has been deleted and we restore the

00:26:28,190 --> 00:26:32,419
backup and this is the backup we have

00:26:29,629 --> 00:26:35,179
created with the old plan and we just

00:26:32,419 --> 00:26:37,809
want to ensure that it still works after

00:26:35,179 --> 00:26:37,809
the plan update

00:26:38,230 --> 00:26:43,490
another interesting thing is what we can

00:26:41,150 --> 00:26:47,600
test with that simple interface is we

00:26:43,490 --> 00:26:49,220
can test platform updates for example we

00:26:47,600 --> 00:26:51,440
release a new product version of our

00:26:49,220 --> 00:26:53,150
platform so that means that we update

00:26:51,440 --> 00:26:56,720
the environment and we want to make sure

00:26:53,150 --> 00:26:59,120
that the update actually worked so the

00:26:56,720 --> 00:27:00,830
test algorithm for that is we have the

00:26:59,120 --> 00:27:02,990
service or the platform deployed in a

00:27:00,830 --> 00:27:04,640
version X so that not only means the

00:27:02,990 --> 00:27:07,790
database itself but also the management

00:27:04,640 --> 00:27:10,130
components like the broker then we

00:27:07,790 --> 00:27:13,400
create a service instance based on that

00:27:10,130 --> 00:27:18,500
alt version we push the application we

00:27:13,400 --> 00:27:21,860
wait every until everything is ready we

00:27:18,500 --> 00:27:24,280
put in some test data we trigger a

00:27:21,860 --> 00:27:27,020
backup right wait until the backup is

00:27:24,280 --> 00:27:29,180
successful and then we update the

00:27:27,020 --> 00:27:30,920
service broker to a new version and we

00:27:29,180 --> 00:27:33,350
update the service instances to new

00:27:30,920 --> 00:27:35,030
version we check that the service

00:27:33,350 --> 00:27:38,330
instances is still running after the

00:27:35,030 --> 00:27:41,630
update we insert some data that we

00:27:38,330 --> 00:27:45,530
delete some data we check that the

00:27:41,630 --> 00:27:47,150
deletion has been successfully and then

00:27:45,530 --> 00:27:48,740
we restore the backup that has been

00:27:47,150 --> 00:27:51,140
created with the old version of the

00:27:48,740 --> 00:27:54,710
platform and ensure that still can be

00:27:51,140 --> 00:27:57,890
restored yeah we have some further plans

00:27:54,710 --> 00:27:59,300
with that testing framework a colleague

00:27:57,890 --> 00:28:00,980
of mine came up with the service

00:27:59,300 --> 00:28:03,830
progress shield I and the idea of the

00:28:00,980 --> 00:28:07,370
service protocol is to mimic the CF CLI

00:28:03,830 --> 00:28:09,740
so the idea is to replace the CF CLI

00:28:07,370 --> 00:28:14,210
within the test suite so we don't have

00:28:09,740 --> 00:28:18,530
to rely on CF anymore and can reuse the

00:28:14,210 --> 00:28:21,980
test logic also for kubernetes I guess

00:28:18,530 --> 00:28:28,190
I'm running out of time so that's good

00:28:21,980 --> 00:28:31,070
point for questions in theory it works

00:28:28,190 --> 00:28:34,030
with every eye is that works for Bosch

00:28:31,070 --> 00:28:37,460
and so underneath we are using Bosch as

00:28:34,030 --> 00:28:40,280
automation tool so that means OpenStack

00:28:37,460 --> 00:28:42,410
vSphere we have some experience the most

00:28:40,280 --> 00:28:45,590
experience we have with AWS and the

00:28:42,410 --> 00:28:47,270
vSphere that is what our customers are

00:28:45,590 --> 00:28:51,200
using the most but we also have some

00:28:47,270 --> 00:28:52,070
setups on OpenStack I guess yeah and and

00:28:51,200 --> 00:28:56,110
on Asia and

00:28:52,070 --> 00:28:56,110
on our cloud Alibaba cloud

00:29:08,360 --> 00:29:11,700
and I'm not sure whether and get the

00:29:10,470 --> 00:29:13,620
question so you're talking about

00:29:11,700 --> 00:29:15,890
multiple platforms and one service

00:29:13,620 --> 00:29:15,890
broker

00:29:27,900 --> 00:29:32,370
and so yeah you have you have one broker

00:29:30,330 --> 00:29:36,630
and you want to deploy the service

00:29:32,370 --> 00:29:39,390
instances to different es that's

00:29:36,630 --> 00:29:42,510
something that is not possible out of

00:29:39,390 --> 00:29:46,020
the box but Bosch provides a concept

00:29:42,510 --> 00:29:48,660
that is called multi CPI and so if that

00:29:46,020 --> 00:29:50,580
concept should be feasible quite easy

00:29:48,660 --> 00:29:53,340
with the multi CPI concept you can

00:29:50,580 --> 00:29:56,850
actually configure Bosch to be able to

00:29:53,340 --> 00:29:58,830
deploy two different es targets so and

00:29:56,850 --> 00:30:01,340
based on that feature it would be quite

00:29:58,830 --> 00:30:09,050
easy I guess

00:30:01,340 --> 00:30:09,050
any further questions okay thank you

00:30:09,120 --> 00:30:12,360

YouTube URL: https://www.youtube.com/watch?v=66-SPt9B_yA


