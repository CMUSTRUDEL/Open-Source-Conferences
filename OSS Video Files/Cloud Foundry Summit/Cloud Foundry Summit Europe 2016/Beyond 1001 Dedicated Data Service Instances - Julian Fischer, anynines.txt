Title: Beyond 1001 Dedicated Data Service Instances - Julian Fischer, anynines
Publication date: 2016-09-30
Playlist: Cloud Foundry Summit Europe 2016
Description: 
	Beyond 1001 Dedicated Data Service Instances - Julian Fischer, anynines

The first on-demand provisioning service brokers have appeared on the radar. This talk inspires to use them at scale leading to new challenges. Provisioning thousands of dedicated service instances, a corresponding number of VMs or even clusters of VMs needs to be managed. Find out whether common practises from manual cluster operations and automation strategies suitable for managing a small number of service instances can still be applied. Striving for a full lifecycle automation, this talk gives insights about obstacles and strategies for deploying dedicated, clustered data services instances at a large scale. Questions such as how to manage thousands of service instance dedicated PostgreSQL or MongoDB clusters will be discussed, exemplarily. Join this talk and the mission to automatically manage the entire lifecycle of complex data services beyond 1001 service instances.

Julian Fischer
Julian Fischer is CEO of Avarteq GmbH, the company behind anynines. He is enthusiastic about lean thinking, agile and cloud. As a Ruby friend he fell in love with Cloud Foundry, early. By running Cloud Foundry for more than two years on VMware, AWS and OpenStack as well as researching deep into the development of Cloud Foundry services, Julian has collected valuable experience about the capabilities of Cloud Foundry.
Captions: 
	00:00:00,000 --> 00:00:09,300
all right let's give it a try so welcome

00:00:05,970 --> 00:00:11,309
to this talk today we're going to talk

00:00:09,300 --> 00:00:17,299
about data service design and how to

00:00:11,309 --> 00:00:17,299
scale data services so why this talk

00:00:17,390 --> 00:00:22,470
we've been automating data service for

00:00:20,340 --> 00:00:24,720
Cloud Foundry for now I think two or

00:00:22,470 --> 00:00:26,760
three years and we've learned that it is

00:00:24,720 --> 00:00:30,000
a challenging task because there are so

00:00:26,760 --> 00:00:31,920
many possibilities also something we've

00:00:30,000 --> 00:00:33,690
seen and observed is that early stage

00:00:31,920 --> 00:00:37,320
development is often very driven by

00:00:33,690 --> 00:00:39,480
feature demand so where it is very

00:00:37,320 --> 00:00:42,300
important to ship certain features to

00:00:39,480 --> 00:00:44,969
the customer of course this is important

00:00:42,300 --> 00:00:47,969
but also we should think of software

00:00:44,969 --> 00:00:50,610
development in general comparable to

00:00:47,969 --> 00:00:52,350
driving a motorcycle so driving a

00:00:50,610 --> 00:00:54,510
motorcycle if you're not familiar with

00:00:52,350 --> 00:00:56,219
that is it's challenging because you're

00:00:54,510 --> 00:00:58,440
not just sitting in the car and you

00:00:56,219 --> 00:01:00,300
drive around but you have to know what

00:00:58,440 --> 00:01:01,649
what's directly in front of you they are

00:01:00,300 --> 00:01:04,580
stone it could be the end of your

00:01:01,649 --> 00:01:07,979
journey so you have to keep on looking

00:01:04,580 --> 00:01:09,420
into the next turn and then directly in

00:01:07,979 --> 00:01:12,200
front of your wheel again and you'll

00:01:09,420 --> 00:01:15,900
keep on changing these two perspectives

00:01:12,200 --> 00:01:19,590
so this talk is about how do you do that

00:01:15,900 --> 00:01:21,720
for software development we would like

00:01:19,590 --> 00:01:24,090
to find some architectural bottlenecks

00:01:21,720 --> 00:01:26,790
and learn from them and see how we can

00:01:24,090 --> 00:01:30,630
improve them so the mission is actually

00:01:26,790 --> 00:01:32,610
to find a you know a way to develop

00:01:30,630 --> 00:01:36,030
Cloud Foundry data stores that work at

00:01:32,610 --> 00:01:39,590
scale so what do I mean with scale I

00:01:36,030 --> 00:01:42,270
mean we have people here in the audience

00:01:39,590 --> 00:01:44,369
representing big IOT platforms for

00:01:42,270 --> 00:01:47,850
example they are very likely to scale

00:01:44,369 --> 00:01:51,270
beyond a few service instances so how

00:01:47,850 --> 00:01:53,790
are we going to do that when looking at

00:01:51,270 --> 00:01:57,240
data service design there's a vast space

00:01:53,790 --> 00:01:59,210
of possibilities so there are many

00:01:57,240 --> 00:02:02,759
designs you can actually come up with

00:01:59,210 --> 00:02:05,430
design decisions include attributes such

00:02:02,759 --> 00:02:08,340
as what are you using as a service

00:02:05,430 --> 00:02:11,099
instance are you are you doing shared

00:02:08,340 --> 00:02:13,770
instances served from a cluster or

00:02:11,099 --> 00:02:14,640
dedicated instances what kind of

00:02:13,770 --> 00:02:17,430
research

00:02:14,640 --> 00:02:20,099
are you applying VMs versus containers

00:02:17,430 --> 00:02:22,250
are you provisioning virtual machines on

00:02:20,099 --> 00:02:24,780
demand or are you pre provisioning them

00:02:22,250 --> 00:02:26,549
how about your failover strategy do you

00:02:24,780 --> 00:02:29,280
want to have one replica of data or

00:02:26,549 --> 00:02:31,650
several so depending on your particular

00:02:29,280 --> 00:02:34,379
requirements there are a lot of design

00:02:31,650 --> 00:02:37,379
decisions to be made and they have

00:02:34,379 --> 00:02:40,590
impact on the quality your service is

00:02:37,379 --> 00:02:42,140
going to offer so for example what is

00:02:40,590 --> 00:02:45,299
your desire time to repair

00:02:42,140 --> 00:02:48,269
I've seen people talking about keep it

00:02:45,299 --> 00:02:51,840
simple and data services they should be

00:02:48,269 --> 00:02:54,510
no more complex and you know building a

00:02:51,840 --> 00:02:56,370
cluster is complex so maybe we should

00:02:54,510 --> 00:02:58,829
keep it simple but on the other hand I

00:02:56,370 --> 00:03:01,769
have to we have to deal with outages

00:02:58,829 --> 00:03:03,209
Hardware outages on regular basis so we

00:03:01,769 --> 00:03:05,790
want to deal with them we want to have

00:03:03,209 --> 00:03:08,220
minimum service impact so you always

00:03:05,790 --> 00:03:11,120
have to make trade-offs when designing a

00:03:08,220 --> 00:03:14,909
data service regarding those quality

00:03:11,120 --> 00:03:17,370
attributes so in order to think of

00:03:14,909 --> 00:03:19,889
scaling data services in general we have

00:03:17,370 --> 00:03:22,500
to narrow down the discussion and be a

00:03:19,889 --> 00:03:24,540
little more concrete than then that

00:03:22,500 --> 00:03:28,290
because otherwise we'll just be lost in

00:03:24,540 --> 00:03:30,349
a vast space of opportunities so and I

00:03:28,290 --> 00:03:34,079
think when when coming to that

00:03:30,349 --> 00:03:36,900
particular design and asking for scale

00:03:34,079 --> 00:03:38,970
the definition of what a service

00:03:36,900 --> 00:03:41,099
instance is is the most important

00:03:38,970 --> 00:03:43,530
question are you going to serve this

00:03:41,099 --> 00:03:45,419
shared database from from a single

00:03:43,530 --> 00:03:47,970
Postgres database for example as a

00:03:45,419 --> 00:03:50,669
service instance or is it going to be in

00:03:47,970 --> 00:03:53,459
your tire Postgres cluster so that that

00:03:50,669 --> 00:03:57,479
actually makes up the the nature of your

00:03:53,459 --> 00:04:01,290
service basically it makes up the nature

00:03:57,479 --> 00:04:03,120
of your service also interesting is how

00:04:01,290 --> 00:04:05,430
many services are you going to operate

00:04:03,120 --> 00:04:08,220
in the long term is it only ten or

00:04:05,430 --> 00:04:09,840
hundred or 200 then basically the

00:04:08,220 --> 00:04:12,780
challenge isn't very big but if they

00:04:09,840 --> 00:04:14,519
look at what Heroku achieved like where

00:04:12,780 --> 00:04:16,979
some of their services Postgres for

00:04:14,519 --> 00:04:18,750
example the non production Postgres i

00:04:16,979 --> 00:04:23,159
think they have a million of service

00:04:18,750 --> 00:04:26,430
instances so how do you do that also a

00:04:23,159 --> 00:04:27,930
challenging question is how many users

00:04:26,430 --> 00:04:30,270
how many concurrent use

00:04:27,930 --> 00:04:32,550
are using your platform for example if

00:04:30,270 --> 00:04:36,800
you have a well it's a fairly slow

00:04:32,550 --> 00:04:41,520
development pace we ship new apps on a

00:04:36,800 --> 00:04:43,949
rather infrequent scale then maybe the

00:04:41,520 --> 00:04:45,530
question of how many service instances

00:04:43,949 --> 00:04:48,840
you are creating at a certain time isn't

00:04:45,530 --> 00:04:50,759
very high where when everything is

00:04:48,840 --> 00:04:52,680
automated in your company and you're

00:04:50,759 --> 00:04:54,750
you're creating and destroying service

00:04:52,680 --> 00:04:57,000
instances automatically you might have a

00:04:54,750 --> 00:05:01,949
different requirement for parallel

00:04:57,000 --> 00:05:06,120
service corporations so looking at scale

00:05:01,949 --> 00:05:08,430
I think it's it makes sense to think

00:05:06,120 --> 00:05:10,349
about common data service design

00:05:08,430 --> 00:05:13,740
patterns and look at the architectural

00:05:10,349 --> 00:05:16,169
scale and within a 20 minute scale 20

00:05:13,740 --> 00:05:19,610
minute talk sadly it's not possible to

00:05:16,169 --> 00:05:23,160
you know cover all the ifs and then so I

00:05:19,610 --> 00:05:26,610
see doctor Nakia we cover something you

00:05:23,160 --> 00:05:34,889
do with dingo tiles so you're you're

00:05:26,610 --> 00:05:40,020
free to interrupt me after so I see it

00:05:34,889 --> 00:05:42,300
coming so three major design patterns

00:05:40,020 --> 00:05:45,030
we've recognized so far it's a shared

00:05:42,300 --> 00:05:47,220
cluster so basically a virtual machine

00:05:45,030 --> 00:05:50,220
or cluster virtual Murphy machines

00:05:47,220 --> 00:05:52,289
split up into into some kind of data

00:05:50,220 --> 00:05:56,130
service specific instances such as

00:05:52,289 --> 00:06:00,750
databases dedicated containers and

00:05:56,130 --> 00:06:04,580
dedicated virtual machines so when you

00:06:00,750 --> 00:06:09,210
look at their scalability capabilities

00:06:04,580 --> 00:06:11,039
looking at a shared cluster is it's like

00:06:09,210 --> 00:06:13,259
the following you have a set of virtual

00:06:11,039 --> 00:06:15,960
machines most likely they are

00:06:13,259 --> 00:06:18,840
distributed across several availability

00:06:15,960 --> 00:06:21,810
zones so why are they three of them so

00:06:18,840 --> 00:06:23,849
in order to cover the outage of an

00:06:21,810 --> 00:06:28,680
availability zone you still want to be

00:06:23,849 --> 00:06:30,630
able to provide a quorum by having two

00:06:28,680 --> 00:06:33,060
nodes being left in the cluster so that

00:06:30,630 --> 00:06:35,789
they can agree upon a new master for

00:06:33,060 --> 00:06:39,240
example so you have those three virtual

00:06:35,789 --> 00:06:41,370
machines and you'll slice up those

00:06:39,240 --> 00:06:43,440
virtual machines into

00:06:41,370 --> 00:06:45,690
in two different service instances

00:06:43,440 --> 00:06:48,389
serving the databases as a service

00:06:45,690 --> 00:06:50,850
instance so what this approach is fairly

00:06:48,389 --> 00:06:52,800
simple to do all you have to do is wrap

00:06:50,850 --> 00:06:55,260
your MongoDB cluster in a in a Bosch

00:06:52,800 --> 00:06:58,710
release and write a service broker

00:06:55,260 --> 00:07:01,380
creating databases it's also very cost

00:06:58,710 --> 00:07:03,919
effective so because what's the overhead

00:07:01,380 --> 00:07:06,780
for a service instance barely nothing

00:07:03,919 --> 00:07:10,740
but at some point your your cluster will

00:07:06,780 --> 00:07:12,300
be full and also having a threshold for

00:07:10,740 --> 00:07:14,669
when the clusters fall is hard because

00:07:12,300 --> 00:07:16,470
it's not the number of service instances

00:07:14,669 --> 00:07:18,090
that make up the utilization of that

00:07:16,470 --> 00:07:22,080
cluster but it's what you do with those

00:07:18,090 --> 00:07:25,520
service instances and so while this has

00:07:22,080 --> 00:07:27,990
a very low cost per service instance and

00:07:25,520 --> 00:07:30,990
apparently a simple service broker logic

00:07:27,990 --> 00:07:33,510
because all you have to do is create a

00:07:30,990 --> 00:07:36,720
database when a new service instances

00:07:33,510 --> 00:07:38,639
requested or a database user when a new

00:07:36,720 --> 00:07:41,430
service binding is requested

00:07:38,639 --> 00:07:44,639
you have pretty weak isolation because

00:07:41,430 --> 00:07:46,860
most of the data services well if they

00:07:44,639 --> 00:07:49,490
come with multi-tenancy capabilities at

00:07:46,860 --> 00:07:51,930
all they are limited in doing that

00:07:49,490 --> 00:07:54,030
sometimes you are able to see the

00:07:51,930 --> 00:07:55,770
database names of other customers or

00:07:54,030 --> 00:07:58,500
even the database schema of other

00:07:55,770 --> 00:08:01,500
customers and sometimes there is no

00:07:58,500 --> 00:08:03,570
multi-tenancy capability at all so

00:08:01,500 --> 00:08:05,880
beside of the weak isolation coming with

00:08:03,570 --> 00:08:07,740
a shared cluster approach you also have

00:08:05,880 --> 00:08:11,130
the problem that the the basic

00:08:07,740 --> 00:08:13,620
architecture cannot be taken as a a

00:08:11,130 --> 00:08:15,810
common a common way to approach every

00:08:13,620 --> 00:08:17,520
data service it's not a generic thing

00:08:15,810 --> 00:08:19,530
and especially when designing a

00:08:17,520 --> 00:08:22,200
framework for integrating a large number

00:08:19,530 --> 00:08:25,650
of data services that is a particular

00:08:22,200 --> 00:08:28,289
bad approach also it comes with the

00:08:25,650 --> 00:08:31,590
structural limitation that once your

00:08:28,289 --> 00:08:36,510
clusters fall you have to somehow answer

00:08:31,590 --> 00:08:39,209
the question on how to scale out so the

00:08:36,510 --> 00:08:41,430
one of the obvious options would be to

00:08:39,209 --> 00:08:44,219
just have a second cluster and fill the

00:08:41,430 --> 00:08:46,920
second cluster with instances and while

00:08:44,219 --> 00:08:49,860
this is surely can be done it also

00:08:46,920 --> 00:08:51,209
causes your simple service broker logic

00:08:49,860 --> 00:08:54,449
to become more calm

00:08:51,209 --> 00:08:57,199
phlex because you have to deal with

00:08:54,449 --> 00:09:00,149
challenges such as fragmentation so

00:08:57,199 --> 00:09:02,009
after a while database instances are

00:09:00,149 --> 00:09:04,139
created and delete and you have a

00:09:02,009 --> 00:09:07,199
fragmented cluster so in this case you

00:09:04,139 --> 00:09:09,569
can see cluster two it's not really

00:09:07,199 --> 00:09:12,540
utilized but it still consumes all the

00:09:09,569 --> 00:09:15,839
infrastructure resources taken by you

00:09:12,540 --> 00:09:18,839
know the virtual machines so you end up

00:09:15,839 --> 00:09:20,639
having a placement problem where you

00:09:18,839 --> 00:09:25,529
have to decide where to put a new

00:09:20,639 --> 00:09:27,990
service instance and which causes the

00:09:25,529 --> 00:09:30,300
desire to have a strategy for cluster

00:09:27,990 --> 00:09:32,129
rebalancing so if that scenario gets

00:09:30,300 --> 00:09:35,069
worse you know the second cluster is

00:09:32,129 --> 00:09:37,860
basically not needed so you want to have

00:09:35,069 --> 00:09:40,350
something that rebalances those service

00:09:37,860 --> 00:09:42,480
instances transparently so you can you

00:09:40,350 --> 00:09:45,869
know tear down the cluster and free the

00:09:42,480 --> 00:09:47,910
infrastructure resources this is all

00:09:45,869 --> 00:09:50,399
doable it's possible and maybe in the

00:09:47,910 --> 00:09:55,410
future that's the way to go if combined

00:09:50,399 --> 00:09:57,329
with the proper context however it it it

00:09:55,410 --> 00:09:59,220
feels somehow like what Cloud Foundry

00:09:57,329 --> 00:10:01,139
does for service instance or for

00:09:59,220 --> 00:10:04,459
application instances already like you

00:10:01,139 --> 00:10:08,309
have this placement problem and so on so

00:10:04,459 --> 00:10:10,980
yes looking at the conclusions from this

00:10:08,309 --> 00:10:13,199
is you come up with two major challenges

00:10:10,980 --> 00:10:16,499
first first you have a scalability issue

00:10:13,199 --> 00:10:18,529
so with a little bit more complexity in

00:10:16,499 --> 00:10:21,269
the service broker you can address that

00:10:18,529 --> 00:10:24,720
but the isolation issues you are ending

00:10:21,269 --> 00:10:28,889
up with they are just they don't allow

00:10:24,720 --> 00:10:31,049
for a generic solution so one way to get

00:10:28,889 --> 00:10:33,629
around that especially if you want to

00:10:31,049 --> 00:10:35,730
have cheap service instances is using

00:10:33,629 --> 00:10:38,670
containers as they come with very little

00:10:35,730 --> 00:10:42,329
overhead compared to para virtualized or

00:10:38,670 --> 00:10:45,029
fully virtualized virtual machines you

00:10:42,329 --> 00:10:49,199
have a better startup time and still you

00:10:45,029 --> 00:10:52,920
have a better isolation so what you can

00:10:49,199 --> 00:10:55,319
do for example is again provide two

00:10:52,920 --> 00:10:57,329
hosts and this time virtual machines are

00:10:55,319 --> 00:10:59,639
equipped as docker hosts and whenever

00:10:57,329 --> 00:11:01,740
you create a service instance you create

00:10:59,639 --> 00:11:03,000
either a single container or a pair of

00:11:01,740 --> 00:11:06,389
containers

00:11:03,000 --> 00:11:09,449
so in this scenario we are using a

00:11:06,389 --> 00:11:11,069
clustered container solution so do you

00:11:09,449 --> 00:11:16,199
have to introduce the complexity of

00:11:11,069 --> 00:11:18,660
having a replication so yeah you have to

00:11:16,199 --> 00:11:20,220
create containers and you have to equip

00:11:18,660 --> 00:11:24,720
the containers with appropriate

00:11:20,220 --> 00:11:26,850
configure 8 configuration so 2 pros

00:11:24,720 --> 00:11:29,670
caste processes are actually running and

00:11:26,850 --> 00:11:32,519
you have replication going on so good

00:11:29,670 --> 00:11:35,360
thing so new service instance is created

00:11:32,519 --> 00:11:38,370
you just create a new pair of containers

00:11:35,360 --> 00:11:40,459
so while this is a good approach and

00:11:38,370 --> 00:11:44,490
it's much better than a shared cluster

00:11:40,459 --> 00:11:46,800
because it has isolation and it also is

00:11:44,490 --> 00:11:50,490
pretty generic all you have to provide

00:11:46,800 --> 00:11:52,740
is an automation to spin up a data

00:11:50,490 --> 00:11:54,750
service instance but once you've done

00:11:52,740 --> 00:11:57,870
that you can basically translate that to

00:11:54,750 --> 00:12:00,899
a lot of different data service types so

00:11:57,870 --> 00:12:03,240
it's already a smarter solution so but

00:12:00,899 --> 00:12:04,680
you also run into the same problem

00:12:03,240 --> 00:12:07,079
because at some point the cluster is

00:12:04,680 --> 00:12:10,559
full and you need more of those docker

00:12:07,079 --> 00:12:13,470
hosts so the same structural limitation

00:12:10,559 --> 00:12:17,250
applies you have to answer the question

00:12:13,470 --> 00:12:18,540
what to do in the cluster is for so you

00:12:17,250 --> 00:12:20,670
also have the same service broker

00:12:18,540 --> 00:12:23,850
challenge so of course you can just you

00:12:20,670 --> 00:12:26,100
know add new virtual machines and you

00:12:23,850 --> 00:12:32,809
know use them to create more service

00:12:26,100 --> 00:12:36,059
instances so with that being said

00:12:32,809 --> 00:12:37,589
problem is what happens if your cluster

00:12:36,059 --> 00:12:39,540
is full in the middle of the night and

00:12:37,589 --> 00:12:40,860
you have you know customers from a

00:12:39,540 --> 00:12:42,689
different time zone and they want to

00:12:40,860 --> 00:12:45,029
create new service instances but they

00:12:42,689 --> 00:12:47,279
can't so that's something I as a

00:12:45,029 --> 00:12:50,550
platform user wouldn't see it would like

00:12:47,279 --> 00:12:52,620
to happen so I think that that in the

00:12:50,550 --> 00:12:55,860
long run the on-demand provisioning is

00:12:52,620 --> 00:12:58,290
somehow avoidable so if we go down that

00:12:55,860 --> 00:13:00,990
road and look how can we actually

00:12:58,290 --> 00:13:02,850
delegate all this complexity of the

00:13:00,990 --> 00:13:05,519
placement problem as well as the

00:13:02,850 --> 00:13:07,620
isolation as well as the challenge of

00:13:05,519 --> 00:13:10,350
orchestrating the creation of several

00:13:07,620 --> 00:13:12,809
virtual machines we can have a look at

00:13:10,350 --> 00:13:14,899
the third common data service design

00:13:12,809 --> 00:13:16,620
pattern which is about the on-demand

00:13:14,899 --> 00:13:18,750
creation of dedicate

00:13:16,620 --> 00:13:20,280
at service instances which could be

00:13:18,750 --> 00:13:23,880
single virtual machines but also

00:13:20,280 --> 00:13:25,920
clusters of varying sizes so in that

00:13:23,880 --> 00:13:28,650
scenario comparable to the shared

00:13:25,920 --> 00:13:31,050
cluster what we have is a set of three

00:13:28,650 --> 00:13:34,740
virtual machines in this example

00:13:31,050 --> 00:13:36,600
comprising a MongoDB replica set spread

00:13:34,740 --> 00:13:39,510
across three availability zones of your

00:13:36,600 --> 00:13:42,800
infrastructure you're not using that

00:13:39,510 --> 00:13:45,840
database to cough it up into different

00:13:42,800 --> 00:13:48,930
service instances but you take it as a

00:13:45,840 --> 00:13:50,460
service instance so facing an

00:13:48,930 --> 00:13:53,100
organization with production

00:13:50,460 --> 00:13:56,580
requirements towards data services most

00:13:53,100 --> 00:13:59,040
likely they either have DBA is running

00:13:56,580 --> 00:14:01,590
around in the company already applying

00:13:59,040 --> 00:14:03,090
some automation magic with chef or

00:14:01,590 --> 00:14:05,760
puppet or whatever

00:14:03,090 --> 00:14:08,690
automation technology they use so our

00:14:05,760 --> 00:14:12,120
solution needs to compete with them

00:14:08,690 --> 00:14:15,180
positioning would be around let's

00:14:12,120 --> 00:14:17,790
automate the 80 to 90% average use case

00:14:15,180 --> 00:14:21,300
of the company by allowing you to create

00:14:17,790 --> 00:14:22,950
on-demand different clusters of Modi be

00:14:21,300 --> 00:14:25,110
here or Postgres or whatever data

00:14:22,950 --> 00:14:26,730
service you prefer either a single

00:14:25,110 --> 00:14:29,640
virtual machines as you can see in the

00:14:26,730 --> 00:14:31,950
lower right or as clusters of virtual

00:14:29,640 --> 00:14:34,200
machines and of course according to the

00:14:31,950 --> 00:14:37,320
choose and service plan different sizes

00:14:34,200 --> 00:14:39,990
of virtual machines a good thing and the

00:14:37,320 --> 00:14:42,420
very strong isolation you get is you get

00:14:39,990 --> 00:14:45,170
the isolation that comes with power of

00:14:42,420 --> 00:14:47,310
full virtualization which is little more

00:14:45,170 --> 00:14:50,010
resilient than the container based

00:14:47,310 --> 00:14:53,060
isolation comes with more overhead but

00:14:50,010 --> 00:14:55,680
also has certain advantages

00:14:53,060 --> 00:14:58,560
so the question could rise and how can

00:14:55,680 --> 00:15:00,750
this be a simple solution and the answer

00:14:58,560 --> 00:15:02,400
to that is we have to delegate the heavy

00:15:00,750 --> 00:15:04,230
lifting because there's a lot of

00:15:02,400 --> 00:15:06,300
complexity in how to orchestrate the

00:15:04,230 --> 00:15:07,710
creation of a large number of clusters

00:15:06,300 --> 00:15:11,280
where each cluster has a certain

00:15:07,710 --> 00:15:13,440
complexity so a possible architecture

00:15:11,280 --> 00:15:13,890
for such an approach has already been

00:15:13,440 --> 00:15:15,810
built

00:15:13,890 --> 00:15:18,830
pivotal is working on something like

00:15:15,810 --> 00:15:21,540
that and we've done that two years ago

00:15:18,830 --> 00:15:23,970
so in that case you have a pretty

00:15:21,540 --> 00:15:26,190
generic service broker that has a data

00:15:23,970 --> 00:15:28,970
service specific plugin called the SPI

00:15:26,190 --> 00:15:32,280
whose responsibility is mainly

00:15:28,970 --> 00:15:35,460
managing the creation of credentials and

00:15:32,280 --> 00:15:38,130
everything it's service-specific talking

00:15:35,460 --> 00:15:40,950
to a deployment component that actually

00:15:38,130 --> 00:15:44,790
manages bosh deployments so what it does

00:15:40,950 --> 00:15:47,430
is it chooses it picks by the given

00:15:44,790 --> 00:15:50,880
service plan you have a certain Bosch

00:15:47,430 --> 00:15:53,280
release and creates a manifest v from it

00:15:50,880 --> 00:15:55,800
so basically you say I want to have a

00:15:53,280 --> 00:15:59,310
single server small or you want to have

00:15:55,800 --> 00:16:02,430
a cluster large so the Bosch release has

00:15:59,310 --> 00:16:05,150
all the magic in it that covers the

00:16:02,430 --> 00:16:07,320
replication of a MongoDB cluster or

00:16:05,150 --> 00:16:09,990
Postgres is more of a challenge because

00:16:07,320 --> 00:16:12,060
you have to add a cluster manager so

00:16:09,990 --> 00:16:16,320
that will be all we in the in the Bosch

00:16:12,060 --> 00:16:18,420
release so with that approach all you

00:16:16,320 --> 00:16:21,090
have to do to create a new data service

00:16:18,420 --> 00:16:22,950
is you have to create an SPI so how to

00:16:21,090 --> 00:16:25,110
create a credential for new newly

00:16:22,950 --> 00:16:27,660
created cluster during a service bending

00:16:25,110 --> 00:16:30,990
binding for example as well as creating

00:16:27,660 --> 00:16:33,840
a proper Bosch release all the

00:16:30,990 --> 00:16:37,230
complexity then is handled by Bosch I

00:16:33,840 --> 00:16:39,510
mean Bosch has been built to create and

00:16:37,230 --> 00:16:44,280
run and maintain lifecycle of large

00:16:39,510 --> 00:16:46,920
complex distributed systems so let Bosch

00:16:44,280 --> 00:16:49,830
do the dirty work

00:16:46,920 --> 00:16:51,840
so with Bosch taking care of virtual

00:16:49,830 --> 00:16:53,940
machine orchestration we can actually

00:16:51,840 --> 00:16:55,320
use the infrastructure to solve problems

00:16:53,940 --> 00:16:57,690
such as placement and fragmentation

00:16:55,320 --> 00:17:00,450
because whether virtual machine hoses

00:16:57,690 --> 00:17:02,450
fully is fully utilized or has free

00:17:00,450 --> 00:17:07,770
spare I mean that's a problem

00:17:02,450 --> 00:17:09,210
you know we've solved many years ago so

00:17:07,770 --> 00:17:11,520
what are the remaining challenges

00:17:09,210 --> 00:17:14,880
because you know we wanted to create a

00:17:11,520 --> 00:17:18,570
thousand service instances think of

00:17:14,880 --> 00:17:20,370
thousand service instances being

00:17:18,570 --> 00:17:22,830
clustered you end up having three

00:17:20,370 --> 00:17:25,560
thousand virtual machines so there are

00:17:22,830 --> 00:17:27,840
some challenges here how to get there

00:17:25,560 --> 00:17:30,480
the most naive approach would be that we

00:17:27,840 --> 00:17:32,400
are going to spin up a thousand service

00:17:30,480 --> 00:17:35,670
instances and just see what happens

00:17:32,400 --> 00:17:37,740
but that's way too expensive for small

00:17:35,670 --> 00:17:41,220
companies like us with just 50 people

00:17:37,740 --> 00:17:42,270
thinking about Cloud Foundry stuff so we

00:17:41,220 --> 00:17:44,390
are not a pivot

00:17:42,270 --> 00:17:46,559
or you know company it just you know

00:17:44,390 --> 00:17:48,210
takes the resources I don't think it's

00:17:46,559 --> 00:17:50,880
meaningful because before you actually

00:17:48,210 --> 00:17:55,950
do that experiment you have to prepare

00:17:50,880 --> 00:17:58,530
yourself in how to do that personally

00:17:55,950 --> 00:18:01,290
I'm a I'm a lean I'm a fan of lean

00:17:58,530 --> 00:18:04,320
management and also a fan of the lean

00:18:01,290 --> 00:18:06,660
startup it's a paradigm so what we

00:18:04,320 --> 00:18:10,170
actually do is we want to create a build

00:18:06,660 --> 00:18:12,450
measure learn loop to enter a process of

00:18:10,170 --> 00:18:13,860
learning as an organization because

00:18:12,450 --> 00:18:15,720
there are certain things we don't know

00:18:13,860 --> 00:18:18,960
about the environment we are we are

00:18:15,720 --> 00:18:21,000
operating in so there are many small and

00:18:18,960 --> 00:18:23,550
little question that that tease us all

00:18:21,000 --> 00:18:25,230
the time because we are unsure how our

00:18:23,550 --> 00:18:27,720
system might behave in one of the other

00:18:25,230 --> 00:18:30,240
scenario so the goal is to find a

00:18:27,720 --> 00:18:33,990
product market fit and this translates

00:18:30,240 --> 00:18:37,620
to our scenario is we've been using our

00:18:33,990 --> 00:18:41,910
existing MongoDB which by the way has

00:18:37,620 --> 00:18:44,550
been released for PCF today and what we

00:18:41,910 --> 00:18:46,350
want to end up having is a MongoDB that

00:18:44,550 --> 00:18:49,290
supports more than a thousand service

00:18:46,350 --> 00:18:51,330
instances so how do we get there the

00:18:49,290 --> 00:18:53,160
next questions we have we have to answer

00:18:51,330 --> 00:18:57,000
is what do we actually need to measure

00:18:53,160 --> 00:18:59,490
and what can we learn so applying this

00:18:57,000 --> 00:19:02,790
paradigm obviously answering those

00:18:59,490 --> 00:19:06,960
questions is key so we ended up

00:19:02,790 --> 00:19:08,820
searching for hypotheses of potential

00:19:06,960 --> 00:19:10,650
bottlenecks which can be answered by

00:19:08,820 --> 00:19:14,490
just looking at the dependency tree of

00:19:10,650 --> 00:19:17,420
the architecture so you can de rÃªve some

00:19:14,490 --> 00:19:20,070
some of these potential bottlenecks and

00:19:17,420 --> 00:19:23,220
obviously each of our component could be

00:19:20,070 --> 00:19:24,690
bottleneck or Bosch itself could be a

00:19:23,220 --> 00:19:28,050
bottleneck as the creation of virtual

00:19:24,690 --> 00:19:30,240
machines is handled by Bosch so if you

00:19:28,050 --> 00:19:32,550
know Bosch little more into detail you

00:19:30,240 --> 00:19:34,710
know that it runs on a on one or several

00:19:32,550 --> 00:19:37,530
virtual machines so the director could

00:19:34,710 --> 00:19:40,950
be overloaded in CPU or i/o weight or

00:19:37,530 --> 00:19:45,000
whatever there could be there could be

00:19:40,950 --> 00:19:48,030
you know not enough workers so that you

00:19:45,000 --> 00:19:49,980
have a queuing of Bosch tasks or maybe

00:19:48,030 --> 00:19:51,990
at some point with thousands of virtual

00:19:49,980 --> 00:19:54,000
machines the heartbeats coming from

00:19:51,990 --> 00:19:56,010
those spiritual machines could could

00:19:54,000 --> 00:19:59,150
actually out crawl the cat

00:19:56,010 --> 00:20:02,340
capacity of your nuts message bus and

00:19:59,150 --> 00:20:05,100
the creation of virtual machines is also

00:20:02,340 --> 00:20:06,560
a heavy heavy task to be executed by

00:20:05,100 --> 00:20:08,220
your infrastructure so at some point

00:20:06,560 --> 00:20:09,600
depending on the size of your

00:20:08,220 --> 00:20:13,740
infrastructure you might append up

00:20:09,600 --> 00:20:17,070
travel trouble with it so now there's a

00:20:13,740 --> 00:20:19,440
customer and the customer needs to to

00:20:17,070 --> 00:20:22,020
implement a certain solution probe you

00:20:19,440 --> 00:20:25,620
know promise to their customers they

00:20:22,020 --> 00:20:27,870
need a proper a runtime a proper data

00:20:25,620 --> 00:20:30,480
service solution on their infrastructure

00:20:27,870 --> 00:20:34,260
do we already know when those things

00:20:30,480 --> 00:20:36,420
cause trouble no we don't and and

00:20:34,260 --> 00:20:38,820
therefore we don't define experiments to

00:20:36,420 --> 00:20:42,540
find out how we actually how the system

00:20:38,820 --> 00:20:44,550
behaves when we scale out so in order to

00:20:42,540 --> 00:20:46,230
do that we have to think about what are

00:20:44,550 --> 00:20:51,270
the quality attributes we would like to

00:20:46,230 --> 00:20:54,300
expect and arbitrarily from just you

00:20:51,270 --> 00:20:57,420
know observing our own patients we came

00:20:54,300 --> 00:20:59,790
up with something that's about three

00:20:57,420 --> 00:21:01,710
minutes I would like to have a new

00:20:59,790 --> 00:21:04,290
service instance being created on demand

00:21:01,710 --> 00:21:06,270
in around three minutes fair enough

00:21:04,290 --> 00:21:08,490
if there's you know a lot of concurrent

00:21:06,270 --> 00:21:10,620
users I might wait up to let's say nine

00:21:08,490 --> 00:21:15,960
minutes but after nine minutes should be

00:21:10,620 --> 00:21:18,150
done on average so it's obvious that the

00:21:15,960 --> 00:21:20,130
question of how many data services are

00:21:18,150 --> 00:21:23,910
there going to be in total somehow

00:21:20,130 --> 00:21:26,910
influences or is influenced by the scale

00:21:23,910 --> 00:21:30,780
of my data service architecture and also

00:21:26,910 --> 00:21:32,750
the number of concurrent creations of

00:21:30,780 --> 00:21:35,700
service instances or update or

00:21:32,750 --> 00:21:38,190
destroying them is determined by the

00:21:35,700 --> 00:21:40,350
system scale so not by the architecture

00:21:38,190 --> 00:21:44,190
but just by how many how many Bosch

00:21:40,350 --> 00:21:45,960
workers I have how big my boss is so in

00:21:44,190 --> 00:21:47,670
order to have an experiment and looking

00:21:45,960 --> 00:21:49,860
into that systematically we have to

00:21:47,670 --> 00:21:54,480
identify relevant metrics and lock and

00:21:49,860 --> 00:21:56,490
locks one of the most important things

00:21:54,480 --> 00:22:00,150
of course because we rely on Bosch is

00:21:56,490 --> 00:22:03,600
the execution time of Bosch tasks in all

00:22:00,150 --> 00:22:05,759
its variations so similar to a Kanban

00:22:03,600 --> 00:22:07,659
board you can defer

00:22:05,759 --> 00:22:10,299
differentiate the lead time and the

00:22:07,659 --> 00:22:12,309
cycle time so the lead time would be I

00:22:10,299 --> 00:22:15,100
want to create a service instance let's

00:22:12,309 --> 00:22:18,549
assume a single server instance for you

00:22:15,100 --> 00:22:20,379
no purpose of simplicity so it ends up

00:22:18,549 --> 00:22:22,960
having a Bosch task creating this

00:22:20,379 --> 00:22:26,100
virtual machine so how long does it take

00:22:22,960 --> 00:22:30,009
from service create until it's done

00:22:26,100 --> 00:22:32,740
because when doing multiple of those of

00:22:30,009 --> 00:22:35,620
those create operations at the same time

00:22:32,740 --> 00:22:37,990
you might have a queuing in Bosch so the

00:22:35,620 --> 00:22:40,990
cycle time the time a Bosch task is

00:22:37,990 --> 00:22:43,509
executed will differ from the overall

00:22:40,990 --> 00:22:47,919
time of creating the service instance

00:22:43,509 --> 00:22:49,539
and also of course the the difference of

00:22:47,919 --> 00:22:52,269
that would be the wait time the time

00:22:49,539 --> 00:22:54,340
that the tasks just sit in the queue and

00:22:52,269 --> 00:22:58,720
wait until some worker has time to

00:22:54,340 --> 00:23:01,299
execute it and in order to learn about

00:22:58,720 --> 00:23:03,669
potential bottlenecks or around that and

00:23:01,299 --> 00:23:06,730
maybe also because to learn why a

00:23:03,669 --> 00:23:09,309
certain Bosch task took so long we also

00:23:06,730 --> 00:23:12,879
need system metrics from all VMs and

00:23:09,309 --> 00:23:15,940
components participating here so

00:23:12,879 --> 00:23:19,440
gathering them require us to co-locate

00:23:15,940 --> 00:23:23,080
our lock stash on all the VMS and

00:23:19,440 --> 00:23:24,970
scripting the Bosch CLI to take relevant

00:23:23,080 --> 00:23:26,919
time stems so feature request to the

00:23:24,970 --> 00:23:30,669
Bosch team would be very nice to have a

00:23:26,919 --> 00:23:33,399
more easy approach to to collect this

00:23:30,669 --> 00:23:35,580
data so scripting Bosch is a big thing I

00:23:33,399 --> 00:23:37,870
guess and it would be nice to have

00:23:35,580 --> 00:23:40,480
metrics from wash maybe there is a way

00:23:37,870 --> 00:23:45,580
to do that and but in in the preparation

00:23:40,480 --> 00:23:50,200
of this talk and yeah could be would be

00:23:45,580 --> 00:23:51,879
would be possible way yeah I would love

00:23:50,200 --> 00:23:53,590
to talk about that later on it's an

00:23:51,879 --> 00:23:55,629
interesting topic so however we

00:23:53,590 --> 00:23:59,350
definitely have to have to gather this

00:23:55,629 --> 00:24:02,379
information so now our experiments look

00:23:59,350 --> 00:24:04,690
experiment looks like this so we have we

00:24:02,379 --> 00:24:08,710
have the product we have the metrics and

00:24:04,690 --> 00:24:12,039
we have theories so hypotheses around

00:24:08,710 --> 00:24:14,649
what bottlenecks may occur so what are

00:24:12,039 --> 00:24:17,770
the influencing factors because this

00:24:14,649 --> 00:24:19,600
doesn't exist in a vacuum but it is

00:24:17,770 --> 00:24:21,520
executed against the certain deployment

00:24:19,600 --> 00:24:23,650
of your data service to on a certain

00:24:21,520 --> 00:24:25,630
infrastructure so looking at the

00:24:23,650 --> 00:24:27,130
architecture again you see there's a

00:24:25,630 --> 00:24:28,660
service broker there is a deployer

00:24:27,130 --> 00:24:30,400
there's the Bosch it's a little

00:24:28,660 --> 00:24:32,770
simplified there are few more components

00:24:30,400 --> 00:24:35,170
but for creating a service instance

00:24:32,770 --> 00:24:37,930
that's that's mainly the components that

00:24:35,170 --> 00:24:41,230
are hit so the scale of these components

00:24:37,930 --> 00:24:44,380
is important scale of your portion scale

00:24:41,230 --> 00:24:46,420
of your infrastructure so obviously it's

00:24:44,380 --> 00:24:49,660
a difference whether you have three

00:24:46,420 --> 00:24:53,230
workers or ten workers or just more of

00:24:49,660 --> 00:24:55,510
them you know what we are aiming at is

00:24:53,230 --> 00:24:57,160
finding out a scale out formula as a

00:24:55,510 --> 00:25:00,520
rule of thumb formula and it's not a

00:24:57,160 --> 00:25:02,440
prices a precise science here but I

00:25:00,520 --> 00:25:05,890
would like to be I would like to be able

00:25:02,440 --> 00:25:08,190
to tell a client to say well this is the

00:25:05,890 --> 00:25:10,600
scale and it's suitable for that many

00:25:08,190 --> 00:25:12,250
for that many service instances and you

00:25:10,600 --> 00:25:14,320
can create ten of them at the same time

00:25:12,250 --> 00:25:17,830
and they all will be done in that amount

00:25:14,320 --> 00:25:21,550
of of time so find a system scale that

00:25:17,830 --> 00:25:23,710
matches a certain SLA so we did some

00:25:21,550 --> 00:25:27,910
experiments I'll go through three of

00:25:23,710 --> 00:25:30,130
them before coming to a conclusion we

00:25:27,910 --> 00:25:32,980
did the first experiment on our inner

00:25:30,130 --> 00:25:35,590
house vSphere it's a fairly small one

00:25:32,980 --> 00:25:38,560
around several hundred gigs of RAM just

00:25:35,590 --> 00:25:41,860
a half a dozen which half of those and

00:25:38,560 --> 00:25:45,910
physical machines I guess so when we

00:25:41,860 --> 00:25:49,870
started with a very small deployment so

00:25:45,910 --> 00:25:52,120
with the small Bosch three workers and

00:25:49,870 --> 00:25:54,370
what we can see is that the creation of

00:25:52,120 --> 00:25:58,570
a single virtual machine when having a

00:25:54,370 --> 00:26:01,240
one concurrent create service happening

00:25:58,570 --> 00:26:03,370
in parallel that we are done in two two

00:26:01,240 --> 00:26:07,390
and a half minutes which passes our

00:26:03,370 --> 00:26:09,850
quality attribute with ten being created

00:26:07,390 --> 00:26:11,860
in parallel we can see that the actual

00:26:09,850 --> 00:26:13,510
execution time of the Bosch task is

00:26:11,860 --> 00:26:15,700
already increased to three minute and

00:26:13,510 --> 00:26:18,940
twenty and that it takes six minutes in

00:26:15,700 --> 00:26:21,250
total with 25 you wait up to 17 minutes

00:26:18,940 --> 00:26:23,800
on the creation of a single service

00:26:21,250 --> 00:26:27,070
instance on average which is way beyond

00:26:23,800 --> 00:26:30,190
what we actually desire so the outcome

00:26:27,070 --> 00:26:31,210
of of that iteration was that we

00:26:30,190 --> 00:26:33,790
actually have

00:26:31,210 --> 00:26:36,340
the ability to create 10 service

00:26:33,790 --> 00:26:38,830
instances at the same time if you're

00:26:36,340 --> 00:26:40,870
willing to wait up to send me seven

00:26:38,830 --> 00:26:43,660
minutes on average on average here means

00:26:40,870 --> 00:26:47,320
we had values between five and nine

00:26:43,660 --> 00:26:49,750
minutes we also confirmed the

00:26:47,320 --> 00:26:52,210
assumptions that the utilization of our

00:26:49,750 --> 00:26:54,040
core components is barely nothing so

00:26:52,210 --> 00:26:56,470
they are not going to be a bottleneck in

00:26:54,040 --> 00:26:59,260
the near future and that scaling Bosch

00:26:56,470 --> 00:27:01,200
is the dominant factor obviously we

00:26:59,260 --> 00:27:04,810
started with a very small bush

00:27:01,200 --> 00:27:07,480
deployment so that was natural to happen

00:27:04,810 --> 00:27:08,980
so the conclusion was we adjust the

00:27:07,480 --> 00:27:12,460
scale of Bosch for the next iteration

00:27:08,980 --> 00:27:15,880
still on vSphere we just scaled the

00:27:12,460 --> 00:27:17,800
number of of of Bosch workers assuming

00:27:15,880 --> 00:27:20,440
that the queuing time would be reduced

00:27:17,800 --> 00:27:22,380
by both what we've actually seen was

00:27:20,440 --> 00:27:25,240
that what we've seen is that the

00:27:22,380 --> 00:27:27,610
execution time heavily varied you can

00:27:25,240 --> 00:27:30,220
see that the actual execution of a Bosch

00:27:27,610 --> 00:27:33,090
tasks the creation of VM suddenly took

00:27:30,220 --> 00:27:37,300
like eight minutes and that was

00:27:33,090 --> 00:27:39,070
unexpected because it it clearly tells

00:27:37,300 --> 00:27:41,200
you that there's something happening on

00:27:39,070 --> 00:27:44,020
the infrastructure level that caused EVM

00:27:41,200 --> 00:27:47,320
creation to be delayed like that and as

00:27:44,020 --> 00:27:50,830
you can see and as expected the overall

00:27:47,320 --> 00:27:52,890
creation time goes a little down so you

00:27:50,830 --> 00:27:56,650
are faster in creating those 25

00:27:52,890 --> 00:27:58,630
instances on average so the outcome was

00:27:56,650 --> 00:28:01,030
that there is a large derivation on the

00:27:58,630 --> 00:28:06,010
completion of the tasks caused by the

00:28:01,030 --> 00:28:09,010
infrastructure we had learned that even

00:28:06,010 --> 00:28:11,980
a small infrastructure cannon can be an

00:28:09,010 --> 00:28:15,430
influencing factor when creating service

00:28:11,980 --> 00:28:17,500
instances in parallel pretty early so

00:28:15,430 --> 00:28:19,030
the conclusion was to see where the

00:28:17,500 --> 00:28:20,890
limits of the architecture are we are

00:28:19,030 --> 00:28:23,560
going to move that to Amazon and see how

00:28:20,890 --> 00:28:25,900
that changes because we've been sure

00:28:23,560 --> 00:28:28,900
that it's really the infrastructure that

00:28:25,900 --> 00:28:32,710
causes those delays so the same

00:28:28,900 --> 00:28:36,490
experiment with like in every iteration

00:28:32,710 --> 00:28:38,860
two we performed it on AWS and we've

00:28:36,490 --> 00:28:41,350
seen that the creation of a virtual

00:28:38,860 --> 00:28:44,080
machine the execution of a Bosch time is

00:28:41,350 --> 00:28:44,900
pretty it's pretty constant there they

00:28:44,080 --> 00:28:48,200
are

00:28:44,900 --> 00:28:51,080
somehow equipped to do that and you can

00:28:48,200 --> 00:28:54,230
also see that the the average time for

00:28:51,080 --> 00:28:57,440
the entire for the entire creation of 25

00:28:54,230 --> 00:28:59,990
instances on average just goes up to 5

00:28:57,440 --> 00:29:02,450
minutes when creating 25 in parallel

00:28:59,990 --> 00:29:04,130
which means that there's barely no

00:29:02,450 --> 00:29:09,350
queuing time for Bosh

00:29:04,130 --> 00:29:13,550
so that would be okay that scale would

00:29:09,350 --> 00:29:16,250
be okay if you would like to create 25

00:29:13,550 --> 00:29:19,100
service instances in parallel and wait

00:29:16,250 --> 00:29:22,160
around whatever 7 minutes to create a

00:29:19,100 --> 00:29:25,450
service instance observed we can see

00:29:22,160 --> 00:29:28,910
that AWS is more stable when it comes to

00:29:25,450 --> 00:29:32,390
dealing with more load obviously and

00:29:28,910 --> 00:29:34,160
also that small infrastructures may

00:29:32,390 --> 00:29:36,590
become a limiting factors so if you

00:29:34,160 --> 00:29:38,930
expect to create a large a lot of

00:29:36,590 --> 00:29:40,880
service instance for example as part of

00:29:38,930 --> 00:29:43,040
your CI pipeline and you want to deploy

00:29:40,880 --> 00:29:46,360
them on demand you definitely have to

00:29:43,040 --> 00:29:49,460
take care of your infrastructure as well

00:29:46,360 --> 00:29:52,100
so yes that is the SLA we came up with

00:29:49,460 --> 00:29:54,350
and the next experiments we are going to

00:29:52,100 --> 00:29:55,820
make is wheel stepwise increase Bosch

00:29:54,350 --> 00:29:58,670
increase the number of parallel

00:29:55,820 --> 00:30:01,880
instances to be created and see how is

00:29:58,670 --> 00:30:04,490
the relation between those two because

00:30:01,880 --> 00:30:07,280
we want to come up with this formula to

00:30:04,490 --> 00:30:09,590
scale out Bosch it depending on the

00:30:07,280 --> 00:30:11,930
number of service instances you want to

00:30:09,590 --> 00:30:14,150
be able to run conclusion from our

00:30:11,930 --> 00:30:15,890
experiments was that sadly it took

00:30:14,150 --> 00:30:17,570
longer than expected because I actually

00:30:15,890 --> 00:30:20,390
planned to show you how to create a

00:30:17,570 --> 00:30:22,340
thousand service instances but we are

00:30:20,390 --> 00:30:25,010
pretty positive that the architecture

00:30:22,340 --> 00:30:27,470
we've chosen is solid to do that because

00:30:25,010 --> 00:30:29,990
we haven't had any architectural

00:30:27,470 --> 00:30:31,070
bottlenecks hit so far and we also

00:30:29,990 --> 00:30:33,320
learned that the number of

00:30:31,070 --> 00:30:35,690
simultaneously service broker operations

00:30:33,320 --> 00:30:37,730
is the first thing we should look at

00:30:35,690 --> 00:30:39,530
it's not a total number it's the the

00:30:37,730 --> 00:30:41,000
number of of service book operations

00:30:39,530 --> 00:30:44,360
happening in parallel that should take

00:30:41,000 --> 00:30:46,340
care us more and this is basically

00:30:44,360 --> 00:30:48,770
determined by the scale of Bosch and the

00:30:46,340 --> 00:30:51,410
scale of your infrastructure so this

00:30:48,770 --> 00:30:53,210
scale out formula should somehow have a

00:30:51,410 --> 00:30:55,490
balance between your infrastructure

00:30:53,210 --> 00:30:58,010
resources your runtime your boss your

00:30:55,490 --> 00:31:00,710
data services

00:30:58,010 --> 00:31:03,590
so we think that the cron work here is

00:31:00,710 --> 00:31:05,870
done and we will keep on doing these

00:31:03,590 --> 00:31:07,760
experiments and stay tuned we are going

00:31:05,870 --> 00:31:10,340
to block and post about that because I

00:31:07,760 --> 00:31:13,460
think that's a valuable contribution to

00:31:10,340 --> 00:31:15,650
the entire Cloud Foundry community so

00:31:13,460 --> 00:31:19,690
thank you for your attention and feel

00:31:15,650 --> 00:31:19,690

YouTube URL: https://www.youtube.com/watch?v=LetxV_3HlnA


