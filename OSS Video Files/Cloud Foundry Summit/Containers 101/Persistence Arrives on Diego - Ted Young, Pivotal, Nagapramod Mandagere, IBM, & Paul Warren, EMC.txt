Title: Persistence Arrives on Diego - Ted Young, Pivotal, Nagapramod Mandagere, IBM, & Paul Warren, EMC
Publication date: 2016-06-06
Playlist: Containers 101
Description: 
	Persistent Storage is coming to Diego! IBM, Pivotal, and EMC have partnered to extend the Cloud Foundry runtime to include persistence storage. We discuss the approach we are taking, how distributed filesystems and block storage services fit well with our service broker abstraction. and how we plan to offer these services in the marketplace. An overview of the types of storage, their differences, and the effect they have on scheduling and the application lifecycle will be discussed. Example use cases for each type of storage will be covered. We will also discuss the pros and cons of running databases and other services directly on Cloud Foundry. 

Nagapramod Mandagere
Research Group Leader, IBM
Nagapramod Mandagere received his PhD from University of Minnesota in Enterprise data management. He has been a researcher at IBM Almaden Research center since 2008 working on various systems technologies. He has coauthored several conference papers and has several patents in domain of systems management

Paul Warren
Software Engineer, EMC

Ted Young
Pivotal
Ted has built distributed computer systems in a variety of environments: computer animation pipelines for VFX, live event coordination, and elastic compute platforms. In 2015 he received a Pivotal Research Grant to explore approaches to running persistent workloads in a multi-tenant environment. He is a core contributor to Diego, and heads up the Diego Persistence Team (Persi).
Captions: 
	00:00:00,030 --> 00:00:06,569
hello hello is this thing on no it is it

00:00:03,060 --> 00:00:09,150
is excellent all right welcome everyone

00:00:06,569 --> 00:00:11,790
thanks for coming my name is Ted young

00:00:09,150 --> 00:00:14,880
I'm the lead on persistence at Cloud

00:00:11,790 --> 00:00:17,310
Foundry a year ago I came and spoke at

00:00:14,880 --> 00:00:19,439
this conference about a proposal to add

00:00:17,310 --> 00:00:21,480
persistence to Cloud Foundry and I'm

00:00:19,439 --> 00:00:23,460
happy to say today that we've gone and

00:00:21,480 --> 00:00:26,820
done it so I'd like to welcome onto

00:00:23,460 --> 00:00:29,340
stage my coworkers promote mana Jerry

00:00:26,820 --> 00:00:31,880
from IBM research and Paul Warren from

00:00:29,340 --> 00:00:34,620
EMC and we're going to talk to you about

00:00:31,880 --> 00:00:36,420
the persistence options we've added to

00:00:34,620 --> 00:00:37,290
Cloud Foundry today and the ones that

00:00:36,420 --> 00:00:41,850
are coming soon

00:00:37,290 --> 00:00:45,390
all right ok good good morning everybody

00:00:41,850 --> 00:00:47,670
a quick outline for today's talk first

00:00:45,390 --> 00:00:50,820
we'll talk about why we need persistence

00:00:47,670 --> 00:00:53,940
for CF applications and by this I mean

00:00:50,820 --> 00:00:55,379
native persistence we'll back it up with

00:00:53,940 --> 00:00:57,809
a few use cases that we have come across

00:00:55,379 --> 00:01:00,690
from multiple customer scenarios and

00:00:57,809 --> 00:01:02,100
multiple application use cases we'll

00:01:00,690 --> 00:01:04,769
follow it up with the architecture we

00:01:02,100 --> 00:01:07,619
came up with and then go to a couple of

00:01:04,769 --> 00:01:09,360
walk throughs for WordPress and one

00:01:07,619 --> 00:01:10,979
sample big data analytics application

00:01:09,360 --> 00:01:15,600
and follow it up with futures and

00:01:10,979 --> 00:01:18,060
comments so Cloud Foundry is a very good

00:01:15,600 --> 00:01:21,360
runtime for web applications built using

00:01:18,060 --> 00:01:24,119
the 12 factor paradigm CF runtime

00:01:21,360 --> 00:01:25,770
managers application instances it

00:01:24,119 --> 00:01:27,780
actually manages cells and VMs and

00:01:25,770 --> 00:01:30,950
deploys the application instances in

00:01:27,780 --> 00:01:33,570
isolated containers using build packs

00:01:30,950 --> 00:01:35,820
there is a ephemeral file system

00:01:33,570 --> 00:01:37,619
available inside a container which is at

00:01:35,820 --> 00:01:40,110
the disposal of the application but as

00:01:37,619 --> 00:01:42,720
the name suggests its FML or there's no

00:01:40,110 --> 00:01:45,270
guarantees on this file system this is

00:01:42,720 --> 00:01:47,810
local to the container if you have

00:01:45,270 --> 00:01:49,890
multiple instances of your application

00:01:47,810 --> 00:01:51,780
chances are they are deployed they're

00:01:49,890 --> 00:01:53,820
deployed in different cells and this

00:01:51,780 --> 00:01:55,740
local ephemeral storage is not shareable

00:01:53,820 --> 00:01:58,979
across instances so there is no common

00:01:55,740 --> 00:02:00,810
namespace the only way to do state

00:01:58,979 --> 00:02:04,020
persistence in Cloud Foundry today is

00:02:00,810 --> 00:02:05,460
why as a service route you have a bunch

00:02:04,020 --> 00:02:07,259
of different types of services available

00:02:05,460 --> 00:02:09,780
in Cloud Foundry today there are

00:02:07,259 --> 00:02:12,120
relational services like db2 Oracle

00:02:09,780 --> 00:02:13,410
there are no sequel engines like

00:02:12,120 --> 00:02:15,390
Cloudant

00:02:13,410 --> 00:02:17,670
they're also the object storage services

00:02:15,390 --> 00:02:20,130
likes swift which you can use for

00:02:17,670 --> 00:02:22,080
persistence but the key point to note

00:02:20,130 --> 00:02:24,510
here is these services themselves don't

00:02:22,080 --> 00:02:26,850
run on Cloud Foundry runtime they run on

00:02:24,510 --> 00:02:31,410
other runtimes externally or directly on

00:02:26,850 --> 00:02:33,720
infrastructures if you look at the

00:02:31,410 --> 00:02:36,420
current model and if you try to look at

00:02:33,720 --> 00:02:38,310
typical legacy applications which have a

00:02:36,420 --> 00:02:40,950
lot of file system dependencies in them

00:02:38,310 --> 00:02:44,310
porting them to Cloud Foundry is not a

00:02:40,950 --> 00:02:46,170
trivial task eliminating all types of

00:02:44,310 --> 00:02:49,410
file system dependencies in your code is

00:02:46,170 --> 00:02:50,880
quite complicated and often you'll end

00:02:49,410 --> 00:02:52,710
up probably rewriting you're better off

00:02:50,880 --> 00:02:53,430
rewriting the entire application code

00:02:52,710 --> 00:02:55,620
from scratch

00:02:53,430 --> 00:02:59,400
but this is not always feasible for all

00:02:55,620 --> 00:03:01,980
application scenarios and when it comes

00:02:59,400 --> 00:03:04,200
to 12 factor conformance achieving the

00:03:01,980 --> 00:03:06,360
last mile of 12 file factor conformance

00:03:04,200 --> 00:03:08,220
is the most crucial or the trivial part

00:03:06,360 --> 00:03:10,440
or non-trivial part and that makes

00:03:08,220 --> 00:03:13,550
porting existing legacy applications to

00:03:10,440 --> 00:03:16,440
cloud foundry really an arduous task

00:03:13,550 --> 00:03:18,840
further most applications use a lot of

00:03:16,440 --> 00:03:20,490
third party dependencies and libraries

00:03:18,840 --> 00:03:22,980
which have been accrued over time for

00:03:20,490 --> 00:03:25,950
different purposes and eliminating file

00:03:22,980 --> 00:03:29,310
system dependencies from them is often a

00:03:25,950 --> 00:03:34,020
non-starter for porting applications to

00:03:29,310 --> 00:03:36,060
cloud foundry the other type of

00:03:34,020 --> 00:03:38,610
applications are composite applications

00:03:36,060 --> 00:03:40,830
nowadays are composed of multiple micro

00:03:38,610 --> 00:03:42,630
services each deployed on different

00:03:40,830 --> 00:03:44,790
runtimes you might have some micro

00:03:42,630 --> 00:03:46,650
services running on Cloud Foundry and

00:03:44,790 --> 00:03:49,310
some others running on infrastructure

00:03:46,650 --> 00:03:52,410
providers directly and in these cases

00:03:49,310 --> 00:03:54,570
porting existing datasets which are the

00:03:52,410 --> 00:03:56,610
heart of all these applications becomes

00:03:54,570 --> 00:03:58,709
an a very arduous task and this is

00:03:56,610 --> 00:04:01,380
unstructured data and it's not always

00:03:58,709 --> 00:04:03,390
possible to API 5 this data and file

00:04:01,380 --> 00:04:05,640
system is the only common way of sharing

00:04:03,390 --> 00:04:09,150
data across them you could potentially

00:04:05,640 --> 00:04:11,550
do a ETL face where you transform all

00:04:09,150 --> 00:04:13,470
the file system related data to objects

00:04:11,550 --> 00:04:17,670
or no sequel but this is often a very

00:04:13,470 --> 00:04:20,400
arduous task and often not all data is

00:04:17,670 --> 00:04:22,710
suitable for no sequel or object or DV

00:04:20,400 --> 00:04:24,810
backends and it's so unstructured that

00:04:22,710 --> 00:04:27,819
it file system is the best choice for

00:04:24,810 --> 00:04:30,979
actually maintaining the state

00:04:27,819 --> 00:04:33,590
further it's not just a matter of doing

00:04:30,979 --> 00:04:35,990
a one-time ETL at the beginning of your

00:04:33,590 --> 00:04:37,669
deployment phase chances are that since

00:04:35,990 --> 00:04:40,159
your deployment strides across multiple

00:04:37,669 --> 00:04:42,740
run times you need to be in a constant

00:04:40,159 --> 00:04:45,080
cycle of doing this ETL on a day to day

00:04:42,740 --> 00:04:47,180
basis so if your data changes in one run

00:04:45,080 --> 00:04:50,960
time you need to again do an ETL from

00:04:47,180 --> 00:04:52,520
file system two objects are no sequel

00:04:50,960 --> 00:04:54,289
engines and ingest it into cloud foundry

00:04:52,520 --> 00:04:56,210
and then actually consume it in your

00:04:54,289 --> 00:04:58,310
cloud foundry applications so it's not a

00:04:56,210 --> 00:04:59,810
one-time ETL process it's a very

00:04:58,310 --> 00:05:01,969
constant process for a lot of

00:04:59,810 --> 00:05:04,340
applications and this is a nightmare and

00:05:01,969 --> 00:05:07,849
such applications typically cannot be

00:05:04,340 --> 00:05:10,460
ported to cloud foundry today there's

00:05:07,849 --> 00:05:12,800
however one alternative to overcome

00:05:10,460 --> 00:05:15,620
these limitations that's basically by

00:05:12,800 --> 00:05:17,750
using fuse enable file systems so what

00:05:15,620 --> 00:05:20,080
you do in this case is you use user

00:05:17,750 --> 00:05:23,000
space file systems like SSH Affairs and

00:05:20,080 --> 00:05:25,370
actually mount remote file systems into

00:05:23,000 --> 00:05:27,740
the container directly by doing this you

00:05:25,370 --> 00:05:30,500
can access external remote storage in

00:05:27,740 --> 00:05:32,569
file systems and your applications don't

00:05:30,500 --> 00:05:33,889
need to eliminate all file system

00:05:32,569 --> 00:05:36,740
dependencies and they can be readily

00:05:33,889 --> 00:05:39,469
ported to Cloud Foundry but however this

00:05:36,740 --> 00:05:42,169
comes with a bunch of challenges first

00:05:39,469 --> 00:05:43,909
and foremost is security what typically

00:05:42,169 --> 00:05:46,009
happens is when you're trying to do a

00:05:43,909 --> 00:05:47,330
fused mount inside a container your

00:05:46,009 --> 00:05:49,400
container needs to be running in

00:05:47,330 --> 00:05:51,889
privileged mode otherwise containers

00:05:49,400 --> 00:05:53,479
does don't support mount commands so

00:05:51,889 --> 00:05:55,069
this is often a non-starter because

00:05:53,479 --> 00:05:56,750
running containers and privileged mode

00:05:55,069 --> 00:05:59,690
opens you up to a lot of security

00:05:56,750 --> 00:06:01,789
challenges you know provider runtime the

00:05:59,690 --> 00:06:04,129
second challenge is that of concurrency

00:06:01,789 --> 00:06:05,659
when you're doing fused mounts and if

00:06:04,129 --> 00:06:08,539
you are mounting the same file system

00:06:05,659 --> 00:06:11,000
across multiple containers there is no

00:06:08,539 --> 00:06:14,419
consistency guarantees that are

00:06:11,000 --> 00:06:16,310
achievable in such systems so the third

00:06:14,419 --> 00:06:18,080
important point is that of performance

00:06:16,310 --> 00:06:20,270
since you're using a user space file

00:06:18,080 --> 00:06:22,039
system it cannot match the native file

00:06:20,270 --> 00:06:24,529
system access that you can get from

00:06:22,039 --> 00:06:27,639
traditional file systems so these

00:06:24,529 --> 00:06:30,219
limitations make the fuze based systems

00:06:27,639 --> 00:06:32,419
non-starter for quite a few situations

00:06:30,219 --> 00:06:35,719
with all these in mind we started

00:06:32,419 --> 00:06:37,430
designing capabilities in the runtime to

00:06:35,719 --> 00:06:38,990
support cluster file systems and

00:06:37,430 --> 00:06:39,720
different types of persistence

00:06:38,990 --> 00:06:42,030
mechanisms

00:06:39,720 --> 00:06:46,260
and Paul will take us through the design

00:06:42,030 --> 00:06:48,720
of this yeah thanks for mate say design

00:06:46,260 --> 00:06:49,980
girls let's have a look at those we knew

00:06:48,720 --> 00:06:51,960
right up front that we weren't going to

00:06:49,980 --> 00:06:53,250
build hooks for every all of the storage

00:06:51,960 --> 00:06:56,610
systems out there and neither did we

00:06:53,250 --> 00:06:59,280
want T so primary design goal was to be

00:06:56,610 --> 00:07:02,580
open and extensible in the solution that

00:06:59,280 --> 00:07:04,500
we provided we also decided for a couple

00:07:02,580 --> 00:07:06,300
of reasons to surface this through the

00:07:04,500 --> 00:07:08,580
standard service broker semantics and

00:07:06,300 --> 00:07:10,380
model two main reasons there really was

00:07:08,580 --> 00:07:12,030
we want it to be familiar just like

00:07:10,380 --> 00:07:14,790
attaching any other service to your app

00:07:12,030 --> 00:07:16,650
and secondly we know that even within

00:07:14,790 --> 00:07:18,690
the same category all of these storage

00:07:16,650 --> 00:07:21,000
systems have have subtleties to the way

00:07:18,690 --> 00:07:23,970
that work so we wanted the app developer

00:07:21,000 --> 00:07:25,920
to be cognizant of the of the type of

00:07:23,970 --> 00:07:27,330
volume that he was attaching to and

00:07:25,920 --> 00:07:28,680
obviously we wanted to solve that

00:07:27,330 --> 00:07:30,680
security problem that promote talked

00:07:28,680 --> 00:07:32,850
about so we wanted to be able to do this

00:07:30,680 --> 00:07:34,500
so that the containers didn't need to

00:07:32,850 --> 00:07:38,550
run with any special elevated privileges

00:07:34,500 --> 00:07:39,900
but they can still mount volumes so

00:07:38,550 --> 00:07:41,520
implementation how do we actually solve

00:07:39,900 --> 00:07:43,710
that we're looking in the community

00:07:41,520 --> 00:07:46,710
there was no standard body out there for

00:07:43,710 --> 00:07:49,410
volume mounting but docker done quite a

00:07:46,710 --> 00:07:51,210
lot of work in this area and having a

00:07:49,410 --> 00:07:54,300
look at their stuff they had a fairly

00:07:51,210 --> 00:07:56,040
nice volume API in a nice ecosystem of

00:07:54,300 --> 00:07:59,070
volume drivers being written to that API

00:07:56,040 --> 00:08:00,960
so we decided to take the daka volume

00:07:59,070 --> 00:08:04,050
plug-in and implement a volume manager

00:08:00,960 --> 00:08:05,550
to that service broker was pretty good

00:08:04,050 --> 00:08:07,620
as is didn't really need to make any

00:08:05,550 --> 00:08:09,540
changes to that apart from in one area

00:08:07,620 --> 00:08:11,370
where we just extended a little bit so

00:08:09,540 --> 00:08:14,490
that it could think it could incorporate

00:08:11,370 --> 00:08:16,020
volume mount instructions diego we

00:08:14,490 --> 00:08:17,840
really only had to change in for the

00:08:16,020 --> 00:08:20,390
first phase anyway in a couple of areas

00:08:17,840 --> 00:08:23,250
one we had to add a little bit of volume

00:08:20,390 --> 00:08:24,690
information to diego so that the

00:08:23,250 --> 00:08:26,790
auctioneer could be a little bit clever

00:08:24,690 --> 00:08:28,500
about where it placed apps on cells

00:08:26,790 --> 00:08:29,940
clearly we don't want it to place an app

00:08:28,500 --> 00:08:31,979
on a cell that doesn't have the right

00:08:29,940 --> 00:08:34,050
volume drivers for the type of volume

00:08:31,979 --> 00:08:36,060
that the app needs to mount to and

00:08:34,050 --> 00:08:39,270
secondly we actually had to obviously

00:08:36,060 --> 00:08:40,589
make diego do the mounting itself in a

00:08:39,270 --> 00:08:42,060
couple of respects needs to mount onto

00:08:40,589 --> 00:08:43,710
the cell firstly and then it needs to

00:08:42,060 --> 00:08:47,070
mount take that volume and mount it into

00:08:43,710 --> 00:08:48,089
the container picture paints a thousand

00:08:47,070 --> 00:08:50,700
words so let's have a look at that

00:08:48,089 --> 00:08:51,870
here's a rather simplified version of

00:08:50,700 --> 00:08:53,850
the Cloud Foundry architecture

00:08:51,870 --> 00:08:56,160
you don't pick me up on this and it's

00:08:53,850 --> 00:08:58,529
simplified so let's see what bits we

00:08:56,160 --> 00:09:00,270
added first and foremost obviously we

00:08:58,529 --> 00:09:03,089
got the storage sitting outside of Cloud

00:09:00,270 --> 00:09:06,570
Foundry some sort of volume service okay

00:09:03,089 --> 00:09:10,410
could be EMC scale i/o for block could

00:09:06,570 --> 00:09:13,110
be EMC Isilon one FS for NFS stuff could

00:09:10,410 --> 00:09:14,940
be IBM spectrum scale we actually chose

00:09:13,110 --> 00:09:15,960
to implement a reference implementations

00:09:14,940 --> 00:09:19,350
there's something that works out of the

00:09:15,960 --> 00:09:21,900
box and we chose set for that because it

00:09:19,350 --> 00:09:24,390
provided what we needed from day one and

00:09:21,900 --> 00:09:26,000
actually provided a little bit of stuff

00:09:24,390 --> 00:09:28,260
for us moving forward

00:09:26,000 --> 00:09:30,570
as I said we surfaced this to be the

00:09:28,260 --> 00:09:32,400
standard service broker semantics so we

00:09:30,570 --> 00:09:34,980
front that with some sort of volume

00:09:32,400 --> 00:09:36,390
specific service broker okay that like

00:09:34,980 --> 00:09:39,240
provides I think in modern lingo that

00:09:36,390 --> 00:09:43,860
provides the control plane for the

00:09:39,240 --> 00:09:46,529
storage system then added a volume

00:09:43,860 --> 00:09:49,470
manager on to each cell and we call this

00:09:46,529 --> 00:09:51,870
thing vol man and that doesn't actually

00:09:49,470 --> 00:09:54,570
really talk to the storage systems right

00:09:51,870 --> 00:09:56,580
open and extensible so we add volume

00:09:54,570 --> 00:09:58,650
drivers we co-locate volume drivers onto

00:09:56,580 --> 00:10:00,660
the cell at the point you install Cloud

00:09:58,650 --> 00:10:03,300
Foundry and those things as we've

00:10:00,660 --> 00:10:05,279
already talked about expose exposed by

00:10:03,300 --> 00:10:06,690
the docker volume API so the vault man

00:10:05,279 --> 00:10:10,320
talks those things using the docker

00:10:06,690 --> 00:10:12,450
volume API calls so those are the pieces

00:10:10,320 --> 00:10:14,250
we add how does it really work again

00:10:12,450 --> 00:10:15,810
reinforcing the point that we just it's

00:10:14,250 --> 00:10:17,700
just another service alright but it's a

00:10:15,810 --> 00:10:19,740
little bit of a special service ok so

00:10:17,700 --> 00:10:22,920
your developer needs to bind his app to

00:10:19,740 --> 00:10:25,230
some volumes it just issues the same CF

00:10:22,920 --> 00:10:26,700
create call my Cloud controller receives

00:10:25,230 --> 00:10:28,500
that and it forwards it on to the volume

00:10:26,700 --> 00:10:30,420
specific service broker the one that can

00:10:28,500 --> 00:10:32,700
handle that and that does whatever it

00:10:30,420 --> 00:10:34,650
needs to do to create an instance a

00:10:32,700 --> 00:10:36,510
service instance in our reference

00:10:34,650 --> 00:10:37,980
implementation for set for a couple of

00:10:36,510 --> 00:10:39,660
reasons we just made it do something

00:10:37,980 --> 00:10:43,440
really dumb so it creates a top-level

00:10:39,660 --> 00:10:45,390
folder named using the service ID but we

00:10:43,440 --> 00:10:46,230
envisage production versions of this

00:10:45,390 --> 00:10:49,890
doing something slightly more

00:10:46,230 --> 00:10:51,390
sophisticated than that obviously then

00:10:49,890 --> 00:10:53,490
when you actually want to bind it you

00:10:51,390 --> 00:10:55,320
just issue a CF bind call Cloud

00:10:53,490 --> 00:10:57,000
Controller receives that force that on

00:10:55,320 --> 00:10:59,430
to the volume specific service broker

00:10:57,000 --> 00:11:01,260
the best way to think about this is the

00:10:59,430 --> 00:11:03,329
in response the volume specific service

00:11:01,260 --> 00:11:05,300
broker issues one or more volume mount

00:11:03,329 --> 00:11:07,250
instructions back to cloud file

00:11:05,300 --> 00:11:09,230
Cloud Foundry then takes them and does a

00:11:07,250 --> 00:11:12,439
couple of things first thing it does is

00:11:09,230 --> 00:11:14,449
BBS talks to volume manager and the

00:11:12,439 --> 00:11:17,480
volume manager talks to the volume

00:11:14,449 --> 00:11:19,730
specific driver on the cell to perform

00:11:17,480 --> 00:11:22,579
the actual mount on to the VM on to the

00:11:19,730 --> 00:11:24,680
cell itself and then secondly whoops

00:11:22,579 --> 00:11:27,019
wrong way and then secondly BBS

00:11:24,680 --> 00:11:29,540
instructs garden to take that volume out

00:11:27,019 --> 00:11:32,569
we just did at cell level and mount it

00:11:29,540 --> 00:11:34,970
into the container that's going to host

00:11:32,569 --> 00:11:36,589
the app that you're pushing and that's

00:11:34,970 --> 00:11:38,660
pretty much it so you can see it's a

00:11:36,589 --> 00:11:40,579
fairly simple picture but whilst it's

00:11:38,660 --> 00:11:42,620
simple we think that this thing caters

00:11:40,579 --> 00:11:44,149
for all of the current use cases and all

00:11:42,620 --> 00:11:45,680
of the future use cases we've got moving

00:11:44,149 --> 00:11:46,490
forward I'm gonna hand you over to Ted

00:11:45,680 --> 00:11:50,300
who's going to talk a little bit about

00:11:46,490 --> 00:11:52,279
those yeah thanks Paul so let's look at

00:11:50,300 --> 00:11:54,589
a couple specific examples to really

00:11:52,279 --> 00:11:57,620
drive home how this works exactly the

00:11:54,589 --> 00:11:59,060
same way other services work so let's

00:11:57,620 --> 00:12:00,920
talk about blank storage this is the

00:11:59,060 --> 00:12:02,779
most obvious example you have an

00:12:00,920 --> 00:12:04,730
application let's say it's a large

00:12:02,779 --> 00:12:07,910
wordpress application you have a very

00:12:04,730 --> 00:12:09,740
high traffic blog and you'd like to use

00:12:07,910 --> 00:12:11,240
WordPress on Cloud Foundry but you'd

00:12:09,740 --> 00:12:12,889
like to use it the way WordPress was

00:12:11,240 --> 00:12:15,199
intended to be used which is to have a

00:12:12,889 --> 00:12:16,819
site administrator be able to install

00:12:15,199 --> 00:12:18,709
themes and plugins directly through the

00:12:16,819 --> 00:12:21,350
admin panel rather than having a

00:12:18,709 --> 00:12:23,149
developer do that for them you'd also

00:12:21,350 --> 00:12:24,649
like to dynamically scale the write load

00:12:23,149 --> 00:12:26,809
in other words it's not enough just to

00:12:24,649 --> 00:12:28,970
cache the responses from WordPress you

00:12:26,809 --> 00:12:31,100
also have a lively comics comment

00:12:28,970 --> 00:12:32,750
session maybe a forum something that has

00:12:31,100 --> 00:12:34,790
a write load that can spike and so you'd

00:12:32,750 --> 00:12:36,439
like to be able to scale horizontally so

00:12:34,790 --> 00:12:38,420
Cloud Foundry allows you to do that

00:12:36,439 --> 00:12:42,079
provided you have a distributed file

00:12:38,420 --> 00:12:44,870
system in your service marketplace so if

00:12:42,079 --> 00:12:47,750
we look at pushing WordPress and then

00:12:44,870 --> 00:12:50,990
creating a cephus service that attaches

00:12:47,750 --> 00:12:53,000
to it and then scaling WordPress up you

00:12:50,990 --> 00:12:55,879
can see works like any other service and

00:12:53,000 --> 00:12:58,370
we focus on the calls we're specifically

00:12:55,879 --> 00:13:00,829
making to the service broker you can see

00:12:58,370 --> 00:13:04,009
we're calling create service with a

00:13:00,829 --> 00:13:05,809
premium plan there's some storage

00:13:04,009 --> 00:13:08,089
specific options here but these are

00:13:05,809 --> 00:13:09,769
specific to this service so it's it

00:13:08,089 --> 00:13:11,899
wants to know which storage tier you'd

00:13:09,769 --> 00:13:14,269
like do you want SSD or spinning

00:13:11,899 --> 00:13:16,399
platters do you want any kind of outer

00:13:14,269 --> 00:13:18,110
band backup to be happening this

00:13:16,399 --> 00:13:19,910
particular service broker provides

00:13:18,110 --> 00:13:21,769
we back up so we're turning that on and

00:13:19,910 --> 00:13:24,170
then when you bind it to your

00:13:21,769 --> 00:13:26,000
application you're asking the service

00:13:24,170 --> 00:13:27,890
broker to bind it to a specific mount

00:13:26,000 --> 00:13:29,570
point within your application so for

00:13:27,890 --> 00:13:31,040
WordPress that's the WP content

00:13:29,570 --> 00:13:35,029
directory that's well the themes and

00:13:31,040 --> 00:13:37,279
plugins go it then you start WordPress

00:13:35,029 --> 00:13:40,670
and then you scale it and it works just

00:13:37,279 --> 00:13:43,490
like usual let's go through another

00:13:40,670 --> 00:13:46,190
example so this is an example of working

00:13:43,490 --> 00:13:49,750
with an external data set so to set this

00:13:46,190 --> 00:13:52,730
up imagine you have satellite images and

00:13:49,750 --> 00:13:53,990
every day new satellite images come into

00:13:52,730 --> 00:13:56,870
your system and you've been doing this

00:13:53,990 --> 00:13:59,329
for years and those images are stored on

00:13:56,870 --> 00:14:01,910
a variety of storage backends the oldest

00:13:59,329 --> 00:14:04,519
stuff is still on tape drive you've got

00:14:01,910 --> 00:14:06,350
things in EMC Isilon you've got

00:14:04,519 --> 00:14:08,720
something new maybe that you're rolling

00:14:06,350 --> 00:14:11,630
out tomorrow you're gonna add to that

00:14:08,720 --> 00:14:14,990
cluster and currently your developers

00:14:11,630 --> 00:14:18,320
have to know where these things live if

00:14:14,990 --> 00:14:20,660
they want to access them and you'd like

00:14:18,320 --> 00:14:23,029
to move those data processing workloads

00:14:20,660 --> 00:14:24,649
on to cloud foundry but you can't move

00:14:23,029 --> 00:14:27,350
all of them onto Cloud Foundry some of

00:14:24,649 --> 00:14:31,279
them still need to be running in virtual

00:14:27,350 --> 00:14:32,630
machines or in some older back-end so

00:14:31,279 --> 00:14:35,510
how would you do that will you do it

00:14:32,630 --> 00:14:37,940
with a custom service broker so if we're

00:14:35,510 --> 00:14:39,800
pushing an image processor app to Cloud

00:14:37,940 --> 00:14:42,320
Foundry and then binding it to our

00:14:39,800 --> 00:14:44,600
satellite image service broker and we

00:14:42,320 --> 00:14:45,829
look at the options that were talking

00:14:44,600 --> 00:14:48,050
about when we buy into that service

00:14:45,829 --> 00:14:50,779
broker we're saying we want to bind to

00:14:48,050 --> 00:14:52,610
the daily snapshots we're interested in

00:14:50,779 --> 00:14:54,320
some image sets specifically we're

00:14:52,610 --> 00:14:55,910
interested in the four-band images and

00:14:54,320 --> 00:14:58,430
the multispectrum images for this

00:14:55,910 --> 00:15:00,709
particular use case and then we bind it

00:14:58,430 --> 00:15:02,899
to application we're giving it a

00:15:00,709 --> 00:15:04,730
top-level data path but our expectation

00:15:02,899 --> 00:15:06,730
is this going to mount multiple things

00:15:04,730 --> 00:15:09,410
under that is going to amount the

00:15:06,730 --> 00:15:11,930
ingestion data the input in a read-only

00:15:09,410 --> 00:15:14,420
mount is going to mount the output

00:15:11,930 --> 00:15:16,670
somewhere else and then possibly mount

00:15:14,420 --> 00:15:21,019
as some local scratch space in which to

00:15:16,670 --> 00:15:22,970
do our ETL workload then we start our

00:15:21,019 --> 00:15:23,329
image processor and we're off to the

00:15:22,970 --> 00:15:26,029
races

00:15:23,329 --> 00:15:27,890
now notice there is nothing in this

00:15:26,029 --> 00:15:31,279
create service or bind service call

00:15:27,890 --> 00:15:33,649
where I talked about what

00:15:31,279 --> 00:15:34,999
type of service was running back there

00:15:33,649 --> 00:15:37,579
I'm not talking about

00:15:34,999 --> 00:15:38,269
please mount this thing from this Isilon

00:15:37,579 --> 00:15:39,860
cluster

00:15:38,269 --> 00:15:41,600
please mount this thing from the tape

00:15:39,860 --> 00:15:43,069
drive is the application developer you

00:15:41,600 --> 00:15:45,740
don't really care about those details

00:15:43,069 --> 00:15:47,930
you just care about I want the four band

00:15:45,740 --> 00:15:50,389
daily snapshots and so the service

00:15:47,930 --> 00:15:51,230
broker is capable of isolating all that

00:15:50,389 --> 00:15:53,540
information from the application

00:15:51,230 --> 00:15:55,009
developer because ultimately it's the

00:15:53,540 --> 00:15:58,009
thing that's in charge of the volume

00:15:55,009 --> 00:16:00,170
mounts so I think this shows that this

00:15:58,009 --> 00:16:02,420
clean separation using service brokers

00:16:00,170 --> 00:16:04,399
really allows you to continue operating

00:16:02,420 --> 00:16:05,870
up at the domain level rather than

00:16:04,399 --> 00:16:09,949
delving into the implementation details

00:16:05,870 --> 00:16:11,449
of how the service actually works all

00:16:09,949 --> 00:16:13,160
right so let's talk about the future

00:16:11,449 --> 00:16:14,360
we've talked about distributed file

00:16:13,160 --> 00:16:16,639
systems and that's what we have

00:16:14,360 --> 00:16:18,350
available today but there's two other

00:16:16,639 --> 00:16:20,720
kinds of storage that we're looking at

00:16:18,350 --> 00:16:24,319
one is local scratch space and the other

00:16:20,720 --> 00:16:27,499
is single attach volumes so what is

00:16:24,319 --> 00:16:29,120
local scratch space well you have a

00:16:27,499 --> 00:16:30,620
ephemeral space available to your app

00:16:29,120 --> 00:16:32,870
but that's running on a layered file

00:16:30,620 --> 00:16:34,100
system in kind of a shared environment

00:16:32,870 --> 00:16:36,649
and it's not necessarily all that

00:16:34,100 --> 00:16:38,420
performant for heavy readwrite workloads

00:16:36,649 --> 00:16:40,129
and you might be running an environment

00:16:38,420 --> 00:16:40,790
where you have better disable and you'd

00:16:40,129 --> 00:16:42,829
like to make those available

00:16:40,790 --> 00:16:46,250
specifically to the apps that have these

00:16:42,829 --> 00:16:48,769
heavy readwrite workloads so we plan on

00:16:46,250 --> 00:16:51,199
extending the docker volume plug-in

00:16:48,769 --> 00:16:54,410
interface to allow drivers to advertise

00:16:51,199 --> 00:16:55,759
local resources and to have Diego be

00:16:54,410 --> 00:16:58,610
able to take advantage of them

00:16:55,759 --> 00:17:00,620
so in this case every new instance would

00:16:58,610 --> 00:17:03,290
get a new volume whenever it's spun up

00:17:00,620 --> 00:17:05,419
and when the application instance is

00:17:03,290 --> 00:17:07,189
spun down that volume would be reaped

00:17:05,419 --> 00:17:08,959
and recycled so there would be no

00:17:07,189 --> 00:17:11,329
permanent data but there would be

00:17:08,959 --> 00:17:12,829
temporary scratch space and this works

00:17:11,329 --> 00:17:15,140
nicely with our current stateless

00:17:12,829 --> 00:17:17,870
environment taking that a step further

00:17:15,140 --> 00:17:19,250
you have single attached volumes so this

00:17:17,870 --> 00:17:21,079
is same thing as scratch space but

00:17:19,250 --> 00:17:23,510
you're saying I would like when I start

00:17:21,079 --> 00:17:25,549
the application instance again for it to

00:17:23,510 --> 00:17:27,350
reattach to the same piece of data and

00:17:25,549 --> 00:17:29,659
to have there be a consistent

00:17:27,350 --> 00:17:34,010
relationship between my app instance and

00:17:29,659 --> 00:17:35,780
the application data this is something

00:17:34,010 --> 00:17:37,940
that we can build from a persistence

00:17:35,780 --> 00:17:39,970
layer but it's problematic on a couple

00:17:37,940 --> 00:17:42,500
other layers notably

00:17:39,970 --> 00:17:43,380
you're now bringing identity and state

00:17:42,500 --> 00:17:45,780
into the mix

00:17:43,380 --> 00:17:47,580
onto a platform that from my primarily

00:17:45,780 --> 00:17:50,430
associated with running stateless

00:17:47,580 --> 00:17:52,440
application logic so in order to do that

00:17:50,430 --> 00:17:56,190
we would need to extend this scheduler

00:17:52,440 --> 00:17:57,810
to allow for consistency so a consistent

00:17:56,190 --> 00:18:00,720
identity can be maintained across

00:17:57,810 --> 00:18:02,880
restarts and also you would need to be

00:18:00,720 --> 00:18:06,030
mounting things that could somehow take

00:18:02,880 --> 00:18:07,530
advantage of these volumes and mostly

00:18:06,030 --> 00:18:09,990
what people think about in those cases

00:18:07,530 --> 00:18:12,000
are databases but the problem with

00:18:09,990 --> 00:18:13,830
databases is the ones that people want

00:18:12,000 --> 00:18:15,630
to run in production today they don't

00:18:13,830 --> 00:18:17,550
really cleanly support a separation

00:18:15,630 --> 00:18:19,560
between DevOps the person operating the

00:18:17,550 --> 00:18:21,090
database and cloud ops the person

00:18:19,560 --> 00:18:24,030
operating the machine the database is

00:18:21,090 --> 00:18:24,750
running on pushing CDs to that things of

00:18:24,030 --> 00:18:27,300
that nature

00:18:24,750 --> 00:18:29,010
so there's a couple other problems that

00:18:27,300 --> 00:18:30,810
need to be solved beyond just

00:18:29,010 --> 00:18:32,850
persistence in order to make the single

00:18:30,810 --> 00:18:34,800
attach volume use case work but we're

00:18:32,850 --> 00:18:37,320
very interested in that on Cloud Foundry

00:18:34,800 --> 00:18:39,450
so you'll see further discussion coming

00:18:37,320 --> 00:18:40,830
in the future about how we can make that

00:18:39,450 --> 00:18:42,840
happen and in fact the container

00:18:40,830 --> 00:18:46,230
networking team down in LA is already

00:18:42,840 --> 00:18:48,480
starting to work on this problem so to

00:18:46,230 --> 00:18:50,580
sum up we currently support distributed

00:18:48,480 --> 00:18:53,340
file systems will soon be adding support

00:18:50,580 --> 00:18:55,050
for scratch space and then we eventually

00:18:53,340 --> 00:18:58,080
hope to support single attach block

00:18:55,050 --> 00:19:00,720
devices and that's where we're at if you

00:18:58,080 --> 00:19:02,700
have any questions feel free to ask them

00:19:00,720 --> 00:19:04,970
at the microphone here or find us after

00:19:02,700 --> 00:19:04,970
the show

00:19:05,340 --> 00:19:11,459

YouTube URL: https://www.youtube.com/watch?v=ajNoPi1uMjQ


