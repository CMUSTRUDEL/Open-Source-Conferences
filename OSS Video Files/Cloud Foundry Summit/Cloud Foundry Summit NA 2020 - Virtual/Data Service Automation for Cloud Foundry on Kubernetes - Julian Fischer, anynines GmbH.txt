Title: Data Service Automation for Cloud Foundry on Kubernetes - Julian Fischer, anynines GmbH
Publication date: 2020-06-22
Playlist: Cloud Foundry Summit NA 2020 - Virtual
Description: 
	Data Service Automation for Cloud Foundry on Kubernetes - Julian Fischer, anynines GmbH 

For more info: https://www.cloudfoundry.org/ 

Running Cloud Foundry on Kubernetes implies that the lifecycle management tool BOSH becomes obsolete. A reason to look at the future of data service automation for Cloud Foundry. In this session you will learn how the entire lifecycle of on-demand provisioning data services can be automated to serve thousands of data service instances of various types. An open source data service framework covering large swathes of the lifecycle automation will be drafted. See how existing technologies such as Operators and Helm can be of use. Furthermore see how the integration with both Kubernetes CRDs and the Open Service Broker API can be combined. At the end of this session you will have gained a conceptual understanding of how to automate stateful workloads for future Cloud Foundry environments.
Captions: 
	00:00:00,160 --> 00:00:04,400
hello and welcome to this talk about

00:00:02,639 --> 00:00:05,040
data service automation for cloud

00:00:04,400 --> 00:00:09,040
foundry

00:00:05,040 --> 00:00:12,559
on kubernetes a few words about me i'm

00:00:09,040 --> 00:00:15,280
julian fisher um ceo of a company called

00:00:12,559 --> 00:00:16,560
any nines we are building platforms for

00:00:15,280 --> 00:00:20,320
living

00:00:16,560 --> 00:00:23,840
we have history in cloud foundry

00:00:20,320 --> 00:00:25,760
and we are now moving along with the

00:00:23,840 --> 00:00:29,359
community to explore kubernetes

00:00:25,760 --> 00:00:32,399
we've also been automating data services

00:00:29,359 --> 00:00:33,120
as part of our platform uh for four

00:00:32,399 --> 00:00:36,239
years

00:00:33,120 --> 00:00:38,960
so there's a set of data services um

00:00:36,239 --> 00:00:40,480
based on on board automation performing

00:00:38,960 --> 00:00:41,920
on-demand provisioning of dedicated

00:00:40,480 --> 00:00:44,079
service instances

00:00:41,920 --> 00:00:45,840
so this is where a lot of the content of

00:00:44,079 --> 00:00:49,760
the talk is inspired by

00:00:45,840 --> 00:00:53,120
actual work that um we are

00:00:49,760 --> 00:00:54,160
ongoingly do so the data service

00:00:53,120 --> 00:00:55,600
automation topic

00:00:54,160 --> 00:00:57,840
is something that i've been speaking

00:00:55,600 --> 00:01:00,000
about for several years so if you're

00:00:57,840 --> 00:01:03,199
familiar with prior talks

00:01:00,000 --> 00:01:04,320
that's good i'll try to explain basic

00:01:03,199 --> 00:01:07,040
concepts

00:01:04,320 --> 00:01:07,040
along the talk

00:01:07,439 --> 00:01:10,960
if you think about data service

00:01:09,119 --> 00:01:12,320
automation in the context of cloud

00:01:10,960 --> 00:01:14,400
foundry

00:01:12,320 --> 00:01:16,000
that's one thing if you're thinking

00:01:14,400 --> 00:01:16,960
about cloud foundry that runs on top of

00:01:16,000 --> 00:01:19,680
kubernetes

00:01:16,960 --> 00:01:20,479
that is a different thing so the context

00:01:19,680 --> 00:01:24,880
of this talk

00:01:20,479 --> 00:01:27,040
just to be clear is

00:01:24,880 --> 00:01:29,680
we're looking into autumn the automation

00:01:27,040 --> 00:01:31,520
of many different data services so it's

00:01:29,680 --> 00:01:34,320
not about a particular database alone

00:01:31,520 --> 00:01:38,079
i'm using postgres examples excessively

00:01:34,320 --> 00:01:40,159
but it's about i think to this point

00:01:38,079 --> 00:01:41,920
uh we alone have eight or ten different

00:01:40,159 --> 00:01:44,159
data services because customers are

00:01:41,920 --> 00:01:46,479
asking for different data services so

00:01:44,159 --> 00:01:48,479
if you're a large corporate uh it is

00:01:46,479 --> 00:01:50,320
very likely that data service automation

00:01:48,479 --> 00:01:52,960
is not a problem

00:01:50,320 --> 00:01:54,240
that restricts to a particular data

00:01:52,960 --> 00:01:57,680
service but a set

00:01:54,240 --> 00:01:59,520
of data services and

00:01:57,680 --> 00:02:01,280
it's interesting to see that a lot of

00:01:59,520 --> 00:02:03,360
existing data solutions

00:02:01,280 --> 00:02:04,640
hyper focus on one data service and one

00:02:03,360 --> 00:02:06,399
data service alone

00:02:04,640 --> 00:02:08,080
missing the opportunity to share

00:02:06,399 --> 00:02:11,200
automation code

00:02:08,080 --> 00:02:12,879
among them so on-demand provisioning of

00:02:11,200 --> 00:02:15,680
dedicated service instances

00:02:12,879 --> 00:02:17,120
is the next constraint we have in this

00:02:15,680 --> 00:02:20,080
talk because

00:02:17,120 --> 00:02:21,599
you could also share a postgres example

00:02:20,080 --> 00:02:24,560
by

00:02:21,599 --> 00:02:25,200
serving databases of a particular data

00:02:24,560 --> 00:02:28,400
service

00:02:25,200 --> 00:02:30,800
server postgres server

00:02:28,400 --> 00:02:32,319
to various clients well while this is

00:02:30,800 --> 00:02:34,480
possible to some degree

00:02:32,319 --> 00:02:36,400
if you do that as a massive at a massive

00:02:34,480 --> 00:02:39,040
scale this won't scale very much

00:02:36,400 --> 00:02:40,720
you'll run into issues such as noisy

00:02:39,040 --> 00:02:44,080
neighborhood

00:02:40,720 --> 00:02:46,720
and and alike so

00:02:44,080 --> 00:02:47,120
it it is the leading paradigm of these

00:02:46,720 --> 00:02:50,640
days

00:02:47,120 --> 00:02:52,959
to provision databases as dedicated

00:02:50,640 --> 00:02:53,760
resources it could be a vm could be a

00:02:52,959 --> 00:02:57,840
set of vm

00:02:53,760 --> 00:03:00,400
could be a port could be set of ports um

00:02:57,840 --> 00:03:01,360
platforms especially if they are big a

00:03:00,400 --> 00:03:04,239
bit bigger

00:03:01,360 --> 00:03:05,840
they tend to be global that means you

00:03:04,239 --> 00:03:06,640
often have different infrastructures

00:03:05,840 --> 00:03:10,239
you'll

00:03:06,640 --> 00:03:13,519
serve for example

00:03:10,239 --> 00:03:16,000
azure or aws ali cloud

00:03:13,519 --> 00:03:17,440
or even on premise infrastructure such

00:03:16,000 --> 00:03:20,800
as vmware

00:03:17,440 --> 00:03:24,000
or openstack

00:03:20,800 --> 00:03:27,360
and and therefore a platform product and

00:03:24,000 --> 00:03:30,840
data service automation product

00:03:27,360 --> 00:03:32,560
should be able to work on all those

00:03:30,840 --> 00:03:34,799
infrastructures

00:03:32,560 --> 00:03:35,680
as you can see with cloud foundry and

00:03:34,799 --> 00:03:37,840
kubernetes

00:03:35,680 --> 00:03:40,480
it is very likely that in your

00:03:37,840 --> 00:03:42,319
organization you want to use both

00:03:40,480 --> 00:03:44,239
some workloads are more suitable for

00:03:42,319 --> 00:03:45,840
running in a kubernetes clusters

00:03:44,239 --> 00:03:48,080
in a kubernetes cluster and other

00:03:45,840 --> 00:03:49,040
workloads will have an affinity towards

00:03:48,080 --> 00:03:51,360
cloud foundry

00:03:49,040 --> 00:03:52,959
so whatever we build when it comes to

00:03:51,360 --> 00:03:55,360
data service automation should integrate

00:03:52,959 --> 00:03:58,080
with both

00:03:55,360 --> 00:04:00,080
um large organizations large platforms

00:03:58,080 --> 00:04:01,519
they come with overhead and therefore it

00:04:00,080 --> 00:04:03,599
is very likely that you will have

00:04:01,519 --> 00:04:05,200
thousands of data service instances

00:04:03,599 --> 00:04:06,879
otherwise you don't have the economy of

00:04:05,200 --> 00:04:09,360
scale to make them

00:04:06,879 --> 00:04:11,519
economically viable so we're looking at

00:04:09,360 --> 00:04:13,920
thousands of data service instances

00:04:11,519 --> 00:04:15,200
in the average case uh striving for full

00:04:13,920 --> 00:04:17,759
life cycle automation

00:04:15,200 --> 00:04:18,479
is imperative if you're looking at

00:04:17,759 --> 00:04:21,280
scales

00:04:18,479 --> 00:04:22,079
like that unless you have plenty of

00:04:21,280 --> 00:04:26,320
operators

00:04:22,079 --> 00:04:28,400
sitting around as

00:04:26,320 --> 00:04:30,160
the life cycle automation is not only

00:04:28,400 --> 00:04:31,759
meant for the platform operator but also

00:04:30,160 --> 00:04:33,680
for the application developer who does

00:04:31,759 --> 00:04:35,199
not necessarily want to wait for

00:04:33,680 --> 00:04:37,040
a platform operator to provision the

00:04:35,199 --> 00:04:40,400
database so i think

00:04:37,040 --> 00:04:44,479
in 2020 we are beyond this

00:04:40,400 --> 00:04:47,360
on the monster service rules um

00:04:44,479 --> 00:04:48,800
so cloud foundry on bosch if you're

00:04:47,360 --> 00:04:49,520
running cloud foundry the traditional

00:04:48,800 --> 00:04:52,720
way

00:04:49,520 --> 00:04:55,759
well which is absolutely fine

00:04:52,720 --> 00:04:56,080
then data service automation on bosch is

00:04:55,759 --> 00:04:59,040
also

00:04:56,080 --> 00:04:59,360
fine so you can still use that and even

00:04:59,040 --> 00:05:01,520
in

00:04:59,360 --> 00:05:04,000
certain cases there's there's good

00:05:01,520 --> 00:05:07,120
reason to keep on using that for example

00:05:04,000 --> 00:05:08,960
the increased isolation level

00:05:07,120 --> 00:05:11,360
that virtual machines provide in

00:05:08,960 --> 00:05:12,560
contrast to pods

00:05:11,360 --> 00:05:15,360
if you're using cloud foundry and

00:05:12,560 --> 00:05:17,919
kubernetes but you still have a

00:05:15,360 --> 00:05:19,680
bosch with access to a virtual machine

00:05:17,919 --> 00:05:22,320
orchestration well then nothing has

00:05:19,680 --> 00:05:25,759
changed for you either

00:05:22,320 --> 00:05:27,120
so this talk basically uh focuses on the

00:05:25,759 --> 00:05:29,039
scenario where you have a pure

00:05:27,120 --> 00:05:31,199
kubernetes environment and for whatever

00:05:29,039 --> 00:05:34,960
reason you don't want to have a bosch

00:05:31,199 --> 00:05:37,440
and you would like to lift or

00:05:34,960 --> 00:05:38,639
delegate the heavy lifting for data

00:05:37,440 --> 00:05:41,600
service automation

00:05:38,639 --> 00:05:43,680
to kubernetes so that's the scope and as

00:05:41,600 --> 00:05:47,039
i said there are already a few ifs

00:05:43,680 --> 00:05:48,400
to get to that particular context

00:05:47,039 --> 00:05:51,039
so data service automation with

00:05:48,400 --> 00:05:54,560
kubernetes what's to be

00:05:51,039 --> 00:05:56,639
said about it first of all

00:05:54,560 --> 00:05:57,600
we are in a cloud-founded context so i

00:05:56,639 --> 00:05:59,600
assume that there

00:05:57,600 --> 00:06:00,800
is a cloud foundry and if there's a

00:05:59,600 --> 00:06:03,520
cloud foundry

00:06:00,800 --> 00:06:04,319
then you want to have the cf create

00:06:03,520 --> 00:06:06,240
service

00:06:04,319 --> 00:06:08,560
experience which ties to having a

00:06:06,240 --> 00:06:10,000
service broker that complies to the open

00:06:08,560 --> 00:06:13,280
service broker api

00:06:10,000 --> 00:06:15,120
so nothing has changed there

00:06:13,280 --> 00:06:16,720
the open service broker api for those

00:06:15,120 --> 00:06:19,840
who are not familiar with it

00:06:16,720 --> 00:06:21,759
postulates two different infra two

00:06:19,840 --> 00:06:24,560
different resources

00:06:21,759 --> 00:06:26,479
a service instance is basically what

00:06:24,560 --> 00:06:28,800
your

00:06:26,479 --> 00:06:30,400
data service instance is going to

00:06:28,800 --> 00:06:33,440
represent

00:06:30,400 --> 00:06:35,520
so if you're looking into postgres

00:06:33,440 --> 00:06:36,560
then a postgres instance would be a

00:06:35,520 --> 00:06:39,600
specific

00:06:36,560 --> 00:06:42,000
postgres that you can use as an

00:06:39,600 --> 00:06:44,000
application developer

00:06:42,000 --> 00:06:45,759
in the service broker api it's not

00:06:44,000 --> 00:06:47,520
specified whether this is a shared

00:06:45,759 --> 00:06:49,599
database

00:06:47,520 --> 00:06:51,520
from a from a shared postgres cluster or

00:06:49,599 --> 00:06:53,280
whether this is a dedicated

00:06:51,520 --> 00:06:55,440
uh virtual machine or a set of virtual

00:06:53,280 --> 00:06:56,240
machines it's up to the implementation

00:06:55,440 --> 00:06:59,520
to decide

00:06:56,240 --> 00:07:02,080
what it actually is

00:06:59,520 --> 00:07:03,360
representing such a service instance the

00:07:02,080 --> 00:07:07,199
other resource that

00:07:03,360 --> 00:07:09,759
often is used is a service binding

00:07:07,199 --> 00:07:11,919
which represents the corresponding

00:07:09,759 --> 00:07:14,960
concept in the in cloud foundry

00:07:11,919 --> 00:07:17,199
the connection between an application

00:07:14,960 --> 00:07:18,639
and its service so if you have an

00:07:17,199 --> 00:07:19,280
application that wants to access a

00:07:18,639 --> 00:07:20,880
postgres

00:07:19,280 --> 00:07:22,479
you will create a service instance for

00:07:20,880 --> 00:07:24,080
postgres and then you will bind your

00:07:22,479 --> 00:07:27,680
application in the service binding to

00:07:24,080 --> 00:07:27,680
that particular postgres instance

00:07:28,000 --> 00:07:34,560
so as i mentioned um

00:07:31,599 --> 00:07:35,919
the service broker design for on-demand

00:07:34,560 --> 00:07:39,599
service broker

00:07:35,919 --> 00:07:42,560
um that that will use bosch for

00:07:39,599 --> 00:07:44,000
creating managed virtual machines is uh

00:07:42,560 --> 00:07:46,080
rather well known so i'm using

00:07:44,000 --> 00:07:48,160
that as an example if you're not

00:07:46,080 --> 00:07:49,680
familiar with that concept i will try to

00:07:48,160 --> 00:07:52,720
uh introduce a bit

00:07:49,680 --> 00:07:55,520
um but you can also look into

00:07:52,720 --> 00:07:56,400
uh prior talks held at the cloud foundry

00:07:55,520 --> 00:07:58,479
summits

00:07:56,400 --> 00:07:59,919
uh they will they will provide you with

00:07:58,479 --> 00:08:03,599
a necessary background

00:07:59,919 --> 00:08:07,120
so in

00:08:03,599 --> 00:08:09,919
in the context of bosch

00:08:07,120 --> 00:08:10,879
a service instance with on-demand

00:08:09,919 --> 00:08:12,960
dedicated

00:08:10,879 --> 00:08:15,360
provisioning with on-demand provisioning

00:08:12,960 --> 00:08:17,039
of dedicated service instances

00:08:15,360 --> 00:08:19,199
a service instance maps to bosch

00:08:17,039 --> 00:08:21,280
deployment which in turn

00:08:19,199 --> 00:08:23,280
will create a virtual machine or a setup

00:08:21,280 --> 00:08:24,560
virtual machine representing the service

00:08:23,280 --> 00:08:28,160
instance

00:08:24,560 --> 00:08:28,800
the service binding will create a

00:08:28,160 --> 00:08:31,599
username

00:08:28,800 --> 00:08:32,560
password will be will have to be stored

00:08:31,599 --> 00:08:35,120
somewhere

00:08:32,560 --> 00:08:38,000
let's say your service broker ideally in

00:08:35,120 --> 00:08:41,599
an encrypted way

00:08:38,000 --> 00:08:45,200
um so this is what service brokers um

00:08:41,599 --> 00:08:47,920
are in the on-demand world with

00:08:45,200 --> 00:08:50,000
uh using bosch so in our case uh we have

00:08:47,920 --> 00:08:53,360
a service broker that has dependency to

00:08:50,000 --> 00:08:55,519
our own postgres so it will store

00:08:53,360 --> 00:08:58,080
this date for example what are the

00:08:55,519 --> 00:08:59,839
service instances that have been created

00:08:58,080 --> 00:09:01,600
what is my service catalog meta

00:08:59,839 --> 00:09:03,120
information look like and

00:09:01,600 --> 00:09:05,680
what are the service spending bindings

00:09:03,120 --> 00:09:10,399
that exist they will all be stored

00:09:05,680 --> 00:09:10,399
in the service brokers backing service

00:09:10,560 --> 00:09:14,720
if you translate that or if you migrate

00:09:12,480 --> 00:09:18,080
that concept into kubernetes

00:09:14,720 --> 00:09:20,240
how is it going to look like

00:09:18,080 --> 00:09:21,760
that's the exciting question and

00:09:20,240 --> 00:09:23,760
obviously you won't be

00:09:21,760 --> 00:09:24,800
you won't orchestrate virtual machines

00:09:23,760 --> 00:09:28,320
um but

00:09:24,800 --> 00:09:29,040
uh ports instead um so instead of

00:09:28,320 --> 00:09:31,519
creating

00:09:29,040 --> 00:09:33,680
a virtual machine for a post press you

00:09:31,519 --> 00:09:36,399
will create a part for postcrest

00:09:33,680 --> 00:09:37,360
if uh your postgres should be clustered

00:09:36,399 --> 00:09:40,880
you'll create

00:09:37,360 --> 00:09:42,480
create a set of pods which leads to the

00:09:40,880 --> 00:09:46,399
interesting question

00:09:42,480 --> 00:09:50,560
how exactly are you doing that

00:09:46,399 --> 00:09:53,760
with using kubernetes resources

00:09:50,560 --> 00:09:54,080
and while there are several options you

00:09:53,760 --> 00:09:56,000
can

00:09:54,080 --> 00:09:57,680
you can possibly use i think one of the

00:09:56,000 --> 00:10:00,399
more

00:09:57,680 --> 00:10:01,519
obvious choices would be to represent

00:10:00,399 --> 00:10:04,800
the service instance

00:10:01,519 --> 00:10:08,079
as a composite of of different

00:10:04,800 --> 00:10:08,079
kubernetes resources

00:10:08,160 --> 00:10:12,000
including a headless service stateful

00:10:11,120 --> 00:10:16,000
sets

00:10:12,000 --> 00:10:20,160
config maps secrets and jobs

00:10:16,000 --> 00:10:24,399
so if we look into these concepts

00:10:20,160 --> 00:10:27,680
for a minute the the most important

00:10:24,399 --> 00:10:31,519
uh resource to understand is

00:10:27,680 --> 00:10:33,920
the stateful set um the stateful set

00:10:31,519 --> 00:10:35,279
comes with usually comes with a headless

00:10:33,920 --> 00:10:36,560
service

00:10:35,279 --> 00:10:38,560
in kubernetes there are several

00:10:36,560 --> 00:10:41,040
resources that you could

00:10:38,560 --> 00:10:42,399
use you could directly create pods but

00:10:41,040 --> 00:10:43,680
they're also replica sets and

00:10:42,399 --> 00:10:45,519
deployments which

00:10:43,680 --> 00:10:46,720
are meant for application deployments

00:10:45,519 --> 00:10:49,120
mostly

00:10:46,720 --> 00:10:50,640
and in recent versions the concept of a

00:10:49,120 --> 00:10:52,880
stateful set has been introduced

00:10:50,640 --> 00:10:56,480
particularly designed

00:10:52,880 --> 00:11:00,320
for data services and they come with

00:10:56,480 --> 00:11:02,240
additional behavior in some

00:11:00,320 --> 00:11:03,440
sense a stateful set behaves like a

00:11:02,240 --> 00:11:05,680
replica set

00:11:03,440 --> 00:11:07,120
in in the way that you can specify a

00:11:05,680 --> 00:11:10,160
number of replicas

00:11:07,120 --> 00:11:12,959
if a part is deleted so you manually

00:11:10,160 --> 00:11:14,320
manually delete a part of a stateful set

00:11:12,959 --> 00:11:15,920
or replica set

00:11:14,320 --> 00:11:18,079
the replica set or stateful set

00:11:15,920 --> 00:11:19,360
controller recognize that and recreate

00:11:18,079 --> 00:11:22,880
the port

00:11:19,360 --> 00:11:25,360
so that's nice um but in

00:11:22,880 --> 00:11:27,600
in in a closer relationship to a

00:11:25,360 --> 00:11:28,399
deployment the stateful set also allows

00:11:27,600 --> 00:11:31,600
you to

00:11:28,399 --> 00:11:32,800
uh to perform subsequent changes to a

00:11:31,600 --> 00:11:35,040
stateful set

00:11:32,800 --> 00:11:36,000
that will perform rolling upgrades for

00:11:35,040 --> 00:11:38,160
example

00:11:36,000 --> 00:11:40,000
a behavior that's unknown to replica

00:11:38,160 --> 00:11:42,959
sets but exclusive to

00:11:40,000 --> 00:11:44,560
deployments these are otherwise

00:11:42,959 --> 00:11:46,959
excluding

00:11:44,560 --> 00:11:49,120
the stateful set so the stateful set in

00:11:46,959 --> 00:11:52,000
this regard is similar to deployment

00:11:49,120 --> 00:11:53,279
um and therefore combines behavior of a

00:11:52,000 --> 00:11:56,560
replica set

00:11:53,279 --> 00:11:56,959
and a deployment without using them by

00:11:56,560 --> 00:11:59,839
the way

00:11:56,959 --> 00:12:01,040
the stateful set controller is grading

00:11:59,839 --> 00:12:04,160
parts

00:12:01,040 --> 00:12:06,000
it also um will create you a stable

00:12:04,160 --> 00:12:07,279
network identity if you use a headless

00:12:06,000 --> 00:12:11,839
service with it

00:12:07,279 --> 00:12:14,399
and each port in a stateful set has

00:12:11,839 --> 00:12:16,560
a stable identity it has a number starts

00:12:14,399 --> 00:12:17,360
with zero so if you have three replicas

00:12:16,560 --> 00:12:20,800
it's one

00:12:17,360 --> 00:12:21,360
two three the headless set the headless

00:12:20,800 --> 00:12:24,639
service

00:12:21,360 --> 00:12:25,440
will add a non layer for load balancing

00:12:24,639 --> 00:12:28,079
so

00:12:25,440 --> 00:12:30,639
usually services in kubernetes perform

00:12:28,079 --> 00:12:32,320
layer for load balancing

00:12:30,639 --> 00:12:34,240
the headless service doesn't do that

00:12:32,320 --> 00:12:38,079
instead it creates dns entries

00:12:34,240 --> 00:12:40,880
um so

00:12:38,079 --> 00:12:42,800
you would you'd be able to refer to a

00:12:40,880 --> 00:12:44,720
particular stateful set

00:12:42,800 --> 00:12:45,920
uh by using its headless service it will

00:12:44,720 --> 00:12:47,839
resolve to

00:12:45,920 --> 00:12:49,600
the port ip addresses that are behind

00:12:47,839 --> 00:12:52,480
that stateful set

00:12:49,600 --> 00:12:53,920
um so for example three replicas your

00:12:52,480 --> 00:12:56,880
dns entry will

00:12:53,920 --> 00:12:58,160
respond with three entries uh with the

00:12:56,880 --> 00:12:59,920
ip addresses

00:12:58,160 --> 00:13:02,160
of the corresponding parts and that will

00:12:59,920 --> 00:13:03,519
respond to changes rather quickly so

00:13:02,160 --> 00:13:07,120
if a port goes down and has to be

00:13:03,519 --> 00:13:09,040
recreated the dns for entry will quickly

00:13:07,120 --> 00:13:10,800
resolve internally to your kubernetes

00:13:09,040 --> 00:13:14,160
cluster to the new pod

00:13:10,800 --> 00:13:14,160
a new ports ip address

00:13:14,399 --> 00:13:20,959
also interesting is that each of the

00:13:17,760 --> 00:13:22,880
pods with its particular number

00:13:20,959 --> 00:13:24,720
and its particular dns entries also

00:13:22,880 --> 00:13:26,320
comes with a particular persistent

00:13:24,720 --> 00:13:28,320
volume

00:13:26,320 --> 00:13:29,519
that's created through a persistent

00:13:28,320 --> 00:13:31,920
volume claim

00:13:29,519 --> 00:13:33,600
that's derived from a persistent volume

00:13:31,920 --> 00:13:36,880
claim template

00:13:33,600 --> 00:13:37,760
so this allows you to be sure that a pod

00:13:36,880 --> 00:13:40,320
number zero

00:13:37,760 --> 00:13:42,959
for example could be the primary of your

00:13:40,320 --> 00:13:45,199
uh streaming replication in postgres

00:13:42,959 --> 00:13:46,399
when it comes back it will come back

00:13:45,199 --> 00:13:49,360
with exact

00:13:46,399 --> 00:13:51,600
the same persistent volume so if there's

00:13:49,360 --> 00:13:54,480
a deviation

00:13:51,600 --> 00:13:55,040
and there's an implicit role associated

00:13:54,480 --> 00:13:58,240
with that

00:13:55,040 --> 00:13:58,959
part of the stateful set for example the

00:13:58,240 --> 00:14:00,560
primary

00:13:58,959 --> 00:14:02,720
you can be sure that it will be the

00:14:00,560 --> 00:14:05,279
primaries disc being reattached

00:14:02,720 --> 00:14:06,320
in the event of a failure which is very

00:14:05,279 --> 00:14:08,560
nice

00:14:06,320 --> 00:14:09,839
also there's ordinality so if you create

00:14:08,560 --> 00:14:12,639
the stateful set

00:14:09,839 --> 00:14:14,079
these ports of the stateful set will be

00:14:12,639 --> 00:14:16,560
created in sequence and they will be

00:14:14,079 --> 00:14:19,920
destroyed in the inverse sequence

00:14:16,560 --> 00:14:22,160
so to 1 0

00:14:19,920 --> 00:14:23,199
during deletion and 0 1 2 during

00:14:22,160 --> 00:14:26,880
creation

00:14:23,199 --> 00:14:29,040
similar things apply to updates

00:14:26,880 --> 00:14:30,000
config maps are basically key value sets

00:14:29,040 --> 00:14:33,040
that you can use

00:14:30,000 --> 00:14:36,639
to to store settings

00:14:33,040 --> 00:14:40,560
or simple configuration files

00:14:36,639 --> 00:14:43,519
these config maps can be can be

00:14:40,560 --> 00:14:44,000
mounted or brought into ports as

00:14:43,519 --> 00:14:46,079
environment

00:14:44,000 --> 00:14:47,279
variables where you have to restart the

00:14:46,079 --> 00:14:49,040
port in order

00:14:47,279 --> 00:14:51,600
to reflect upon changes of a conflict

00:14:49,040 --> 00:14:53,120
map or interestingly you can also mount

00:14:51,600 --> 00:14:55,279
them as volumes which

00:14:53,120 --> 00:14:56,399
will turn the keys of the config map

00:14:55,279 --> 00:14:58,959
into files

00:14:56,399 --> 00:14:59,839
and if your process reads them

00:14:58,959 --> 00:15:01,920
repeatedly

00:14:59,839 --> 00:15:04,079
they will also reflect upon changes of

00:15:01,920 --> 00:15:06,160
the conflict map

00:15:04,079 --> 00:15:07,360
secrets are basically also key value

00:15:06,160 --> 00:15:10,399
pairs

00:15:07,360 --> 00:15:12,079
meant for storing sensitive information

00:15:10,399 --> 00:15:13,920
interestingly by default they are not

00:15:12,079 --> 00:15:17,199
encrypted but i think

00:15:13,920 --> 00:15:20,160
the main purpose of that is to

00:15:17,199 --> 00:15:21,279
let kubernetes administrators plug in

00:15:20,160 --> 00:15:24,079
encryption

00:15:21,279 --> 00:15:27,040
uh for secrets so it's wise to use them

00:15:24,079 --> 00:15:29,279
to store sensitive information

00:15:27,040 --> 00:15:30,320
a job is a concept that allows you to

00:15:29,279 --> 00:15:33,360
create ports

00:15:30,320 --> 00:15:34,480
number of ports repeat repeatedly

00:15:33,360 --> 00:15:36,720
execute them

00:15:34,480 --> 00:15:38,720
to perform certain tasks you could for

00:15:36,720 --> 00:15:42,079
example

00:15:38,720 --> 00:15:42,800
run p sql commands against your postgres

00:15:42,079 --> 00:15:46,160
cluster

00:15:42,800 --> 00:15:48,480
if you want to so it's all these are

00:15:46,160 --> 00:15:49,360
necessary and and useful tools to have

00:15:48,480 --> 00:15:51,920
in the tool belt

00:15:49,360 --> 00:15:55,279
when performing or when doing data

00:15:51,920 --> 00:15:59,600
service automation with kubernetes

00:15:55,279 --> 00:16:01,920
so after the stateful set we also

00:15:59,600 --> 00:16:03,920
after having the service instances well

00:16:01,920 --> 00:16:06,000
which basically go down to creating

00:16:03,920 --> 00:16:09,440
stateful sets and

00:16:06,000 --> 00:16:11,040
and satellite resources around it um

00:16:09,440 --> 00:16:12,560
it's interesting how would you implement

00:16:11,040 --> 00:16:15,120
a service binding

00:16:12,560 --> 00:16:16,880
and there is no equivalent of service

00:16:15,120 --> 00:16:19,600
binding in kubernetes

00:16:16,880 --> 00:16:20,800
in conversations people ask me often uh

00:16:19,600 --> 00:16:23,519
but what about

00:16:20,800 --> 00:16:24,880
secrets are secrets close to service

00:16:23,519 --> 00:16:28,320
bindings

00:16:24,880 --> 00:16:29,839
and i would say a secret is actually a

00:16:28,320 --> 00:16:32,399
set of credentials so

00:16:29,839 --> 00:16:33,600
as a service binding also represents a

00:16:32,399 --> 00:16:37,040
set of credentials

00:16:33,600 --> 00:16:39,759
then yes they are similar but

00:16:37,040 --> 00:16:41,040
where the difference is if you create a

00:16:39,759 --> 00:16:43,120
service

00:16:41,040 --> 00:16:44,880
a binding in cloud foundry ui for

00:16:43,120 --> 00:16:48,480
example for your postgres

00:16:44,880 --> 00:16:49,040
you expect a username a user a database

00:16:48,480 --> 00:16:51,120
user

00:16:49,040 --> 00:16:52,160
and the password to be created and that

00:16:51,120 --> 00:16:55,040
for example is

00:16:52,160 --> 00:16:56,800
what the secret doesn't do for you so

00:16:55,040 --> 00:16:59,199
service binding will

00:16:56,800 --> 00:17:00,240
possibly use a secret to store the

00:16:59,199 --> 00:17:03,440
credentials

00:17:00,240 --> 00:17:04,079
and delegate the persistency to

00:17:03,440 --> 00:17:07,679
kubernetes

00:17:04,079 --> 00:17:09,600
etcd but it won't do

00:17:07,679 --> 00:17:11,439
the actual user creation for you so you

00:17:09,600 --> 00:17:14,880
have to do that yourself

00:17:11,439 --> 00:17:15,760
so you need a bit of logic to create

00:17:14,880 --> 00:17:18,240
that username

00:17:15,760 --> 00:17:20,160
and password which brings us to an

00:17:18,240 --> 00:17:23,120
interesting concept in kubernetes which

00:17:20,160 --> 00:17:25,439
is the kubernetes crd

00:17:23,120 --> 00:17:27,760
a cid allows you to specify a new data

00:17:25,439 --> 00:17:31,280
type that's owned by the kubernetes api

00:17:27,760 --> 00:17:34,640
and stored in its etcd so think of

00:17:31,280 --> 00:17:36,720
um the kubernetes

00:17:34,640 --> 00:17:37,679
objects that you can natively create for

00:17:36,720 --> 00:17:41,039
example a pod

00:17:37,679 --> 00:17:43,919
you create a specification for a part

00:17:41,039 --> 00:17:45,520
you uh cube card will apply it and then

00:17:43,919 --> 00:17:47,440
kubernetes knows that you would like to

00:17:45,520 --> 00:17:49,039
have a port with particular name using a

00:17:47,440 --> 00:17:50,799
particular image

00:17:49,039 --> 00:17:53,360
and you'll get a feedback from

00:17:50,799 --> 00:17:56,880
kubernetes immediately that

00:17:53,360 --> 00:17:58,559
yeah that that object is taken care of

00:17:56,880 --> 00:18:00,720
but it doesn't tell you anything about

00:17:58,559 --> 00:18:04,640
whether this actually worked or not

00:18:00,720 --> 00:18:07,039
because it's a declarative system and

00:18:04,640 --> 00:18:08,720
storing the information that you want to

00:18:07,039 --> 00:18:10,640
have a particular port

00:18:08,720 --> 00:18:12,960
does not mean that the port is already

00:18:10,640 --> 00:18:15,520
there because there is a controller

00:18:12,960 --> 00:18:17,039
being responsible for taking care that

00:18:15,520 --> 00:18:20,320
this resource is created

00:18:17,039 --> 00:18:21,760
afterwards so similar to that a custom

00:18:20,320 --> 00:18:24,960
resource definition

00:18:21,760 --> 00:18:27,520
allows you to describe a particular

00:18:24,960 --> 00:18:28,080
new data type to kubernetes which the

00:18:27,520 --> 00:18:30,559
kubernetes

00:18:28,080 --> 00:18:31,679
api will then take care of and also

00:18:30,559 --> 00:18:36,640
persist it for you

00:18:31,679 --> 00:18:39,120
in its etcd here's a simple example

00:18:36,640 --> 00:18:40,480
this specification describes a custom

00:18:39,120 --> 00:18:43,120
resource definition

00:18:40,480 --> 00:18:44,000
that will teach kubernetes how to store

00:18:43,120 --> 00:18:47,840
uh

00:18:44,000 --> 00:18:50,960
postgres um objects so in this case

00:18:47,840 --> 00:18:53,039
um the uh object

00:18:50,960 --> 00:18:54,000
that we would like to specify has only

00:18:53,039 --> 00:18:56,559
two attributes

00:18:54,000 --> 00:18:57,760
the number of replicas and the postgres

00:18:56,559 --> 00:19:02,080
version

00:18:57,760 --> 00:19:07,679
um you can see that we specify the name

00:19:02,080 --> 00:19:10,320
here the kind is a89 postgre square

00:19:07,679 --> 00:19:13,039
or you could name it postgresql i think

00:19:10,320 --> 00:19:15,919
i'll do that in the next slide

00:19:13,039 --> 00:19:17,280
you can also define short names for it

00:19:15,919 --> 00:19:20,720
so for example

00:19:17,280 --> 00:19:22,160
a persistent volume claim is a regular

00:19:20,720 --> 00:19:25,600
name

00:19:22,160 --> 00:19:26,559
and the short name is pvc so you can do

00:19:25,600 --> 00:19:29,600
the same thing with

00:19:26,559 --> 00:19:33,600
crds it's also possible to spy

00:19:29,600 --> 00:19:36,160
to specify uh validation rules uh but

00:19:33,600 --> 00:19:36,960
well dig into crds if you are

00:19:36,160 --> 00:19:40,480
interesting

00:19:36,960 --> 00:19:43,120
so the takeaway here is that once you

00:19:40,480 --> 00:19:45,280
apply this specification and you ask

00:19:43,120 --> 00:19:47,120
kubernetes to list the api resources is

00:19:45,280 --> 00:19:48,799
that there's a new entry showing up

00:19:47,120 --> 00:19:50,320
uh providing you with the new resource

00:19:48,799 --> 00:19:53,840
kind in this case a

00:19:50,320 --> 00:19:53,840
89's postgresql

00:19:54,000 --> 00:20:00,000
if you specify such an object

00:19:57,520 --> 00:20:01,679
you can create them objects of this kind

00:20:00,000 --> 00:20:03,039
um in this slide i renamed it to

00:20:01,679 --> 00:20:06,960
postgres because it's

00:20:03,039 --> 00:20:09,520
easier to read but

00:20:06,960 --> 00:20:11,120
despite of that it it complies to the

00:20:09,520 --> 00:20:13,679
same specification

00:20:11,120 --> 00:20:15,360
so you can declare a postgres version

00:20:13,679 --> 00:20:18,720
and the number of replicas

00:20:15,360 --> 00:20:21,840
for a postgres database for example

00:20:18,720 --> 00:20:24,000
if you apply that uh

00:20:21,840 --> 00:20:25,039
you'll create that object kubernetes api

00:20:24,000 --> 00:20:26,930
will take care of

00:20:25,039 --> 00:20:28,159
of it and stores it

00:20:26,930 --> 00:20:31,039
[Music]

00:20:28,159 --> 00:20:32,640
let's say as a desired state yes a

00:20:31,039 --> 00:20:35,200
postgres is to be created

00:20:32,640 --> 00:20:36,720
but it doesn't do anything else so the

00:20:35,200 --> 00:20:40,240
custom resource definition

00:20:36,720 --> 00:20:43,360
and can be imagined as a class

00:20:40,240 --> 00:20:45,039
in java and the objects you create

00:20:43,360 --> 00:20:47,600
are like the objects of the class in

00:20:45,039 --> 00:20:50,799
java but there's no behavior

00:20:47,600 --> 00:20:52,559
in order to do to add behavior to that

00:20:50,799 --> 00:20:57,919
you need a controller

00:20:52,559 --> 00:20:57,919
so cid controllers here's an example

00:20:58,080 --> 00:21:02,480
we'll have to take care that if you hand

00:21:00,240 --> 00:21:05,360
them over the specification of whatever

00:21:02,480 --> 00:21:07,520
the resource it will dispatch that into

00:21:05,360 --> 00:21:09,600
the usual cut operations

00:21:07,520 --> 00:21:11,200
creating a resource updating the

00:21:09,600 --> 00:21:13,760
resource and deleting it

00:21:11,200 --> 00:21:14,880
for example creating the postgres

00:21:13,760 --> 00:21:17,600
instance

00:21:14,880 --> 00:21:19,200
updating it or deleting it with all its

00:21:17,600 --> 00:21:21,120
different

00:21:19,200 --> 00:21:22,480
subtleties for example could be an

00:21:21,120 --> 00:21:23,760
upgrade that changes the number of

00:21:22,480 --> 00:21:26,400
replicas

00:21:23,760 --> 00:21:28,400
um or it could be an upgrade that even

00:21:26,400 --> 00:21:31,919
changes the version

00:21:28,400 --> 00:21:34,080
major or minor so the controller will

00:21:31,919 --> 00:21:36,640
have to take care of all the complexity

00:21:34,080 --> 00:21:38,000
and dissecting this change in state in

00:21:36,640 --> 00:21:40,000
the specification

00:21:38,000 --> 00:21:41,760
and make sense of it by translating it

00:21:40,000 --> 00:21:44,320
back into

00:21:41,760 --> 00:21:45,280
changing resources of other kinds so for

00:21:44,320 --> 00:21:46,720
example

00:21:45,280 --> 00:21:48,640
changing the specification of the

00:21:46,720 --> 00:21:51,840
replica set if your

00:21:48,640 --> 00:21:55,280
replicas are go up from one to three

00:21:51,840 --> 00:21:56,880
well you'd be updating the replicas in

00:21:55,280 --> 00:22:00,960
your stateful set

00:21:56,880 --> 00:22:03,520
as you can imagine the postgres example

00:22:00,960 --> 00:22:04,000
here if you have one node well that's

00:22:03,520 --> 00:22:05,760
fine

00:22:04,000 --> 00:22:08,240
if you specify three nodes that doesn't

00:22:05,760 --> 00:22:09,919
bring you a distributed database yet you

00:22:08,240 --> 00:22:11,440
have to set up replication you have to

00:22:09,919 --> 00:22:13,360
set up a replication user

00:22:11,440 --> 00:22:14,960
so all these additional semantics will

00:22:13,360 --> 00:22:18,159
have to be covered

00:22:14,960 --> 00:22:20,000
in the controller maybe not executed but

00:22:18,159 --> 00:22:22,400
at least taken care of and delegated

00:22:20,000 --> 00:22:22,400
somewhere

00:22:22,640 --> 00:22:26,640
so writing those controllers and

00:22:24,799 --> 00:22:28,480
defining custom resources seems to be a

00:22:26,640 --> 00:22:30,400
repetitive task if you're

00:22:28,480 --> 00:22:31,919
doing data service automation and

00:22:30,400 --> 00:22:34,000
therefore tools emerged

00:22:31,919 --> 00:22:35,760
that will help you do that the operator

00:22:34,000 --> 00:22:38,559
sdk is one of them

00:22:35,760 --> 00:22:39,360
it oversimplifying speaking it it helps

00:22:38,559 --> 00:22:43,039
you to describe

00:22:39,360 --> 00:22:45,360
clds and and deploy it employ them

00:22:43,039 --> 00:22:47,280
apply them to the caster cluster and

00:22:45,360 --> 00:22:49,919
will also allow you to create

00:22:47,280 --> 00:22:52,159
cid controllers more easily and it does

00:22:49,919 --> 00:22:54,240
that with a lot of go code

00:22:52,159 --> 00:22:56,559
to be fair there's also support for

00:22:54,240 --> 00:22:59,600
ansible and help support but

00:22:56,559 --> 00:23:02,720
as a data service guy i would say um

00:22:59,600 --> 00:23:06,000
helm and ansbo it's not particularly

00:23:02,720 --> 00:23:09,039
what i what i'd expect here so uh

00:23:06,000 --> 00:23:10,720
the mightiness of go for describing the

00:23:09,039 --> 00:23:11,679
life cycle of data service is a good

00:23:10,720 --> 00:23:14,159
thing

00:23:11,679 --> 00:23:14,799
but you need to structure life cycle

00:23:14,159 --> 00:23:17,039
events

00:23:14,799 --> 00:23:19,120
and the library that performs those

00:23:17,039 --> 00:23:20,799
things very very wisely unless you're

00:23:19,120 --> 00:23:24,000
ending up in a nightmare

00:23:20,799 --> 00:23:26,559
spaghetti code operator

00:23:24,000 --> 00:23:28,159
so the operator sdk comes with code

00:23:26,559 --> 00:23:30,240
generators for custom resource

00:23:28,159 --> 00:23:32,720
definitions and controllers

00:23:30,240 --> 00:23:34,720
it'll postulate a reconcile loop that

00:23:32,720 --> 00:23:36,559
will basically give you two versions of

00:23:34,720 --> 00:23:38,159
the specification of an object

00:23:36,559 --> 00:23:41,280
and then the controller has to take

00:23:38,159 --> 00:23:41,280
corrective measures

00:23:41,600 --> 00:23:45,360
there are also other abstractions or

00:23:44,640 --> 00:23:48,480
other

00:23:45,360 --> 00:23:51,360
tools emerging to build operators

00:23:48,480 --> 00:23:52,799
one example is kudo which is a generic

00:23:51,360 --> 00:23:55,919
operator

00:23:52,799 --> 00:23:57,760
very opinionated um so while

00:23:55,919 --> 00:23:59,279
well let's say the operator sdk is more

00:23:57,760 --> 00:24:02,240
like ruby sinatra

00:23:59,279 --> 00:24:04,640
then kudos more like rails ah it's not a

00:24:02,240 --> 00:24:07,600
particular accurate comparison but

00:24:04,640 --> 00:24:07,600
yeah forgive me that

00:24:07,919 --> 00:24:11,679
kudo introduces a concept called

00:24:09,840 --> 00:24:12,320
workflows where you can specify a

00:24:11,679 --> 00:24:16,000
sequence

00:24:12,320 --> 00:24:17,919
of resource changes because a lot

00:24:16,000 --> 00:24:19,520
in the life cycle management of data

00:24:17,919 --> 00:24:22,080
cells will be around

00:24:19,520 --> 00:24:23,039
modifying the stateful set changing the

00:24:22,080 --> 00:24:24,720
service

00:24:23,039 --> 00:24:26,159
adding the config map changing the

00:24:24,720 --> 00:24:28,880
configmap and so on

00:24:26,159 --> 00:24:30,400
so it seems natural that having a more

00:24:28,880 --> 00:24:32,320
control over that and be more

00:24:30,400 --> 00:24:33,679
descriptive about that sounds

00:24:32,320 --> 00:24:36,640
interesting

00:24:33,679 --> 00:24:36,960
and well you could first think that way

00:24:36,640 --> 00:24:39,200
is

00:24:36,960 --> 00:24:41,440
is a workflow imperative but at some

00:24:39,200 --> 00:24:42,640
point each declarative system has to do

00:24:41,440 --> 00:24:45,520
something

00:24:42,640 --> 00:24:46,880
so there must be a converging algorithm

00:24:45,520 --> 00:24:49,279
to be executed

00:24:46,880 --> 00:24:49,919
so a workflow description in data

00:24:49,279 --> 00:24:51,760
service

00:24:49,919 --> 00:24:52,960
automation is actually a good

00:24:51,760 --> 00:24:55,600
abstraction

00:24:52,960 --> 00:24:57,200
the question here is kudo expressive

00:24:55,600 --> 00:25:00,240
enough or does it get

00:24:57,200 --> 00:25:00,960
in your way so maybe you try it yourself

00:25:00,240 --> 00:25:03,840
and

00:25:00,960 --> 00:25:04,320
form your own opinion um another

00:25:03,840 --> 00:25:06,559
question

00:25:04,320 --> 00:25:08,559
is is that the way it should be done

00:25:06,559 --> 00:25:10,159
because in data service automation you

00:25:08,559 --> 00:25:11,760
also have different layers

00:25:10,159 --> 00:25:14,080
of abstraction where you need to take

00:25:11,760 --> 00:25:16,240
action so it's not only about changing

00:25:14,080 --> 00:25:19,840
the stateful set

00:25:16,240 --> 00:25:23,120
alone in its specification but

00:25:19,840 --> 00:25:26,159
taking the uh example of of

00:25:23,120 --> 00:25:27,200
changing a postgres from one replica to

00:25:26,159 --> 00:25:30,799
three

00:25:27,200 --> 00:25:32,080
also requires setting up um a database

00:25:30,799 --> 00:25:34,000
user for replication

00:25:32,080 --> 00:25:35,760
changing the compute changing the

00:25:34,000 --> 00:25:38,960
configuration file

00:25:35,760 --> 00:25:40,159
uh and maybe invoking commands such as

00:25:38,960 --> 00:25:43,760
pg base echo

00:25:40,159 --> 00:25:46,799
backup on only a few on the cluster

00:25:43,760 --> 00:25:50,320
nodes so in this case not on the primary

00:25:46,799 --> 00:25:52,400
but on the secondaries so this requires

00:25:50,320 --> 00:25:54,960
a bit more fine-grain control of what

00:25:52,400 --> 00:25:57,760
happens inside of the

00:25:54,960 --> 00:25:59,760
of the containers and pods and there's

00:25:57,760 --> 00:26:03,039
no declarative workflow like

00:25:59,760 --> 00:26:03,600
description in kudo here and this is not

00:26:03,039 --> 00:26:05,840
maybe

00:26:03,600 --> 00:26:07,039
kudu will do that at some point but i i

00:26:05,840 --> 00:26:10,240
just believe that there's

00:26:07,039 --> 00:26:12,559
room for improvement left

00:26:10,240 --> 00:26:13,919
but try it out yourself make your own

00:26:12,559 --> 00:26:17,200
opinion

00:26:13,919 --> 00:26:18,320
so the picture drawing the picture

00:26:17,200 --> 00:26:21,200
together

00:26:18,320 --> 00:26:23,039
um you have a cloud foundry that sits on

00:26:21,200 --> 00:26:25,120
top of kubernetes

00:26:23,039 --> 00:26:26,559
then you'll have a cloud controller this

00:26:25,120 --> 00:26:27,120
is where you register your service

00:26:26,559 --> 00:26:28,640
broker

00:26:27,120 --> 00:26:30,880
so if you're familiar with service

00:26:28,640 --> 00:26:33,200
brokers nothing has changed so far

00:26:30,880 --> 00:26:34,960
the service broker will delegate the

00:26:33,200 --> 00:26:38,400
cloud controller will delegate to the

00:26:34,960 --> 00:26:41,279
service broker the service broker uh

00:26:38,400 --> 00:26:42,080
in in in our case uh delegates to a

00:26:41,279 --> 00:26:46,559
deployment

00:26:42,080 --> 00:26:49,120
um uh component then we'll then

00:26:46,559 --> 00:26:50,240
for example write bosch manifests now in

00:26:49,120 --> 00:26:52,640
kubernetes

00:26:50,240 --> 00:26:53,440
you could instead write kubernetes

00:26:52,640 --> 00:26:55,679
manifests

00:26:53,440 --> 00:26:59,200
for example to create a stateful set for

00:26:55,679 --> 00:27:02,000
example to create config maps and so on

00:26:59,200 --> 00:27:03,919
and in order to bring that together more

00:27:02,000 --> 00:27:05,919
easily and more consistently

00:27:03,919 --> 00:27:07,440
especially if you're also targeting

00:27:05,919 --> 00:27:10,720
kubernetes folks

00:27:07,440 --> 00:27:13,360
directly that are not necessarily using

00:27:10,720 --> 00:27:14,000
cloud foundry they'd expect for sure

00:27:13,360 --> 00:27:16,720
higher

00:27:14,000 --> 00:27:17,520
level abstractions as custom resources

00:27:16,720 --> 00:27:20,720
so

00:27:17,520 --> 00:27:21,760
my feelings tells me that you want to

00:27:20,720 --> 00:27:24,240
have

00:27:21,760 --> 00:27:26,080
a custom resource representing your

00:27:24,240 --> 00:27:27,360
service bindings

00:27:26,080 --> 00:27:29,919
and most importantly your service

00:27:27,360 --> 00:27:32,640
instances and then

00:27:29,919 --> 00:27:33,679
your service broker can delegate the

00:27:32,640 --> 00:27:36,080
heavy lifting

00:27:33,679 --> 00:27:37,440
to those controllers behind those custom

00:27:36,080 --> 00:27:40,159
resources

00:27:37,440 --> 00:27:41,600
and in the end what you then do is

00:27:40,159 --> 00:27:44,320
translate that into

00:27:41,600 --> 00:27:45,600
lower level more kubernetes native

00:27:44,320 --> 00:27:48,640
resources

00:27:45,600 --> 00:27:51,039
well nobody will hinder you to

00:27:48,640 --> 00:27:52,320
to use higher level or external

00:27:51,039 --> 00:27:56,480
third-party things like

00:27:52,320 --> 00:27:59,200
service mesh in in in a controller

00:27:56,480 --> 00:28:02,000
when creating service instances so it's

00:27:59,200 --> 00:28:04,159
absolutely up to you

00:28:02,000 --> 00:28:07,200
so the takeaway here would be cloud

00:28:04,159 --> 00:28:09,279
foundry service brokers that's the same

00:28:07,200 --> 00:28:10,640
the responsibility of the service broker

00:28:09,279 --> 00:28:13,600
will change a bit

00:28:10,640 --> 00:28:15,440
for example because if you in case you

00:28:13,600 --> 00:28:19,279
delegate the heavy lifting

00:28:15,440 --> 00:28:21,279
of storing and reconciling uh resources

00:28:19,279 --> 00:28:24,080
such as the data service in systems

00:28:21,279 --> 00:28:25,360
themselves to controllers running inside

00:28:24,080 --> 00:28:28,640
of kubernetes

00:28:25,360 --> 00:28:30,720
do you have to to store much of state

00:28:28,640 --> 00:28:32,080
in the service broker anymore or could

00:28:30,720 --> 00:28:34,320
you run your service broker in

00:28:32,080 --> 00:28:35,679
kubernetes and rely on its etcd for

00:28:34,320 --> 00:28:38,000
persistency

00:28:35,679 --> 00:28:38,880
so the design of a service broker will

00:28:38,000 --> 00:28:42,399
change

00:28:38,880 --> 00:28:44,480
the way you express your and implement

00:28:42,399 --> 00:28:45,600
your service instances and bindings will

00:28:44,480 --> 00:28:48,080
change

00:28:45,600 --> 00:28:48,720
using kubernetes resources but the tools

00:28:48,080 --> 00:28:52,640
are there

00:28:48,720 --> 00:28:52,640
all you have to do is assemble it

00:28:52,720 --> 00:28:58,480
so there is much to talk about left

00:28:56,799 --> 00:29:00,559
for example how do you package ship and

00:28:58,480 --> 00:29:02,159
run these operators

00:29:00,559 --> 00:29:05,039
what about the weaker container

00:29:02,159 --> 00:29:08,399
isolation are there workarounds for that

00:29:05,039 --> 00:29:11,520
what's with this additional effort

00:29:08,399 --> 00:29:12,960
logging metrics scaling harmonized

00:29:11,520 --> 00:29:15,520
backup restore

00:29:12,960 --> 00:29:16,240
so if you want to talk if you want me to

00:29:15,520 --> 00:29:18,880
talk about this

00:29:16,240 --> 00:29:20,000
ask questions or leave comments if you

00:29:18,880 --> 00:29:22,240
are

00:29:20,000 --> 00:29:23,360
watching this as a recording i'd be

00:29:22,240 --> 00:29:26,880
happy to

00:29:23,360 --> 00:29:30,240
produce more talk proposals in this

00:29:26,880 --> 00:29:33,679
area all right so summing it up

00:29:30,240 --> 00:29:36,080
um we need to keep an eye on the weaker

00:29:33,679 --> 00:29:37,039
container isolation as this creates some

00:29:36,080 --> 00:29:40,080
challenges that

00:29:37,039 --> 00:29:40,960
can be mitigated but create additional

00:29:40,080 --> 00:29:44,320
overhead

00:29:40,960 --> 00:29:48,159
in the automation uh for example host

00:29:44,320 --> 00:29:50,799
affinity anti-affinity teams and so on

00:29:48,159 --> 00:29:52,320
kubernetes can do resource data services

00:29:50,799 --> 00:29:54,480
there are resources for that there are

00:29:52,320 --> 00:29:56,399
stateful sets services conflict maps and

00:29:54,480 --> 00:29:58,240
secrets

00:29:56,399 --> 00:30:00,240
and there are also utilities emerging

00:29:58,240 --> 00:30:00,640
that will help you to deal with these

00:30:00,240 --> 00:30:02,480
things

00:30:00,640 --> 00:30:04,880
more gracefully including the operator

00:30:02,480 --> 00:30:06,720
sdk and kudo

00:30:04,880 --> 00:30:08,480
my personal belief is that there is

00:30:06,720 --> 00:30:10,880
still room for improvement

00:30:08,480 --> 00:30:14,640
because it's still a tedious task to

00:30:10,880 --> 00:30:16,799
describe a data service lifecycle

00:30:14,640 --> 00:30:17,760
so watch out for new technologies to pop

00:30:16,799 --> 00:30:19,360
up

00:30:17,760 --> 00:30:21,600
if you have questions and opinions feel

00:30:19,360 --> 00:30:24,640
free to provide feedback and questions

00:30:21,600 --> 00:30:25,520
and so far thanks for your time one

00:30:24,640 --> 00:30:27,919
advice

00:30:25,520 --> 00:30:29,520
if you felt lost a bit because i threw

00:30:27,919 --> 00:30:31,520
around with a lot of kubernetes

00:30:29,520 --> 00:30:34,039
terms that you are not familiar with

00:30:31,520 --> 00:30:35,840
maybe you want to have a look at

00:30:34,039 --> 00:30:37,360
learn.kubernetes.com it's a free

00:30:35,840 --> 00:30:39,120
kubernetes tutorial

00:30:37,360 --> 00:30:40,399
we use that internally to train up new

00:30:39,120 --> 00:30:43,120
engineers

00:30:40,399 --> 00:30:43,760
to do the basic kubernetes and get

00:30:43,120 --> 00:30:46,799
through the

00:30:43,760 --> 00:30:48,000
the kubernetes basics so

00:30:46,799 --> 00:30:50,240
maybe you want to have a look at it

00:30:48,000 --> 00:30:52,720
there's um this tutorial there's

00:30:50,240 --> 00:30:53,919
on our youtube channel the 89's youtube

00:30:52,720 --> 00:30:57,200
channel there's also

00:30:53,919 --> 00:30:59,919
a 12-hour recording um of us

00:30:57,200 --> 00:31:00,559
of a session where we go through this uh

00:30:59,919 --> 00:31:03,039
tutorial

00:31:00,559 --> 00:31:04,159
in depth so have fun with it and thank

00:31:03,039 --> 00:31:06,480
you very much

00:31:04,159 --> 00:31:09,200
for your attention reach out if you have

00:31:06,480 --> 00:31:13,600
questions i'm on twitter

00:31:09,200 --> 00:31:13,600

YouTube URL: https://www.youtube.com/watch?v=WDbz11GujA0


