Title: Deep Dive: How Cloud Foundry Goes "Bang" - Toby Lorne Welch-Richards, UK Government Digital Service
Publication date: 2020-10-26
Playlist: Cloud Foundry Summit Europe 2020
Description: 
	Deep Dive: How Cloud Foundry Goes "Bang" - Toby Lorne Welch-Richards, UK Government Digital Service

Cloud Foundry is a complex distributed system capable of running complex distributed systems, and has interesting and unusual failure modes. The UK Government have been running open-source Cloud Foundry in production for over 5 years. Toby Lorne will present and analyse several bugs and incidents the GOV.UK PaaS team have experienced whilst operating open-source Cloud Foundry deployments at scale.
Captions: 
	00:00:00,160 --> 00:00:04,000
hello and welcome to how cloud foundry

00:00:02,399 --> 00:00:06,000
goes bang

00:00:04,000 --> 00:00:08,080
i'm toby lawn a site reliability

00:00:06,000 --> 00:00:11,040
engineer at golf dot uk pass and an

00:00:08,080 --> 00:00:14,000
occasional cloud foundry contributor

00:00:11,040 --> 00:00:15,759
carpet uk pass is uk based web hosting

00:00:14,000 --> 00:00:17,039
powered by cloud foundry

00:00:15,759 --> 00:00:18,480
so you can get on delivering your

00:00:17,039 --> 00:00:20,320
government service without needing

00:00:18,480 --> 00:00:22,000
specialist infrastructure skills or

00:00:20,320 --> 00:00:23,760
knowledge of infrastructures or service

00:00:22,000 --> 00:00:25,599
providers

00:00:23,760 --> 00:00:26,800
we power things like the department for

00:00:25,599 --> 00:00:29,599
international trades

00:00:26,800 --> 00:00:30,000
great.gov uk to help people import and

00:00:29,599 --> 00:00:33,280
export

00:00:30,000 --> 00:00:35,600
goods into and from the uk

00:00:33,280 --> 00:00:37,520
we power governor uk notify which helps

00:00:35,600 --> 00:00:40,640
government services send emails text

00:00:37,520 --> 00:00:42,640
messages and letters to their users

00:00:40,640 --> 00:00:44,000
we power things like simple energy

00:00:42,640 --> 00:00:46,079
advice so you

00:00:44,000 --> 00:00:48,879
a citizen can reduce your energy bills

00:00:46,079 --> 00:00:50,960
and get advice to make your home greener

00:00:48,879 --> 00:00:53,120
or if you recently purchased a drone or

00:00:50,960 --> 00:00:55,760
model aircraft you can register to fly

00:00:53,120 --> 00:00:59,680
online by the civil aviation authority

00:00:55,760 --> 00:01:01,440
who's now the governor uk powers user

00:00:59,680 --> 00:01:03,280
got the uk powers have been running open

00:01:01,440 --> 00:01:07,200
source cloud foundry since december

00:01:03,280 --> 00:01:10,240
2015. we started out in aws

00:01:07,200 --> 00:01:13,600
ireland eu west one and in

00:01:10,240 --> 00:01:17,119
may 2018 we added a london region us2

00:01:13,600 --> 00:01:18,640
again in aws wk powers is part of the

00:01:17,119 --> 00:01:20,960
government digital service

00:01:18,640 --> 00:01:22,640
we're hiring right now we're quite

00:01:20,960 --> 00:01:24,880
similar to cloud.gov

00:01:22,640 --> 00:01:26,640
and cloud.com au which are both

00:01:24,880 --> 00:01:28,799
government passes

00:01:26,640 --> 00:01:30,720
run by united states and australia

00:01:28,799 --> 00:01:32,880
respectively

00:01:30,720 --> 00:01:33,759
we have and have had a strong commitment

00:01:32,880 --> 00:01:36,479
to open source

00:01:33,759 --> 00:01:38,720
since the start we published a blog post

00:01:36,479 --> 00:01:40,159
back in 2015 on why we were choosing

00:01:38,720 --> 00:01:41,759
cloud foundry as our choice through our

00:01:40,159 --> 00:01:44,960
platform as a service

00:01:41,759 --> 00:01:46,000
and we've contributed and made open our

00:01:44,960 --> 00:01:48,640
code

00:01:46,000 --> 00:01:50,079
since the get-go we have over 65 open

00:01:48,640 --> 00:01:52,640
source repositories

00:01:50,079 --> 00:01:54,880
from our pal cf repository which is our

00:01:52,640 --> 00:01:58,479
cf deployment repository

00:01:54,880 --> 00:02:02,159
or cf cli plugins like conduit

00:01:58,479 --> 00:02:05,360
or open source brokers for aws rds s3

00:02:02,159 --> 00:02:05,360
or for avon.io

00:02:06,320 --> 00:02:10,080
today i'm going to tell you three

00:02:07,759 --> 00:02:11,280
stories a short story about browsers and

00:02:10,080 --> 00:02:12,560
complexity

00:02:11,280 --> 00:02:14,480
a slightly longer story about

00:02:12,560 --> 00:02:15,680
abstraction and service discovery in

00:02:14,480 --> 00:02:17,680
cloud foundry

00:02:15,680 --> 00:02:20,480
and finally i'm going to recount a large

00:02:17,680 --> 00:02:23,200
incident that we had earlier this year

00:02:20,480 --> 00:02:26,080
first up is a short story about browsers

00:02:23,200 --> 00:02:28,160
and complexity

00:02:26,080 --> 00:02:29,599
when a user signs into cloud binary

00:02:28,160 --> 00:02:30,640
they're interacting with a component

00:02:29,599 --> 00:02:32,400
called the uaa

00:02:30,640 --> 00:02:33,680
the user accounts and authorization

00:02:32,400 --> 00:02:36,239
server

00:02:33,680 --> 00:02:37,040
a user shown a form they should fill in

00:02:36,239 --> 00:02:38,879
that form

00:02:37,040 --> 00:02:41,200
and submit probably a username and

00:02:38,879 --> 00:02:44,000
password or following the single sign-on

00:02:41,200 --> 00:02:46,480
journey with an identity provider

00:02:44,000 --> 00:02:47,519
uaa will check the cross-site request

00:02:46,480 --> 00:02:49,360
forgery token

00:02:47,519 --> 00:02:51,680
and validate the username and password

00:02:49,360 --> 00:02:53,840
or the single sign-on token

00:02:51,680 --> 00:02:57,280
and if everything is valid then it will

00:02:53,840 --> 00:02:57,280
successfully sign the user in

00:02:58,239 --> 00:03:02,080
a colleague discovered that when users

00:03:00,239 --> 00:03:04,400
were signing in using microsoft edge

00:03:02,080 --> 00:03:05,760
they'd be shown the form and fill it in

00:03:04,400 --> 00:03:08,720
and they'd submit it

00:03:05,760 --> 00:03:10,000
uaa would always decide that the csrf

00:03:08,720 --> 00:03:11,599
token was invalid

00:03:10,000 --> 00:03:14,400
and the user would not be able to sign

00:03:11,599 --> 00:03:17,680
in this is quite odd behavior and it was

00:03:14,400 --> 00:03:20,080
isolated to microsoft edge

00:03:17,680 --> 00:03:21,840
this is what our uaa looks like it's

00:03:20,080 --> 00:03:22,720
slightly customized to fit the concrete

00:03:21,840 --> 00:03:24,400
uk styling

00:03:22,720 --> 00:03:27,360
and to be a bit more accessible than the

00:03:24,400 --> 00:03:27,360
default uaa

00:03:27,920 --> 00:03:31,760
and my colleagues posted in slack i

00:03:30,000 --> 00:03:32,799
think the crown is stopping edge users

00:03:31,760 --> 00:03:35,920
from logging in

00:03:32,799 --> 00:03:38,959
and then did some further investigation

00:03:35,920 --> 00:03:43,280
this is slightly odd why would a crown

00:03:38,959 --> 00:03:46,799
svg stop a user from signing in

00:03:43,280 --> 00:03:48,640
this is our reaction

00:03:46,799 --> 00:03:51,360
what was happening is the browser is

00:03:48,640 --> 00:03:54,239
requesting the uaa login page

00:03:51,360 --> 00:03:55,040
the uaa login page contains an svg which

00:03:54,239 --> 00:03:59,599
is the crown

00:03:55,040 --> 00:04:00,239
the wk logo svgs aren't supported in all

00:03:59,599 --> 00:04:02,000
browsers

00:04:00,239 --> 00:04:03,439
so it's common practice to include a

00:04:02,000 --> 00:04:05,200
fallback png

00:04:03,439 --> 00:04:08,400
that will be rendered if the browser is

00:04:05,200 --> 00:04:10,400
unable to render svgs

00:04:08,400 --> 00:04:12,159
you do this by putting an image tag

00:04:10,400 --> 00:04:14,640
inside the svg

00:04:12,159 --> 00:04:15,840
usually browsers won't render the image

00:04:14,640 --> 00:04:17,680
and they won't request

00:04:15,840 --> 00:04:19,440
the image specified in the source

00:04:17,680 --> 00:04:21,600
attribute of the image element

00:04:19,440 --> 00:04:24,639
however microsoft edge was always

00:04:21,600 --> 00:04:26,800
requesting this png

00:04:24,639 --> 00:04:29,520
furthermore we were actually hosting the

00:04:26,800 --> 00:04:31,680
png within uaa we forgot to include it

00:04:29,520 --> 00:04:34,720
in its assets bundle

00:04:31,680 --> 00:04:36,240
the behavior of uaa when it's asked for

00:04:34,720 --> 00:04:39,919
an asset that doesn't have

00:04:36,240 --> 00:04:43,520
is to give you a 302 to the login page

00:04:39,919 --> 00:04:45,440
this was followed by the browser edge

00:04:43,520 --> 00:04:49,040
and when it went to the login page and

00:04:45,440 --> 00:04:51,360
this invalidated the csrf token

00:04:49,040 --> 00:04:52,960
once the users submitted the form they

00:04:51,360 --> 00:04:54,240
had submitted the form with an invalid

00:04:52,960 --> 00:04:57,440
csrf token

00:04:54,240 --> 00:05:00,400
they were no longer able to log in

00:04:57,440 --> 00:05:02,000
this is an extremely arcane behavior

00:05:00,400 --> 00:05:04,000
that we couldn't have predicted when

00:05:02,000 --> 00:05:05,039
we're adding a crown logo to our login

00:05:04,000 --> 00:05:08,080
page

00:05:05,039 --> 00:05:10,400
yeah it resulted in quite a few users

00:05:08,080 --> 00:05:13,759
not being able to log into the platform

00:05:10,400 --> 00:05:16,160
because of a strange bug

00:05:13,759 --> 00:05:17,199
the bug was unless you have an x-link

00:05:16,160 --> 00:05:19,919
href

00:05:17,199 --> 00:05:20,720
attribute that's empty inside the image

00:05:19,919 --> 00:05:23,039
element

00:05:20,720 --> 00:05:25,520
inside the svg then the microsoft edge

00:05:23,039 --> 00:05:28,479
will always request the fallback png

00:05:25,520 --> 00:05:29,440
even though it's able to render svgs

00:05:28,479 --> 00:05:31,600
this was fixed

00:05:29,440 --> 00:05:33,440
in the following pull requests wherein

00:05:31,600 --> 00:05:36,320
we specify the x-link

00:05:33,440 --> 00:05:38,960
href element and we will also host the

00:05:36,320 --> 00:05:41,759
png so it won't respond with a 302 to

00:05:38,960 --> 00:05:41,759
the login page

00:05:42,400 --> 00:05:46,000
the next story we're going to look at is

00:05:44,160 --> 00:05:47,039
one involving cloud foundry service

00:05:46,000 --> 00:05:50,560
discovery

00:05:47,039 --> 00:05:52,800
and is a story about abstraction one day

00:05:50,560 --> 00:05:54,320
one of our tenants has an incident they

00:05:52,800 --> 00:05:56,800
write to us on slack

00:05:54,320 --> 00:05:58,080
we just saw a big spike of http 500

00:05:56,800 --> 00:05:59,680
requests

00:05:58,080 --> 00:06:01,039
we replied to them we did a deploy

00:05:59,680 --> 00:06:02,479
during that time and we start

00:06:01,039 --> 00:06:04,479
investigating

00:06:02,479 --> 00:06:05,680
they write to us were you rolling the

00:06:04,479 --> 00:06:08,319
diego cells

00:06:05,680 --> 00:06:10,400
no the only vms that were being deployed

00:06:08,319 --> 00:06:11,759
were the schedule of vms

00:06:10,400 --> 00:06:13,840
well they saw a lot of stats the

00:06:11,759 --> 00:06:16,440
exceptions when resolving dns

00:06:13,840 --> 00:06:18,960
this is interesting we looked at

00:06:16,440 --> 00:06:21,039
apps.internal dns request failures

00:06:18,960 --> 00:06:22,400
across the platform and we see that

00:06:21,039 --> 00:06:24,800
there is indeed a spike

00:06:22,400 --> 00:06:25,520
of failures we can also just look at

00:06:24,800 --> 00:06:28,160
requests

00:06:25,520 --> 00:06:28,720
generally we can see that the dns

00:06:28,160 --> 00:06:31,759
request

00:06:28,720 --> 00:06:34,160
volume suddenly spikes by four times

00:06:31,759 --> 00:06:37,919
this wasn't correlated with traffic wire

00:06:34,160 --> 00:06:39,919
requests suddenly four times higher

00:06:37,919 --> 00:06:42,960
we can see in bosch dns adapter's source

00:06:39,919 --> 00:06:44,800
code that there's a naive retry loop

00:06:42,960 --> 00:06:46,080
which if it doesn't get a successful

00:06:44,800 --> 00:06:50,080
response

00:06:46,080 --> 00:06:52,080
it instantly retries without waiting

00:06:50,080 --> 00:06:53,680
this retry loop instantly wraps up

00:06:52,080 --> 00:06:57,360
requests when failures are happening

00:06:53,680 --> 00:06:58,080
and exacerbates the problem we can also

00:06:57,360 --> 00:07:00,720
see

00:06:58,080 --> 00:07:02,560
tcp listen overflow on the service

00:07:00,720 --> 00:07:04,479
discovery vm

00:07:02,560 --> 00:07:06,160
this occurs when the service discovery

00:07:04,479 --> 00:07:07,680
controller is not able to respond to

00:07:06,160 --> 00:07:10,880
incoming requests

00:07:07,680 --> 00:07:12,400
it can't accept a new tcp connection

00:07:10,880 --> 00:07:13,919
we can see in the logs that there are

00:07:12,400 --> 00:07:16,000
too many open files

00:07:13,919 --> 00:07:18,080
the issue usually implies that there's a

00:07:16,000 --> 00:07:20,319
small u limit somewhere which needs to

00:07:18,080 --> 00:07:22,800
be increased

00:07:20,319 --> 00:07:25,039
but let's zoom out a little bit why is

00:07:22,800 --> 00:07:27,759
that http in my dns

00:07:25,039 --> 00:07:29,360
why do i need to make an http request

00:07:27,759 --> 00:07:30,000
let's look at the lifecycle of an app

00:07:29,360 --> 00:07:33,360
store internal

00:07:30,000 --> 00:07:34,880
dns request so

00:07:33,360 --> 00:07:37,440
when my application makes to do an s

00:07:34,880 --> 00:07:39,120
request i will make a request like this

00:07:37,440 --> 00:07:40,960
and i'll probably expect an almost

00:07:39,120 --> 00:07:43,759
immediate dns response

00:07:40,960 --> 00:07:45,919
but what happens is quite different any

00:07:43,759 --> 00:07:47,360
app store internal dns requests that

00:07:45,919 --> 00:07:49,759
bosch dns sees

00:07:47,360 --> 00:07:51,440
are forwarded via hdp to a component

00:07:49,759 --> 00:07:55,039
called the bosch dns adapter

00:07:51,440 --> 00:07:56,960
which also lives on the diego cell vm

00:07:55,039 --> 00:07:58,960
the bosch dns adapter needs to look up

00:07:56,960 --> 00:08:00,160
the app using the service discovery

00:07:58,960 --> 00:08:02,160
controller

00:08:00,160 --> 00:08:04,080
but because this service discovery

00:08:02,160 --> 00:08:05,840
controller lives on a different vm

00:08:04,080 --> 00:08:08,240
it needs to do this securely using

00:08:05,840 --> 00:08:10,080
mutual tls

00:08:08,240 --> 00:08:11,280
once the mutual tls connection has been

00:08:10,080 --> 00:08:13,199
established

00:08:11,280 --> 00:08:16,080
the adapter can make its request and

00:08:13,199 --> 00:08:17,840
receive a response for http

00:08:16,080 --> 00:08:20,879
because we only deployed two scheduled

00:08:17,840 --> 00:08:22,240
vms but we have three availability zones

00:08:20,879 --> 00:08:24,479
this request could happen across

00:08:22,240 --> 00:08:28,400
availability zones further increasing

00:08:24,479 --> 00:08:30,319
latency inside a single dns request

00:08:28,400 --> 00:08:32,000
now that bosch dns adapter has received

00:08:30,319 --> 00:08:33,120
its response from the service discovery

00:08:32,000 --> 00:08:36,800
controller

00:08:33,120 --> 00:08:39,440
it can in turn respond to the bosch dns

00:08:36,800 --> 00:08:40,640
request which will in turn send a dns

00:08:39,440 --> 00:08:44,880
response back to the

00:08:40,640 --> 00:08:46,880
original app and this was happening

00:08:44,880 --> 00:08:49,600
approximately four times per incoming

00:08:46,880 --> 00:08:50,959
http request to the tenant app

00:08:49,600 --> 00:08:52,560
the tenant's code was doing this

00:08:50,959 --> 00:08:53,760
multiple times whenever they were

00:08:52,560 --> 00:08:56,880
recording a metric

00:08:53,760 --> 00:08:58,800
let's look in version control here we

00:08:56,880 --> 00:09:01,040
can see the developer writing the code

00:08:58,800 --> 00:09:02,160
is making an assumption that dns is in

00:09:01,040 --> 00:09:05,600
fact dns

00:09:02,160 --> 00:09:08,399
locally via udp this is not true

00:09:05,600 --> 00:09:11,040
in this case dns is in fact http via

00:09:08,399 --> 00:09:14,240
mtls remotely

00:09:11,040 --> 00:09:16,560
we can raise several requests one

00:09:14,240 --> 00:09:18,399
ensuring that the platform deploys more

00:09:16,560 --> 00:09:21,279
than two scheduling instances

00:09:18,399 --> 00:09:22,000
and at least one per availability zone

00:09:21,279 --> 00:09:23,680
we can also

00:09:22,000 --> 00:09:25,839
increase the new limit in the services

00:09:23,680 --> 00:09:27,279
discovery controller so instead of only

00:09:25,839 --> 00:09:29,360
being able to serve this

00:09:27,279 --> 00:09:31,120
a thousand concurrent requests you can

00:09:29,360 --> 00:09:33,360
service many more

00:09:31,120 --> 00:09:35,360
we can also delay bosch dns adapter

00:09:33,360 --> 00:09:37,040
retries and our jitter to avoid a

00:09:35,360 --> 00:09:38,959
thundering hive problem

00:09:37,040 --> 00:09:40,480
and finally we can work with our tenants

00:09:38,959 --> 00:09:43,200
so that they cash the ip

00:09:40,480 --> 00:09:45,040
of their stats dns client requests so

00:09:43,200 --> 00:09:46,399
they aren't making lots of dns requests

00:09:45,040 --> 00:09:49,920
within a single

00:09:46,399 --> 00:09:51,519
http request context i've detailed a

00:09:49,920 --> 00:09:53,360
single pull request here

00:09:51,519 --> 00:09:55,440
that has links to all of the other pull

00:09:53,360 --> 00:09:58,240
requests

00:09:55,440 --> 00:09:59,040
our last story today is one that's about

00:09:58,240 --> 00:10:01,360
certificates

00:09:59,040 --> 00:10:04,640
and persistent environments versus

00:10:01,360 --> 00:10:06,640
short-lived environments

00:10:04,640 --> 00:10:08,480
earlier this year we had a major outage

00:10:06,640 --> 00:10:09,440
of applications hosted on property uk

00:10:08,480 --> 00:10:11,200
pass

00:10:09,440 --> 00:10:16,160
for which there's a public report on our

00:10:11,200 --> 00:10:18,160
status page status.cloud.service.gov.uk

00:10:16,160 --> 00:10:19,279
it's been our only p1 incident in five

00:10:18,160 --> 00:10:21,360
years of production

00:10:19,279 --> 00:10:23,200
is caused by certificate rotation you

00:10:21,360 --> 00:10:24,240
can find a link to the status report on

00:10:23,200 --> 00:10:25,519
screen now

00:10:24,240 --> 00:10:27,680
before we talk about certificate

00:10:25,519 --> 00:10:29,360
rotation we should remind ourselves of

00:10:27,680 --> 00:10:32,000
what a certificate is

00:10:29,360 --> 00:10:33,279
and why they're used there are two kinds

00:10:32,000 --> 00:10:35,360
of cryptography

00:10:33,279 --> 00:10:36,959
symmetric cryptography and asymmetric

00:10:35,360 --> 00:10:39,200
cryptography

00:10:36,959 --> 00:10:41,040
symmetric cryptography is when the key

00:10:39,200 --> 00:10:41,839
used for encryption and decryption is

00:10:41,040 --> 00:10:43,600
the same

00:10:41,839 --> 00:10:47,279
and asymmetric cryptography is when

00:10:43,600 --> 00:10:50,079
there's a public and a private component

00:10:47,279 --> 00:10:50,880
the private key is used for decryption

00:10:50,079 --> 00:10:52,880
operations

00:10:50,880 --> 00:10:54,640
but you only need the public key to

00:10:52,880 --> 00:10:56,079
encrypt something

00:10:54,640 --> 00:10:58,240
this means that you can distribute the

00:10:56,079 --> 00:11:00,399
public key to everyone

00:10:58,240 --> 00:11:02,079
and keep the private key safe everyone

00:11:00,399 --> 00:11:04,000
can encrypt messages to you

00:11:02,079 --> 00:11:05,279
but only you can decrypt the encrypted

00:11:04,000 --> 00:11:07,680
messages

00:11:05,279 --> 00:11:09,519
likewise you can use your private key to

00:11:07,680 --> 00:11:11,360
sign messages

00:11:09,519 --> 00:11:13,040
you can distribute the public key and

00:11:11,360 --> 00:11:14,959
everyone can use the public key to

00:11:13,040 --> 00:11:17,200
verify that the messages really were

00:11:14,959 --> 00:11:19,839
signed by you

00:11:17,200 --> 00:11:21,360
a certificate is a public key and some

00:11:19,839 --> 00:11:22,880
metadata

00:11:21,360 --> 00:11:25,200
for instance the comment name of the

00:11:22,880 --> 00:11:25,839
certificate and all of that is bundled

00:11:25,200 --> 00:11:28,079
together

00:11:25,839 --> 00:11:31,040
and signed by a certificate authority

00:11:28,079 --> 00:11:31,040
known as the ca

00:11:32,880 --> 00:11:38,240
as an example an application or computer

00:11:36,480 --> 00:11:39,040
will have a certificate authority bundle

00:11:38,240 --> 00:11:40,839
which contains

00:11:39,040 --> 00:11:43,839
multiple certificate authority

00:11:40,839 --> 00:11:46,000
certificates when you receive a message

00:11:43,839 --> 00:11:47,040
and an accompanying certificate you can

00:11:46,000 --> 00:11:49,120
verify

00:11:47,040 --> 00:11:51,760
that the message has been signed by the

00:11:49,120 --> 00:11:53,120
private key relating to that certificate

00:11:51,760 --> 00:11:54,720
and you can also check that the

00:11:53,120 --> 00:11:57,519
certificate has been signed by a

00:11:54,720 --> 00:12:00,560
certificate authority that you trust

00:11:57,519 --> 00:12:03,600
this tells you two things one the sender

00:12:00,560 --> 00:12:05,360
has the private key of this certificate

00:12:03,600 --> 00:12:07,600
two that this certificate has been

00:12:05,360 --> 00:12:09,279
signed by a trusted party

00:12:07,600 --> 00:12:12,320
these two things together means that you

00:12:09,279 --> 00:12:12,320
should trust the message

00:12:15,120 --> 00:12:18,959
here we have two components within cloud

00:12:17,120 --> 00:12:21,040
foundry on the left we've got

00:12:18,959 --> 00:12:23,279
goruta which is responsible for

00:12:21,040 --> 00:12:24,000
receiving http requests into cloud

00:12:23,279 --> 00:12:25,200
foundry

00:12:24,000 --> 00:12:27,680
and routing them to the right

00:12:25,200 --> 00:12:29,519
application on the right

00:12:27,680 --> 00:12:30,720
we have a vm which might represent a

00:12:29,519 --> 00:12:32,560
diego cell

00:12:30,720 --> 00:12:35,200
there's an application there which has

00:12:32,560 --> 00:12:37,680
an end voice side card proxy

00:12:35,200 --> 00:12:39,519
when the go to receives an http request

00:12:37,680 --> 00:12:39,920
it will look up in its root table where

00:12:39,519 --> 00:12:43,040
it should

00:12:39,920 --> 00:12:46,000
route the http request to or pick

00:12:43,040 --> 00:12:46,480
a back end destination and then it will

00:12:46,000 --> 00:12:49,600
forward

00:12:46,480 --> 00:12:51,440
that request to the back end in this

00:12:49,600 --> 00:12:54,560
case it picks our

00:12:51,440 --> 00:12:57,760
envoy sidecar

00:12:54,560 --> 00:13:00,160
it opens a mutual tls tunnel

00:12:57,760 --> 00:13:01,680
it sends the hp request down the mutual

00:13:00,160 --> 00:13:04,160
tls tunnel

00:13:01,680 --> 00:13:04,880
envoy accepts the http request and

00:13:04,160 --> 00:13:07,920
forwards it

00:13:04,880 --> 00:13:10,480
over local http in the network namespace

00:13:07,920 --> 00:13:12,639
to the application the application

00:13:10,480 --> 00:13:14,880
responds to envoy which then sends the

00:13:12,639 --> 00:13:15,440
hp response back through the mutual tls

00:13:14,880 --> 00:13:18,839
tunnel

00:13:15,440 --> 00:13:20,240
to go to which you can then reply to the

00:13:18,839 --> 00:13:22,240
client

00:13:20,240 --> 00:13:23,279
in order for this connection to be

00:13:22,240 --> 00:13:26,160
trusted

00:13:23,279 --> 00:13:26,800
goethe has a certificate that has been

00:13:26,160 --> 00:13:29,279
signed

00:13:26,800 --> 00:13:30,480
by this that has been issued by this

00:13:29,279 --> 00:13:34,079
typical authority

00:13:30,480 --> 00:13:36,560
that is trusted by envoy and likewise

00:13:34,079 --> 00:13:38,720
envoy has a certificate that has been

00:13:36,560 --> 00:13:42,000
issued by a certificate authority

00:13:38,720 --> 00:13:42,000
that go to trusts

00:13:42,560 --> 00:13:46,000
once we get more components in the mix

00:13:44,399 --> 00:13:47,760
we can see things become a little bit

00:13:46,000 --> 00:13:50,000
more complicated

00:13:47,760 --> 00:13:52,000
in this example we show gorouto which is

00:13:50,000 --> 00:13:52,720
able to forge requests to cloud

00:13:52,000 --> 00:13:56,320
controller

00:13:52,720 --> 00:14:00,399
uaa and diego cells cloud control

00:13:56,320 --> 00:14:02,880
additionally has to send messages to uaa

00:14:00,399 --> 00:14:03,760
so we see that there's quite a few

00:14:02,880 --> 00:14:06,639
components

00:14:03,760 --> 00:14:07,600
that are trusting multiple cas so they

00:14:06,639 --> 00:14:09,760
can validate

00:14:07,600 --> 00:14:12,160
and trust requests from multiple

00:14:09,760 --> 00:14:14,480
different components

00:14:12,160 --> 00:14:15,199
to use our earlier example of go to an

00:14:14,480 --> 00:14:16,560
envoy

00:14:15,199 --> 00:14:18,880
this is what happened during our

00:14:16,560 --> 00:14:18,880
incident

00:14:19,519 --> 00:14:23,760
goethe was not able to trust envoy envoy

00:14:21,839 --> 00:14:27,120
was not able to trust goethe

00:14:23,760 --> 00:14:30,320
because the relationship between the ca

00:14:27,120 --> 00:14:31,920
and the issuing certificate was broken

00:14:30,320 --> 00:14:33,680
this was happening between every single

00:14:31,920 --> 00:14:35,040
component across the platform

00:14:33,680 --> 00:14:37,440
for which the certificates had been

00:14:35,040 --> 00:14:38,959
rotated

00:14:37,440 --> 00:14:41,839
this was not a particularly good

00:14:38,959 --> 00:14:45,839
situation to be in

00:14:41,839 --> 00:14:45,839
so why did everything break

00:14:46,959 --> 00:14:52,720
x509 is the standard which describes how

00:14:50,320 --> 00:14:53,839
certificates should be formatted and

00:14:52,720 --> 00:14:55,920
there are a number of optional

00:14:53,839 --> 00:14:58,480
extensions to x509

00:14:55,920 --> 00:15:00,560
two of which are subject key identifiers

00:14:58,480 --> 00:15:02,399
and authority key identifiers

00:15:00,560 --> 00:15:05,199
it helps speed up certificate validation

00:15:02,399 --> 00:15:07,279
and provides an additional guarantee

00:15:05,199 --> 00:15:09,199
a common way of generating subject key

00:15:07,279 --> 00:15:10,000
identifiers is deriving them from the

00:15:09,199 --> 00:15:14,480
public key

00:15:10,000 --> 00:15:16,800
as shown here authority key identifiers

00:15:14,480 --> 00:15:18,480
for certificates should be derived from

00:15:16,800 --> 00:15:19,920
the separate key identifier of the

00:15:18,480 --> 00:15:22,720
certificate authority that

00:15:19,920 --> 00:15:23,920
signs the certificate authority key

00:15:22,720 --> 00:15:25,680
identifiers for

00:15:23,920 --> 00:15:27,519
certificate authority certificates

00:15:25,680 --> 00:15:31,680
should be derived from the subject key

00:15:27,519 --> 00:15:31,680
identifier of the certificate authority

00:15:31,759 --> 00:15:34,800
a very common library for cryptographic

00:15:33,759 --> 00:15:37,519
operations

00:15:34,800 --> 00:15:39,279
is openssl and it's used in many cloud

00:15:37,519 --> 00:15:42,079
foundry components

00:15:39,279 --> 00:15:43,839
openssl validates certificates if they

00:15:42,079 --> 00:15:46,800
have an authority key identifier

00:15:43,839 --> 00:15:48,959
in the following way if the certificate

00:15:46,800 --> 00:15:52,240
has an authority key identifier

00:15:48,959 --> 00:15:53,360
and the issuer i.e the ca has a subject

00:15:52,240 --> 00:15:54,959
key identifier

00:15:53,360 --> 00:15:57,519
then in order for the certificate to be

00:15:54,959 --> 00:15:59,839
valid the authority key identifier and

00:15:57,519 --> 00:16:02,240
the subject key identifier of the ca

00:15:59,839 --> 00:16:03,440
must have the same asn 1 string

00:16:02,240 --> 00:16:06,959
representation

00:16:03,440 --> 00:16:10,079
otherwise the certificate is invalid

00:16:06,959 --> 00:16:11,440
on the left we see cred hub's behavior

00:16:10,079 --> 00:16:13,519
for generating certificates

00:16:11,440 --> 00:16:14,480
and on the right we see open ssl's

00:16:13,519 --> 00:16:16,800
expectation

00:16:14,480 --> 00:16:18,639
for how certs should be generated if

00:16:16,800 --> 00:16:21,120
they have subject key identifiers and

00:16:18,639 --> 00:16:23,519
authority key identifiers

00:16:21,120 --> 00:16:25,040
cred hub's behavior for generating

00:16:23,519 --> 00:16:27,199
subject key identifiers

00:16:25,040 --> 00:16:29,759
for certificates and ca certificates is

00:16:27,199 --> 00:16:31,759
exactly what openssl expects

00:16:29,759 --> 00:16:34,079
but kradhov's behavior for generating

00:16:31,759 --> 00:16:35,839
the authority key identifier

00:16:34,079 --> 00:16:37,680
of a certificate is different to what

00:16:35,839 --> 00:16:39,839
openssl expects

00:16:37,680 --> 00:16:41,519
instead of deriving the authority key

00:16:39,839 --> 00:16:43,839
identifier from the subject key

00:16:41,519 --> 00:16:47,040
identifier of the ca certificate

00:16:43,839 --> 00:16:48,160
credhub instead derives the authority

00:16:47,040 --> 00:16:50,480
key identifier

00:16:48,160 --> 00:16:52,079
from the public key of the ca

00:16:50,480 --> 00:16:54,240
certificate

00:16:52,079 --> 00:16:56,720
these two different behaviors are

00:16:54,240 --> 00:16:58,320
equivalent if the function used

00:16:56,720 --> 00:16:59,920
to generate the subject key and

00:16:58,320 --> 00:17:02,480
authority key identifiers

00:16:59,920 --> 00:17:03,279
of the certificate are identical to the

00:17:02,480 --> 00:17:04,959
function

00:17:03,279 --> 00:17:07,679
used to derive the subject key

00:17:04,959 --> 00:17:10,880
identifier of the ca certificate

00:17:07,679 --> 00:17:12,880
for example if we use sha1

00:17:10,880 --> 00:17:14,160
to generate the subject key identifier

00:17:12,880 --> 00:17:16,720
of the ca certificate

00:17:14,160 --> 00:17:18,880
and we use sha1 to derive the authority

00:17:16,720 --> 00:17:21,679
key identifier

00:17:18,880 --> 00:17:23,039
of the certificate then this certificate

00:17:21,679 --> 00:17:26,000
is valid

00:17:23,039 --> 00:17:28,559
but say we used sha256 to derive the

00:17:26,000 --> 00:17:31,120
subject key identifier of the ca

00:17:28,559 --> 00:17:32,080
and sha1 to derive the authority key

00:17:31,120 --> 00:17:35,600
identifier

00:17:32,080 --> 00:17:37,520
of the certificate then the certificate

00:17:35,600 --> 00:17:41,679
when validated against the ca

00:17:37,520 --> 00:17:41,679
will not be valid according to openssl

00:17:42,799 --> 00:17:46,640
so how did we respond during the

00:17:44,480 --> 00:17:47,679
incident well understandably we were

00:17:46,640 --> 00:17:50,400
quite confused

00:17:47,679 --> 00:17:52,320
and stressed we saw that significant

00:17:50,400 --> 00:17:54,559
rotation had happened

00:17:52,320 --> 00:17:56,160
and eventually after lots of wrangling

00:17:54,559 --> 00:17:57,440
with the open ssl command line we

00:17:56,160 --> 00:17:59,600
observed that certificates had been

00:17:57,440 --> 00:18:01,120
generated incorrectly

00:17:59,600 --> 00:18:03,520
we used our concourse pipelines to

00:18:01,120 --> 00:18:06,160
generate entirely new key pki

00:18:03,520 --> 00:18:08,559
all the certificates all the cas and we

00:18:06,160 --> 00:18:10,160
forced a redeployment of the platform

00:18:08,559 --> 00:18:12,000
eventually the platform came back to

00:18:10,160 --> 00:18:14,720
life

00:18:12,000 --> 00:18:16,080
why did this reach production production

00:18:14,720 --> 00:18:17,679
environments are long-lived

00:18:16,080 --> 00:18:19,120
but our development environments are

00:18:17,679 --> 00:18:20,720
short-lived

00:18:19,120 --> 00:18:23,200
we turn off our development environments

00:18:20,720 --> 00:18:25,039
every night to save money

00:18:23,200 --> 00:18:27,600
our long-lived environments had imported

00:18:25,039 --> 00:18:30,000
cas from when the cas were generated

00:18:27,600 --> 00:18:31,840
which predated our use of credit hub and

00:18:30,000 --> 00:18:33,120
that means that subject key identifiers

00:18:31,840 --> 00:18:34,559
were not generated in the way that

00:18:33,120 --> 00:18:35,919
kredhar generates subject key

00:18:34,559 --> 00:18:38,960
identifiers

00:18:35,919 --> 00:18:40,799
this meant during rotation the authority

00:18:38,960 --> 00:18:43,200
key identifier did not match the subject

00:18:40,799 --> 00:18:45,360
key identify of the cas

00:18:43,200 --> 00:18:46,400
when we implemented certificate rotation

00:18:45,360 --> 00:18:48,480
and cred hub

00:18:46,400 --> 00:18:49,520
we tested them using development

00:18:48,480 --> 00:18:51,280
environments

00:18:49,520 --> 00:18:52,559
and then tested certificate rotation

00:18:51,280 --> 00:18:53,919
using those same development

00:18:52,559 --> 00:18:55,120
environments that we used to implement

00:18:53,919 --> 00:18:57,360
credhub

00:18:55,120 --> 00:18:58,640
this meant that they didn't have the cas

00:18:57,360 --> 00:19:01,039
with the odd

00:18:58,640 --> 00:19:01,919
subject key identifiers which caused the

00:19:01,039 --> 00:19:04,320
code path

00:19:01,919 --> 00:19:05,600
to be different this meant that when

00:19:04,320 --> 00:19:07,679
production underwent

00:19:05,600 --> 00:19:08,720
a certificate rotation a few months

00:19:07,679 --> 00:19:11,200
later

00:19:08,720 --> 00:19:14,240
it went kaboom instead of certificates

00:19:11,200 --> 00:19:16,640
being smoothly rotated

00:19:14,240 --> 00:19:19,280
what was the user impact our outage

00:19:16,640 --> 00:19:20,880
began at roughly 5 pm on the working day

00:19:19,280 --> 00:19:24,320
and affected both of our production

00:19:20,880 --> 00:19:26,720
regions at roughly the same time

00:19:24,320 --> 00:19:27,600
some apps were able to return to serving

00:19:26,720 --> 00:19:30,240
trafficker

00:19:27,600 --> 00:19:30,720
approximately five hours later and we

00:19:30,240 --> 00:19:33,919
had

00:19:30,720 --> 00:19:36,320
completely redeployed both platforms at

00:19:33,919 --> 00:19:38,000
roughly seven hours later during this

00:19:36,320 --> 00:19:41,360
time end users were unable to

00:19:38,000 --> 00:19:42,080
access any apps after we sort of patched

00:19:41,360 --> 00:19:43,919
ourselves up

00:19:42,080 --> 00:19:45,360
and got back on our feet we took some

00:19:43,919 --> 00:19:47,440
instant action items which we

00:19:45,360 --> 00:19:49,919
communicated to our users

00:19:47,440 --> 00:19:53,440
the first of which was to patch cred hub

00:19:49,919 --> 00:19:55,760
or roll that update to a patched version

00:19:53,440 --> 00:19:56,480
the second was to ensure that our

00:19:55,760 --> 00:19:58,080
pipeline

00:19:56,480 --> 00:19:59,840
rotated certificates more often

00:19:58,080 --> 00:20:01,840
development and staging so that we would

00:19:59,840 --> 00:20:03,440
have a better chance of catching future

00:20:01,840 --> 00:20:05,280
errors

00:20:03,440 --> 00:20:08,559
third was to improve our tests and

00:20:05,280 --> 00:20:10,159
certificate rotation pipeline output

00:20:08,559 --> 00:20:11,840
fourth was to practice instance more

00:20:10,159 --> 00:20:13,360
regularly

00:20:11,840 --> 00:20:16,799
and the last was to improve the user

00:20:13,360 --> 00:20:16,799
experience of platform outages

00:20:17,200 --> 00:20:20,960
we wanted to improve our certificate

00:20:18,640 --> 00:20:23,679
rotation tests our certificate rotation

00:20:20,960 --> 00:20:26,000
relies on a maximum hierarchy of two

00:20:23,679 --> 00:20:27,200
that being there can be one ca and then

00:20:26,000 --> 00:20:29,360
immediately below it

00:20:27,200 --> 00:20:31,360
leaf nodes we don't have lots and lots

00:20:29,360 --> 00:20:33,120
of layers of certificates and

00:20:31,360 --> 00:20:34,720
certificate authorities

00:20:33,120 --> 00:20:36,640
and we now know that it is very

00:20:34,720 --> 00:20:37,440
important to check that authority key

00:20:36,640 --> 00:20:40,000
identifiers

00:20:37,440 --> 00:20:41,760
match the sn1 string representation of

00:20:40,000 --> 00:20:44,400
subject key identifiers as typical

00:20:41,760 --> 00:20:44,400
authorities

00:20:44,640 --> 00:20:48,000
we also made a curtain to practice

00:20:46,320 --> 00:20:49,200
instances more regularly

00:20:48,000 --> 00:20:51,200
we have a commitment to practicing

00:20:49,200 --> 00:20:53,520
incidents more than once per month

00:20:51,200 --> 00:20:55,360
we've developed a lightweight dungeons

00:20:53,520 --> 00:20:56,400
dragon style incident process that our

00:20:55,360 --> 00:20:59,120
team has called

00:20:56,400 --> 00:21:02,159
unlucky did we continue to run slightly

00:20:59,120 --> 00:21:05,440
more heavyweight game days

00:21:02,159 --> 00:21:07,200
lastly the end user experience for

00:21:05,440 --> 00:21:10,559
platform outage

00:21:07,200 --> 00:21:12,960
um is a bit sub-optimal when you just

00:21:10,559 --> 00:21:15,280
have plain text error messages

00:21:12,960 --> 00:21:17,280
so we raised the pro request with thanks

00:21:15,280 --> 00:21:19,440
to the cf networking team

00:21:17,280 --> 00:21:20,720
to allow operators to customize error

00:21:19,440 --> 00:21:23,520
messages

00:21:20,720 --> 00:21:25,919
so that we can improve and iterate on

00:21:23,520 --> 00:21:26,799
the user experience of platform outages

00:21:25,919 --> 00:21:31,840
either total

00:21:26,799 --> 00:21:31,840
or partial

00:21:31,919 --> 00:21:36,000
so what have we learned in our three

00:21:33,440 --> 00:21:37,520
examples over the last 20 minutes

00:21:36,000 --> 00:21:39,280
we first learned that abstractions are

00:21:37,520 --> 00:21:40,720
leaky although perhaps we already knew

00:21:39,280 --> 00:21:42,480
this

00:21:40,720 --> 00:21:44,640
our developer in the service discovery

00:21:42,480 --> 00:21:45,760
and dns example assumed that dns

00:21:44,640 --> 00:21:48,000
resolution would be quick

00:21:45,760 --> 00:21:49,120
because it's just a single dns request

00:21:48,000 --> 00:21:52,000
over udp

00:21:49,120 --> 00:21:53,120
to a local dns resolver when in fact the

00:21:52,000 --> 00:21:55,919
request chain was

00:21:53,120 --> 00:21:56,720
two http requests including a mutual tls

00:21:55,919 --> 00:22:00,559
tunnel

00:21:56,720 --> 00:22:02,240
that crossed an availability zone

00:22:00,559 --> 00:22:03,679
we also learned that distribute systems

00:22:02,240 --> 00:22:05,280
hard both

00:22:03,679 --> 00:22:07,200
in terms of the auxiliary systems that

00:22:05,280 --> 00:22:09,520
we have to build to get them to work

00:22:07,200 --> 00:22:10,799
for instance service discovery and app

00:22:09,520 --> 00:22:12,960
store internal dns

00:22:10,799 --> 00:22:13,919
but also in trying to get them to trust

00:22:12,960 --> 00:22:16,000
each other

00:22:13,919 --> 00:22:17,600
our difficulty with implementing safe

00:22:16,000 --> 00:22:21,760
certificate rotation

00:22:17,600 --> 00:22:23,919
and a huge failure mode and blast radius

00:22:21,760 --> 00:22:27,200
of when that goes wrong

00:22:23,919 --> 00:22:27,840
we also learned in our starting trigger

00:22:27,200 --> 00:22:30,159
example

00:22:27,840 --> 00:22:31,919
that browsers distribute systems it's

00:22:30,159 --> 00:22:33,440
quite challenging to work out this

00:22:31,919 --> 00:22:35,440
isolated failure mode

00:22:33,440 --> 00:22:37,200
in a single browser that wasn't used by

00:22:35,440 --> 00:22:39,440
the developers

00:22:37,200 --> 00:22:41,280
we had to debug back and forth with the

00:22:39,440 --> 00:22:42,799
user and eventually download edge

00:22:41,280 --> 00:22:45,200
ourselves to verify

00:22:42,799 --> 00:22:47,679
the hypothesis that it was a fallback

00:22:45,200 --> 00:22:47,679
png

00:22:47,919 --> 00:22:51,760
we also learned that certificates are

00:22:49,280 --> 00:22:53,360
not as simple as they look

00:22:51,760 --> 00:22:55,120
not only do we have to manage a lot of

00:22:53,360 --> 00:22:56,400
them in a large distributed system like

00:22:55,120 --> 00:22:58,880
cloud foundry

00:22:56,400 --> 00:23:00,640
but we're also operating them at machine

00:22:58,880 --> 00:23:02,320
scale so we have to make sure that our

00:23:00,640 --> 00:23:04,799
tooling generates certificates

00:23:02,320 --> 00:23:06,960
in an appropriate way and in a way that

00:23:04,799 --> 00:23:09,760
all of the other compatible tooling

00:23:06,960 --> 00:23:11,360
accepts this was probably not the

00:23:09,760 --> 00:23:12,880
consideration that was being made the

00:23:11,360 --> 00:23:14,240
first time credit hub implemented

00:23:12,880 --> 00:23:17,679
certificate rotation

00:23:14,240 --> 00:23:19,520
generation lastly

00:23:17,679 --> 00:23:21,760
although we probably also already knew

00:23:19,520 --> 00:23:23,039
this cloud foundry open source community

00:23:21,760 --> 00:23:24,559
is amazing

00:23:23,039 --> 00:23:26,640
we wouldn't be able to operate this

00:23:24,559 --> 00:23:27,520
large platform and serve all of our

00:23:26,640 --> 00:23:29,360
users

00:23:27,520 --> 00:23:31,760
across two regions with such a small

00:23:29,360 --> 00:23:33,440
team without standing on the shoulders

00:23:31,760 --> 00:23:35,120
of some open source giants

00:23:33,440 --> 00:23:37,039
so thank you again to the cloud foundry

00:23:35,120 --> 00:23:39,280
community

00:23:37,039 --> 00:23:40,960
we also learned and we definitely

00:23:39,280 --> 00:23:42,480
already knew this that network failures

00:23:40,960 --> 00:23:44,159
happen often

00:23:42,480 --> 00:23:49,279
those are a few ways that cloud foundry

00:23:44,159 --> 00:23:49,279

YouTube URL: https://www.youtube.com/watch?v=7oJBNcUvKOA


