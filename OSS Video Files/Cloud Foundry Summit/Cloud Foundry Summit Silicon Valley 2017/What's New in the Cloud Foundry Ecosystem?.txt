Title: What's New in the Cloud Foundry Ecosystem?
Publication date: 2017-06-21
Playlist: Cloud Foundry Summit Silicon Valley 2017
Description: 
	What's New in the Cloud Foundry Ecosystem? - Alex Zalesov, Altoros   

A lot has changed in the Cloud Foundry ecosystem in the recent year. But how have these changes influenced the everyday life of the platform operations engineer? What has changed in the developer’s workflow? A year ago one needed to provision a virtual machine for the broker—today, a broker can be deployed as an ordinary application. Previously, a developer had to add authentication code to each of the applications—now a single routing service on top of all apps is used. In this talk, Alex will describe the changes accommodated by our engineers interacting with Cloud Foundry on a day-to-day basis. Alex will describe the features that saved us most time, and the ones that increased our confidence in the platform’s ability to self-heal in the face of failure. Also, Alex will touch upon the most anticipated features that we believe will make our lives much easier.

Alex Zalesov
Altoros
Cloud Foundry Engineer
Minsk, Belarus
Aleksey Zalesov is a Cloud Foundry/DevOps Engineer at Altoros. He has seven years of experience in managing computer systems, both traditional and cloud-based. Currently, he designs and implements distributed PaaS systems based on Cloud Foundry. Aleksey is passionate about automating cloud infrastructure management with BOSH and using DevOps techniques in everyday activities. His research interests also include self-managing computer systems and techno-social interactions.
Captions: 
	00:00:00,210 --> 00:00:07,500
hello little buddy I would like to greet

00:00:04,680 --> 00:00:12,049
you everybody here on Cloud Foundry

00:00:07,500 --> 00:00:15,450
sunny and today I will be talking about

00:00:12,049 --> 00:00:19,020
the latest features and technologies a

00:00:15,450 --> 00:00:22,500
melt in cloud foundry ecosystem for the

00:00:19,020 --> 00:00:28,050
last year I will be talking about their

00:00:22,500 --> 00:00:32,250
good parts and the bad parts also I will

00:00:28,050 --> 00:00:34,500
try to share some storage stories from

00:00:32,250 --> 00:00:39,149
my personal experience with these

00:00:34,500 --> 00:00:44,280
technologies and from experience of my

00:00:39,149 --> 00:00:49,230
colleagues with these technologies let's

00:00:44,280 --> 00:00:52,530
let me introduce myself my name is Alex

00:00:49,230 --> 00:00:57,660
ELISA and I'm a cloud foundry engineer

00:00:52,530 --> 00:01:01,680
at Arturo's during last two years I do

00:00:57,660 --> 00:01:04,229
cloud foundry deployments I do design of

00:01:01,680 --> 00:01:09,510
cloud foundry deployments and also I do

00:01:04,229 --> 00:01:14,460
the implementation and operation after

00:01:09,510 --> 00:01:17,520
the deployment is complete I often go to

00:01:14,460 --> 00:01:21,119
the trainings to educate our clients

00:01:17,520 --> 00:01:26,040
better about the operations of cloud

00:01:21,119 --> 00:01:28,790
foundry and best use of our solutions in

00:01:26,040 --> 00:01:28,790
their daily work

00:01:29,479 --> 00:01:35,460
previous to cloud foundry I built

00:01:33,060 --> 00:01:38,100
management systems for traditional

00:01:35,460 --> 00:01:44,240
infrastructures like Enterprise Server

00:01:38,100 --> 00:01:48,450
systems like telecom systems so I know

00:01:44,240 --> 00:01:51,170
different types of infrastructures from

00:01:48,450 --> 00:01:57,090
physical service to the cloud

00:01:51,170 --> 00:02:00,390
infrastructures we see now the first

00:01:57,090 --> 00:02:04,350
feature I want to talk about is a cloud

00:02:00,390 --> 00:02:07,409
config cloud config is a way to extract

00:02:04,350 --> 00:02:11,720
air specific configuration from your

00:02:07,409 --> 00:02:13,650
manifest typically you have pretty much

00:02:11,720 --> 00:02:18,480
information that

00:02:13,650 --> 00:02:21,299
your cloud and it is and it is the same

00:02:18,480 --> 00:02:24,510
across all the deployments

00:02:21,299 --> 00:02:28,080
one example is availabilities of

00:02:24,510 --> 00:02:30,390
availability zones so typically you use

00:02:28,080 --> 00:02:35,120
the same availability zone separation

00:02:30,390 --> 00:02:40,049
for your services for cloud founding and

00:02:35,120 --> 00:02:42,950
the next type is networks in a reference

00:02:40,049 --> 00:02:47,819
koala found the architecture you have

00:02:42,950 --> 00:02:50,849
networks for elastic runtime and you

00:02:47,819 --> 00:02:53,940
have service network that is used by all

00:02:50,849 --> 00:02:59,069
your services for my sequel red is

00:02:53,940 --> 00:03:02,220
possible scale and in some types of

00:02:59,069 --> 00:03:05,640
clouds like aw years you have predefined

00:03:02,220 --> 00:03:07,680
VM types you can't change these but for

00:03:05,640 --> 00:03:12,569
example in OpenStack deployments the way

00:03:07,680 --> 00:03:16,650
which we do when customer requires Cloud

00:03:12,569 --> 00:03:20,250
Foundry deployment in in the private

00:03:16,650 --> 00:03:23,220
data center you can define your own with

00:03:20,250 --> 00:03:25,739
no machine types or code flavors so

00:03:23,220 --> 00:03:28,829
these information is a good candidate

00:03:25,739 --> 00:03:32,879
for sharing across all the deployments

00:03:28,829 --> 00:03:36,680
and when you extract it with the cloud

00:03:32,879 --> 00:03:41,069
config you don't repeat yourself

00:03:36,680 --> 00:03:44,250
defining it in each manifest you have or

00:03:41,069 --> 00:03:49,530
deploy in your systems it reduces there

00:03:44,250 --> 00:03:54,060
and allows you to have deployment

00:03:49,530 --> 00:03:56,280
portability so you have so if you have

00:03:54,060 --> 00:03:59,730
for example two deployments in a double

00:03:56,280 --> 00:04:03,329
years of Cloud Foundry I'm sorry in

00:03:59,730 --> 00:04:06,269
Cloud Foundry you maintain two manifests

00:04:03,329 --> 00:04:08,489
and two separate cloud configs and this

00:04:06,269 --> 00:04:10,790
manifests can be pretty similar or

00:04:08,489 --> 00:04:10,790
identical

00:04:14,980 --> 00:04:22,190
as far as you know before provisioning

00:04:18,259 --> 00:04:28,789
borscht or something is it something

00:04:22,190 --> 00:04:31,370
wrong is it okay okay

00:04:28,789 --> 00:04:33,919
before provisioning boss or you need to

00:04:31,370 --> 00:04:37,280
create some kind of several

00:04:33,919 --> 00:04:41,479
infrastructure like networks define

00:04:37,280 --> 00:04:44,449
security groups and so on we typically

00:04:41,479 --> 00:04:47,509
use terraform for this purpose terraform

00:04:44,449 --> 00:04:49,820
is an awesome tool you define is a

00:04:47,509 --> 00:04:53,380
configuration in your cloud using JSON

00:04:49,820 --> 00:04:56,900
file and then you can check it to

00:04:53,380 --> 00:05:00,740
version control system you can defeat

00:04:56,900 --> 00:05:03,160
and terraform is smart enough to apply

00:05:00,740 --> 00:05:08,479
these differences to your infrastructure

00:05:03,160 --> 00:05:13,099
in a resilient manner before our post

00:05:08,479 --> 00:05:17,060
boot loader we go to outputs from these

00:05:13,099 --> 00:05:21,620
terraform script that contained network

00:05:17,060 --> 00:05:24,889
IDs subnet names and so on and we need

00:05:21,620 --> 00:05:28,750
to put these information inside of our

00:05:24,889 --> 00:05:31,580
boss manifest and then borsch understand

00:05:28,750 --> 00:05:34,750
where it is deployed and it is ready to

00:05:31,580 --> 00:05:38,750
deploy as a software possible to other

00:05:34,750 --> 00:05:41,570
automates the workflow when you create

00:05:38,750 --> 00:05:47,270
infrastructure with terraform script and

00:05:41,570 --> 00:05:49,940
then it automatically inserts the

00:05:47,270 --> 00:05:52,760
information about your particular crowd

00:05:49,940 --> 00:05:59,090
cloud into the cloud config and you will

00:05:52,760 --> 00:06:00,680
get bosh already already ready to deploy

00:05:59,090 --> 00:06:05,229
your software

00:06:00,680 --> 00:06:09,259
I like this technology because it

00:06:05,229 --> 00:06:13,699
because it saves my time in two ways

00:06:09,259 --> 00:06:17,240
first of all I can just start the

00:06:13,699 --> 00:06:19,610
deployment and it will proceed

00:06:17,240 --> 00:06:24,349
unattended and in 40 minutes I will get

00:06:19,610 --> 00:06:26,840
support deployment and the second is as

00:06:24,349 --> 00:06:30,410
it is fully automated it can be

00:06:26,840 --> 00:06:33,169
tested and I don't know I don't need to

00:06:30,410 --> 00:06:35,479
verify the infrastructure after the

00:06:33,169 --> 00:06:38,780
deployment before I need to do the

00:06:35,479 --> 00:06:41,930
verification in case I had some tipis

00:06:38,780 --> 00:06:47,830
when copying passed in these values from

00:06:41,930 --> 00:06:47,830
the terraform output to to the manifest

00:06:48,669 --> 00:06:56,090
now it is limited to AWS and Google

00:06:52,790 --> 00:06:59,570
cloud platforms but it is rapidly

00:06:56,090 --> 00:07:03,040
developing and I think soon you will be

00:06:59,570 --> 00:07:13,010
able to deploy both on this fear and

00:07:03,040 --> 00:07:15,770
OpenStack with both the cloud or two I

00:07:13,010 --> 00:07:20,389
would like to say that the technologies

00:07:15,770 --> 00:07:22,880
in Cloud Foundry ecosystem are dependent

00:07:20,389 --> 00:07:26,300
on each other for example our bottlings

00:07:22,880 --> 00:07:30,380
is a service discovery mechanism and it

00:07:26,300 --> 00:07:33,889
allows you to utilize as a technology I

00:07:30,380 --> 00:07:40,880
have talked about the cloud config more

00:07:33,889 --> 00:07:43,970
effectively if you have if you have IPS

00:07:40,880 --> 00:07:47,300
in your manifests you can't truly

00:07:43,970 --> 00:07:51,979
separate and work in configuration from

00:07:47,300 --> 00:07:55,750
the system and you can't truly use cloud

00:07:51,979 --> 00:08:00,470
config with both links you can reference

00:07:55,750 --> 00:08:04,190
one job or from another in manifest and

00:08:00,470 --> 00:08:09,050
then Bush does all the job of making

00:08:04,190 --> 00:08:13,630
these jobs to speak to each other so it

00:08:09,050 --> 00:08:17,240
is a static service discovery mechanism

00:08:13,630 --> 00:08:21,440
we use this technology to replace all

00:08:17,240 --> 00:08:24,740
the static IPS from our from our

00:08:21,440 --> 00:08:30,860
releases that we maintain for example

00:08:24,740 --> 00:08:34,010
elasticsearch one and some releases that

00:08:30,860 --> 00:08:37,760
are already grated to this technology

00:08:34,010 --> 00:08:39,440
like concourse we can deploy them using

00:08:37,760 --> 00:08:42,590
a cloud config and using

00:08:39,440 --> 00:08:57,220
poor things and not doing these static

00:08:42,590 --> 00:08:57,220
IP is right now something about security

00:08:58,990 --> 00:09:05,930
components of Cloud Foundry installation

00:09:02,650 --> 00:09:09,400
sync majority of them now I can't talk

00:09:05,930 --> 00:09:15,410
to each other using TLS encryption so

00:09:09,400 --> 00:09:18,980
nobody nobody except these two instances

00:09:15,410 --> 00:09:21,530
can understand their traffic and it is a

00:09:18,980 --> 00:09:24,020
big deal when you want to deploy your

00:09:21,530 --> 00:09:27,020
Cloud Foundry infrastructure in inside

00:09:24,020 --> 00:09:30,200
of public cloud in private cloud you can

00:09:27,020 --> 00:09:32,750
have a rack and a switch in it and you

00:09:30,200 --> 00:09:37,060
can physically secure access to your

00:09:32,750 --> 00:09:41,090
network but in a public cloud like AWS

00:09:37,060 --> 00:09:43,460
you have your network shot within shared

00:09:41,090 --> 00:09:46,070
between several instances and you can't

00:09:43,460 --> 00:09:50,630
guarantee is that somebody else is

00:09:46,070 --> 00:09:57,140
tapping into your traffic I have a story

00:09:50,630 --> 00:09:59,180
of a customer who had who had a main

00:09:57,140 --> 00:10:02,140
installation of Cloud Foundry on

00:09:59,180 --> 00:10:05,480
OpenStack in private data center and

00:10:02,140 --> 00:10:08,750
wanted to add AWS installation for

00:10:05,480 --> 00:10:11,180
elasticity because when you have

00:10:08,750 --> 00:10:14,390
installation in private data center it

00:10:11,180 --> 00:10:17,450
is elastic on the own application level

00:10:14,390 --> 00:10:19,520
on hardware level you still need to buy

00:10:17,450 --> 00:10:22,220
new hardware and to manually install it

00:10:19,520 --> 00:10:27,500
to the data center in a double years you

00:10:22,220 --> 00:10:32,510
can buy resources on demand and then

00:10:27,500 --> 00:10:34,820
freeze them and because we have no such

00:10:32,510 --> 00:10:40,360
feature at that time it was about a year

00:10:34,820 --> 00:10:45,320
to ago we could only move as a testing

00:10:40,360 --> 00:10:47,150
the testing to this environment to the

00:10:45,320 --> 00:10:50,270
AWS environment

00:10:47,150 --> 00:10:51,950
it was pretty elastic you run tests and

00:10:50,270 --> 00:10:57,200
then you can tear down

00:10:51,950 --> 00:11:01,640
this environment and it it doesn't work

00:10:57,200 --> 00:11:04,460
with the customer data so it has free

00:11:01,640 --> 00:11:12,220
tools security restrictions but now it

00:11:04,460 --> 00:11:18,380
is possible oh this is my favorite one

00:11:12,220 --> 00:11:21,800
this is boss to CLI that is written in

00:11:18,380 --> 00:11:25,090
go when I first read about it I thought

00:11:21,800 --> 00:11:35,060
okay what is a big deal of rewriting

00:11:25,090 --> 00:11:36,530
boss CLI from Ruby to go but when I do

00:11:35,060 --> 00:11:39,650
hands-on with this technology I

00:11:36,530 --> 00:11:44,300
understood that it is completely right

00:11:39,650 --> 00:11:49,490
with a separate set of features it

00:11:44,300 --> 00:11:52,090
allows you to substitute to so for

00:11:49,490 --> 00:11:55,850
merging manifests like spiffing spruce

00:11:52,090 --> 00:11:59,870
with native CLI so you don't need a

00:11:55,850 --> 00:12:06,470
separate tool to merge several parts of

00:11:59,870 --> 00:12:08,930
manifests together it it can generate a

00:12:06,470 --> 00:12:12,410
default secrets for you called foundry

00:12:08,930 --> 00:12:16,160
use a lot of secrets inside for example

00:12:12,410 --> 00:12:19,520
it uses secret that is shared between

00:12:16,160 --> 00:12:23,510
garrote or and Nats error and they use

00:12:19,520 --> 00:12:27,830
the secret to talk together is operator

00:12:23,510 --> 00:12:31,720
I don't care about how my router is

00:12:27,830 --> 00:12:34,180
talking to not cell since it is secure I

00:12:31,720 --> 00:12:37,160
care about some secrets like

00:12:34,180 --> 00:12:40,840
administrative security when I use it to

00:12:37,160 --> 00:12:43,970
log into Cloud Foundry and I care about

00:12:40,840 --> 00:12:48,490
the certificate that I place on a lot

00:12:43,970 --> 00:12:53,120
balancer or go router because it is

00:12:48,490 --> 00:12:55,550
encrypting client traffic but internal

00:12:53,120 --> 00:12:59,610
certificates between the Cloud Foundry

00:12:55,550 --> 00:13:02,100
components can be so generated and

00:12:59,610 --> 00:13:07,560
while they managed correctly I don't

00:13:02,100 --> 00:13:09,740
care what they are before - CL

00:13:07,560 --> 00:13:13,890
I was available

00:13:09,740 --> 00:13:15,630
- doom TLS encryption of traffic in

00:13:13,890 --> 00:13:18,600
Cloud Foundry you need to generate

00:13:15,630 --> 00:13:24,200
certificates manually and then insert

00:13:18,600 --> 00:13:27,180
them to the manifest and the manifests

00:13:24,200 --> 00:13:32,850
become became huge

00:13:27,180 --> 00:13:37,020
I think I saw about 5,000 lines manifest

00:13:32,850 --> 00:13:37,940
in the practice and they were work hard

00:13:37,020 --> 00:13:40,650
to match

00:13:37,940 --> 00:13:45,200
now you can generate these secrets and

00:13:40,650 --> 00:13:45,200
they will be stored locally in your file

00:13:49,670 --> 00:13:57,000
hub project e moves this pixel

00:13:54,300 --> 00:13:59,310
technology even first it automates

00:13:57,000 --> 00:14:03,330
credential generation storage and

00:13:59,310 --> 00:14:08,100
lifecycle management but it adds another

00:14:03,330 --> 00:14:11,340
feature when you when you do local

00:14:08,100 --> 00:14:13,950
credential management it is good when

00:14:11,340 --> 00:14:18,360
you are the only person that operates

00:14:13,950 --> 00:14:21,750
this deployment but when you have a team

00:14:18,360 --> 00:14:24,180
of the engineers you need a secure way

00:14:21,750 --> 00:14:29,910
to distribute credentials between them

00:14:24,180 --> 00:14:33,300
and to maintain these credentials in

00:14:29,910 --> 00:14:37,490
second in a consistent way you can use

00:14:33,300 --> 00:14:42,180
github to collaborate on configuration

00:14:37,490 --> 00:14:44,850
but for credentials the best possible

00:14:42,180 --> 00:14:48,930
solution was so something like passing

00:14:44,850 --> 00:14:53,100
these Yama file between these developed

00:14:48,930 --> 00:14:56,700
engineers Craig Harper provides you with

00:14:53,100 --> 00:14:59,970
the centralized server where you can

00:14:56,700 --> 00:15:02,400
upload these credentials and you can

00:14:59,970 --> 00:15:05,610
extract these credentials from your boss

00:15:02,400 --> 00:15:10,830
manifests so the boss manifests become

00:15:05,610 --> 00:15:12,390
is easy to manage you can just put in

00:15:10,830 --> 00:15:14,750
put them into

00:15:12,390 --> 00:15:21,570
github account and don't worry that

00:15:14,750 --> 00:15:27,930
thumb of AWS access keys leak to the bad

00:15:21,570 --> 00:15:32,760
place I want to share my personal story

00:15:27,930 --> 00:15:36,800
or with both manifests when I just

00:15:32,760 --> 00:15:40,700
started working with Cloud Foundry I

00:15:36,800 --> 00:15:43,530
deployed some stuff with Bosch and I

00:15:40,700 --> 00:15:47,870
wanted to share manifest with other

00:15:43,530 --> 00:15:52,560
people to ask for help I applauded it to

00:15:47,870 --> 00:15:57,330
to say get to the github gist and didn't

00:15:52,560 --> 00:16:01,320
delete my AWS credentials and then I

00:15:57,330 --> 00:16:04,620
discovered that there are two kinds of

00:16:01,320 --> 00:16:08,430
boats constantly searching through the

00:16:04,620 --> 00:16:13,170
github for the keys one kind of boss are

00:16:08,430 --> 00:16:14,850
the footballs they will tell you that

00:16:13,170 --> 00:16:17,820
your credentials are compromised when

00:16:14,850 --> 00:16:20,370
you when they find your key and the

00:16:17,820 --> 00:16:25,230
other boats are dead bolts so they will

00:16:20,370 --> 00:16:29,940
span about 500 VMC in the AWS account

00:16:25,230 --> 00:16:32,400
and start mining bitcoins I was lucky I

00:16:29,940 --> 00:16:34,410
received the email that my credentials

00:16:32,400 --> 00:16:39,500
were compromised and I quickly recycled

00:16:34,410 --> 00:16:45,840
them but it was not pleasant experience

00:16:39,500 --> 00:16:47,550
so I think great hub will make this

00:16:45,840 --> 00:16:51,330
credential management for the future

00:16:47,550 --> 00:16:57,570
generations of divorce engineers in call

00:16:51,330 --> 00:17:00,590
foundry less painful story sorry how

00:16:57,570 --> 00:17:00,590
much time do I have

00:17:05,580 --> 00:17:14,000
ten is yeah so then we have ten minutes

00:17:11,490 --> 00:17:18,570
I will left or five minutes for answers

00:17:14,000 --> 00:17:21,570
for questions and answers and in the

00:17:18,570 --> 00:17:25,130
last five minutes I will talk about two

00:17:21,570 --> 00:17:30,060
other interesting technologies first is

00:17:25,130 --> 00:17:34,020
CF deployment it is the way of deploying

00:17:30,060 --> 00:17:41,430
Cloud Foundry using many of previously

00:17:34,020 --> 00:17:55,100
mentioned mentioned technologies well

00:17:41,430 --> 00:17:57,690
150 okay that's great one moment it uses

00:17:55,100 --> 00:18:00,930
many of the previously managed my

00:17:57,690 --> 00:18:04,140
mentioned technologies Bosch links cloud

00:18:00,930 --> 00:18:08,270
config it separates releases of

00:18:04,140 --> 00:18:11,790
individual Cloud Foundry components to

00:18:08,270 --> 00:18:16,530
separate entities and you do deployment

00:18:11,790 --> 00:18:21,780
out of these many releases either from

00:18:16,530 --> 00:18:25,440
the single model is cf release we use

00:18:21,780 --> 00:18:29,730
these deployment for our proof of

00:18:25,440 --> 00:18:33,560
concept deployment now and we also use

00:18:29,730 --> 00:18:37,950
it for the training purposes for example

00:18:33,560 --> 00:18:42,660
yesterday my colleagues gave a training

00:18:37,950 --> 00:18:48,480
here on board and they utilize this CF

00:18:42,660 --> 00:18:50,720
deployment and both to CLI to install it

00:18:48,480 --> 00:18:55,500
is not a production yet already yet

00:18:50,720 --> 00:18:59,700
because there is no way to migrate

00:18:55,500 --> 00:19:03,390
deployment from CF release to the CF

00:18:59,700 --> 00:19:08,580
deployment release yet and it is not

00:19:03,390 --> 00:19:11,040
necessary good experience were upgrading

00:19:08,580 --> 00:19:14,220
one CF deployment to another you can you

00:19:11,040 --> 00:19:17,900
may want to delete it and to start from

00:19:14,220 --> 00:19:21,890
scratch but it is very promising and

00:19:17,900 --> 00:19:25,640
what I love what I love about this

00:19:21,890 --> 00:19:27,590
technology is that it makes deployment

00:19:25,640 --> 00:19:32,080
process of Cloud Foundry understandable

00:19:27,590 --> 00:19:36,290
it is a big deal when you speak with

00:19:32,080 --> 00:19:38,750
students in the training to make them

00:19:36,290 --> 00:19:41,090
understand how the Cloud Foundry is

00:19:38,750 --> 00:19:45,320
deployed and to make them understand

00:19:41,090 --> 00:19:48,580
this fast without driving in into

00:19:45,320 --> 00:19:54,290
specifics of speed manifest generation

00:19:48,580 --> 00:19:59,180
into specifics of deployed networking

00:19:54,290 --> 00:20:01,390
staff in the years this can be a dozens

00:19:59,180 --> 00:20:04,460
of steps doesn't solve separate steps

00:20:01,390 --> 00:20:07,730
for provisioning infrastructure in the

00:20:04,460 --> 00:20:10,580
OpenStack or a double years there can be

00:20:07,730 --> 00:20:13,310
a lot of steps merging manifests

00:20:10,580 --> 00:20:16,730
together and then adding Diego runtime

00:20:13,310 --> 00:20:20,600
inside of them and with the CF

00:20:16,730 --> 00:20:24,800
deployment you can do pretty diagrams

00:20:20,600 --> 00:20:27,890
and actually follow this process and it

00:20:24,800 --> 00:20:31,760
will consist of three or four steps and

00:20:27,890 --> 00:20:34,640
the manifests that you get using a CF

00:20:31,760 --> 00:20:37,430
deployment are pretty small so you can

00:20:34,640 --> 00:20:41,750
just go through these manifests with

00:20:37,430 --> 00:20:45,620
your students and they will understand

00:20:41,750 --> 00:20:52,160
each line of them all the deployment

00:20:45,620 --> 00:20:55,370
manifests oh it's cool on the top of

00:20:52,160 --> 00:21:01,400
your CF deployment you can put a

00:20:55,370 --> 00:21:04,460
container networking what is it it is a

00:21:01,400 --> 00:21:09,380
policy server and a dorm to Governor on

00:21:04,460 --> 00:21:13,370
see before before container networking

00:21:09,380 --> 00:21:16,700
you if one of your applications want to

00:21:13,370 --> 00:21:22,280
talk to another applications it need to

00:21:16,700 --> 00:21:27,860
go through growth on OpenStack it was

00:21:22,280 --> 00:21:31,690
even worse because the rotor typically

00:21:27,860 --> 00:21:34,870
has a floating IP and it is a separate

00:21:31,690 --> 00:21:37,600
and you have a lot of traffic between

00:21:34,870 --> 00:21:42,000
the physical machines and on different

00:21:37,600 --> 00:21:44,710
levels we and the reason was that

00:21:42,000 --> 00:21:49,890
policies were enforced on the rotor

00:21:44,710 --> 00:21:53,050
level and it it was hard to put

00:21:49,890 --> 00:21:56,620
container to container networking now

00:21:53,050 --> 00:22:00,460
the policies are enforced on Diego cell

00:21:56,620 --> 00:22:02,530
level and your containers can talk to

00:22:00,460 --> 00:22:06,880
each other without going through garrote

00:22:02,530 --> 00:22:08,770
it will decrease the amount of traffic

00:22:06,880 --> 00:22:14,050
going through the rotor so you need

00:22:08,770 --> 00:22:17,140
possibly less VMs and it will decrease

00:22:14,050 --> 00:22:20,830
the latency of your micro-services

00:22:17,140 --> 00:22:23,890
communications so sometimes you have a

00:22:20,830 --> 00:22:26,890
pretty deep stack when clients request

00:22:23,890 --> 00:22:31,420
his a first micro-services and it runs

00:22:26,890 --> 00:22:34,360
out to other levels and this

00:22:31,420 --> 00:22:37,780
microservices reply back and the final

00:22:34,360 --> 00:22:46,450
answer goes to the client now this

00:22:37,780 --> 00:22:50,890
latency is lower another story was a

00:22:46,450 --> 00:22:57,180
performance story one day we discovered

00:22:50,890 --> 00:22:59,980
that our client has timeout problems so

00:22:57,180 --> 00:23:03,580
engineers try to push applications to

00:22:59,980 --> 00:23:06,850
Cloud Foundry and a timeout it and we

00:23:03,580 --> 00:23:14,500
looked in this deployment and found out

00:23:06,850 --> 00:23:18,430
that etcd was about 1 + 100 % busy it

00:23:14,500 --> 00:23:22,920
was busy on this kilo and these

00:23:18,430 --> 00:23:28,750
deployment was on pretty slow HDD drives

00:23:22,920 --> 00:23:32,290
we tried to age 80 CD to be faster but

00:23:28,750 --> 00:23:37,510
it turns out that the possible solution

00:23:32,290 --> 00:23:40,500
was just to replace HDD with STDs it

00:23:37,510 --> 00:23:45,600
doesn't suit us we did

00:23:40,500 --> 00:23:49,770
a hack like we created a ram disk and

00:23:45,600 --> 00:23:53,070
make etcd right to this RAM disk instead

00:23:49,770 --> 00:23:56,850
of actual hard drive disk to go this

00:23:53,070 --> 00:24:03,770
problem what it is was a nasty shock and

00:23:56,850 --> 00:24:08,130
I didn't like it sometimes later diego

00:24:03,770 --> 00:24:11,670
was able to store its data on relational

00:24:08,130 --> 00:24:15,840
databases like post ways like my sequel

00:24:11,670 --> 00:24:23,880
and we moved to post press and now we

00:24:15,840 --> 00:24:27,180
have known this problem and so us the

00:24:23,880 --> 00:24:31,650
technology i want to speak about and I

00:24:27,180 --> 00:24:32,330
wish I would have this technology one

00:24:31,650 --> 00:24:36,690
ergo

00:24:32,330 --> 00:24:38,640
these are isolation segments reservation

00:24:36,690 --> 00:24:42,570
segments allow you to define group

00:24:38,640 --> 00:24:44,790
public a group of applications that run

00:24:42,570 --> 00:24:49,800
apotheke on particular set of hardware

00:24:44,790 --> 00:24:54,450
so you can efficiently so you can

00:24:49,800 --> 00:24:57,060
efficiently separate separate different

00:24:54,450 --> 00:24:59,750
environments not on only on the

00:24:57,060 --> 00:25:03,840
container level but on the VM level and

00:24:59,750 --> 00:25:08,100
combined with routine groups you can

00:25:03,840 --> 00:25:10,600
separate traffic from these isolation

00:25:08,100 --> 00:25:11,930
segments to separate Goro terms

00:25:10,600 --> 00:25:16,560
[Music]

00:25:11,930 --> 00:25:20,820
typically client typically clients want

00:25:16,560 --> 00:25:25,590
to separate environments like production

00:25:20,820 --> 00:25:29,940
or staging development and before we

00:25:25,590 --> 00:25:34,320
used to deploy several Cloud Foundry

00:25:29,940 --> 00:25:38,940
instances for these purposes but it

00:25:34,320 --> 00:25:42,750
resulted in increased effort required to

00:25:38,940 --> 00:25:46,110
maintain these instances of call family

00:25:42,750 --> 00:25:48,450
at this required more resources because

00:25:46,110 --> 00:25:49,650
you are duplicating stuff like Cloud

00:25:48,450 --> 00:25:52,860
Controller

00:25:49,650 --> 00:25:58,170
and everything else that can be pretty

00:25:52,860 --> 00:26:01,020
easily shocked now cease technology is

00:25:58,170 --> 00:26:04,500
generally available we haven't used it

00:26:01,020 --> 00:26:06,510
in our projects yet but we are looking

00:26:04,500 --> 00:26:10,520
forward to do some proof of concepts

00:26:06,510 --> 00:26:17,309
with it and to make future deployments

00:26:10,520 --> 00:26:21,600
use this technology this is just a small

00:26:17,309 --> 00:26:24,870
part okay this is just as part of the

00:26:21,600 --> 00:26:27,059
technologies that were matched in the

00:26:24,870 --> 00:26:32,240
cloud foundry system during the last

00:26:27,059 --> 00:26:37,290
year when I did research for this talk I

00:26:32,240 --> 00:26:41,190
outlined for about 50 techno 50 separate

00:26:37,290 --> 00:26:45,360
technologies and if you want to talk to

00:26:41,190 --> 00:26:48,090
me about other technologies and to share

00:26:45,360 --> 00:26:50,730
your stories of operating them please

00:26:48,090 --> 00:26:57,870
come to our booth to all ultros booze

00:26:50,730 --> 00:27:01,520
that is in the hope and now it is yeah

00:26:57,870 --> 00:27:01,520
now it is time for the questions

00:27:19,800 --> 00:27:29,470
question about mutual TLS okay is it the

00:27:26,500 --> 00:27:31,870
TLS between wash components or it is

00:27:29,470 --> 00:27:37,390
between bars and other deployments for

00:27:31,870 --> 00:27:40,570
example CF this is TLS between 15-year

00:27:37,390 --> 00:27:43,450
Bosch detector no no I'm speaking about

00:27:40,570 --> 00:27:50,380
TLS between Cloud Foundry components

00:27:43,450 --> 00:27:53,530
like etcd and Diego like go rotor and

00:27:50,380 --> 00:27:55,030
nuts get you a lesson is there any

00:27:53,530 --> 00:27:57,400
additional functionality because I think

00:27:55,030 --> 00:28:00,760
in the current deployment will you we do

00:27:57,400 --> 00:28:02,860
have certificates like for example air

00:28:00,760 --> 00:28:05,950
city has its own CA and certificates so

00:28:02,860 --> 00:28:07,720
similarly council just wanted to

00:28:05,950 --> 00:28:11,730
understand what's the new functionality

00:28:07,720 --> 00:28:11,730
that we're bringing using the mutual TLS

00:28:13,170 --> 00:28:19,270
here I speak about the previous

00:28:16,900 --> 00:28:23,200
deployments when you don't put

00:28:19,270 --> 00:28:26,440
certificates on your VMs or so they talk

00:28:23,200 --> 00:28:31,000
to using unencrypted traffic and it was

00:28:26,440 --> 00:28:34,170
pretty easy to debug for example because

00:28:31,000 --> 00:28:36,970
you can capture packets and now you put

00:28:34,170 --> 00:28:39,730
certificates to the VMS to the different

00:28:36,970 --> 00:28:40,360
VM so deployed by Bosch and when they

00:28:39,730 --> 00:28:43,480
communicate

00:28:40,360 --> 00:28:48,040
you can't more see this traffic flowing

00:28:43,480 --> 00:28:54,450
make sense this is involved we have time

00:28:48,040 --> 00:28:57,040
for another question anyway all right

00:28:54,450 --> 00:29:00,460
the isolation segments that you

00:28:57,040 --> 00:29:04,450
mentioned last what are their criteria

00:29:00,460 --> 00:29:09,910
you can use to like segment your abs can

00:29:04,450 --> 00:29:13,350
you do it like by org by space do you

00:29:09,910 --> 00:29:13,350
have any information on that

00:29:13,600 --> 00:29:20,440
you have to define your isolation

00:29:16,990 --> 00:29:23,920
segments in two places first you define

00:29:20,440 --> 00:29:28,810
them in your borscht deployment where

00:29:23,920 --> 00:29:30,650
you mark other selves that from one

00:29:28,810 --> 00:29:32,870
segment or another segment

00:29:30,650 --> 00:29:38,300
and then you can bind your isolation

00:29:32,870 --> 00:29:43,550
segment to we go organization oh ah we

00:29:38,300 --> 00:29:45,200
have one more all right oh so I guess

00:29:43,550 --> 00:29:46,700
you can just space that isolation

00:29:45,200 --> 00:29:48,590
yeah all right thank you very much Alex

00:29:46,700 --> 00:29:51,720
I appreciate it thank you

00:29:48,590 --> 00:29:51,720

YouTube URL: https://www.youtube.com/watch?v=kDvsLDAYXRY


