Title: To Kill a Monolith: Slaying the Demons of a Monolith with Node.js Microservices on CloudFoundry
Publication date: 2017-06-22
Playlist: Cloud Foundry Summit Silicon Valley 2017
Description: 
	To Kill a Monolith: Slaying the Demons of a Monolith with Node.js Microservices on CloudFoundry [I] - Tony Erwin, IBM    

The Bluemix UI (which runs on CloudFoundry) is the front-end to Bluemix, IBM's open cloud hosting platform. The original implementation as a single-page, monolithic Java web app brought with it many demons, such as poor performance, lack of scalability, inability to push small updates, and difficulty for other teams to contribute code. Over the last 2 years, the team has been on a mission to slay these demons by embracing cloud native principles and splitting the monolith into smaller Node.js microservices. The effort to migrate to a more modern and scalable architecture has paid large dividends, but has also left behind a few battle scars from wrestling with the added complexity cloud native can bring. The team had to tackle problems in a wide variety of areas, including: large-scale deployments, continuous integration, monitoring, problem determination, high availability, and security. Tony Erwin will discuss the advantages of microservice architectures, ways that Node.js has increased developer productivity, approaches to phasing microservices into a live product, and real-life lessons learned in the deployment and management of Node.js microservices across multiple CloudFoundry environments. His war stories will prepare you to wage your own battles against monoliths everywhere -- happy slaying!


Tony Erwin
IBM
Senior Software Engineer
Austin, Texas Area
Twitter Tweet Facebook Message  Websitehttps://tonyerwin.com
Tony Erwin is a Senior Software Engineer at IBM and currently the Lead Architect for the IBM Bluemix UI. He's been with IBM for 18 years and has extensive full-stack experience building UIs using a wide variety of technologies. Current interests include cloud, Node.js, microservices, reliability, and performance. In addition, he's a semi-regular blogger on Bluemix and CloudFoundry-related topics.
Captions: 
	00:00:00,030 --> 00:00:04,830
my name is Tony Erwin and I'm going to

00:00:01,709 --> 00:00:09,840
talk to you today about some work my

00:00:04,830 --> 00:00:13,219
team has done with the bluemix UI which

00:00:09,840 --> 00:00:16,109
is the front end to IBM's cloud offering

00:00:13,219 --> 00:00:20,400
with cloud foundry as the pass layer

00:00:16,109 --> 00:00:22,220
there talk about the the process we went

00:00:20,400 --> 00:00:25,380
through in migrating a monolithic

00:00:22,220 --> 00:00:30,210
application to microservices nodejs

00:00:25,380 --> 00:00:31,380
mantra services on cloud foundry and

00:00:30,210 --> 00:00:34,440
during this talk we'll talk about the

00:00:31,380 --> 00:00:38,219
origins of the bluemix UI some of the

00:00:34,440 --> 00:00:40,710
demons of the monolith our original

00:00:38,219 --> 00:00:45,649
monolithic architecture how my cursor

00:00:40,710 --> 00:00:47,940
versus helped slay those demons and then

00:00:45,649 --> 00:00:49,410
sometimes you trade one sets of prop set

00:00:47,940 --> 00:00:54,960
of problems for another so we have some

00:00:49,410 --> 00:00:57,270
new demons to sway as well so as I sort

00:00:54,960 --> 00:00:59,820
of alluded to that the bluemix UI serves

00:00:57,270 --> 00:01:02,789
as the front end to bluemix it lets

00:00:59,820 --> 00:01:06,270
users create view and manage Cloud

00:01:02,789 --> 00:01:08,159
Foundry resources but not just Cloud

00:01:06,270 --> 00:01:11,210
Foundry we also have containers and

00:01:08,159 --> 00:01:13,890
virtual servers and other resource types

00:01:11,210 --> 00:01:15,270
coming around so when we first started

00:01:13,890 --> 00:01:18,000
bluemix some of you may know it was

00:01:15,270 --> 00:01:20,159
pretty Cloud Foundry focused now cloud

00:01:18,000 --> 00:01:24,479
foundry is just a part of our bluemix

00:01:20,159 --> 00:01:28,860
offering it runs on top of the bluemix

00:01:24,479 --> 00:01:30,900
pass layer which is cloud foundry as I

00:01:28,860 --> 00:01:34,049
alluded to it started as a monolithic

00:01:30,900 --> 00:01:37,950
app it was a single page application so

00:01:34,049 --> 00:01:39,810
all the HTML CSS JavaScript was served

00:01:37,950 --> 00:01:43,560
yeah I had to be loaded to the page at

00:01:39,810 --> 00:01:47,630
once basically in one HTML page and this

00:01:43,560 --> 00:01:51,180
was all served by a single Java app

00:01:47,630 --> 00:01:55,320
which was also deployed to Cloud Foundry

00:01:51,180 --> 00:01:58,130
this was a common stack in IBM not all

00:01:55,320 --> 00:02:01,680
that long ago where everyone was using

00:01:58,130 --> 00:02:06,090
the dojo JavaScript framework for the UI

00:02:01,680 --> 00:02:09,030
piece and serving it with Java so that

00:02:06,090 --> 00:02:13,670
was very popular in IBM in recent years

00:02:09,030 --> 00:02:13,670
but we sort of move into other stacks

00:02:14,469 --> 00:02:18,040
and this is just sort of a stream that

00:02:16,569 --> 00:02:21,069
shows that the bluemix UI is pretty

00:02:18,040 --> 00:02:22,750
large there's you know this is five or

00:02:21,069 --> 00:02:25,810
six different pages therefore dashboard

00:02:22,750 --> 00:02:27,760
catalog resource details like manage

00:02:25,810 --> 00:02:30,010
your Cloud Foundry Apps users billing

00:02:27,760 --> 00:02:35,170
and there's a whole lot more so it's a

00:02:30,010 --> 00:02:37,480
large application and this was this is

00:02:35,170 --> 00:02:40,599
sort of a diagram or it is a diagram

00:02:37,480 --> 00:02:42,069
that sort of a diagram of the monolithic

00:02:40,599 --> 00:02:45,099
architecture that we started with the

00:02:42,069 --> 00:02:47,620
the top where it says bluemix UI or a

00:02:45,099 --> 00:02:50,650
client is basically the web browser and

00:02:47,620 --> 00:02:52,299
the orange box is there a roughly

00:02:50,650 --> 00:02:55,419
correspond to the pages I showed on the

00:02:52,299 --> 00:02:57,389
previous slide but that's just to show

00:02:55,419 --> 00:03:01,269
that's really just all the JavaScript

00:02:57,389 --> 00:03:04,060
logic is out on the browser essentially

00:03:01,269 --> 00:03:06,489
in our single page app and then on the

00:03:04,060 --> 00:03:10,419
backend the Cloud Foundry side we've got

00:03:06,489 --> 00:03:14,019
a single UI server which is Java it was

00:03:10,419 --> 00:03:17,739
bound to a db2 service for some of the

00:03:14,019 --> 00:03:20,650
data persistence we had to do and you

00:03:17,739 --> 00:03:22,569
know basically that pass through then to

00:03:20,650 --> 00:03:24,489
a whole lot of back-end API you know

00:03:22,569 --> 00:03:32,379
including Cloud Foundry of a Cloud

00:03:24,489 --> 00:03:34,870
Foundry Cloud Controller you AAA etc so

00:03:32,379 --> 00:03:37,840
the monolith had some problems or this

00:03:34,870 --> 00:03:39,549
architecture had some problems we had

00:03:37,840 --> 00:03:41,919
performance issues

00:03:39,549 --> 00:03:44,349
the heavyweight dojo is a rather large

00:03:41,919 --> 00:03:47,290
library and we also wrote a lot of

00:03:44,349 --> 00:03:48,939
JavaScript bewafa logic on the client so

00:03:47,290 --> 00:03:53,349
heavyweight JavaScript loaded to the

00:03:48,939 --> 00:03:55,030
browser can be slow and also in a single

00:03:53,349 --> 00:03:56,620
page app if you know a number of you

00:03:55,030 --> 00:03:59,409
probably worked on them you're relying

00:03:56,620 --> 00:04:02,109
totally on Ajax calls back from the the

00:03:59,409 --> 00:04:06,489
client so that can also create some

00:04:02,109 --> 00:04:09,909
bottlenecks because really nothing is in

00:04:06,489 --> 00:04:12,939
your initial payload except for the code

00:04:09,909 --> 00:04:14,500
that loads your your front-end there's

00:04:12,939 --> 00:04:18,250
nothing about the data that you actually

00:04:14,500 --> 00:04:20,950
want to see another problem was it's

00:04:18,250 --> 00:04:23,760
very difficult to integrate code from

00:04:20,950 --> 00:04:25,590
other teams so as we'll talk more about

00:04:23,760 --> 00:04:27,770
here in a bit

00:04:25,590 --> 00:04:30,600
with you know there's probably 15 and

00:04:27,770 --> 00:04:34,080
Melissa's growing other teams that want

00:04:30,600 --> 00:04:36,960
to plug in to our UI and so we all look

00:04:34,080 --> 00:04:39,060
like kind of one big product with the

00:04:36,960 --> 00:04:41,550
stack we had that was really not a

00:04:39,060 --> 00:04:43,560
practical you know you have to tell

00:04:41,550 --> 00:04:45,990
people to write dojo we had fewer people

00:04:43,560 --> 00:04:49,080
and fewer groups that wanted to write

00:04:45,990 --> 00:04:51,120
Toto for one thing and it's just not

00:04:49,080 --> 00:04:56,460
that easy to integrate these sorts of

00:04:51,120 --> 00:04:58,710
plugins into a single page app you have

00:04:56,460 --> 00:05:01,290
to push the whole prod product just for

00:04:58,710 --> 00:05:03,169
small changes so you know I fix a

00:05:01,290 --> 00:05:06,030
nullpointerexception I have to redeploy

00:05:03,169 --> 00:05:08,450
the whole product as opposed to just

00:05:06,030 --> 00:05:12,860
being able to deploy a part of the

00:05:08,450 --> 00:05:15,060
poorer SEO search engine optimization

00:05:12,860 --> 00:05:18,060
because as I alluded to there's not a

00:05:15,060 --> 00:05:20,310
whole lot of content in the one HTML

00:05:18,060 --> 00:05:24,180
page that was served and there wasn't

00:05:20,310 --> 00:05:27,870
much that was crawlable by Google and

00:05:24,180 --> 00:05:30,360
other search engines and new hires

00:05:27,870 --> 00:05:32,690
wanted nothing to do with dojo as we

00:05:30,360 --> 00:05:35,280
brought on some new front-end developers

00:05:32,690 --> 00:05:38,220
they're like why are you guys using dojo

00:05:35,280 --> 00:05:39,570
there's better stuff for or you know if

00:05:38,220 --> 00:05:41,280
you've been in UI development you know

00:05:39,570 --> 00:05:44,450
there's always you know every six months

00:05:41,280 --> 00:05:47,220
there's a there's a new cool toy

00:05:44,450 --> 00:05:48,930
dojo had a lot of good things about it

00:05:47,220 --> 00:05:55,200
but it was also kind of viewed as as

00:05:48,930 --> 00:05:57,960
legacy the old old IBM kind of stuff so

00:05:55,200 --> 00:06:00,660
now we'll get a bit into the micro

00:05:57,960 --> 00:06:04,500
services architecture that we migrated

00:06:00,660 --> 00:06:06,510
to and I call this slide the weapons of

00:06:04,500 --> 00:06:10,590
micro services that we used to slay

00:06:06,510 --> 00:06:14,010
those demons from the previous page but

00:06:10,590 --> 00:06:15,900
it the the approach helped us to migrate

00:06:14,010 --> 00:06:17,849
to a more modern lighter weight stack

00:06:15,900 --> 00:06:21,870
now based on nodejs

00:06:17,849 --> 00:06:25,440
and other tools without starting over so

00:06:21,870 --> 00:06:27,030
we were able to keep this was also in a

00:06:25,440 --> 00:06:29,070
live running product that people were

00:06:27,030 --> 00:06:32,280
reusing and we couldn't just suddenly

00:06:29,070 --> 00:06:34,500
throw away all the stuff we had during

00:06:32,280 --> 00:06:36,479
this reaaargh kotecha but with the micro

00:06:34,500 --> 00:06:38,669
services we were allowed to slowly break

00:06:36,479 --> 00:06:39,480
pieces of the monolith apart while still

00:06:38,669 --> 00:06:41,940
leaving the core

00:06:39,480 --> 00:06:46,170
of the monolithic app there and I'll

00:06:41,940 --> 00:06:48,390
show a diagram of that here in a bit the

00:06:46,170 --> 00:06:50,910
goal is to break the monolith into

00:06:48,390 --> 00:06:53,400
smaller services to improve performance

00:06:50,910 --> 00:06:59,550
because these services be optimized for

00:06:53,400 --> 00:07:01,200
speed and page size this architecture we

00:06:59,550 --> 00:07:01,890
believe would increase developer

00:07:01,200 --> 00:07:04,410
productivity

00:07:01,890 --> 00:07:06,440
you can push smaller changes there's

00:07:04,410 --> 00:07:11,280
less chance of breaking the entire

00:07:06,440 --> 00:07:14,370
product loosely coupled services can

00:07:11,280 --> 00:07:17,280
deploy at their own schedule teams can

00:07:14,370 --> 00:07:19,680
use stack of their choice as they they

00:07:17,280 --> 00:07:22,860
plugin and you don't have to wait on

00:07:19,680 --> 00:07:25,380
others because yeah I lead the core what

00:07:22,860 --> 00:07:27,780
we call the core team and you know I

00:07:25,380 --> 00:07:29,730
think we've got about 25 micro services

00:07:27,780 --> 00:07:32,640
we have some of the core components like

00:07:29,730 --> 00:07:34,470
catalog and dashboard but we have a lot

00:07:32,640 --> 00:07:36,330
of other teams that you know want to do

00:07:34,470 --> 00:07:38,040
custom things and they don't want to

00:07:36,330 --> 00:07:39,720
wait on my team doesn't have the

00:07:38,040 --> 00:07:41,550
resources for example to provide that

00:07:39,720 --> 00:07:46,130
and and teams don't want to wait on us

00:07:41,550 --> 00:07:46,130
either so this helps solve that problem

00:07:46,250 --> 00:07:51,480
the way we started serving pages led to

00:07:49,080 --> 00:07:52,680
improved SEO because we started using a

00:07:51,480 --> 00:07:56,250
little bit more server-side templating

00:07:52,680 --> 00:07:58,650
so that was more of the user data in the

00:07:56,250 --> 00:08:03,210
initial payload so there was more

00:07:58,650 --> 00:08:05,280
crawlable content and the way we did it

00:08:03,210 --> 00:08:08,250
and I'll show a diagram of this as well

00:08:05,280 --> 00:08:10,080
you know when you when teams plug-in or

00:08:08,250 --> 00:08:12,410
micro surfaces plug-in you want them all

00:08:10,080 --> 00:08:15,510
to appear to be part of the same product

00:08:12,410 --> 00:08:18,030
so we were able to help promote some UI

00:08:15,510 --> 00:08:25,380
consistency with some micro surface

00:08:18,030 --> 00:08:29,000
composition so this slide basically

00:08:25,380 --> 00:08:31,890
shows our general microservice pattern

00:08:29,000 --> 00:08:34,950
kind of focusing on UI micro services we

00:08:31,890 --> 00:08:38,460
also have micro services that just serve

00:08:34,950 --> 00:08:42,840
api's as well but all of our micro

00:08:38,460 --> 00:08:45,300
services are written in nodejs they

00:08:42,840 --> 00:08:48,180
serve lightweight HTML CSS javascript

00:08:45,300 --> 00:08:49,970
trying to go for the simplest approach

00:08:48,180 --> 00:08:53,070
that work so if we could use vanilla

00:08:49,970 --> 00:08:56,850
JavaScript for a particular page

00:08:53,070 --> 00:08:59,790
we did it we do have some teams using

00:08:56,850 --> 00:09:02,490
other frameworks you know including my

00:08:59,790 --> 00:09:04,199
team like react where it makes sense for

00:09:02,490 --> 00:09:06,509
some of our richer pages and stuff but

00:09:04,199 --> 00:09:09,240
still a far smaller footprint than we

00:09:06,509 --> 00:09:12,980
had with our dojo

00:09:09,240 --> 00:09:16,560
we use server-side templating just j/s

00:09:12,980 --> 00:09:17,310
in particular to make as much data as

00:09:16,560 --> 00:09:18,930
possible

00:09:17,310 --> 00:09:20,639
now of course you don't want to spend a

00:09:18,930 --> 00:09:22,589
bunch of time on the server collecting

00:09:20,639 --> 00:09:24,329
data to include in the payload but but

00:09:22,589 --> 00:09:26,880
some things that we have cache like the

00:09:24,329 --> 00:09:28,589
user name and picture and and you know a

00:09:26,880 --> 00:09:31,110
lot of the stuff in the header we can

00:09:28,589 --> 00:09:35,029
include so when the page comes up you

00:09:31,110 --> 00:09:38,430
know the header renders right away and

00:09:35,029 --> 00:09:42,149
kind of that goes to the next point if

00:09:38,430 --> 00:09:44,550
you look at this diagram there's a

00:09:42,149 --> 00:09:47,279
common header microservice that was

00:09:44,550 --> 00:09:50,759
added and all of our UI micro services

00:09:47,279 --> 00:09:54,269
call that to get the HTML for the top

00:09:50,759 --> 00:09:55,949
top row the stuff at the top which I'll

00:09:54,269 --> 00:10:00,600
show us the green shot of that here

00:09:55,949 --> 00:10:04,410
shortly we introduced a shared session

00:10:00,600 --> 00:10:06,209
store it wasn't so wasn't as required

00:10:04,410 --> 00:10:09,050
with our Java app because you just have

00:10:06,209 --> 00:10:11,550
an in-memory session but when you have

00:10:09,050 --> 00:10:13,980
distributed apps you know want to share

00:10:11,550 --> 00:10:17,670
things like user tokens and things we

00:10:13,980 --> 00:10:20,459
had to add Redis to the mix there and

00:10:17,670 --> 00:10:23,190
then of course these UI micro services

00:10:20,459 --> 00:10:29,010
can call any of the other back into API

00:10:23,190 --> 00:10:30,329
is or other API micro services the one

00:10:29,010 --> 00:10:33,319
the one thing I always kind of glossed

00:10:30,329 --> 00:10:35,730
over in this picture is is the proxy

00:10:33,319 --> 00:10:38,490
there at the top in the Cloud Foundry

00:10:35,730 --> 00:10:41,189
box the proxy is really what sort of

00:10:38,490 --> 00:10:43,199
holds the whole thing together just now

00:10:41,189 --> 00:10:45,630
instead of having a route you know say

00:10:43,199 --> 00:10:48,240
console bluemix net that just goes to a

00:10:45,630 --> 00:10:50,759
Java app we now have that route go into

00:10:48,240 --> 00:10:55,459
the proxy and based on the path of the

00:10:50,759 --> 00:10:55,459
URL it routes to the right Micra service

00:10:57,889 --> 00:11:03,300
so this shows what I was talking about

00:11:01,079 --> 00:11:04,920
with page composition and you know I

00:11:03,300 --> 00:11:06,870
mentioned the common header that the

00:11:04,920 --> 00:11:08,700
Micra services call

00:11:06,870 --> 00:11:11,190
so basically you know the greenbox is

00:11:08,700 --> 00:11:13,620
they any micro service let's say the

00:11:11,190 --> 00:11:16,500
catalog on the server side it will

00:11:13,620 --> 00:11:20,460
invoke the common API and get the HTML

00:11:16,500 --> 00:11:23,430
for what we call the common header and

00:11:20,460 --> 00:11:24,900
there's sort of a picture of it here and

00:11:23,430 --> 00:11:27,180
then that's combined with the

00:11:24,900 --> 00:11:29,850
server-side templating into one payload

00:11:27,180 --> 00:11:32,310
and sent to the browser and so then all

00:11:29,850 --> 00:11:34,320
pages that use this approach you know

00:11:32,310 --> 00:11:36,180
look like they're there's other things

00:11:34,320 --> 00:11:38,190
in common too that you access like some

00:11:36,180 --> 00:11:41,970
common style sheets and things too so

00:11:38,190 --> 00:11:47,960
that plus the header really enables the

00:11:41,970 --> 00:11:50,490
product to look consistent and here's a

00:11:47,960 --> 00:11:53,550
picture of our first stage of the

00:11:50,490 --> 00:11:59,160
migration and this was as of about

00:11:53,550 --> 00:12:02,820
December 2015 we formally introduced the

00:11:59,160 --> 00:12:05,580
proxy layer that I alluded to we added

00:12:02,820 --> 00:12:08,820
three micro services alongside of our

00:12:05,580 --> 00:12:10,560
our Java monolith one was the common

00:12:08,820 --> 00:12:13,020
header we had a home

00:12:10,560 --> 00:12:16,470
Micra service just for the the home page

00:12:13,020 --> 00:12:19,230
and solutions which was some marketing

00:12:16,470 --> 00:12:23,970
material which we no longer have in the

00:12:19,230 --> 00:12:26,190
core product but so we started trying to

00:12:23,970 --> 00:12:28,680
pick pieces that we thought would be the

00:12:26,190 --> 00:12:32,880
simplest to migrate you know kind of as

00:12:28,680 --> 00:12:35,270
a proof of concept we we also introduced

00:12:32,880 --> 00:12:38,130
two additional Cloud Foundry services

00:12:35,270 --> 00:12:40,080
the I alluded to the shared session I

00:12:38,130 --> 00:12:42,000
guess we actually use data cache for

00:12:40,080 --> 00:12:44,780
that back in this time frame we're using

00:12:42,000 --> 00:12:48,660
Redis now data cache was an IBM product

00:12:44,780 --> 00:12:50,490
and a no SQL DB for some data storage so

00:12:48,660 --> 00:12:54,060
these are just these micro services or

00:12:50,490 --> 00:13:02,970
just Cloud Foundry apps and bound to

00:12:54,060 --> 00:13:06,030
those services phase two

00:13:02,970 --> 00:13:10,430
basically I'm just showing more boxes

00:13:06,030 --> 00:13:14,690
moved from the top browser side of the

00:13:10,430 --> 00:13:17,550
product and down into the cloud foundry

00:13:14,690 --> 00:13:18,790
at this point which was about a year

00:13:17,550 --> 00:13:21,550
after

00:13:18,790 --> 00:13:23,560
the slide the previous slide we were

00:13:21,550 --> 00:13:26,290
probably about 90% complete we still had

00:13:23,560 --> 00:13:30,520
some account you know user management

00:13:26,290 --> 00:13:35,710
and things that were not migrated yet to

00:13:30,520 --> 00:13:38,260
the new architecture and then you know

00:13:35,710 --> 00:13:42,310
this is our end goal which were more or

00:13:38,260 --> 00:13:44,230
less at today except we do still have

00:13:42,310 --> 00:13:46,540
our Java server we do want to pork that

00:13:44,230 --> 00:13:48,940
it's working fine for us I think we do

00:13:46,540 --> 00:13:51,910
want to still port it to to node before

00:13:48,940 --> 00:13:55,300
we're done and we do still have a small

00:13:51,910 --> 00:13:58,540
amount of legacy dojo code so so it was

00:13:55,300 --> 00:14:01,450
a you know we had to balance new

00:13:58,540 --> 00:14:03,220
function over r-e architecture so you

00:14:01,450 --> 00:14:06,970
know over about two years we were able

00:14:03,220 --> 00:14:09,790
to do a pretty complete migration of the

00:14:06,970 --> 00:14:13,650
original product while you know adding

00:14:09,790 --> 00:14:13,650
some new function and things like that

00:14:14,400 --> 00:14:21,000
this slide I loaded too earlier we have

00:14:17,980 --> 00:14:27,910
a bunch of other teams that wants you

00:14:21,000 --> 00:14:29,400
plug in I the the the the what I've been

00:14:27,910 --> 00:14:31,660
showing are really the core

00:14:29,400 --> 00:14:34,360
microservices the core pieces of the

00:14:31,660 --> 00:14:35,890
architecture that the my team owns but

00:14:34,360 --> 00:14:38,800
we have things like you know Watson

00:14:35,890 --> 00:14:43,270
Internet of Things our new kubernetes

00:14:38,800 --> 00:14:46,180
service open whisk that want to be part

00:14:43,270 --> 00:14:47,710
of the console but not necessarily

00:14:46,180 --> 00:14:49,690
deployed with all the core micro

00:14:47,710 --> 00:14:51,730
services or owned by my core team and

00:14:49,690 --> 00:14:54,720
this just kind of shows that the proxy

00:14:51,730 --> 00:14:59,200
so we have proxy rules for like slash

00:14:54,720 --> 00:15:00,640
Watson that will route to a server owned

00:14:59,200 --> 00:15:03,520
by the Watson team that could be

00:15:00,640 --> 00:15:05,320
deployed deployed everywhere anywhere we

00:15:03,520 --> 00:15:07,990
proxy through they can use our common

00:15:05,320 --> 00:15:14,110
header and you know look like all part

00:15:07,990 --> 00:15:16,330
of the same same product and I might do

00:15:14,110 --> 00:15:20,170
eight well I guess I don't have the I

00:15:16,330 --> 00:15:23,710
was going to do a demo switched to a new

00:15:20,170 --> 00:15:25,240
PC thank you just before before this I

00:15:23,710 --> 00:15:28,450
don't think I'm going to try to fire up

00:15:25,240 --> 00:15:30,340
the browser and login and stuff but what

00:15:28,450 --> 00:15:32,140
I really wanted to show is that you know

00:15:30,340 --> 00:15:36,010
if it as you're clicking on different

00:15:32,140 --> 00:15:38,470
of the UI you'll seamlessly go to stuff

00:15:36,010 --> 00:15:41,050
owned by other teams you'll to see the

00:15:38,470 --> 00:15:43,180
path in the URL change the proxy you

00:15:41,050 --> 00:15:49,210
know routes through across appropriately

00:15:43,180 --> 00:15:52,810
and it all kind of blends together so

00:15:49,210 --> 00:15:54,460
there are a number of new challenges as

00:15:52,810 --> 00:15:59,260
I mentioned earlier sometimes you trade

00:15:54,460 --> 00:16:01,510
one set of problems for another I think

00:15:59,260 --> 00:16:02,920
you know we're glad we did it but there

00:16:01,510 --> 00:16:06,220
were other things we had to worry about

00:16:02,920 --> 00:16:07,960
now there's more more moving parts more

00:16:06,220 --> 00:16:10,570
complexity

00:16:07,960 --> 00:16:13,450
it makes your your build pipeline and

00:16:10,570 --> 00:16:16,050
test automation and all those things all

00:16:13,450 --> 00:16:18,640
the more important I think we probably

00:16:16,050 --> 00:16:22,840
underestimated that when we started down

00:16:18,640 --> 00:16:25,110
this path collecting federated status

00:16:22,840 --> 00:16:29,230
monitoring the health of the system

00:16:25,110 --> 00:16:31,120
something goes wrong at 2:00 a.m. you

00:16:29,230 --> 00:16:32,770
know the console is not rendering you

00:16:31,120 --> 00:16:35,590
know which of our micro services is a

00:16:32,770 --> 00:16:37,840
problem or you know and sometimes it's

00:16:35,590 --> 00:16:40,630
you know outside of our control like the

00:16:37,840 --> 00:16:45,250
Cloud Foundry environment we've been

00:16:40,630 --> 00:16:49,660
deployed to has problems so we needed a

00:16:45,250 --> 00:16:52,720
way to monitor those aspects of health

00:16:49,660 --> 00:16:55,780
and to kind of yeah we're all

00:16:52,720 --> 00:16:58,240
responsible for reliability and h.a and

00:16:55,780 --> 00:17:00,550
everything but sometimes you know just

00:16:58,240 --> 00:17:01,840
to get past a particular problem you

00:17:00,550 --> 00:17:05,980
need to find the right team to actually

00:17:01,840 --> 00:17:07,630
get looking at the issue so we quickly

00:17:05,980 --> 00:17:12,160
developed some monitoring tools to help

00:17:07,630 --> 00:17:14,470
you know point the problem at the right

00:17:12,160 --> 00:17:20,079
team now sometimes it was my team too

00:17:14,470 --> 00:17:24,160
but not always the granularity of Micra

00:17:20,079 --> 00:17:26,560
services and memory allocation talked

00:17:24,160 --> 00:17:30,490
that the previous talk mentioned you

00:17:26,560 --> 00:17:33,430
know 512 megabyte is good for a four

00:17:30,490 --> 00:17:35,260
nodejs app when we had the monolith you

00:17:33,430 --> 00:17:38,590
know we had three or four instances at

00:17:35,260 --> 00:17:41,110
at two gigabytes apiece for Java and so

00:17:38,590 --> 00:17:44,350
that's about six gigabytes total if you

00:17:41,110 --> 00:17:45,110
have you know about 27 micro services

00:17:44,350 --> 00:17:46,970
with

00:17:45,110 --> 00:17:48,799
you know three or four or five instances

00:17:46,970 --> 00:17:51,559
a piece I think at one point we had

00:17:48,799 --> 00:17:55,040
about 95 total instances they're all at

00:17:51,559 --> 00:17:56,960
512 megabyte or even a little bit higher

00:17:55,040 --> 00:18:00,100
in some cases you know you end up with a

00:17:56,960 --> 00:18:03,530
system that's you know taking 55 60

00:18:00,100 --> 00:18:06,500
gigabytes now to deliver a lot of the

00:18:03,530 --> 00:18:08,330
same function not as big of a deal for

00:18:06,500 --> 00:18:11,929
our public offerings but we also do

00:18:08,330 --> 00:18:14,210
deploy into some customers local and

00:18:11,929 --> 00:18:17,090
dedicated environments and they are not

00:18:14,210 --> 00:18:19,400
necessarily happy about paying for a lot

00:18:17,090 --> 00:18:23,570
of memory just around the console so

00:18:19,400 --> 00:18:25,220
that's you know a consideration we did

00:18:23,570 --> 00:18:28,580
have to solve some issues with seamless

00:18:25,220 --> 00:18:31,820
navigation between our new micro service

00:18:28,580 --> 00:18:34,280
you eyes and and our dojo UI or monolith

00:18:31,820 --> 00:18:36,950
trying to make things look as close to

00:18:34,280 --> 00:18:40,010
the same as we could those sorts of

00:18:36,950 --> 00:18:42,200
things Bluegreen deployments you know

00:18:40,010 --> 00:18:44,000
what one question it's one thing to do a

00:18:42,200 --> 00:18:47,600
Bluegreen to pointment of a single app

00:18:44,000 --> 00:18:50,960
but if you have 25 apps how do you do

00:18:47,600 --> 00:18:53,929
that we ended up having an on deck

00:18:50,960 --> 00:18:56,179
basically version and a production

00:18:53,929 --> 00:18:57,740
version of the apps so all the so we had

00:18:56,179 --> 00:18:59,809
two versions of all the micro services

00:18:57,740 --> 00:19:03,049
deployed and we would do a Bluegreen

00:18:59,809 --> 00:19:05,900
swap on the proxy for each of those and

00:19:03,049 --> 00:19:08,150
so basically you're sending it setting

00:19:05,900 --> 00:19:12,410
up a whole new you're suddenly routing

00:19:08,150 --> 00:19:14,150
to a whole new set of micro services we

00:19:12,410 --> 00:19:15,110
want to be more granular with something

00:19:14,150 --> 00:19:17,630
we're still working on in our pipeline

00:19:15,110 --> 00:19:20,510
is to be more granular than that and do

00:19:17,630 --> 00:19:24,710
Bluegreen swaps at the individual micro

00:19:20,510 --> 00:19:27,910
service level this is a big problem is

00:19:24,710 --> 00:19:32,000
promoting uniformity and consistency

00:19:27,910 --> 00:19:35,390
while still giving teams freedom so we

00:19:32,000 --> 00:19:37,580
have we have a large set of UI designers

00:19:35,390 --> 00:19:41,290
in IBM and there's different UI

00:19:37,580 --> 00:19:44,990
designers applied to the different teams

00:19:41,290 --> 00:19:48,799
they often have differing views on how

00:19:44,990 --> 00:19:50,510
the UI should look and behave so you

00:19:48,799 --> 00:19:53,360
want to give people let their

00:19:50,510 --> 00:19:55,880
imaginations go and you know come up

00:19:53,360 --> 00:19:58,640
with you know innovative UIs but then

00:19:55,880 --> 00:19:59,059
you run the risk of having you know half

00:19:58,640 --> 00:20:01,220
the you

00:19:59,059 --> 00:20:03,769
by looking and behaving one way and you

00:20:01,220 --> 00:20:06,549
know other pieces a totally different

00:20:03,769 --> 00:20:11,210
way so that's still an ongoing challenge

00:20:06,549 --> 00:20:13,220
for us another point I threw in here and

00:20:11,210 --> 00:20:15,590
I guess I've got another slide on it too

00:20:13,220 --> 00:20:18,529
to go in a little bit more detail you

00:20:15,590 --> 00:20:21,649
know we did this work to be a micro

00:20:18,529 --> 00:20:23,860
service based within one deployment of

00:20:21,649 --> 00:20:26,769
Cloud Foundry and you know a lot of

00:20:23,860 --> 00:20:29,600
resiliency and h.a work as part of that

00:20:26,769 --> 00:20:34,789
but then how do you go and make that

00:20:29,600 --> 00:20:38,360
more globally available and I'll just go

00:20:34,789 --> 00:20:42,350
ahead and go to the next well I'll get

00:20:38,360 --> 00:20:44,149
to that in a second I did want to drill

00:20:42,350 --> 00:20:46,580
down into the importance of monitoring

00:20:44,149 --> 00:20:50,600
and I alluded to that we kind of

00:20:46,580 --> 00:20:52,220
underestimated how important monitoring

00:20:50,600 --> 00:20:54,259
was when we first deployed our first two

00:20:52,220 --> 00:20:55,970
or three micro services so as we

00:20:54,259 --> 00:20:57,799
deployed more and more this became all

00:20:55,970 --> 00:21:00,259
the more important

00:20:57,799 --> 00:21:04,009
lots of things can go wrong when you

00:21:00,259 --> 00:21:05,480
have this many components and you know

00:21:04,009 --> 00:21:07,999
root cause determination can be

00:21:05,480 --> 00:21:10,399
difficult so we did a lot of work to

00:21:07,999 --> 00:21:14,059
start collecting metrics on every

00:21:10,399 --> 00:21:17,029
inbound and outbound requests for every

00:21:14,059 --> 00:21:19,850
micro service with response times and

00:21:17,029 --> 00:21:22,190
error codes and you know as much detail

00:21:19,850 --> 00:21:22,669
as we could get and we can look at those

00:21:22,190 --> 00:21:25,220
things

00:21:22,669 --> 00:21:27,740
Indra fauna which there's a little

00:21:25,220 --> 00:21:29,869
screenshot here at the bottom and in

00:21:27,740 --> 00:21:32,179
fact I have a talk tomorrow that's going

00:21:29,869 --> 00:21:35,210
to go even deeper into all this of if

00:21:32,179 --> 00:21:38,720
you're interested we were very

00:21:35,210 --> 00:21:41,840
interested in memory usage CPU usage and

00:21:38,720 --> 00:21:45,139
uptime crashes for all of our micro

00:21:41,840 --> 00:21:47,059
services so so now we are monitoring a

00:21:45,139 --> 00:21:49,190
set up so if an app crashes for example

00:21:47,059 --> 00:21:52,669
and we send an alert to the appropriate

00:21:49,190 --> 00:21:56,659
people general health of ourselves and

00:21:52,669 --> 00:22:00,259
our dependencies for example if we can't

00:21:56,659 --> 00:22:02,509
get to our Redis server we can't share

00:22:00,259 --> 00:22:03,830
the token and you know people can't

00:22:02,509 --> 00:22:06,619
really authenticate and do what they

00:22:03,830 --> 00:22:08,750
need to do in the UI so that's part of

00:22:06,619 --> 00:22:11,419
the health we have to keep monitoring

00:22:08,750 --> 00:22:13,020
and we also do work where we have

00:22:11,419 --> 00:22:15,720
synthetic

00:22:13,020 --> 00:22:20,520
page loads so we run site speed io

00:22:15,720 --> 00:22:23,700
strips regularly in the background so we

00:22:20,520 --> 00:22:25,260
can always see how certain pages are

00:22:23,700 --> 00:22:30,240
performing and from different parts of

00:22:25,260 --> 00:22:33,420
the world now back to the global load

00:22:30,240 --> 00:22:35,540
balancing so we've got four currently

00:22:33,420 --> 00:22:38,280
have for public regions of bluemix

00:22:35,540 --> 00:22:40,410
Dallas London Frankfurt and Sydney and

00:22:38,280 --> 00:22:43,590
we've got all these micro services

00:22:40,410 --> 00:22:46,860
deployed in each one and each one of

00:22:43,590 --> 00:22:52,260
those has its own URL you know it's

00:22:46,860 --> 00:22:54,960
console dot region bluemix.net so as I

00:22:52,260 --> 00:22:56,970
mentioned you know we wanted to do some

00:22:54,960 --> 00:22:59,940
more h.a work here because you know if

00:22:56,970 --> 00:23:01,110
one region goes down you know maybe

00:22:59,940 --> 00:23:03,290
there's a problem with Cloud Foundry

00:23:01,110 --> 00:23:06,660
which could never happen I suppose right

00:23:03,290 --> 00:23:09,420
or there's a networking problem or or

00:23:06,660 --> 00:23:11,580
some other thing users going directly to

00:23:09,420 --> 00:23:12,840
a regional URL are gonna say well this

00:23:11,580 --> 00:23:15,780
thing doesn't work you know they see

00:23:12,840 --> 00:23:19,680
errors and white pages and all this

00:23:15,780 --> 00:23:21,630
stuff so well we're currently rolling

00:23:19,680 --> 00:23:23,280
out we call global console

00:23:21,630 --> 00:23:26,550
so basically we're starting to

00:23:23,280 --> 00:23:29,640
distribute the load over the micro

00:23:26,550 --> 00:23:31,590
service systems in all of our regions so

00:23:29,640 --> 00:23:34,530
we have one global we'll have one global

00:23:31,590 --> 00:23:39,450
URL which is actually live today console

00:23:34,530 --> 00:23:41,460
bluemix.net and there we did have a

00:23:39,450 --> 00:23:43,830
region switch concept of a region switch

00:23:41,460 --> 00:23:47,010
or even in the old model but it would

00:23:43,830 --> 00:23:48,990
totally switch URLs to the deployment in

00:23:47,010 --> 00:23:53,700
the other region now it just really does

00:23:48,990 --> 00:23:55,800
a filter within the current so wherever

00:23:53,700 --> 00:23:59,090
the UI is being served from you're doing

00:23:55,800 --> 00:24:04,890
a filter within that we're using Dyne

00:23:59,090 --> 00:24:07,200
geo load balancing so that whichever of

00:24:04,890 --> 00:24:09,600
our regions is closest to you

00:24:07,200 --> 00:24:13,080
when you go to the browser is where

00:24:09,600 --> 00:24:15,900
you'll get the UI serve from so if

00:24:13,080 --> 00:24:19,670
you're in new zealand you would probably

00:24:15,900 --> 00:24:23,430
get routed to our sydney australia

00:24:19,670 --> 00:24:24,840
console and the other thing that's you

00:24:23,430 --> 00:24:26,390
know I kind of alluded to monitoring and

00:24:24,840 --> 00:24:28,130
health checks and stuff we

00:24:26,390 --> 00:24:31,100
gotten our health check to the point now

00:24:28,130 --> 00:24:32,830
where you know dine consults in the

00:24:31,100 --> 00:24:36,590
different regions if a region is

00:24:32,830 --> 00:24:38,270
considered down by our health check the

00:24:36,590 --> 00:24:41,210
Dynel stop route in there for a bit

00:24:38,270 --> 00:24:43,580
until it's healthy again so in this way

00:24:41,210 --> 00:24:46,310
you know hopefully the user never sees

00:24:43,580 --> 00:24:48,950
you know what what looks like a full

00:24:46,310 --> 00:24:51,230
outage there may be you know they may

00:24:48,950 --> 00:24:53,840
not be able to manage their Dallas CF

00:24:51,230 --> 00:24:55,600
resources at any time at any given time

00:24:53,840 --> 00:24:57,740
but they could still manage their

00:24:55,600 --> 00:24:59,930
kubernetes clusters and stuff because

00:24:57,740 --> 00:25:02,270
the UI they still have a UI that works

00:24:59,930 --> 00:25:08,200
and is able to talk to the appropriate

00:25:02,270 --> 00:25:11,810
back-end api's across the world and that

00:25:08,200 --> 00:25:23,240
takes me to the end anything I see yeah

00:25:11,810 --> 00:25:25,100
already have a question well while the

00:25:23,240 --> 00:25:27,290
jump in memory was because we had only

00:25:25,100 --> 00:25:30,440
had one app with two gigabytes and

00:25:27,290 --> 00:25:33,470
instance and then we had suddenly had 25

00:25:30,440 --> 00:25:35,840
apps with you know 512 megabytes and

00:25:33,470 --> 00:25:37,970
instance and even if you just do to do

00:25:35,840 --> 00:25:39,920
the math that's a lot more memory now we

00:25:37,970 --> 00:25:41,660
have looked at you know some of our

00:25:39,920 --> 00:25:43,490
micro services were probably allocated

00:25:41,660 --> 00:25:45,200
more memory than they needed and we've

00:25:43,490 --> 00:25:48,380
you know we've caught some of that stuff

00:25:45,200 --> 00:25:50,840
back over time you know sometimes a if

00:25:48,380 --> 00:25:56,060
the Micra service isn't doing much you

00:25:50,840 --> 00:25:57,770
know 256 or even 128 may work but we

00:25:56,060 --> 00:26:03,160
were never able to cut it all the way

00:25:57,770 --> 00:26:03,160
down to the memory usage of one app

00:26:08,590 --> 00:26:13,910
it's it's not some material to our

00:26:11,030 --> 00:26:16,220
public deployments more so when we

00:26:13,910 --> 00:26:17,300
deploy onto a customers hardware and

00:26:16,220 --> 00:26:19,670
they're like well why do we have to pay

00:26:17,300 --> 00:26:22,250
extra for this but you know I do get to

00:26:19,670 --> 00:26:25,910
see you know obviously my team is not

00:26:22,250 --> 00:26:30,830
paying for the use of the Cloud Foundry

00:26:25,910 --> 00:26:34,490
resources but I do get to see what we

00:26:30,830 --> 00:26:38,090
would be billed if if we did have to pay

00:26:34,490 --> 00:26:40,520
and it does add up I mean so certainly

00:26:38,090 --> 00:26:42,970
if you're a you have not lucky enough to

00:26:40,520 --> 00:26:45,410
be able to work at IBM and deploy on IBM

00:26:42,970 --> 00:26:48,440
resources if you need to use 55

00:26:45,410 --> 00:26:50,780
gigabytes for your microcircuit system

00:26:48,440 --> 00:26:52,160
there's gonna be some cost involved

00:26:50,780 --> 00:26:54,650
there I think no matter which Cloud

00:26:52,160 --> 00:26:58,880
Foundry provider you use you'd have to

00:26:54,650 --> 00:27:01,130
pay pay more for them so well your

00:26:58,880 --> 00:27:04,460
global load balancing how did you keep

00:27:01,130 --> 00:27:06,890
the different regions in sync different

00:27:04,460 --> 00:27:09,320
versions or different regions and oh

00:27:06,890 --> 00:27:13,640
yeah so well that's a good question

00:27:09,320 --> 00:27:15,770
because we we do try when we roll out a

00:27:13,640 --> 00:27:18,410
new I mentioned we have an on deck and a

00:27:15,770 --> 00:27:20,890
net production set of Micra services in

00:27:18,410 --> 00:27:23,600
each of our regions we do typically

00:27:20,890 --> 00:27:26,060
upgrade them at the same roughly the

00:27:23,600 --> 00:27:28,250
same time so we'll usually upgrade like

00:27:26,060 --> 00:27:30,680
Sydney and then Frankfurt and then

00:27:28,250 --> 00:27:33,200
London and then Dallas but there's

00:27:30,680 --> 00:27:37,600
certainly times there where they're not

00:27:33,200 --> 00:27:42,380
exactly the same version so I think it's

00:27:37,600 --> 00:27:44,570
in general where it doesn't hurt us too

00:27:42,380 --> 00:27:46,310
bad yet because you know you're if

00:27:44,570 --> 00:27:48,320
you're getting your UI from Sydney and

00:27:46,310 --> 00:27:50,570
it's not exactly the same version as

00:27:48,320 --> 00:27:52,400
Dallas at least the Sydney stuff is

00:27:50,570 --> 00:27:55,130
going to work with whatever API is and

00:27:52,400 --> 00:27:58,520
such are there it may not quite have a

00:27:55,130 --> 00:28:00,410
new function that we've deployed to

00:27:58,520 --> 00:28:03,320
Frankfurt so there could be you know a

00:28:00,410 --> 00:28:05,120
half hour where you know a user being

00:28:03,320 --> 00:28:07,540
routed to Sydney won't see the same

00:28:05,120 --> 00:28:11,150
thing that they saw in Frankfurt exactly

00:28:07,540 --> 00:28:14,870
but it hasn't been a huge issue for us

00:28:11,150 --> 00:28:19,010
yet but if we do a do a major upgrade of

00:28:14,870 --> 00:28:19,490
of like our feeding and stuff at some

00:28:19,010 --> 00:28:21,350
point

00:28:19,490 --> 00:28:23,179
I thought what right to think about that

00:28:21,350 --> 00:28:25,790
a little bit harder because you know you

00:28:23,179 --> 00:28:28,070
get routed to Sydney and you see you

00:28:25,790 --> 00:28:30,350
know green and orange and whatever and

00:28:28,070 --> 00:28:33,230
then maybe you fail over to Dallas and

00:28:30,350 --> 00:28:36,650
you see the old style that might be a

00:28:33,230 --> 00:28:38,960
bit bit jarring also varying the process

00:28:36,650 --> 00:28:40,880
of doing something very similar okay

00:28:38,960 --> 00:28:43,850
what what suggestions or tips would you

00:28:40,880 --> 00:28:45,770
give us based on your experience so

00:28:43,850 --> 00:28:49,220
we've had pretty good luck with with

00:28:45,770 --> 00:28:50,929
dine for this where actually have a

00:28:49,220 --> 00:28:55,550
little bit of a weird situation because

00:28:50,929 --> 00:28:58,370
we're we're using Akamai for our CDN and

00:28:55,550 --> 00:29:02,210
woth DDoS and such so we really have a

00:28:58,370 --> 00:29:03,679
combination of Akamai and Dine which is

00:29:02,210 --> 00:29:07,400
a little bit weird

00:29:03,679 --> 00:29:11,360
I would we're actually looking to see if

00:29:07,400 --> 00:29:13,670
we can move move one way or the other so

00:29:11,360 --> 00:29:17,960
that so all the configuration for this

00:29:13,670 --> 00:29:20,090
is housed in one place there's there's

00:29:17,960 --> 00:29:22,309
also issues that we're working through

00:29:20,090 --> 00:29:25,220
with with dine right now about you know

00:29:22,309 --> 00:29:27,679
when there is a failover why did it

00:29:25,220 --> 00:29:29,420
happen because sometimes you know could

00:29:27,679 --> 00:29:30,290
be our health check legitimately

00:29:29,420 --> 00:29:33,260
returned

00:29:30,290 --> 00:29:34,850
we're down because Redis is down but

00:29:33,260 --> 00:29:37,070
sometimes requests don't even get to our

00:29:34,850 --> 00:29:39,260
health check you know if there's a

00:29:37,070 --> 00:29:41,660
networking problem or a firewall can

00:29:39,260 --> 00:29:43,570
fake problem and it's not always easy to

00:29:41,660 --> 00:29:47,090
tell from the dinah lurtz

00:29:43,570 --> 00:29:48,740
why that was so we have to you know

00:29:47,090 --> 00:29:50,660
start getting our networking guys

00:29:48,740 --> 00:29:52,100
involved to say well why can't why

00:29:50,660 --> 00:29:54,950
weren't these requests getting through

00:29:52,100 --> 00:30:01,490
and things like that i'm effete does

00:29:54,950 --> 00:30:02,690
that answer the question kind of oh I'm

00:30:01,490 --> 00:30:04,610
sorry I thought you were trying to move

00:30:02,690 --> 00:30:06,920
to a global global load balancer yeah

00:30:04,610 --> 00:30:08,690
I'm sorry oh so okay so tips on okay I

00:30:06,920 --> 00:30:11,360
know I understand the question so tips

00:30:08,690 --> 00:30:14,179
on moving from a monolith to a to micro

00:30:11,360 --> 00:30:16,490
services yeah so that's yeah I think

00:30:14,179 --> 00:30:19,309
I've got a lot of those probably not

00:30:16,490 --> 00:30:21,620
enough time but yeah wonder if we could

00:30:19,309 --> 00:30:23,090
let people go to lunch than anyone could

00:30:21,620 --> 00:30:26,690
come up to the front and talk to Tony

00:30:23,090 --> 00:30:31,779
okay thank you very much Tony thank you

00:30:26,690 --> 00:30:31,779

YouTube URL: https://www.youtube.com/watch?v=psvBZkrkiRM


