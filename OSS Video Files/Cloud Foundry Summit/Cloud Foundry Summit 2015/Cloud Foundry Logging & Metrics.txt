Title: Cloud Foundry Logging & Metrics
Publication date: 2015-05-12
Playlist: Cloud Foundry Summit 2015
Description: 
	Cloud Foundry Logging & Metrics - 05 Colin Humphreys, Ed King 720p
Captions: 
	00:00:00,949 --> 00:00:07,500
hello everyone thank you very much to

00:00:05,790 --> 00:00:12,809
everyone that has remained from the last

00:00:07,500 --> 00:00:14,759
talk I apologize I'm here again you'll

00:00:12,809 --> 00:00:16,740
be really glad to know that I'm not on

00:00:14,759 --> 00:00:18,510
the next time slot here you get a break

00:00:16,740 --> 00:00:21,500
from me but then I'm on the last time

00:00:18,510 --> 00:00:23,460
slot so it's kind of like alternating me

00:00:21,500 --> 00:00:26,039
so thank you for the people that have

00:00:23,460 --> 00:00:28,260
remained I appreciate your patience it's

00:00:26,039 --> 00:00:29,820
also going to be quite amusing to see if

00:00:28,260 --> 00:00:32,460
I can remain awake because I'm quite

00:00:29,820 --> 00:00:34,500
badly jet-lagged so the last talk might

00:00:32,460 --> 00:00:38,850
be a little bit dodgy we will give it a

00:00:34,500 --> 00:00:42,149
go so welcome to our talk on logging and

00:00:38,850 --> 00:00:44,640
metrics my name as you know from the

00:00:42,149 --> 00:00:49,050
last talk is Colin Humphries I'm the CEO

00:00:44,640 --> 00:00:52,289
cloud credo presenting to my left is ed

00:00:49,050 --> 00:00:58,370
king hello who also works for cloud

00:00:52,289 --> 00:01:01,289
credo we are a cloud foundry and Bosch

00:00:58,370 --> 00:01:02,699
consultancy based in London so very

00:01:01,289 --> 00:01:04,760
quick sales pitch if you want help with

00:01:02,699 --> 00:01:09,360
Cloud Foundry please do get in contact

00:01:04,760 --> 00:01:10,650
come and talk to us we love working with

00:01:09,360 --> 00:01:17,240
organizations that are trying to do

00:01:10,650 --> 00:01:17,240
interesting things with Cloud Foundry so

00:01:19,180 --> 00:01:26,409
I'm going to talk very briefly about why

00:01:22,759 --> 00:01:29,350
logging and monitoring are so important

00:01:26,409 --> 00:01:32,869
the first time I used Cloud Foundry I

00:01:29,350 --> 00:01:33,740
thought it was fantastic absolutely

00:01:32,869 --> 00:01:39,380
amazing

00:01:33,740 --> 00:01:40,420
I had an application I called CF push my

00:01:39,380 --> 00:01:44,149
application

00:01:40,420 --> 00:01:46,939
the staging happened and it ran and I

00:01:44,149 --> 00:01:50,119
thought this is going to change my

00:01:46,939 --> 00:01:53,080
career this is going to change IT this

00:01:50,119 --> 00:01:55,819
is going to change how we deliver value

00:01:53,080 --> 00:02:00,159
the world is a better place now I can

00:01:55,819 --> 00:02:04,369
see us push applications and it was

00:02:00,159 --> 00:02:07,330
until my application broke and then I

00:02:04,369 --> 00:02:11,510
had absolutely no idea what was going on

00:02:07,330 --> 00:02:14,989
because the black box of the path made

00:02:11,510 --> 00:02:21,010
it also a page I trusted the past around

00:02:14,989 --> 00:02:25,489
my app and if it broke what was going on

00:02:21,010 --> 00:02:28,519
particularly if staging failed the user

00:02:25,489 --> 00:02:30,200
journey used to be I have absolutely no

00:02:28,519 --> 00:02:32,150
idea why the staging has gone wrong

00:02:30,200 --> 00:02:36,040
quite often you've got no logs at all

00:02:32,150 --> 00:02:41,269
you just had staging has failed

00:02:36,040 --> 00:02:47,930
so things have been proved in Cloud

00:02:41,269 --> 00:02:51,950
Foundry but I want to know how do I get

00:02:47,930 --> 00:02:55,220
access to logs to metrics and to

00:02:51,950 --> 00:02:58,160
monitoring and Ed's going to take us

00:02:55,220 --> 00:03:03,560
through the components in Cloud Foundry

00:02:58,160 --> 00:03:06,190
and how they allow us to do that ok

00:03:03,560 --> 00:03:06,190
Thank You Colin

00:03:06,989 --> 00:03:12,700
so yeah I just like to start by covering

00:03:10,690 --> 00:03:14,200
some of the main components involved in

00:03:12,700 --> 00:03:17,200
the Cloud Foundry logging a metric

00:03:14,200 --> 00:03:19,299
system and as with most things in Cloud

00:03:17,200 --> 00:03:21,940
Foundry the system is constantly being

00:03:19,299 --> 00:03:23,440
updated and improved and especially

00:03:21,940 --> 00:03:24,909
recently there have been some fairly big

00:03:23,440 --> 00:03:27,610
changes to the way that the logging of

00:03:24,909 --> 00:03:29,140
metric system works and so I think it'd

00:03:27,610 --> 00:03:30,760
just be a good idea just to start by

00:03:29,140 --> 00:03:34,150
taking a look at the current state of

00:03:30,760 --> 00:03:39,220
the system what components are there and

00:03:34,150 --> 00:03:40,750
how they work as well so the first

00:03:39,220 --> 00:03:43,989
component I'd like to talk about is

00:03:40,750 --> 00:03:45,879
local Gator and local Gator is really at

00:03:43,989 --> 00:03:48,370
the core of the Cloud Foundry logging

00:03:45,879 --> 00:03:51,000
and metric system it's currently

00:03:48,370 --> 00:03:55,000
comprised of a few smaller components

00:03:51,000 --> 00:03:57,459
namely these sources Metron doppler and

00:03:55,000 --> 00:03:59,650
the traffic controller and I'll talk a

00:03:57,459 --> 00:04:01,840
little bit more about those in just a

00:03:59,650 --> 00:04:03,579
second but just to sort of give you a

00:04:01,840 --> 00:04:07,480
general overview of the logger Gator

00:04:03,579 --> 00:04:09,700
system as a whole this is the component

00:04:07,480 --> 00:04:12,459
that allows developers to stream their

00:04:09,700 --> 00:04:15,669
application logs in real time down to

00:04:12,459 --> 00:04:19,329
the CLR CLI and that can be achieved by

00:04:15,669 --> 00:04:22,270
running the CF logs my app command from

00:04:19,329 --> 00:04:24,900
the CLI it also allows developers to

00:04:22,270 --> 00:04:27,729
dump a recent subset of their logs and

00:04:24,900 --> 00:04:29,620
it can also provide functionality for

00:04:27,729 --> 00:04:34,289
draining those logs off to third-party

00:04:29,620 --> 00:04:37,510
syslog dreams and how this all works

00:04:34,289 --> 00:04:39,729
the Doppler component basically sits

00:04:37,510 --> 00:04:42,789
there and gathers all of the logging and

00:04:39,729 --> 00:04:44,500
metrics data from the platform and it

00:04:42,789 --> 00:04:48,030
stores this logging a metrics data in

00:04:44,500 --> 00:04:50,410
temporary buffers on the Dhokla servers

00:04:48,030 --> 00:04:52,240
but it's really really important to

00:04:50,410 --> 00:04:56,530
realize that that the Doppler servers

00:04:52,240 --> 00:04:59,229
don't retain that data on a long-term

00:04:56,530 --> 00:05:01,120
basis and actually Cloud Foundry doesn't

00:04:59,229 --> 00:05:04,030
really ship with a component to provide

00:05:01,120 --> 00:05:07,570
long-term storage and indexing and

00:05:04,030 --> 00:05:09,400
pausing of the log messages and so we'll

00:05:07,570 --> 00:05:10,389
be taking a look at a project that we

00:05:09,400 --> 00:05:13,560
can use to actually provide that

00:05:10,389 --> 00:05:15,789
functionality in just a few minutes time

00:05:13,560 --> 00:05:17,370
but just be aware that once those

00:05:15,789 --> 00:05:19,080
buffers fill up

00:05:17,370 --> 00:05:23,040
your logs are kind of dropped off the

00:05:19,080 --> 00:05:24,360
end and lost and the other component I

00:05:23,040 --> 00:05:26,850
briefly mentioned was the traffic

00:05:24,360 --> 00:05:29,130
controller so this is the component that

00:05:26,850 --> 00:05:31,530
actually accepts the incoming requests

00:05:29,130 --> 00:05:33,240
for our logging a metrics data and it

00:05:31,530 --> 00:05:39,780
can then forward that those requests on

00:05:33,240 --> 00:05:42,210
to the Doppler servers in the back so

00:05:39,780 --> 00:05:43,770
this diagram here I've I've shamelessly

00:05:42,210 --> 00:05:46,680
ripped this diagram straight from the

00:05:43,770 --> 00:05:48,210
github page for the log regatta but it

00:05:46,680 --> 00:05:50,010
gives a pretty good overview of the log

00:05:48,210 --> 00:05:52,860
regatta system and some of the

00:05:50,010 --> 00:05:55,470
components involved so just to briefly

00:05:52,860 --> 00:05:58,169
run through some of these then on the

00:05:55,470 --> 00:06:00,449
left-hand side we have these sources and

00:05:58,169 --> 00:06:02,220
the sources are the components that are

00:06:00,449 --> 00:06:05,190
actually generating the logging and

00:06:02,220 --> 00:06:08,280
metrics data so an example of the source

00:06:05,190 --> 00:06:11,190
would be the DEA logging agent for

00:06:08,280 --> 00:06:12,539
example and the sources generate all of

00:06:11,190 --> 00:06:15,360
this logging and metrics data and

00:06:12,539 --> 00:06:20,880
forward it onto a component known as

00:06:15,360 --> 00:06:22,650
Metron Metron is a small go library that

00:06:20,880 --> 00:06:25,320
basically sits there and gathers all of

00:06:22,650 --> 00:06:27,330
that incoming data and is responsible

00:06:25,320 --> 00:06:32,310
for forwarding it on to the relevant

00:06:27,330 --> 00:06:35,190
Dhokla servers and so as such the Metron

00:06:32,310 --> 00:06:36,800
agents get co-located across every VM in

00:06:35,190 --> 00:06:40,080
your Cloud Foundry deployment and

00:06:36,800 --> 00:06:42,419
typically these sources will log to the

00:06:40,080 --> 00:06:44,280
local Metron agent and Metron will

00:06:42,419 --> 00:06:47,760
forward that off to the Doppler servers

00:06:44,280 --> 00:06:49,620
and then as I said once the data comes

00:06:47,760 --> 00:06:51,750
in to the Doppler servers stored in

00:06:49,620 --> 00:06:53,610
those temporary buffers and we have

00:06:51,750 --> 00:06:57,479
functionality for shipping those logs

00:06:53,610 --> 00:06:59,789
off to syslog drains or the local data

00:06:57,479 --> 00:07:03,240
traffic controller can accept incoming

00:06:59,789 --> 00:07:09,510
requests and forward those on to the

00:07:03,240 --> 00:07:11,250
doctor servers so that's all great but

00:07:09,510 --> 00:07:13,740
really the most the most awesome feature

00:07:11,250 --> 00:07:15,930
of log Regatta or in my opinion the most

00:07:13,740 --> 00:07:19,440
awesome feature is a relatively new

00:07:15,930 --> 00:07:21,539
feature called the firehose and I guess

00:07:19,440 --> 00:07:25,650
the definition of the firehose is a

00:07:21,539 --> 00:07:27,599
stream of every applications logs Plus

00:07:25,650 --> 00:07:29,040
metrics data from cloud boundaries

00:07:27,599 --> 00:07:31,600
components

00:07:29,040 --> 00:07:34,060
so application logs should be fairly

00:07:31,600 --> 00:07:36,190
self-explanatory but the the metric

00:07:34,060 --> 00:07:37,750
states are slightly more interesting so

00:07:36,190 --> 00:07:40,960
to give you an example of the metrics

00:07:37,750 --> 00:07:43,600
data the Cloud Foundry router is is

00:07:40,960 --> 00:07:46,690
constantly emitting metrics events for

00:07:43,600 --> 00:07:49,540
every single HTTP request coming in and

00:07:46,690 --> 00:07:52,260
out of the platform so for example it

00:07:49,540 --> 00:07:55,570
will emit metrics events detailing

00:07:52,260 --> 00:07:57,820
response times or status codes for each

00:07:55,570 --> 00:07:59,740
of the requests and all of these events

00:07:57,820 --> 00:08:02,940
get get gathered up and forward it down

00:07:59,740 --> 00:08:02,940
the firehose

00:08:03,220 --> 00:08:08,110
but obviously by default every single

00:08:06,190 --> 00:08:10,830
application log as well as every piece

00:08:08,110 --> 00:08:13,840
of metrics data it's quite a lot of data

00:08:10,830 --> 00:08:16,420
and so because of that this notion or

00:08:13,840 --> 00:08:19,990
this concept of nozzles has been

00:08:16,420 --> 00:08:22,450
introduced and a nozzle is basically a

00:08:19,990 --> 00:08:25,300
pluggable component that can attach to

00:08:22,450 --> 00:08:26,919
the firehose pulldown just a subset of

00:08:25,300 --> 00:08:30,130
the data that you might be interested in

00:08:26,919 --> 00:08:33,400
do some processing on that data and then

00:08:30,130 --> 00:08:36,789
forward it off to some third party for

00:08:33,400 --> 00:08:39,219
example syslog or graphite so I believe

00:08:36,789 --> 00:08:42,190
there's a there's a fire hose to syslog

00:08:39,219 --> 00:08:44,080
nozzle which i think is available now in

00:08:42,190 --> 00:08:47,200
the Cloud Foundry community github page

00:08:44,080 --> 00:08:49,510
which as the name suggests connects to

00:08:47,200 --> 00:08:54,089
the firehose pulls downs and logs and

00:08:49,510 --> 00:08:54,089
forwards them off into syslog

00:08:57,080 --> 00:09:01,130
and that sort of concludes just talking

00:08:59,150 --> 00:09:04,550
about the main components involved in

00:09:01,130 --> 00:09:08,630
the Cloud Foundry system I think back

00:09:04,550 --> 00:09:11,420
over to Colin to introduce logs so logs

00:09:08,630 --> 00:09:13,760
the reason I wanted to talk a little bit

00:09:11,420 --> 00:09:17,330
and interject into Ed's talk it's

00:09:13,760 --> 00:09:18,770
because the the first time I really

00:09:17,330 --> 00:09:21,050
needs to get logs out of Cloud Foundry

00:09:18,770 --> 00:09:23,920
it was a Cloud Foundry version one

00:09:21,050 --> 00:09:28,040
installation we were working on with a

00:09:23,920 --> 00:09:29,300
client we've given them the set up and

00:09:28,040 --> 00:09:30,230
it was working pretty well and then

00:09:29,300 --> 00:09:32,810
something went wrong with their

00:09:30,230 --> 00:09:35,480
application and they said how do we see

00:09:32,810 --> 00:09:37,190
the logs from this it's all going wrong

00:09:35,480 --> 00:09:39,380
I need to see some logs I've got yeah

00:09:37,190 --> 00:09:42,950
big problem here how do we get to the

00:09:39,380 --> 00:09:45,440
logs and I said ok what we're going to

00:09:42,950 --> 00:09:48,890
do you're going to subscribe to the nats

00:09:45,440 --> 00:09:51,140
message bus you're going to filter the

00:09:48,890 --> 00:09:52,490
messages for something that looks kind

00:09:51,140 --> 00:09:54,020
of log shaped when you think things are

00:09:52,490 --> 00:09:55,310
going on and then were going to try and

00:09:54,020 --> 00:09:58,060
work out what's happening from there

00:09:55,310 --> 00:10:00,500
onwards and it was an atrocious journey

00:09:58,060 --> 00:10:02,690
it really was so this for me is one of

00:10:00,500 --> 00:10:05,030
the biggest pain points well has been a

00:10:02,690 --> 00:10:09,290
big pain point in Cloud Foundry and now

00:10:05,030 --> 00:10:12,530
it's got a lot better so over to you add

00:10:09,290 --> 00:10:14,330
how do we now get to these logs how do

00:10:12,530 --> 00:10:16,010
we now get some insight into what's

00:10:14,330 --> 00:10:19,970
happening when our application isn't

00:10:16,010 --> 00:10:22,790
working right sure so I'm going to talk

00:10:19,970 --> 00:10:25,400
about a project called log search and

00:10:22,790 --> 00:10:28,460
the log search project allows us to

00:10:25,400 --> 00:10:33,230
essentially integrate the elk stack with

00:10:28,460 --> 00:10:35,360
our Cloud Foundry deployment so one of

00:10:33,230 --> 00:10:37,430
the things that log search does is to

00:10:35,360 --> 00:10:41,240
package up the components of the elk

00:10:37,430 --> 00:10:43,220
stack as a Porsche release for those of

00:10:41,240 --> 00:10:45,830
you not familiar with the elk stock

00:10:43,220 --> 00:10:49,370
it stands for elasticsearch log stash

00:10:45,830 --> 00:10:51,890
and Cabana where elasticsearch provides

00:10:49,370 --> 00:10:54,650
the backend storage and indexing of your

00:10:51,890 --> 00:10:56,270
looks log stash provides the the

00:10:54,650 --> 00:10:58,520
filtering and pausing of your log

00:10:56,270 --> 00:11:01,760
messages and Cabana provides the

00:10:58,520 --> 00:11:03,710
front-end web interface and so log

00:11:01,760 --> 00:11:06,080
search takes all of those components and

00:11:03,710 --> 00:11:08,650
neatly packages them up makes them

00:11:06,080 --> 00:11:11,560
available to us by a Bosch

00:11:08,650 --> 00:11:13,060
and the good news about log search is

00:11:11,560 --> 00:11:14,830
that it's completely open source and

00:11:13,060 --> 00:11:17,740
free you can go and download it right

00:11:14,830 --> 00:11:19,750
now it's available and github and I'd

00:11:17,740 --> 00:11:22,270
like to just say a quick thanks to David

00:11:19,750 --> 00:11:24,850
lang and to all of the contributors and

00:11:22,270 --> 00:11:26,980
maintainer zuv log search because it's

00:11:24,850 --> 00:11:28,980
really awesome and I'm just going to

00:11:26,980 --> 00:11:31,420
spend the next few minutes talking about

00:11:28,980 --> 00:11:34,960
what a log search deployment looks like

00:11:31,420 --> 00:11:37,450
what the components are involved and

00:11:34,960 --> 00:11:39,820
also how we can integrate it with our

00:11:37,450 --> 00:11:44,730
Cloud Foundry deployment to provide that

00:11:39,820 --> 00:11:44,730
long-term storage of of our log messages

00:11:44,850 --> 00:11:50,530
so on that note very excited to announce

00:11:48,250 --> 00:11:53,380
a new project the the log search for

00:11:50,530 --> 00:11:56,320
Cloud Foundry project and this project

00:11:53,380 --> 00:11:58,390
is really focused towards customizing

00:11:56,320 --> 00:12:02,380
log search to work with Cloud Foundry

00:11:58,390 --> 00:12:05,620
data and it does this by allowing log

00:12:02,380 --> 00:12:08,830
search to accept logs from our Cloud

00:12:05,620 --> 00:12:12,430
Foundry deployments from two main sort

00:12:08,830 --> 00:12:16,320
of input streams so the first stream is

00:12:12,430 --> 00:12:20,050
from the Cloud Foundry component syslog

00:12:16,320 --> 00:12:22,540
message messages so every component in

00:12:20,050 --> 00:12:25,180
Cloud Foundry emits syslog messages and

00:12:22,540 --> 00:12:27,610
we can tell Cloud Foundry to forward

00:12:25,180 --> 00:12:31,030
those messages off into our log search

00:12:27,610 --> 00:12:32,980
deployment that's the first way and the

00:12:31,030 --> 00:12:36,250
second way is that we can also tell log

00:12:32,980 --> 00:12:41,950
search to talk to the firehose and pull

00:12:36,250 --> 00:12:43,600
down log messages from there as well and

00:12:41,950 --> 00:12:46,420
the project is really sort of focused

00:12:43,600 --> 00:12:48,670
towards two main user groups so there's

00:12:46,420 --> 00:12:50,260
the Cloud Foundry developer who's going

00:12:48,670 --> 00:12:52,480
to be mostly interested in getting

00:12:50,260 --> 00:12:55,540
application logs or their own

00:12:52,480 --> 00:12:57,610
application logs and there's also the

00:12:55,540 --> 00:12:59,710
Cloud Foundry operator who's going to

00:12:57,610 --> 00:13:02,740
want to see all of the system logs for

00:12:59,710 --> 00:13:04,300
the for the system as a whole and log

00:13:02,740 --> 00:13:06,640
search for Cloud Foundry provides some

00:13:04,300 --> 00:13:09,160
really nice multi-tenancy options to

00:13:06,640 --> 00:13:10,810
ensure that users can only access the

00:13:09,160 --> 00:13:14,170
logs that they are actually responsible

00:13:10,810 --> 00:13:16,120
for and it's pretty cool how it does

00:13:14,170 --> 00:13:18,430
that so there's a there's essentially a

00:13:16,120 --> 00:13:21,190
proxy that sits in front of the system

00:13:18,430 --> 00:13:22,499
that goes off and talks to the UAA

00:13:21,190 --> 00:13:26,290
server

00:13:22,499 --> 00:13:29,350
determines which spaces the user has

00:13:26,290 --> 00:13:33,339
access to and then it uses that along

00:13:29,350 --> 00:13:35,769
with elasticsearch aliases to basically

00:13:33,339 --> 00:13:38,049
filter down the amount of logs that the

00:13:35,769 --> 00:13:39,939
users can see and so we end up where

00:13:38,049 --> 00:13:44,019
users can only see the logs that they

00:13:39,939 --> 00:13:45,549
actually have access to if you want a

00:13:44,019 --> 00:13:47,170
little bit more detail about how that's

00:13:45,549 --> 00:13:50,619
and that's all setup there's a couple of

00:13:47,170 --> 00:13:52,089
links there to some YouTube videos where

00:13:50,619 --> 00:13:58,600
where David goes into more detail about

00:13:52,089 --> 00:14:02,049
about that so this image here this is

00:13:58,600 --> 00:14:04,480
the Cabana web interface as I'm sure

00:14:02,049 --> 00:14:07,389
you'll agree it's it's very very pretty

00:14:04,480 --> 00:14:11,319
like it looks great what we're actually

00:14:07,389 --> 00:14:14,110
seeing here is a dashboard showing three

00:14:11,319 --> 00:14:17,499
demo applications logging through the

00:14:14,110 --> 00:14:19,149
log search system one of the nice things

00:14:17,499 --> 00:14:21,610
to point out here is that we've got the

00:14:19,149 --> 00:14:23,619
actual application names coming up in

00:14:21,610 --> 00:14:26,079
the dashboard as opposed to just the the

00:14:23,619 --> 00:14:27,189
you IDs so it's a very user friendly

00:14:26,079 --> 00:14:29,829
interface

00:14:27,189 --> 00:14:36,309
great to use and it's just generally

00:14:29,829 --> 00:14:38,649
looks looks pretty good this slightly

00:14:36,309 --> 00:14:41,379
less impressive looking diagram is one I

00:14:38,649 --> 00:14:43,119
drew myself but it helps to sort of give

00:14:41,379 --> 00:14:45,279
an overview of all of the components

00:14:43,119 --> 00:14:47,980
that are involved in a typical blog

00:14:45,279 --> 00:14:50,259
search deployment and it sort of shows

00:14:47,980 --> 00:14:52,269
that the the journey that a log message

00:14:50,259 --> 00:14:54,699
would take starting with Cloud Foundry

00:14:52,269 --> 00:14:57,100
on the left and going through the log

00:14:54,699 --> 00:14:59,499
search system to end up in elasticsearch

00:14:57,100 --> 00:15:00,999
at the back so I'm just going to run

00:14:59,499 --> 00:15:02,169
through each of these components to sort

00:15:00,999 --> 00:15:08,410
of give you a quick overview of what

00:15:02,169 --> 00:15:10,839
they do the first component is the in

00:15:08,410 --> 00:15:13,149
gesture and the ingest is are really

00:15:10,839 --> 00:15:17,350
responsible for accepting incoming logs

00:15:13,149 --> 00:15:19,540
into the blog search system and look

00:15:17,350 --> 00:15:23,619
search ships with a couple of default in

00:15:19,540 --> 00:15:26,529
jesters namely syslog with TLS and a

00:15:23,619 --> 00:15:28,569
rope and gesture as well and if you also

00:15:26,529 --> 00:15:31,360
happen to be deploying the log search

00:15:28,569 --> 00:15:34,240
for Cloud Foundry bas-reliefs we get an

00:15:31,360 --> 00:15:34,880
additional in gesture and that is the

00:15:34,240 --> 00:15:36,950
component

00:15:34,880 --> 00:15:40,460
could go and pull down logs from the

00:15:36,950 --> 00:15:42,440
fire hose so the investors are basically

00:15:40,460 --> 00:15:47,180
the entry point to logs search for your

00:15:42,440 --> 00:15:47,780
log messages once the logs have been

00:15:47,180 --> 00:15:50,330
adjusted

00:15:47,780 --> 00:15:52,850
they are then forwarded on to the Q and

00:15:50,330 --> 00:15:54,950
the Q component is currently provided by

00:15:52,850 --> 00:15:57,020
a Redis and this is actually a really

00:15:54,950 --> 00:15:59,360
really nice addition to the standard ELQ

00:15:57,020 --> 00:16:02,870
stack it provides us with a couple of

00:15:59,360 --> 00:16:06,800
benefits so the first benefit is that it

00:16:02,870 --> 00:16:08,750
helps to keep the system stable if you

00:16:06,800 --> 00:16:11,480
were to experience a sudden increase in

00:16:08,750 --> 00:16:13,730
the volume of logs coming through your

00:16:11,480 --> 00:16:15,980
system so Redis provides a nice

00:16:13,730 --> 00:16:17,780
temporary buffer and it gives you a

00:16:15,980 --> 00:16:19,850
little bit of time to just go and scale

00:16:17,780 --> 00:16:21,710
the relevant components so that you're

00:16:19,850 --> 00:16:25,780
able to keep up with the demand

00:16:21,710 --> 00:16:28,190
of the increase in in logging traffic

00:16:25,780 --> 00:16:30,890
one of the other other things it does is

00:16:28,190 --> 00:16:34,130
help to prevent against message loss in

00:16:30,890 --> 00:16:36,260
some certain scenarios so for example if

00:16:34,130 --> 00:16:39,140
you were to lose your elasticsearch

00:16:36,260 --> 00:16:40,850
backend for whatever reason again Redis

00:16:39,140 --> 00:16:43,190
gives you that temporary buffer buffer

00:16:40,850 --> 00:16:44,540
and gives you a little bit of time to go

00:16:43,190 --> 00:16:46,550
and figure out what's wrong

00:16:44,540 --> 00:16:49,280
before you start losing your logging

00:16:46,550 --> 00:16:51,200
messages so it just generally helps to

00:16:49,280 --> 00:16:54,050
keep the whole thing a lot more stable

00:16:51,200 --> 00:16:55,820
and that just ships with it with a

00:16:54,050 --> 00:17:01,370
standard blog search bas-reliefs which

00:16:55,820 --> 00:17:02,630
it which is great once the messages have

00:17:01,370 --> 00:17:05,449
made it through the queue they are then

00:17:02,630 --> 00:17:07,520
forwarded on to the pauses and this is

00:17:05,449 --> 00:17:11,150
where the actual filtering and parsing

00:17:07,520 --> 00:17:13,699
of the log messages actually occurs so

00:17:11,150 --> 00:17:15,380
as I'm sure you're aware every single

00:17:13,699 --> 00:17:18,319
log message Under the Sun is going to be

00:17:15,380 --> 00:17:20,120
in a slightly different format and the

00:17:18,319 --> 00:17:23,540
pauses are really attempting to take

00:17:20,120 --> 00:17:25,550
that mishmash of logging data and turn

00:17:23,540 --> 00:17:29,660
it into something that we actually want

00:17:25,550 --> 00:17:32,840
to use and to store so the pauses are

00:17:29,660 --> 00:17:36,170
running log stash in order to do this

00:17:32,840 --> 00:17:39,260
and log search ships with a few default

00:17:36,170 --> 00:17:42,460
filters to do some standard filtering

00:17:39,260 --> 00:17:44,900
and parsing of log messages such as

00:17:42,460 --> 00:17:47,750
cleaning up the white space for example

00:17:44,900 --> 00:17:51,110
and it's also to

00:17:47,750 --> 00:17:53,720
also enables you to write your own

00:17:51,110 --> 00:17:55,160
filters as well and local search

00:17:53,720 --> 00:17:56,720
provides some some Oh stalling around

00:17:55,160 --> 00:17:59,240
helping you to write those filters and

00:17:56,720 --> 00:18:04,940
get them included in your log search

00:17:59,240 --> 00:18:05,720
deployment once the logs have been

00:18:04,940 --> 00:18:07,300
parsed

00:18:05,720 --> 00:18:09,440
they are then finally forwarded onto

00:18:07,300 --> 00:18:11,780
elasticsearch where they can be stored

00:18:09,440 --> 00:18:16,030
and indexed and they can remain there

00:18:11,780 --> 00:18:18,650
for as long as you need them to be and

00:18:16,030 --> 00:18:20,780
the final component then is the the

00:18:18,650 --> 00:18:23,090
Cabana web interface which we saw

00:18:20,780 --> 00:18:25,340
earlier and as I said this provides the

00:18:23,090 --> 00:18:29,060
front-end web interface and the nice

00:18:25,340 --> 00:18:31,580
dashboards as well and it's probably

00:18:29,060 --> 00:18:35,360
worth me also mentioning that alongside

00:18:31,580 --> 00:18:39,080
Cabana log search exposes a read-only

00:18:35,360 --> 00:18:40,940
elasticsearch API as well and this is

00:18:39,080 --> 00:18:44,510
great for for integrating with with

00:18:40,940 --> 00:18:47,210
other third parties so for example at

00:18:44,510 --> 00:18:49,730
the CF CLI we could write a plug-in to

00:18:47,210 --> 00:18:56,510
go and grab logs out of out of the

00:18:49,730 --> 00:18:58,910
system using using that endpoint just to

00:18:56,510 --> 00:19:02,180
show this again and that's sort of the

00:18:58,910 --> 00:19:04,340
system as a whole so we export our logs

00:19:02,180 --> 00:19:06,860
from Cloud Foundry they get ingested

00:19:04,340 --> 00:19:08,870
pass through the queue has been

00:19:06,860 --> 00:19:15,980
formatted and then end up in

00:19:08,870 --> 00:19:17,990
elasticsearch so that's all great by now

00:19:15,980 --> 00:19:19,760
you must surely be wondering how do I

00:19:17,990 --> 00:19:22,940
actually do this how do I get this all

00:19:19,760 --> 00:19:25,490
set up and the good news is is that it's

00:19:22,940 --> 00:19:27,200
actually not too difficult assuming

00:19:25,490 --> 00:19:29,990
you've got a little bit of Bosch

00:19:27,200 --> 00:19:32,540
knowledge and I know that might be quite

00:19:29,990 --> 00:19:34,190
a lot to ask but it's it's really really

00:19:32,540 --> 00:19:36,350
worth investing some time in becoming

00:19:34,190 --> 00:19:39,290
more familiar with Bosch because it

00:19:36,350 --> 00:19:42,110
helps with deploying a complicated stack

00:19:39,290 --> 00:19:43,700
such as the elk stack and it helps to

00:19:42,110 --> 00:19:48,080
make the the deployment and management

00:19:43,700 --> 00:19:50,510
of that much much easier so just as sort

00:19:48,080 --> 00:19:53,540
of a very high-level what you'd need to

00:19:50,510 --> 00:19:56,540
do then the first step is you need to

00:19:53,540 --> 00:19:57,639
upload the Boche releases so the boss

00:19:56,540 --> 00:20:00,190
release

00:19:57,639 --> 00:20:01,450
contains the actual source packages that

00:20:00,190 --> 00:20:03,789
are going to be running as part of this

00:20:01,450 --> 00:20:06,579
deployment and there's currently two of

00:20:03,789 --> 00:20:08,379
them there's the standard log search

00:20:06,579 --> 00:20:10,299
wash release which contains the elk

00:20:08,379 --> 00:20:12,309
stack and then there's the new

00:20:10,299 --> 00:20:14,440
additional log search for Cloud Foundry

00:20:12,309 --> 00:20:17,049
bas-reliefs which contains that

00:20:14,440 --> 00:20:20,499
additional in gesture that can talk to

00:20:17,049 --> 00:20:22,149
the fire to the fire hose and this is

00:20:20,499 --> 00:20:25,599
simply just a case of wash upload

00:20:22,149 --> 00:20:29,079
release from the command line so fairly

00:20:25,599 --> 00:20:30,639
simple so far the next step is that we

00:20:29,079 --> 00:20:33,820
need to configure a few properties

00:20:30,639 --> 00:20:35,649
within the deployment manifests so

00:20:33,820 --> 00:20:38,259
because we are running a Cloud Foundry

00:20:35,649 --> 00:20:40,539
deployment and a log search deployment

00:20:38,259 --> 00:20:43,629
as well we're going to end up with two

00:20:40,539 --> 00:20:44,679
separate deployment manifests and if

00:20:43,629 --> 00:20:47,229
you're not familiar with the deployment

00:20:44,679 --> 00:20:49,779
manifest this is basically the file that

00:20:47,229 --> 00:20:52,419
details what your deployment actually

00:20:49,779 --> 00:20:55,119
looks like so for example in the log

00:20:52,419 --> 00:20:58,419
search manifest you could say I want to

00:20:55,119 --> 00:21:03,159
have ten elasticsearch nodes I want them

00:20:58,419 --> 00:21:05,200
to run on these IP addresses etc etc and

00:21:03,159 --> 00:21:08,259
we can use the properties to customize

00:21:05,200 --> 00:21:10,839
those installations so the first

00:21:08,259 --> 00:21:13,509
properties we need to set our properties

00:21:10,839 --> 00:21:15,399
in the Cloud Foundry deployment and we

00:21:13,509 --> 00:21:17,829
need to set these syslog daemon config

00:21:15,399 --> 00:21:20,019
properties and these are the properties

00:21:17,829 --> 00:21:23,200
that tell Cloud Foundry to forward all

00:21:20,019 --> 00:21:26,049
of these syslog messages and we will

00:21:23,200 --> 00:21:30,190
point it at the syslog in gesture of our

00:21:26,049 --> 00:21:31,479
lock search deployment and the second

00:21:30,190 --> 00:21:33,279
set of properties that we need to set

00:21:31,479 --> 00:21:36,249
are the in gesture Cloud Foundry

00:21:33,279 --> 00:21:39,940
firehose properties and these live in

00:21:36,249 --> 00:21:43,389
the block search deployment manifest and

00:21:39,940 --> 00:21:45,339
these properties just detail a user that

00:21:43,389 --> 00:21:50,229
can access the firehose the actual

00:21:45,339 --> 00:21:52,419
firehose endpoint etc etc so we set

00:21:50,229 --> 00:21:55,209
those properties save the file and then

00:21:52,419 --> 00:21:58,419
hit wash deploy at which point Bosch

00:21:55,209 --> 00:22:01,809
will go and do its thing and go and set

00:21:58,419 --> 00:22:03,249
up everything for you and that's

00:22:01,809 --> 00:22:04,959
actually really really awesome

00:22:03,249 --> 00:22:08,769
because we've essentially gone from

00:22:04,959 --> 00:22:11,200
having nothing to a fully scaleable elq

00:22:08,769 --> 00:22:12,909
stack with Cloud Foundry forwarding all

00:22:11,200 --> 00:22:16,450
of the logs through the system in

00:22:12,909 --> 00:22:20,559
essentially just a few commands which i

00:22:16,450 --> 00:22:23,320
think is really really great but I am

00:22:20,559 --> 00:22:24,940
aware that not everyone loves Bosch and

00:22:23,320 --> 00:22:25,779
it can be very difficult to get started

00:22:24,940 --> 00:22:28,260
with it

00:22:25,779 --> 00:22:31,120
so I'd like to point you towards a

00:22:28,260 --> 00:22:33,429
separate project the log search wash

00:22:31,120 --> 00:22:35,710
workspace which really aims to sort of

00:22:33,429 --> 00:22:38,559
help you get up and running with this as

00:22:35,710 --> 00:22:44,740
quickly as possible and that's available

00:22:38,559 --> 00:22:48,039
in the cloud foundry community so why

00:22:44,740 --> 00:22:51,309
should you choose log search obviously

00:22:48,039 --> 00:22:52,690
it's open source that's awesome but

00:22:51,309 --> 00:22:54,610
really my my sort of favorite thing

00:22:52,690 --> 00:22:57,490
about log search is that as I say it

00:22:54,610 --> 00:22:59,559
extracts all of the complexity or most

00:22:57,490 --> 00:23:01,659
of the complexity of managing the elks

00:22:59,559 --> 00:23:03,279
stack and being able to define

00:23:01,659 --> 00:23:06,549
everything in a single deployment

00:23:03,279 --> 00:23:08,889
manifest is really great and it actually

00:23:06,549 --> 00:23:11,470
makes it very easy to scale the system

00:23:08,889 --> 00:23:14,470
as well so for example if we want to

00:23:11,470 --> 00:23:17,200
scale our elasticsearch cluster all we

00:23:14,470 --> 00:23:20,350
need to do is edit one value in that

00:23:17,200 --> 00:23:21,850
file and then hit wash deploy and Bausch

00:23:20,350 --> 00:23:25,899
and log search will go and handle the

00:23:21,850 --> 00:23:27,970
rest for you and while we're talking

00:23:25,899 --> 00:23:30,850
about scaling the graph on the right

00:23:27,970 --> 00:23:32,850
there gives an overview of the number of

00:23:30,850 --> 00:23:36,909
VMs that you need to run for log search

00:23:32,850 --> 00:23:40,600
in order to ingest X logs per a minute

00:23:36,909 --> 00:23:43,600
so I think that's ten VMs for three

00:23:40,600 --> 00:23:45,159
hundred thousand logs a minute which

00:23:43,600 --> 00:23:46,330
which is fairly reasonable but I guess

00:23:45,159 --> 00:23:49,179
the thing to note there is that it's

00:23:46,330 --> 00:23:51,899
pretty it's pretty linear so it seems to

00:23:49,179 --> 00:23:51,899
scale pretty well

00:23:54,240 --> 00:24:00,570
and that's the end of the the logging

00:23:56,249 --> 00:24:03,809
section and to you very much thanks Edie

00:24:00,570 --> 00:24:06,749
I just wanna say at this point we've

00:24:03,809 --> 00:24:09,059
we've got a good solution for logs but

00:24:06,749 --> 00:24:10,440
there is more to life than logs there's

00:24:09,059 --> 00:24:13,289
more to life than just looking at when

00:24:10,440 --> 00:24:16,769
things go wrong we want to support

00:24:13,289 --> 00:24:20,369
patterns we want to spot trends we want

00:24:16,769 --> 00:24:22,259
to see that we can address issues before

00:24:20,369 --> 00:24:24,450
they become the kind of things that crop

00:24:22,259 --> 00:24:30,269
up in logs there's no explosions and

00:24:24,450 --> 00:24:33,690
stack traces so how do we look at the

00:24:30,269 --> 00:24:36,509
metrics in the system as is like a black

00:24:33,690 --> 00:24:38,519
box so how do we get the metrics out get

00:24:36,509 --> 00:24:40,399
them viewed and how do we graph all the

00:24:38,519 --> 00:24:45,690
things

00:24:40,399 --> 00:24:47,220
yes so let's talk about graphite and how

00:24:45,690 --> 00:24:51,149
we can integrate graphite with cloud

00:24:47,220 --> 00:24:53,129
foundry so graphite is a graphing a

00:24:51,149 --> 00:24:55,100
metrics tool that's gained quite a lot

00:24:53,129 --> 00:24:57,690
of popularity over the past few years

00:24:55,100 --> 00:25:00,809
and I think that this has been partly

00:24:57,690 --> 00:25:04,409
due to - a famed blog post written by

00:25:00,809 --> 00:25:05,610
the team at Etsy and one of the things

00:25:04,409 --> 00:25:07,980
that they mention in that in that

00:25:05,610 --> 00:25:10,470
article is how HC they worship at the

00:25:07,980 --> 00:25:14,070
Church of graphing and how they use

00:25:10,470 --> 00:25:16,200
graphite to achieve this and so it would

00:25:14,070 --> 00:25:18,590
be great if we could have our own church

00:25:16,200 --> 00:25:21,330
of graphing available for cloud foundry

00:25:18,590 --> 00:25:22,710
and fortunately we can and there's

00:25:21,330 --> 00:25:24,720
actually quite a few ways that we can do

00:25:22,710 --> 00:25:26,309
this and I'm just going to talk about

00:25:24,720 --> 00:25:29,249
two of the ways that we can integrate

00:25:26,309 --> 00:25:34,889
graphite with cloud foundry to provide a

00:25:29,249 --> 00:25:36,600
really nice metrics solution so the

00:25:34,889 --> 00:25:39,149
first approach is that we could use the

00:25:36,600 --> 00:25:41,249
cloud foundry collector and the

00:25:39,149 --> 00:25:42,749
collector is an optional component that

00:25:41,249 --> 00:25:46,019
ships with a standard cloud foundry

00:25:42,749 --> 00:25:47,789
deployment but I kind of refrained from

00:25:46,019 --> 00:25:50,190
talking about the collector too much in

00:25:47,789 --> 00:25:52,440
this talk the reason being that it's

00:25:50,190 --> 00:25:55,740
it's sort of being deprecated in favor

00:25:52,440 --> 00:25:57,330
of the fire hose but actually it is

00:25:55,740 --> 00:25:59,730
pretty quick and easy to set up and

00:25:57,330 --> 00:26:01,080
maybe if you're stuck running an older

00:25:59,730 --> 00:26:04,830
version of cloud foundry for whatever

00:26:01,080 --> 00:26:06,720
reason and the collector is a quick

00:26:04,830 --> 00:26:07,470
solution that you can use to get some

00:26:06,720 --> 00:26:12,150
metrics

00:26:07,470 --> 00:26:15,150
out of the system so the collector works

00:26:12,150 --> 00:26:19,350
by basically querying the slash health

00:26:15,150 --> 00:26:22,409
said and /var ZD h TT endpoints of all

00:26:19,350 --> 00:26:25,020
of the Cloud Foundry components so every

00:26:22,409 --> 00:26:28,049
component in Cloud Foundry exposes these

00:26:25,020 --> 00:26:30,840
endpoints slash health said will return

00:26:28,049 --> 00:26:32,900
either a 1 or a 0 depending on whether

00:26:30,840 --> 00:26:35,880
or not the process is healthy or not and

00:26:32,900 --> 00:26:39,390
/ba Zed will return some more detailed

00:26:35,880 --> 00:26:41,820
information about the process so for

00:26:39,390 --> 00:26:44,340
example if you query slash bar Zed on a

00:26:41,820 --> 00:26:45,900
UA a server it will return some

00:26:44,340 --> 00:26:48,960
information about the underlying Java

00:26:45,900 --> 00:26:50,760
process and so the collector sits there

00:26:48,960 --> 00:26:52,919
querying all of these endpoints for all

00:26:50,760 --> 00:26:57,720
of the components and gathering all the

00:26:52,919 --> 00:27:01,110
data and it then uses what is called a

00:26:57,720 --> 00:27:04,860
historian to forward that data on to

00:27:01,110 --> 00:27:08,190
some third party unfortunately there's a

00:27:04,860 --> 00:27:09,240
graphite historian to do this I should

00:27:08,190 --> 00:27:11,130
just mention though that this is

00:27:09,240 --> 00:27:13,260
currently considered to be a community

00:27:11,130 --> 00:27:15,510
maintain feature so it's not being

00:27:13,260 --> 00:27:17,610
actively developed anymore but it does

00:27:15,510 --> 00:27:21,270
work and as I said it's quite quick and

00:27:17,610 --> 00:27:23,190
easy to get to get it set up so to

00:27:21,270 --> 00:27:25,440
actually do this all we need to do is

00:27:23,190 --> 00:27:27,650
ensure that the collector is included as

00:27:25,440 --> 00:27:31,169
part of your Cloud Foundry deployment

00:27:27,650 --> 00:27:33,210
and then we just need to set a couple of

00:27:31,169 --> 00:27:35,159
properties so you want to say use

00:27:33,210 --> 00:27:36,720
graphite to be true and then you don't

00:27:35,159 --> 00:27:42,690
want to provide the IP address and port

00:27:36,720 --> 00:27:45,539
of a graphite server fairly quick and

00:27:42,690 --> 00:27:47,610
easy and you go and run Bosch deploy and

00:27:45,539 --> 00:27:49,980
it just goes and does it thing does its

00:27:47,610 --> 00:27:53,070
thing and we end up with our metrics in

00:27:49,980 --> 00:27:54,630
graphite so that's kind of okay but

00:27:53,070 --> 00:27:57,120
there are a few problems with using that

00:27:54,630 --> 00:28:00,330
approach namely that it's being

00:27:57,120 --> 00:28:02,669
deprecated so an alternate approach that

00:28:00,330 --> 00:28:05,820
we could use then is to grab our metrics

00:28:02,669 --> 00:28:09,000
directly from the firehose using a

00:28:05,820 --> 00:28:10,860
nozzle so I was kind of interested to

00:28:09,000 --> 00:28:13,830
see what a nozzle might look like for

00:28:10,860 --> 00:28:17,669
graphite and so so I wrote the the

00:28:13,830 --> 00:28:19,860
creatively named graphite - nozzle which

00:28:17,669 --> 00:28:21,350
is available on the cloud credo github

00:28:19,860 --> 00:28:24,509
page

00:28:21,350 --> 00:28:27,059
and this is a small NGO program that as

00:28:24,509 --> 00:28:29,549
I said connects to the firehose listens

00:28:27,059 --> 00:28:31,950
for all of the metrics events pulls them

00:28:29,549 --> 00:28:34,019
down does some parsing and processing on

00:28:31,950 --> 00:28:37,379
them and then forwards them off to a

00:28:34,019 --> 00:28:40,230
graphite server and that's generally

00:28:37,379 --> 00:28:42,299
working quite quite well so please feel

00:28:40,230 --> 00:28:47,669
free to check that out and let me know

00:28:42,299 --> 00:28:49,470
what you think this picture this is an

00:28:47,669 --> 00:28:52,559
example of what a graphic server might

00:28:49,470 --> 00:28:55,379
look like probably not all that that

00:28:52,559 --> 00:28:56,999
impressive at the moment but it's got

00:28:55,379 --> 00:28:59,159
some nice pretty colors and and graphite

00:28:56,999 --> 00:29:04,950
always looks good on monitors throughout

00:28:59,159 --> 00:29:08,700
your office which is great and I think

00:29:04,950 --> 00:29:11,039
that that is everything so thank you

00:29:08,700 --> 00:29:13,909
very much for listening and I hope that

00:29:11,039 --> 00:29:13,909

YouTube URL: https://www.youtube.com/watch?v=jTxnCV7wjeA


