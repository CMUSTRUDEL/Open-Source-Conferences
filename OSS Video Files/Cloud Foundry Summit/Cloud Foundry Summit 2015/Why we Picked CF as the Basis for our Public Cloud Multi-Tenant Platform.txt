Title: Why we Picked CF as the Basis for our Public Cloud Multi-Tenant Platform
Publication date: 2015-05-12
Playlist: Cloud Foundry Summit 2015
Description: 
	Why we Picked CF as the Basis for our Public Cloud Multi-Tenant Platform - 05 Michael Root, Jeroen van Rotterdam 720p
Captions: 
	00:00:00,030 --> 00:00:05,490
thanks for joining let me start with a

00:00:03,419 --> 00:00:07,919
quick introduction so I'm Jerome from

00:00:05,490 --> 00:00:10,099
Rotterdam as they say in Dutch or they

00:00:07,919 --> 00:00:12,900
call me Jay if you try to plant it and

00:00:10,099 --> 00:00:15,360
I'm actually the CTO for the EMC

00:00:12,900 --> 00:00:18,300
enterprise content division and Mike

00:00:15,360 --> 00:00:20,070
Sears welcome at Mike and I'm architect

00:00:18,300 --> 00:00:24,090
on the cloud sass platform that we're

00:00:20,070 --> 00:00:25,680
building for content applications all

00:00:24,090 --> 00:00:27,779
right ok so what we want to explain

00:00:25,680 --> 00:00:30,599
today is how we're using cloud foundry

00:00:27,779 --> 00:00:33,000
and how we're using it in production and

00:00:30,599 --> 00:00:36,000
sort of the struggles we had deploying

00:00:33,000 --> 00:00:38,340
it and why we actually picked that in

00:00:36,000 --> 00:00:42,180
the first place what the business

00:00:38,340 --> 00:00:43,920
benefits were so if you look at our

00:00:42,180 --> 00:00:46,680
environment so several years ago we

00:00:43,920 --> 00:00:49,789
started to build a new content platform

00:00:46,680 --> 00:00:53,520
and so that first quarter division is

00:00:49,789 --> 00:00:55,920
it's primarily focused on content

00:00:53,520 --> 00:00:57,870
management type of applications rich

00:00:55,920 --> 00:00:59,579
content apps as well as a process

00:00:57,870 --> 00:01:01,710
centric apps and apps that are sort of

00:00:59,579 --> 00:01:05,369
on the intersection between content and

00:01:01,710 --> 00:01:06,869
process and it's pretty sizable division

00:01:05,369 --> 00:01:09,240
by the way so it's around seven hundred

00:01:06,869 --> 00:01:10,830
million in revenue two and half thousand

00:01:09,240 --> 00:01:15,810
people's that sort of that order of

00:01:10,830 --> 00:01:17,580
magnitude and you go back alright so

00:01:15,810 --> 00:01:21,030
what we wanted to do is basically build

00:01:17,580 --> 00:01:23,850
a new platform for our customers and our

00:01:21,030 --> 00:01:26,970
partners to build application upon and

00:01:23,850 --> 00:01:29,549
as well as we are in the business of

00:01:26,970 --> 00:01:32,640
building applications or solution on top

00:01:29,549 --> 00:01:36,000
of our own platform as well and so we

00:01:32,640 --> 00:01:39,780
were shooting to to build a base

00:01:36,000 --> 00:01:41,729
platform that is sort of handling this

00:01:39,780 --> 00:01:44,189
matrix where you can have multiple

00:01:41,729 --> 00:01:47,520
applications running in the same

00:01:44,189 --> 00:01:50,340
environment for multiple tenants so we

00:01:47,520 --> 00:01:54,329
build a multi tenant platform from from

00:01:50,340 --> 00:01:55,979
the ground up and you know the many

00:01:54,329 --> 00:01:57,540
challenges if you go multi-turn but

00:01:55,979 --> 00:01:59,969
there are many promises if you do that

00:01:57,540 --> 00:02:01,200
as well obviously the cost of ownership

00:01:59,969 --> 00:02:02,700
is one of them

00:02:01,200 --> 00:02:04,320
and so we wanted to build a single

00:02:02,700 --> 00:02:05,750
environment for multiple apps and

00:02:04,320 --> 00:02:08,129
multiple tenants

00:02:05,750 --> 00:02:10,110
man it's like life on if we start to

00:02:08,129 --> 00:02:13,410
adopt I found Mike about two and a half

00:02:10,110 --> 00:02:13,920
years ago yep soon of years ago it was

00:02:13,410 --> 00:02:15,930
still with

00:02:13,920 --> 00:02:17,640
VMware before I moved to pivitol before

00:02:15,930 --> 00:02:20,880
it moved to open-source so we were

00:02:17,640 --> 00:02:22,709
extremely early in making that

00:02:20,880 --> 00:02:24,300
commitment and we said well you know

00:02:22,709 --> 00:02:27,239
there's a good promise which I'll

00:02:24,300 --> 00:02:29,760
explain today and let's go for that but

00:02:27,239 --> 00:02:31,470
if you look at it there's a you know be

00:02:29,760 --> 00:02:33,120
behind the scenes you know you've got

00:02:31,470 --> 00:02:36,270
this single environment running multiple

00:02:33,120 --> 00:02:38,400
apps and multiple tenants but they

00:02:36,270 --> 00:02:41,280
actually share a lot of sort of content

00:02:38,400 --> 00:02:43,980
centric and process centric services or

00:02:41,280 --> 00:02:46,800
micro services as they are called these

00:02:43,980 --> 00:02:48,600
days and so them these are services

00:02:46,800 --> 00:02:50,580
around for instance transforming content

00:02:48,600 --> 00:02:53,459
from one form to a different format or

00:02:50,580 --> 00:02:57,090
things like the process engine to run

00:02:53,459 --> 00:03:00,360
business processes we've got an engine

00:02:57,090 --> 00:03:02,640
for case management we do secure certs

00:03:00,360 --> 00:03:04,980
of secure full text and metadata certs

00:03:02,640 --> 00:03:07,650
things like storing basic metadata

00:03:04,980 --> 00:03:11,010
modeling Yavanna they be storing content

00:03:07,650 --> 00:03:14,760
files in a scale out way and each of

00:03:11,010 --> 00:03:18,150
these underlying micro services are need

00:03:14,760 --> 00:03:20,250
to be skilled independently because some

00:03:18,150 --> 00:03:25,170
are more CPU hour or memory intensive

00:03:20,250 --> 00:03:27,750
then then others and you know all these

00:03:25,170 --> 00:03:29,880
apps they sort of reuse the underlying

00:03:27,750 --> 00:03:32,400
common application platform as a service

00:03:29,880 --> 00:03:35,640
as we call it with these common services

00:03:32,400 --> 00:03:37,650
and you know the reason why we started

00:03:35,640 --> 00:03:40,290
to build this multi at multi-tenant

00:03:37,650 --> 00:03:44,280
platform is that in our business sort of

00:03:40,290 --> 00:03:46,980
there's a trend towards smaller

00:03:44,280 --> 00:03:49,130
applications with a smaller life cycle

00:03:46,980 --> 00:03:52,440
so the big monolithic apps are sort of

00:03:49,130 --> 00:03:56,070
are gone and specifically mobile you see

00:03:52,440 --> 00:03:57,780
a need for more targeted apps that solve

00:03:56,070 --> 00:03:59,190
one specific business problem really

00:03:57,780 --> 00:04:03,480
really well with excellent user

00:03:59,190 --> 00:04:05,880
experience rather than have a big app

00:04:03,480 --> 00:04:07,709
that tries to address many different use

00:04:05,880 --> 00:04:09,600
cases and so if you think about it

00:04:07,709 --> 00:04:12,930
modeled and there's sort of an

00:04:09,600 --> 00:04:15,569
interesting direction that the apps get

00:04:12,930 --> 00:04:16,950
smaller and and it takes much smaller to

00:04:15,569 --> 00:04:19,799
build an app like that but the life

00:04:16,950 --> 00:04:21,930
cycle bit is much smaller as well and so

00:04:19,799 --> 00:04:24,040
in fact the underlying data in that

00:04:21,930 --> 00:04:27,760
platform in the continent data

00:04:24,040 --> 00:04:30,970
data will outlive the lifespan of that

00:04:27,760 --> 00:04:32,890
small little app and so you actually

00:04:30,970 --> 00:04:36,400
need to think about how to decouple your

00:04:32,890 --> 00:04:39,040
data model from the applications and you

00:04:36,400 --> 00:04:43,000
need to have a very fast-paced model to

00:04:39,040 --> 00:04:45,700
build these apps and deploy them as well

00:04:43,000 --> 00:04:48,610
as minutes now many of those different

00:04:45,700 --> 00:04:50,980
apps in the you know on the same shared

00:04:48,610 --> 00:04:53,650
environment so there's absolutely a need

00:04:50,980 --> 00:04:55,480
for managing multiple applications in an

00:04:53,650 --> 00:04:58,360
environment over the same share data and

00:04:55,480 --> 00:05:00,580
content but there's also a need to do

00:04:58,360 --> 00:05:02,920
multi-tenancy and effect you know we're

00:05:00,580 --> 00:05:05,680
in the enterprise segment but we're

00:05:02,920 --> 00:05:09,220
moving down market and if you go go down

00:05:05,680 --> 00:05:11,020
market then the marginal cost to to

00:05:09,220 --> 00:05:14,890
onboard a new customer needs to be as

00:05:11,020 --> 00:05:16,750
small as you can can get it and so we

00:05:14,890 --> 00:05:20,230
were really focused on you know bringing

00:05:16,750 --> 00:05:22,540
the TCO down for adding new new tenants

00:05:20,230 --> 00:05:24,460
and and at the same time for our own

00:05:22,540 --> 00:05:26,440
premise customers a lot of enterprise

00:05:24,460 --> 00:05:29,830
customers that actually have a

00:05:26,440 --> 00:05:31,600
multi-tenant problem so you know

00:05:29,830 --> 00:05:34,990
typically there are divisions or their

00:05:31,600 --> 00:05:37,420
country organizations that have a level

00:05:34,990 --> 00:05:41,110
of isolation so even in the enterprise

00:05:37,420 --> 00:05:46,660
segment you see a need for multi-tenancy

00:05:41,110 --> 00:05:48,490
within one company all right all right

00:05:46,660 --> 00:05:53,560
so another thing we did is so we build

00:05:48,490 --> 00:05:55,840
this multi tenant public cloud platform

00:05:53,560 --> 00:05:57,610
as well as solutions on top of that and

00:05:55,840 --> 00:05:59,140
one thing that the customers really like

00:05:57,610 --> 00:06:03,520
is that they can sort of personalise

00:05:59,140 --> 00:06:06,310
their URI and there as well as the

00:06:03,520 --> 00:06:09,130
behavior in the application and so what

00:06:06,310 --> 00:06:11,830
we're running is running these shared

00:06:09,130 --> 00:06:14,380
services in you know all of these

00:06:11,830 --> 00:06:17,590
components these micro services in our

00:06:14,380 --> 00:06:19,240
application platform as a service on top

00:06:17,590 --> 00:06:21,400
of our persistence model and so we've

00:06:19,240 --> 00:06:23,440
got three main per system all using

00:06:21,400 --> 00:06:27,880
Cassandra as the key value store we're

00:06:23,440 --> 00:06:30,730
using our own database X DB for metadata

00:06:27,880 --> 00:06:34,780
it's an metadata repository scale-out

00:06:30,730 --> 00:06:36,580
repository as well as all of these

00:06:34,780 --> 00:06:37,750
buckets of shared servers that we need

00:06:36,580 --> 00:06:39,880
to scale in the video

00:06:37,750 --> 00:06:41,350
and on top of that our application and

00:06:39,880 --> 00:06:45,880
the first application we build is the

00:06:41,350 --> 00:06:47,730
so-called supplier exchange app and and

00:06:45,880 --> 00:06:52,090
each of these customers will have their

00:06:47,730 --> 00:06:54,220
dedicated URI to hook into this platform

00:06:52,090 --> 00:06:57,000
and we need to manage sort of the

00:06:54,220 --> 00:06:59,560
versioning of the underlying

00:06:57,000 --> 00:07:01,150
applications as well as the dependency

00:06:59,560 --> 00:07:02,740
between the version of a specific

00:07:01,150 --> 00:07:05,680
application and underlying shared

00:07:02,740 --> 00:07:08,230
services as well as the persistence

00:07:05,680 --> 00:07:10,660
layer and specifically if you're doing

00:07:08,230 --> 00:07:13,570
is you know if you're upgrading your

00:07:10,660 --> 00:07:16,390
cloud which we do for all tenants then

00:07:13,570 --> 00:07:19,060
use sort of migrated tenants towards the

00:07:16,390 --> 00:07:22,000
new bits from the application level down

00:07:19,060 --> 00:07:24,490
so this dependency between hey you know

00:07:22,000 --> 00:07:28,350
the router configuration as well as the

00:07:24,490 --> 00:07:31,390
shared services and the application bits

00:07:28,350 --> 00:07:33,550
and the version of those are something

00:07:31,390 --> 00:07:34,960
that we're managing using cloud foundry

00:07:33,550 --> 00:07:36,250
and we didn't want to set all those up

00:07:34,960 --> 00:07:38,290
by hand we didn't want to set all these

00:07:36,250 --> 00:07:40,120
up by hand we wanted some tool to help

00:07:38,290 --> 00:07:40,870
us do that well to do this by n will be

00:07:40,120 --> 00:07:42,610
a disaster

00:07:40,870 --> 00:07:47,530
right so there's the last thing you want

00:07:42,610 --> 00:07:50,890
to do and so we sort of this is our real

00:07:47,530 --> 00:07:53,500
production set up so we like I said we

00:07:50,890 --> 00:07:55,660
started two and a half years ago and it

00:07:53,500 --> 00:07:58,300
was really early and we filled this

00:07:55,660 --> 00:08:00,040
brand-new you know application services

00:07:58,300 --> 00:08:01,900
and reported a bunch of assets we had

00:08:00,040 --> 00:08:04,690
from our on premise platform made it

00:08:01,900 --> 00:08:08,560
multi tenant as well as building a new

00:08:04,690 --> 00:08:09,850
app and and so it took a long time to

00:08:08,560 --> 00:08:13,240
actually build the sort of the

00:08:09,850 --> 00:08:15,550
foundation first and the radar and then

00:08:13,240 --> 00:08:18,130
we sort of made made of the full project

00:08:15,550 --> 00:08:21,100
within the division and then sort of

00:08:18,130 --> 00:08:24,240
early 2014 we really started to focus on

00:08:21,100 --> 00:08:28,000
the deployment of the environment and

00:08:24,240 --> 00:08:30,820
and so this is the real-life situation

00:08:28,000 --> 00:08:34,719
so I current in production or even in

00:08:30,820 --> 00:08:36,280
production on August 1st 14 where we're

00:08:34,719 --> 00:08:39,520
running as much as we can on the

00:08:36,280 --> 00:08:42,339
non-persistent services in wardens and

00:08:39,520 --> 00:08:45,580
so they're actually three applications

00:08:42,339 --> 00:08:48,070
so we got the sort of end user facing

00:08:45,580 --> 00:08:50,130
supplier exchange but we created two

00:08:48,070 --> 00:08:52,980
other applications one is

00:08:50,130 --> 00:08:54,270
tenant management consoles or super user

00:08:52,980 --> 00:08:56,400
from the tenant in a multi-tenant

00:08:54,270 --> 00:08:58,890
platform can manage his own environment

00:08:56,400 --> 00:09:02,400
invite users have sort of this viral

00:08:58,890 --> 00:09:04,020
effect on within the organization or

00:09:02,400 --> 00:09:05,760
beyond his organization and then we

00:09:04,020 --> 00:09:08,730
created a platform management console

00:09:05,760 --> 00:09:10,740
which is an environment for us to men as

00:09:08,730 --> 00:09:12,750
the tenant so for instance if a tenant

00:09:10,740 --> 00:09:15,840
is refusing to paid and we can pause it

00:09:12,750 --> 00:09:18,030
right so there's an example or that's

00:09:15,840 --> 00:09:20,130
where we actually provision new tenants

00:09:18,030 --> 00:09:22,680
if they give us a purchase order for

00:09:20,130 --> 00:09:25,110
that and then you got these warden

00:09:22,680 --> 00:09:26,820
contains with the micro services like

00:09:25,110 --> 00:09:30,510
the process engine and metadata server

00:09:26,820 --> 00:09:32,820
and transformation services as well as

00:09:30,510 --> 00:09:34,820
analytics capabilities so we've got this

00:09:32,820 --> 00:09:39,150
really nice analytic analytics engine

00:09:34,820 --> 00:09:41,400
that sort of is a configuration attached

00:09:39,150 --> 00:09:42,900
to the data model of an app and you can

00:09:41,400 --> 00:09:45,090
think of it as a tap right so we can

00:09:42,900 --> 00:09:47,580
figure that and either more or less data

00:09:45,090 --> 00:09:49,950
or more or less data comes into our

00:09:47,580 --> 00:09:54,330
analytics platform which is based on

00:09:49,950 --> 00:09:57,420
hock and PhD and then there are a bunch

00:09:54,330 --> 00:10:00,870
of funny services though we built our

00:09:57,420 --> 00:10:02,370
own brokers or services later and for

00:10:00,870 --> 00:10:07,700
xdb and we had to build one for

00:10:02,370 --> 00:10:10,460
Cassandra and using rabbit as the mq and

00:10:07,700 --> 00:10:13,800
and then there are a few boss managed

00:10:10,460 --> 00:10:15,570
virtual machines like for instance the

00:10:13,800 --> 00:10:19,440
Swift store this is the interface to

00:10:15,570 --> 00:10:21,390
store files as well as our full text

00:10:19,440 --> 00:10:24,540
engine that we ported over into this

00:10:21,390 --> 00:10:27,090
environment and you know a multi tenant

00:10:24,540 --> 00:10:30,060
federated authentication module which

00:10:27,090 --> 00:10:32,040
was not easy to build and then few sort

00:10:30,060 --> 00:10:37,380
of outliers that we got a bunch of VMs

00:10:32,040 --> 00:10:40,290
for you know we're actually moving you

00:10:37,380 --> 00:10:42,030
know hoc and pivotal HDE as towards the

00:10:40,290 --> 00:10:45,180
services model that was just a temporary

00:10:42,030 --> 00:10:48,000
thing and unfortunately we've got one

00:10:45,180 --> 00:10:51,660
Windows VM with you know you know ad

00:10:48,000 --> 00:10:56,400
sync type of set up so all the rest is

00:10:51,660 --> 00:10:58,260
just Linux all right so after we after

00:10:56,400 --> 00:11:00,210
we decided that we wanted all this bosch

00:10:58,260 --> 00:11:01,440
and Cloud Foundry stuff then we took it

00:11:00,210 --> 00:11:03,640
to the security guys and they said well

00:11:01,440 --> 00:11:04,840
how can we set up this security no

00:11:03,640 --> 00:11:06,190
we like to have firewalls around our

00:11:04,840 --> 00:11:08,620
little applications and around our

00:11:06,190 --> 00:11:09,970
services how are we going to do that so

00:11:08,620 --> 00:11:11,320
it took us a little bit of convincing a

00:11:09,970 --> 00:11:13,840
little bit of talking to say the CF

00:11:11,320 --> 00:11:16,840
router is one VM and we can put that in

00:11:13,840 --> 00:11:18,910
its own network and the Bosch stuff can

00:11:16,840 --> 00:11:21,430
be in its own network Bosch director all

00:11:18,910 --> 00:11:22,450
the Bosch mediums all the das can be in

00:11:21,430 --> 00:11:24,670
their own Network because I are gonna

00:11:22,450 --> 00:11:28,600
run the applications themselves and all

00:11:24,670 --> 00:11:30,460
the CF stuff cloud controller you AAA

00:11:28,600 --> 00:11:32,380
all those things can run in its own

00:11:30,460 --> 00:11:34,450
network so we've actually partitioned

00:11:32,380 --> 00:11:36,280
our production environment into the six

00:11:34,450 --> 00:11:38,560
networks with the CF services all the

00:11:36,280 --> 00:11:41,680
persistent stores are in its own network

00:11:38,560 --> 00:11:43,330
it took them Bosch was working fine it

00:11:41,680 --> 00:11:44,680
took a little just the firewalls and

00:11:43,330 --> 00:11:46,420
things like that was a lot of work but

00:11:44,680 --> 00:11:47,800
washed by itself just deployed

00:11:46,420 --> 00:11:49,600
everything in the right networks fine we

00:11:47,800 --> 00:11:52,480
were really happy with that hey Mike

00:11:49,600 --> 00:11:54,430
what was the decision to make these two

00:11:52,480 --> 00:11:56,230
partition a network right I can see that

00:11:54,430 --> 00:11:58,600
you want to isolate Windows right yeah

00:11:56,230 --> 00:12:02,920
keep that apart and you don't trust that

00:11:58,600 --> 00:12:04,720
but yeah so the key decisions were that

00:12:02,920 --> 00:12:06,100
the CF router was the end point where

00:12:04,720 --> 00:12:07,570
everything is coming in from the

00:12:06,100 --> 00:12:09,040
Internet's coming to the CF router so we

00:12:07,570 --> 00:12:10,690
wanted that and isolated if it was

00:12:09,040 --> 00:12:13,270
hacked or broken into we didn't want

00:12:10,690 --> 00:12:14,140
that facing yeah publicly facing and

00:12:13,270 --> 00:12:15,790
then the DA's were where the

00:12:14,140 --> 00:12:17,440
applications were running so the guys

00:12:15,790 --> 00:12:18,550
are gonna route the rest calls and

00:12:17,440 --> 00:12:20,200
things are going to get all the way into

00:12:18,550 --> 00:12:24,250
the DEA so we wanted to isolate that as

00:12:20,200 --> 00:12:25,690
well and then the boss really was seen

00:12:24,250 --> 00:12:27,700
as the more secure that's the one that's

00:12:25,690 --> 00:12:30,370
that can talk to the vCenter that's can

00:12:27,700 --> 00:12:32,380
create VMs DBMS we wanted Vash to be

00:12:30,370 --> 00:12:34,510
isolated also and that's kind of where

00:12:32,380 --> 00:12:36,700
we fell into this like since the

00:12:34,510 --> 00:12:38,440
persistence stores and its own network

00:12:36,700 --> 00:12:40,360
router public facing in its own network

00:12:38,440 --> 00:12:42,910
DEA is running applications in their own

00:12:40,360 --> 00:12:44,320
network and Bosch has all the secrets

00:12:42,910 --> 00:12:45,940
and passwords we want someone breaking

00:12:44,320 --> 00:12:47,620
in and deleting our VMs and things like

00:12:45,940 --> 00:12:49,240
that so that was kind of our our

00:12:47,620 --> 00:12:51,280
thinking around that she's not uncommon

00:12:49,240 --> 00:12:53,470
in a real enterprise setting is not

00:12:51,280 --> 00:12:55,690
uncommon right yeah although if you look

00:12:53,470 --> 00:12:57,340
at all the swift manifests in a cloud

00:12:55,690 --> 00:12:58,720
foundry manifests they don't break it up

00:12:57,340 --> 00:13:01,480
like this right so we had to do a little

00:12:58,720 --> 00:13:02,980
bit of work to add the extra networks to

00:13:01,480 --> 00:13:04,600
lay it all in there but once you add the

00:13:02,980 --> 00:13:08,320
extra networks and lay it all out it

00:13:04,600 --> 00:13:10,480
deploys quite nicely so after talking to

00:13:08,320 --> 00:13:12,400
the security guys then you talk to the

00:13:10,480 --> 00:13:13,720
operations guys and the engineers and

00:13:12,400 --> 00:13:15,580
developers and then they kept asking

00:13:13,720 --> 00:13:16,410
like why are we using this Cloud Foundry

00:13:15,580 --> 00:13:18,240
thing why are we

00:13:16,410 --> 00:13:21,360
using this Bosch thing so it came up

00:13:18,240 --> 00:13:24,389
with like the short list of why we chose

00:13:21,360 --> 00:13:25,709
to do this one is we wanted a

00:13:24,389 --> 00:13:27,029
standardized on Bosch so we talked about

00:13:25,709 --> 00:13:29,550
lowering the khatola cost of ownership

00:13:27,029 --> 00:13:31,470
and this is the total cost of ownership

00:13:29,550 --> 00:13:34,800
is not just building the application but

00:13:31,470 --> 00:13:36,660
it's also the operational side so having

00:13:34,800 --> 00:13:40,079
everything deployed with Bosch is a big

00:13:36,660 --> 00:13:42,329
time saver on our on our plate we can

00:13:40,079 --> 00:13:43,949
deploy all our environments it's one

00:13:42,329 --> 00:13:46,529
command we have several Bloch manifests

00:13:43,949 --> 00:13:48,990
but Bosch deploy sets up the data center

00:13:46,529 --> 00:13:49,709
the exactly the way you want to you

00:13:48,990 --> 00:13:51,449
don't have to worry about the

00:13:49,709 --> 00:13:54,600
operational people typing the wrong

00:13:51,449 --> 00:13:57,720
command or miss configuring things we

00:13:54,600 --> 00:13:59,879
like Cloud Foundry for the having the

00:13:57,720 --> 00:14:03,360
scalability and then also the tenant URL

00:13:59,879 --> 00:14:06,870
so we talked about Acme DMC Oh Andy calm

00:14:03,360 --> 00:14:08,670
or fubar EMC Oh Andy calm we one of the

00:14:06,870 --> 00:14:09,959
requirements was that the customer

00:14:08,670 --> 00:14:12,839
needed to be able to pick their own URL

00:14:09,959 --> 00:14:14,490
so we have our application they can type

00:14:12,839 --> 00:14:16,829
in what URL they want we can check with

00:14:14,490 --> 00:14:19,350
Cloud Foundry to see if it's in use if

00:14:16,829 --> 00:14:20,639
it's not used we'll just deploy it talk

00:14:19,350 --> 00:14:22,680
with cloth founder through the api's

00:14:20,639 --> 00:14:25,139
give them the new URL and now they can

00:14:22,680 --> 00:14:28,230
go to that new URL and login and be on

00:14:25,139 --> 00:14:29,880
their way and then on top of that

00:14:28,230 --> 00:14:32,310
we were able to use the cloth founder

00:14:29,880 --> 00:14:33,899
api's to build our own upgrade tool to

00:14:32,310 --> 00:14:35,519
do blue green deployments so we don't

00:14:33,899 --> 00:14:37,290
have any downtime of any of our services

00:14:35,519 --> 00:14:38,339
or any of our applications and I'll talk

00:14:37,290 --> 00:14:41,660
through that a little more later

00:14:38,339 --> 00:14:44,670
and this relying on Bosch really

00:14:41,660 --> 00:14:46,439
exemplified it and really hit home with

00:14:44,670 --> 00:14:47,910
everyone when we hit the shell shock

00:14:46,439 --> 00:14:50,639
event that happened last year right big

00:14:47,910 --> 00:14:53,360
security event the OS was compromised we

00:14:50,639 --> 00:14:55,470
needed to upgrade the OS and for us

00:14:53,360 --> 00:14:56,880
there was like no worry right because

00:14:55,470 --> 00:14:59,309
this was built into our plan our plan

00:14:56,880 --> 00:15:01,949
was get the new stem cell run Bosch

00:14:59,309 --> 00:15:03,720
deploy we're all done so it took a

00:15:01,949 --> 00:15:05,910
little while we waited for pivotal to

00:15:03,720 --> 00:15:09,600
update the stem cell patch patch the OS

00:15:05,910 --> 00:15:11,459
got the new stem cell and fairly quickly

00:15:09,600 --> 00:15:12,540
over the course of a couple days running

00:15:11,459 --> 00:15:14,670
through all the environments doing the

00:15:12,540 --> 00:15:18,120
proper testing we had everything up and

00:15:14,670 --> 00:15:21,839
running so upgrade the stem cell run

00:15:18,120 --> 00:15:22,829
Bosch deploy go drink beer so am so

00:15:21,839 --> 00:15:25,110
let's talk a little bit about it so

00:15:22,829 --> 00:15:27,360
these are almost 500 virtual machines

00:15:25,110 --> 00:15:28,340
that were updated automatically right

00:15:27,360 --> 00:15:30,830
yep

00:15:28,340 --> 00:15:32,000
now they explain so there are 16 clock

00:15:30,830 --> 00:15:34,550
counting environments I think that's

00:15:32,000 --> 00:15:37,400
good to understand that we have a quite

00:15:34,550 --> 00:15:39,740
a slew of test environments on falconry

00:15:37,400 --> 00:15:41,180
as well as a you know a pea broth

00:15:39,740 --> 00:15:45,260
integration pea protein production

00:15:41,180 --> 00:15:47,570
environment right and you want to

00:15:45,260 --> 00:15:49,790
elaborate on that yes so the beauty is

00:15:47,570 --> 00:15:51,320
that once we did at once right so we did

00:15:49,790 --> 00:15:53,120
it in the first cloth under environment

00:15:51,320 --> 00:15:55,250
our sea environment the CI tests were

00:15:53,120 --> 00:15:57,230
running automatically an hour later we

00:15:55,250 --> 00:15:59,000
had a green build that said everything's

00:15:57,230 --> 00:16:01,820
running good on this new CI environment

00:15:59,000 --> 00:16:03,260
at that point we were highly confident

00:16:01,820 --> 00:16:05,240
that every all the other environment is

00:16:03,260 --> 00:16:07,670
going to work right so we took this

00:16:05,240 --> 00:16:09,350
manifest we took this stem cell we said

00:16:07,670 --> 00:16:11,690
okay give it to the next guy down the

00:16:09,350 --> 00:16:13,490
line run Bosch deploy that worked also

00:16:11,690 --> 00:16:15,620
okay next line down line run Bosch

00:16:13,490 --> 00:16:17,660
deployed just down line and there was

00:16:15,620 --> 00:16:19,130
once it was working in one we knew that

00:16:17,660 --> 00:16:20,270
it was going to work everywhere so it

00:16:19,130 --> 00:16:22,700
gives us that high degree of confidence

00:16:20,270 --> 00:16:25,250
using Bosch that we can run Bosch deploy

00:16:22,700 --> 00:16:26,360
in one we can hand off the manifest down

00:16:25,250 --> 00:16:28,100
to the next guy that needs to update

00:16:26,360 --> 00:16:30,260
their environment and it just upgrades

00:16:28,100 --> 00:16:31,730
so I think the level of automation is

00:16:30,260 --> 00:16:33,500
super-important right so in this

00:16:31,730 --> 00:16:35,330
environment we've got you know

00:16:33,500 --> 00:16:38,180
applications with end-user user

00:16:35,330 --> 00:16:40,970
interfaces as well as micro services and

00:16:38,180 --> 00:16:43,790
persistence and with rest services in

00:16:40,970 --> 00:16:45,920
between right so we test everything in

00:16:43,790 --> 00:16:48,830
an automated fashion right so we got a

00:16:45,920 --> 00:16:50,960
full test automation suite and the new

00:16:48,830 --> 00:16:52,850
UI technology which is this is bootstrap

00:16:50,960 --> 00:16:55,580
and angularjs right it makes it so much

00:16:52,850 --> 00:16:57,260
easier to build an automated test we

00:16:55,580 --> 00:17:00,740
with decent code coverage measurement

00:16:57,260 --> 00:17:02,540
etc right so you tell me about the

00:17:00,740 --> 00:17:07,060
California ap why did you guys go the

00:17:02,540 --> 00:17:09,079
API route yes so the CF COI by itself

00:17:07,060 --> 00:17:12,020
would allow us to do Bluegreen

00:17:09,079 --> 00:17:13,760
deployments if you're played around with

00:17:12,020 --> 00:17:15,350
a little bit you can push a new new

00:17:13,760 --> 00:17:16,939
version of the app you can map your

00:17:15,350 --> 00:17:18,709
route to move your routes and add new

00:17:16,939 --> 00:17:20,449
routes and delete routes but that was it

00:17:18,709 --> 00:17:21,800
that would be a munch event manual steps

00:17:20,449 --> 00:17:26,410
right we didn't want to hand this off to

00:17:21,800 --> 00:17:26,410
an operational person question

00:17:33,430 --> 00:17:39,290
so all of our the multi-tenancy is built

00:17:36,920 --> 00:17:42,580
through the stack so when you go to the

00:17:39,290 --> 00:17:42,580
URL the URL

00:17:43,390 --> 00:17:47,110
what is multi-tenancy

00:17:58,540 --> 00:18:04,730
so for us so for us what we mean by

00:18:02,150 --> 00:18:06,620
multi-tenancy is that so we can have

00:18:04,730 --> 00:18:09,530
multiple customers of supplier exchange

00:18:06,620 --> 00:18:11,180
all using the same supplier application

00:18:09,530 --> 00:18:13,010
running in Cloud Foundry and so we

00:18:11,180 --> 00:18:21,470
differentiate the tenant based on the

00:18:13,010 --> 00:18:24,800
URL so if they come in to Acme ok ok

00:18:21,470 --> 00:18:26,450
that's what yeah so so one other thing

00:18:24,800 --> 00:18:28,400
about the IOC around the cloud foundry

00:18:26,450 --> 00:18:30,470
API right so I think what's also

00:18:28,400 --> 00:18:32,930
important to understand is that if you

00:18:30,470 --> 00:18:34,850
if you do Bluegreen upgrades right and

00:18:32,930 --> 00:18:37,130
you push new bits and many upgrade

00:18:34,850 --> 00:18:39,800
scenarios this is like from very simple

00:18:37,130 --> 00:18:42,080
like you push just the abbot's right or

00:18:39,800 --> 00:18:44,900
new versions of the micro services or a

00:18:42,080 --> 00:18:48,290
new database of versions right or etc

00:18:44,900 --> 00:18:50,780
and sometimes they're more or less

00:18:48,290 --> 00:18:53,900
complex upgrade scenarios maybe your

00:18:50,780 --> 00:18:57,320
data model changes right but in a lot of

00:18:53,900 --> 00:18:59,420
cases when you push new logic into your

00:18:57,320 --> 00:19:01,310
production environment you might have to

00:18:59,420 --> 00:19:03,340
tweak some configuration as well right

00:19:01,310 --> 00:19:05,360
so it's not just pushing the bits the

00:19:03,340 --> 00:19:08,200
configuration of the existing customers

00:19:05,360 --> 00:19:10,460
need to change yeah that's what we do so

00:19:08,200 --> 00:19:11,750
yeah so our upgrade tool actually does

00:19:10,460 --> 00:19:12,740
two things when it does the upgrade it

00:19:11,750 --> 00:19:14,630
does the blue/green upgrade of the

00:19:12,740 --> 00:19:16,100
application bits but then it'll also

00:19:14,630 --> 00:19:19,370
talk to the micro services themselves

00:19:16,100 --> 00:19:22,370
and reconfigure them as needed or update

00:19:19,370 --> 00:19:24,080
their data model or let them know about

00:19:22,370 --> 00:19:25,520
you need to now talk to this new service

00:19:24,080 --> 00:19:27,290
so this new service available and new

00:19:25,520 --> 00:19:28,610
versions available it'll update the

00:19:27,290 --> 00:19:33,170
version along the whole stack and I have

00:19:28,610 --> 00:19:34,460
a slide on that later and then this is

00:19:33,170 --> 00:19:36,500
the stuff that we didn't have to build

00:19:34,460 --> 00:19:38,240
right by choosing it Cloud Foundry we

00:19:36,500 --> 00:19:40,280
didn't have to monitor the VMS we didn't

00:19:38,240 --> 00:19:41,210
need a bunch of clustering to make sure

00:19:40,280 --> 00:19:42,470
everything was

00:19:41,210 --> 00:19:44,929
up and running you do a botch deploy

00:19:42,470 --> 00:19:46,700
Bosch makes sure all the bosch beans and

00:19:44,929 --> 00:19:48,500
a Bosch software is running do a Cloud

00:19:46,700 --> 00:19:51,169
Foundry push Cloud Foundry make sure the

00:19:48,500 --> 00:19:53,750
applications are up and running resource

00:19:51,169 --> 00:19:55,429
scaling so we've had it to kind of teach

00:19:53,750 --> 00:19:57,770
our performance team to think a little

00:19:55,429 --> 00:19:58,520
bit different about scaling so before

00:19:57,770 --> 00:20:02,570
they would think

00:19:58,520 --> 00:20:04,580
can I get 20,000 people all to use this

00:20:02,570 --> 00:20:06,380
server that we've just deployed all

00:20:04,580 --> 00:20:08,750
using it at the same time it's like well

00:20:06,380 --> 00:20:10,760
if we're gonna scale out we don't have

00:20:08,750 --> 00:20:13,250
to worry about 20,000 people all using

00:20:10,760 --> 00:20:15,860
the same app at the same time we just

00:20:13,250 --> 00:20:18,679
have to know how many users can use this

00:20:15,860 --> 00:20:20,299
app is that 50 users is it 100 users is

00:20:18,679 --> 00:20:22,130
a thousand users if it's a thousand

00:20:20,299 --> 00:20:23,990
users then we're just going to scale out

00:20:22,130 --> 00:20:25,909
if we need 20,000 we're just going to

00:20:23,990 --> 00:20:27,350
deploy 20 of them and it gets the

00:20:25,909 --> 00:20:30,200
performance team testing in a little bit

00:20:27,350 --> 00:20:32,360
different way high availability is all

00:20:30,200 --> 00:20:34,399
taken care of with Cloud Foundry log

00:20:32,360 --> 00:20:35,960
collection they all log grader will send

00:20:34,399 --> 00:20:38,240
it all the place that we can collect

00:20:35,960 --> 00:20:40,760
them in one place and then health

00:20:38,240 --> 00:20:42,710
metrics we don't have to worry about is

00:20:40,760 --> 00:20:47,510
the system healthy or not Cloud Foundry

00:20:42,710 --> 00:20:50,779
will tell us so we really like Bosh Bosh

00:20:47,510 --> 00:20:53,240
lays out the data center for us a ridge

00:20:50,779 --> 00:20:55,549
we start out without spiff before spiff

00:20:53,240 --> 00:20:57,890
it was a little hard because it was

00:20:55,549 --> 00:20:59,779
communicating modified the manifest in

00:20:57,890 --> 00:21:02,419
this way right it was how do you patch a

00:20:59,779 --> 00:21:05,600
manifest it was a little complicated out

00:21:02,419 --> 00:21:07,370
we run sniff 16 times in our in our

00:21:05,600 --> 00:21:10,659
build system so a team City will run

00:21:07,370 --> 00:21:15,529
this spiff command just every time just

00:21:10,659 --> 00:21:17,690
create a new cloud foundry configuration

00:21:15,529 --> 00:21:19,700
for us and so a developer can actually

00:21:17,690 --> 00:21:20,809
go to the team city build and look and

00:21:19,700 --> 00:21:23,390
see what's in production

00:21:20,809 --> 00:21:26,090
how many das are running how many nodes

00:21:23,390 --> 00:21:27,529
are in Swift what are the settings being

00:21:26,090 --> 00:21:29,029
used in production they can just go look

00:21:27,529 --> 00:21:31,279
at our build and they can see what's

00:21:29,029 --> 00:21:34,580
being deployed in production and this is

00:21:31,279 --> 00:21:36,200
key for us to be able to push things

00:21:34,580 --> 00:21:37,700
throughout the throughout the

00:21:36,200 --> 00:21:40,220
infrastructure right we have CI which is

00:21:37,700 --> 00:21:43,039
one way performance guys maybe on a week

00:21:40,220 --> 00:21:44,450
old Bosch manifest and if they come back

00:21:43,039 --> 00:21:46,760
and say I'm running this version this

00:21:44,450 --> 00:21:48,350
build of the Bosch manifest we can say

00:21:46,760 --> 00:21:50,480
ok we'll go update we think this has

00:21:48,350 --> 00:21:52,870
changed and everyone can stay on the

00:21:50,480 --> 00:21:52,870
same page

00:21:53,590 --> 00:21:57,919
so this is a kind of more detail on how

00:21:56,240 --> 00:21:59,299
we do this upgrade with our upgrade tool

00:21:57,919 --> 00:22:02,600
so you look on the top you have our

00:21:59,299 --> 00:22:04,610
tenants t0 with using application 1 t0

00:22:02,600 --> 00:22:07,460
using application for we may have a

00:22:04,610 --> 00:22:11,570
stack of the the supplier exchange blue

00:22:07,460 --> 00:22:12,950
and in just blue one of our services the

00:22:11,570 --> 00:22:15,289
main service that we use all running in

00:22:12,950 --> 00:22:18,740
Cloud Foundry so we start up the green

00:22:15,289 --> 00:22:20,929
version avenges and then start up a old

00:22:18,740 --> 00:22:22,880
a new blue version of n just so that we

00:22:20,929 --> 00:22:25,010
can move tenants one at a time over to

00:22:22,880 --> 00:22:27,200
the old version the application running

00:22:25,010 --> 00:22:28,370
on the new services Angie's not in micro

00:22:27,200 --> 00:22:33,100
services right and just this is the

00:22:28,370 --> 00:22:36,279
Microsoft yeah it's an in cooking but um

00:22:33,100 --> 00:22:39,320
so we have the old application with

00:22:36,279 --> 00:22:41,120
running on the new services but we've

00:22:39,320 --> 00:22:42,470
only moved over to tenants now we can

00:22:41,120 --> 00:22:43,880
test out those tenants make sure they're

00:22:42,470 --> 00:22:45,980
going to work make sure the new services

00:22:43,880 --> 00:22:48,200
don't fall on fall flat on the ground

00:22:45,980 --> 00:22:50,179
and production load then we can start up

00:22:48,200 --> 00:22:51,710
a new version of the application move

00:22:50,179 --> 00:22:53,480
tenants over to that new application

00:22:51,710 --> 00:22:55,159
running on the new services and

00:22:53,480 --> 00:22:56,990
everybody's happy and we can do all this

00:22:55,159 --> 00:22:59,450
without downtime because our application

00:22:56,990 --> 00:23:01,760
tool or upgrade tool will just do it one

00:22:59,450 --> 00:23:03,200
by one we can list upgrade all the

00:23:01,760 --> 00:23:06,830
tenants we can list which tenants we

00:23:03,200 --> 00:23:09,769
want to upgrade so Mike this is sort of

00:23:06,830 --> 00:23:11,779
the environment during an upgrade of due

00:23:09,769 --> 00:23:13,429
during the move of tenants right so we

00:23:11,779 --> 00:23:15,590
if you want to keep them all in the same

00:23:13,429 --> 00:23:17,990
stack right right but you know you do

00:23:15,590 --> 00:23:20,149
this in a period of time because you

00:23:17,990 --> 00:23:21,919
have thousands of thousands of of

00:23:20,149 --> 00:23:24,169
tenants right here if you move them all

00:23:21,919 --> 00:23:28,669
at once and you've got issues then you

00:23:24,169 --> 00:23:30,169
know and then it hits a fan oh we've set

00:23:28,669 --> 00:23:33,110
up one test tenant so we can move that

00:23:30,169 --> 00:23:34,940
one test ten at first and we can do some

00:23:33,110 --> 00:23:36,409
basic validation a little bit of scale

00:23:34,940 --> 00:23:37,549
testing if we want and if that's all

00:23:36,409 --> 00:23:40,779
good then we can run the command a

00:23:37,549 --> 00:23:43,070
second time and move everybody else over

00:23:40,779 --> 00:23:45,169
so this is a bunch of the gaps that we

00:23:43,070 --> 00:23:47,899
had some of these are because we started

00:23:45,169 --> 00:23:49,549
way back in version 1 before everything

00:23:47,899 --> 00:23:52,610
went to version 2 you know we talked

00:23:49,549 --> 00:23:54,230
about we first wrote our service brokers

00:23:52,610 --> 00:23:57,950
with the service gateways and then we

00:23:54,230 --> 00:24:00,529
made them service brokers the CLI api's

00:23:57,950 --> 00:24:03,080
are changing quite for the sea lice

00:24:00,529 --> 00:24:04,639
changing so even last week on Wednesday

00:24:03,080 --> 00:24:06,830
we found a production issue whether it

00:24:04,639 --> 00:24:08,570
was a customer couldn't do something

00:24:06,830 --> 00:24:11,480
because of a something that had been

00:24:08,570 --> 00:24:13,730
pushed the week before and so on

00:24:11,480 --> 00:24:15,560
Thursday we fixed it and on Friday the

00:24:13,730 --> 00:24:18,230
developers wrote the script of what's

00:24:15,560 --> 00:24:21,830
Cloud Foundry commands to to apply the

00:24:18,230 --> 00:24:23,060
patch because it it was just a one jar

00:24:21,830 --> 00:24:24,650
that had a change so there was some

00:24:23,060 --> 00:24:27,020
manual steps that we want to do just to

00:24:24,650 --> 00:24:28,130
get in as quick as possible but the

00:24:27,020 --> 00:24:29,540
steps that they wrote the Cloud Foundry

00:24:28,130 --> 00:24:32,780
steps that they had written were based

00:24:29,540 --> 00:24:34,310
on the version 5 CLI and so on Saturday

00:24:32,780 --> 00:24:36,500
when we were doing the promotion they

00:24:34,310 --> 00:24:38,090
got to the step well on Friday we found

00:24:36,500 --> 00:24:40,490
it because they were doing the the

00:24:38,090 --> 00:24:42,830
pretest in pre prod we were writing the

00:24:40,490 --> 00:24:45,200
steps in the commands were actually not

00:24:42,830 --> 00:24:47,600
working because in production and in pre

00:24:45,200 --> 00:24:49,550
prod we had the version 6 CLI so we had

00:24:47,600 --> 00:24:51,410
to go rework it we found it before we

00:24:49,550 --> 00:24:55,700
put it into production but see lies

00:24:51,410 --> 00:24:58,550
changing a bunch of other stuffs we're

00:24:55,700 --> 00:25:00,560
running out of time we've contributed a

00:24:58,550 --> 00:25:03,100
bunch of this stuff that isn't core to

00:25:00,560 --> 00:25:04,790
our services so clan Navy which is

00:25:03,100 --> 00:25:06,710
antivirus

00:25:04,790 --> 00:25:08,150
open source admin and a virus thing

00:25:06,710 --> 00:25:10,850
we've have a block deployment for that

00:25:08,150 --> 00:25:13,580
swift and H a proxy in front of Swift a

00:25:10,850 --> 00:25:15,350
deployment VM which is the Bosch CLI and

00:25:13,580 --> 00:25:16,690
the Cloud Foundry CLI so if you want to

00:25:15,350 --> 00:25:21,140
run these in your production environment

00:25:16,690 --> 00:25:23,240
we didn't want a non Bosch VM where the

00:25:21,140 --> 00:25:25,130
ops people logged in and then did stuff

00:25:23,240 --> 00:25:26,330
and it had to be like a Windows VM or

00:25:25,130 --> 00:25:29,450
something like that so we just wanted a

00:25:26,330 --> 00:25:31,010
Bosch deployed VM so we have a the end

00:25:29,450 --> 00:25:33,920
that just has CL eyes and some basic

00:25:31,010 --> 00:25:35,780
user add users add passwords so so I

00:25:33,920 --> 00:25:38,300
think this is sort of the base principle

00:25:35,780 --> 00:25:41,330
right so what as a business we want to

00:25:38,300 --> 00:25:43,790
focus on high level application micro

00:25:41,330 --> 00:25:46,190
services around content and process in

00:25:43,790 --> 00:25:48,260
case and then we're building solutions

00:25:46,190 --> 00:25:50,000
on top of that and our customers build

00:25:48,260 --> 00:25:52,220
solution so we really want to focus on

00:25:50,000 --> 00:25:54,860
building the apps and enabling to build

00:25:52,220 --> 00:25:56,090
these apps but everything underneath it

00:25:54,860 --> 00:25:57,920
first of all you don't want to own it

00:25:56,090 --> 00:26:00,440
right so the fact that we had to do

00:25:57,920 --> 00:26:02,630
something like this that's because it

00:26:00,440 --> 00:26:04,190
was early early days but we really want

00:26:02,630 --> 00:26:07,340
to contribute this to the open source

00:26:04,190 --> 00:26:10,190
community yeah because you know that's

00:26:07,340 --> 00:26:11,420
not our primary focus ideally there

00:26:10,190 --> 00:26:13,010
would just be a catalog of Bosch

00:26:11,420 --> 00:26:15,110
releases and you could go up there and

00:26:13,010 --> 00:26:16,640
say I need this Bosch release for Hadoop

00:26:15,110 --> 00:26:17,960
or I need this Bosch release for Swift

00:26:16,640 --> 00:26:19,940
and I could just pull it down from a

00:26:17,960 --> 00:26:21,980
common spot and it would

00:26:19,940 --> 00:26:23,299
some version or some updates and I could

00:26:21,980 --> 00:26:25,190
just get whatever one and know that it's

00:26:23,299 --> 00:26:27,350
going to work for me and then we also

00:26:25,190 --> 00:26:29,269
did two service brokers Cassandra Dex DB

00:26:27,350 --> 00:26:30,830
we didn't do a swift service broker

00:26:29,269 --> 00:26:32,929
because we just basically have one swift

00:26:30,830 --> 00:26:34,639
user and we didn't need to share the

00:26:32,929 --> 00:26:36,620
Swift cluster but we're looking to the

00:26:34,639 --> 00:26:38,570
point of with adding more micro services

00:26:36,620 --> 00:26:40,100
that we're going to need to share Swift

00:26:38,570 --> 00:26:43,299
a little more so we will probably be

00:26:40,100 --> 00:26:45,740
adding that over the next few months

00:26:43,299 --> 00:26:47,299
right so let me tell you a little bit

00:26:45,740 --> 00:26:49,190
about the environments we have so you

00:26:47,299 --> 00:26:51,399
see the Cloud Foundry environment so

00:26:49,190 --> 00:26:55,490
when we push something in production

00:26:51,399 --> 00:26:58,429
then you know we basically have a full

00:26:55,490 --> 00:27:00,289
CI CD environment and so we at any point

00:26:58,429 --> 00:27:03,679
in time we can take a green bill to use

00:27:00,289 --> 00:27:06,860
the 8:03 example and then we push that

00:27:03,679 --> 00:27:09,350
into a wide variety of environment so

00:27:06,860 --> 00:27:12,649
one for functional testing as well as

00:27:09,350 --> 00:27:13,909
internationalization testing then

00:27:12,649 --> 00:27:15,919
there's a separate class learning

00:27:13,909 --> 00:27:19,759
environment for performance testing and

00:27:15,919 --> 00:27:22,759
one for longevity testing then there's

00:27:19,759 --> 00:27:24,769
an integration test environment because

00:27:22,759 --> 00:27:26,450
our solutions typically are sort of a

00:27:24,769 --> 00:27:27,970
hybrid cloud structure you have a public

00:27:26,450 --> 00:27:31,700
cloud environment that might integrate

00:27:27,970 --> 00:27:33,980
with a private cloud single tenant

00:27:31,700 --> 00:27:36,019
environment then we do upgrade

00:27:33,980 --> 00:27:38,570
validation in a separate environment as

00:27:36,019 --> 00:27:41,210
well and and there is a pre prod

00:27:38,570 --> 00:27:43,820
environment that mimics the environment

00:27:41,210 --> 00:27:46,340
of production before we push it in in

00:27:43,820 --> 00:27:50,210
production and this is run in parallel

00:27:46,340 --> 00:27:52,519
and so you know we actually have the

00:27:50,210 --> 00:27:55,190
performance test all these tests by the

00:27:52,519 --> 00:27:57,470
way from end to end you know you I to

00:27:55,190 --> 00:27:59,389
perform santiago d everything is fully

00:27:57,470 --> 00:28:03,440
automated and the performance test takes

00:27:59,389 --> 00:28:05,330
the longer so about 12 hours and never

00:28:03,440 --> 00:28:07,779
you've got predefined quality exit

00:28:05,330 --> 00:28:10,429
criteria for each of the test results

00:28:07,779 --> 00:28:12,740
and we automatically lock the test

00:28:10,429 --> 00:28:14,889
reports into a portal as well so we have

00:28:12,740 --> 00:28:17,330
a fully stay it meets the exit criteria

00:28:14,889 --> 00:28:20,029
this is the test result from that

00:28:17,330 --> 00:28:24,110
particular push and so it takes about us

00:28:20,029 --> 00:28:27,139
about 12 12 hours from hitting the

00:28:24,110 --> 00:28:30,559
button to getting the bits in production

00:28:27,139 --> 00:28:32,299
live which is an enormous improvement

00:28:30,559 --> 00:28:33,500
and by the way to longevity test T we

00:28:32,299 --> 00:28:36,710
can make that

00:28:33,500 --> 00:28:38,960
we can let it run because in a 12-hour

00:28:36,710 --> 00:28:41,990
window normally for our enterprise

00:28:38,960 --> 00:28:43,940
software we do 7-day longevity testing

00:28:41,990 --> 00:28:44,929
to see whether there are memory leaks so

00:28:43,940 --> 00:28:47,740
it sort of makes sense to have that

00:28:44,929 --> 00:28:52,850
environment just keep it up and running

00:28:47,740 --> 00:28:54,559
for a couple of days as well and yeah

00:28:52,850 --> 00:28:57,289
and so you see the actual deployment

00:28:54,559 --> 00:29:00,440
like I said we went in production da on

00:28:57,289 --> 00:29:03,620
August 1st and so it took a long time to

00:29:00,440 --> 00:29:06,860
build the basis as well as the first

00:29:03,620 --> 00:29:08,840
application and so we did a bunch of

00:29:06,860 --> 00:29:13,820
upgrade this is the actual deployment

00:29:08,840 --> 00:29:16,309
history so you know in the first twenty

00:29:13,820 --> 00:29:22,510
nine in the first thirty seven weeks

00:29:16,309 --> 00:29:25,130
since GA we we did 29 full releases and

00:29:22,510 --> 00:29:27,429
there were a bus upgrades to Cloud

00:29:25,130 --> 00:29:29,929
Foundry upgrades as well

00:29:27,429 --> 00:29:31,580
22 upgrades of the applications for each

00:29:29,929 --> 00:29:35,690
of the three applications so it's at

00:29:31,580 --> 00:29:37,220
least sixty six upgrades and and one

00:29:35,690 --> 00:29:40,429
stem cell upgrade that was the

00:29:37,220 --> 00:29:42,559
shellshock issue and and so there were

00:29:40,429 --> 00:29:45,530
situations like this is 8 9 and 10

00:29:42,559 --> 00:29:48,830
October 3 days 3 promotions in

00:29:45,530 --> 00:29:54,919
production earlier this year we went to

00:29:48,830 --> 00:29:57,440
weekly cycles on Thursday the production

00:29:54,919 --> 00:30:00,140
the dev team the ops team I mean comes

00:29:57,440 --> 00:30:01,490
comes together and say alright does it

00:30:00,140 --> 00:30:04,280
make sense to push something in

00:30:01,490 --> 00:30:06,500
production yes or no and then they hit

00:30:04,280 --> 00:30:09,980
the button and then you know end of the

00:30:06,500 --> 00:30:13,159
day it's done so really going to a

00:30:09,980 --> 00:30:15,440
weekly weekly deployment model and by

00:30:13,159 --> 00:30:18,260
the way in between some early October we

00:30:15,440 --> 00:30:21,500
rewrote the entire application then from

00:30:18,260 --> 00:30:24,740
ECG s to angular and bootstrap in that

00:30:21,500 --> 00:30:27,140
timeframe and push the entire app again

00:30:24,740 --> 00:30:33,890
you know Brill built from scratch in a

00:30:27,140 --> 00:30:36,049
very short period of time right so you

00:30:33,890 --> 00:30:39,530
know the the real disruption here is the

00:30:36,049 --> 00:30:41,570
JLT we're getting right so if you look

00:30:39,530 --> 00:30:43,760
at our enterprise software that is

00:30:41,570 --> 00:30:47,000
deployed on-premise we typically do

00:30:43,760 --> 00:30:48,740
12-month release cycles so every

00:30:47,000 --> 00:30:51,200
of months we do a new release of our

00:30:48,740 --> 00:30:53,600
enterprise software in our public cloud

00:30:51,200 --> 00:30:56,300
environment were at weekly and we Kudo

00:30:53,600 --> 00:30:59,330
do it every day but in principle every

00:30:56,300 --> 00:31:00,830
week we push in production and that sort

00:30:59,330 --> 00:31:02,870
of changed the game right so the model

00:31:00,830 --> 00:31:05,780
is now certainly different the concept

00:31:02,870 --> 00:31:07,280
of a release becomes really weird it

00:31:05,780 --> 00:31:09,440
doesn't make sense anymore to talk about

00:31:07,280 --> 00:31:12,650
releases because you push when you want

00:31:09,440 --> 00:31:15,020
to things like pet releases so a pet

00:31:12,650 --> 00:31:17,720
strain or pets releases are irrelevant

00:31:15,020 --> 00:31:19,910
because if you have an issue you just

00:31:17,720 --> 00:31:22,520
fix it on the latest base you get a new

00:31:19,910 --> 00:31:24,800
green build and you push that green

00:31:22,520 --> 00:31:27,620
build you know with the patches and the

00:31:24,800 --> 00:31:31,460
new functionality that comes in we also

00:31:27,620 --> 00:31:33,620
build in switches so so every customer

00:31:31,460 --> 00:31:35,780
facing functionality that is new it has

00:31:33,620 --> 00:31:38,630
a switch so that we can push in

00:31:35,780 --> 00:31:40,460
production and maybe the marketing team

00:31:38,630 --> 00:31:42,950
isn't ready to announce that right so

00:31:40,460 --> 00:31:44,300
then we turn the switch off but then we

00:31:42,950 --> 00:31:47,600
can turn it on to expose that

00:31:44,300 --> 00:31:49,550
functionality to our customers and so

00:31:47,600 --> 00:31:51,590
the you know the whole concept of a

00:31:49,550 --> 00:31:53,900
release and release management is sort

00:31:51,590 --> 00:31:57,380
of weird or concept of road map is weird

00:31:53,900 --> 00:32:02,240
you're sort of talking about more like

00:31:57,380 --> 00:32:05,200
investment themes and and priorities in

00:32:02,240 --> 00:32:07,430
your backlog rather than then releases

00:32:05,200 --> 00:32:09,590
all right so Romasanta time any

00:32:07,430 --> 00:32:17,210
questions in the audience

00:32:09,590 --> 00:32:19,670
go ahead so good so the stem cell is

00:32:17,210 --> 00:32:23,060
basically OS packaged up with a Bosch

00:32:19,670 --> 00:32:25,670
agent so that you can run Bosch deploy

00:32:23,060 --> 00:32:27,770
and it'll create the VM with that OS

00:32:25,670 --> 00:32:29,150
base image and then the Bosch can talk

00:32:27,770 --> 00:32:31,340
to the Bosch agent and deploy your

00:32:29,150 --> 00:32:33,260
software for you so it's basically stem

00:32:31,340 --> 00:32:35,960
cell is the OS image that you want to

00:32:33,260 --> 00:32:37,280
run and we're running whoo boon to the

00:32:35,960 --> 00:32:40,570
right we're running Ubuntu we just

00:32:37,280 --> 00:32:40,570
updated a 1404

00:32:45,090 --> 00:32:52,450
right yeah warded to docker so they're

00:32:50,560 --> 00:32:54,280
both containers right so we don't care

00:32:52,450 --> 00:32:55,570
necessarily about the container we just

00:32:54,280 --> 00:32:59,350
care that our application is up and

00:32:55,570 --> 00:33:01,720
running and we don't have developers

00:32:59,350 --> 00:33:03,040
running Cloud Foundry on their desktops

00:33:01,720 --> 00:33:04,780
when they're doing development so when

00:33:03,040 --> 00:33:10,330
they do development they're still doing

00:33:04,780 --> 00:33:13,030
a Java Gradle build maven run kind of

00:33:10,330 --> 00:33:14,770
stuff on their laptops and then they

00:33:13,030 --> 00:33:16,450
they push the Cloud Foundry for the CI

00:33:14,770 --> 00:33:20,050
part of it once we once the code is

00:33:16,450 --> 00:33:21,670
committed and ready to run so we haven't

00:33:20,050 --> 00:33:23,140
gotten to the problem of developers

00:33:21,670 --> 00:33:25,840
wanting to build docker containers and

00:33:23,140 --> 00:33:27,970
run them yet although we might get there

00:33:25,840 --> 00:33:29,650
over the next year yeah there's a sort

00:33:27,970 --> 00:33:32,470
of fundamental difference right I mean

00:33:29,650 --> 00:33:36,100
yes my family can handle wardens as well

00:33:32,470 --> 00:33:37,810
as Cloud Foundry docker containers but

00:33:36,100 --> 00:33:40,240
there's a bit of a principle difference

00:33:37,810 --> 00:33:42,040
right so if you take your dev

00:33:40,240 --> 00:33:43,600
environment your docker container and

00:33:42,040 --> 00:33:45,850
you push that actually in production the

00:33:43,600 --> 00:33:48,220
same environment versus Cloud Foundry

00:33:45,850 --> 00:33:50,140
smaller where you develop your app and

00:33:48,220 --> 00:33:52,900
you push your application in production

00:33:50,140 --> 00:33:55,330
and some I would be a little bit nervous

00:33:52,900 --> 00:33:56,980
pushing your dev environment in a

00:33:55,330 --> 00:33:59,770
production environment right so you have

00:33:56,980 --> 00:34:01,750
to do a lot of controls around that the

00:33:59,770 --> 00:34:03,370
moment that you deploy your app you have

00:34:01,750 --> 00:34:05,800
a ton of controls we have a ton of

00:34:03,370 --> 00:34:08,700
controls in our test suite in the way we

00:34:05,800 --> 00:34:08,700
deploy etc

00:34:29,770 --> 00:34:35,480
more more than an hour but so we do we

00:34:33,169 --> 00:34:37,550
we do Multi multi Tennessee at various

00:34:35,480 --> 00:34:40,099
layers right so every process in the

00:34:37,550 --> 00:34:42,829
engine where it's micro service or an

00:34:40,099 --> 00:34:45,919
app server they can handle logs from any

00:34:42,829 --> 00:34:47,659
tenant so and that gives you really nice

00:34:45,919 --> 00:34:50,329
horizontal scaling you just spin up more

00:34:47,659 --> 00:34:52,520
of those instances the router you know

00:34:50,329 --> 00:34:55,460
every every request is in the context of

00:34:52,520 --> 00:34:57,980
the tenant so we identified it in the

00:34:55,460 --> 00:34:59,300
key when we authenticate so we know for

00:34:57,980 --> 00:35:00,890
every request doesn't matter where it's

00:34:59,300 --> 00:35:03,109
being processed in which particular node

00:35:00,890 --> 00:35:06,319
we know from which tenant that is at the

00:35:03,109 --> 00:35:08,329
database level we do we partition we

00:35:06,319 --> 00:35:10,609
have a single database instance which

00:35:08,329 --> 00:35:13,309
skills are horizontally as well and we

00:35:10,609 --> 00:35:16,490
we partition the data the metadata in a

00:35:13,309 --> 00:35:20,059
separate separate database instances

00:35:16,490 --> 00:35:21,559
within the same database engine and we

00:35:20,059 --> 00:35:24,170
don't do that for maybe you can explain

00:35:21,559 --> 00:35:25,940
so we made a decision early on that we

00:35:24,170 --> 00:35:27,589
didn't want to spend up services or

00:35:25,940 --> 00:35:29,180
applications for every tenant right

00:35:27,589 --> 00:35:31,369
because we want tenants to be self sign

00:35:29,180 --> 00:35:32,270
up they can sign up for our service and

00:35:31,369 --> 00:35:35,329
they can start using the application

00:35:32,270 --> 00:35:37,730
right away so we don't run Bosch

00:35:35,329 --> 00:35:39,410
commands or CF commands to provision new

00:35:37,730 --> 00:35:41,839
databases or new tenants or create more

00:35:39,410 --> 00:35:43,609
spaces but we do have multi-tenancy

00:35:41,839 --> 00:35:45,500
built in throughout so the application

00:35:43,609 --> 00:35:47,900
knows that urls come from a specific

00:35:45,500 --> 00:35:49,970
tenant it makes request in the context

00:35:47,900 --> 00:35:52,010
of that tenant the security at the

00:35:49,970 --> 00:35:53,299
services layer is based on the tenant so

00:35:52,010 --> 00:35:54,619
there's a ticket associated with that

00:35:53,299 --> 00:35:57,049
tenant so you can only read data from

00:35:54,619 --> 00:35:58,640
that tenant we make us secure checks at

00:35:57,049 --> 00:36:00,440
the security hour and then in the

00:35:58,640 --> 00:36:04,490
database we also partition things so we

00:36:00,440 --> 00:36:06,980
we create a database within X DB for

00:36:04,490 --> 00:36:08,240
different tenants in Cassandra it's all

00:36:06,980 --> 00:36:09,770
mixed together but there are certain

00:36:08,240 --> 00:36:14,240
tables for certain tenants and then in

00:36:09,770 --> 00:36:15,980
Swift the data is partitioned into

00:36:14,240 --> 00:36:18,829
spaces for that tenant so it's easier to

00:36:15,980 --> 00:36:21,440
manage but it's a logical partitioning

00:36:18,829 --> 00:36:23,569
not a physical partition so because none

00:36:21,440 --> 00:36:25,609
of the processes are a database or any

00:36:23,569 --> 00:36:27,880
of the tiers right it's dedicated to one

00:36:25,609 --> 00:36:32,430
tenant now you got full

00:36:27,880 --> 00:36:32,430
zatia of your resources that makes it

00:36:46,610 --> 00:36:54,090
so they're so so you're saying you know

00:36:52,530 --> 00:36:55,920
what about the privacy or compliance

00:36:54,090 --> 00:36:57,630
issue and we're in a highly regulated

00:36:55,920 --> 00:37:00,600
industry that's actually you know

00:36:57,630 --> 00:37:03,050
majority of our market so we do see

00:37:00,600 --> 00:37:05,790
issues with for instance the Patriot Act

00:37:03,050 --> 00:37:08,340
where European customers don't want to

00:37:05,790 --> 00:37:09,480
run in a u.s. datacenter and the other

00:37:08,340 --> 00:37:11,070
way around you as customers don't want

00:37:09,480 --> 00:37:14,760
to run on a European datacenter by the

00:37:11,070 --> 00:37:16,260
way and so our environment we actually

00:37:14,760 --> 00:37:17,880
have multiple data centers around the

00:37:16,260 --> 00:37:20,490
world and so we replicate the entire

00:37:17,880 --> 00:37:22,650
environment in a local data center you

00:37:20,490 --> 00:37:25,410
know to adhere to the geo boundaries and

00:37:22,650 --> 00:37:28,800
compliance issues we don't see a real

00:37:25,410 --> 00:37:31,260
issue with you know having shared

00:37:28,800 --> 00:37:33,150
environment so the world is rapidly

00:37:31,260 --> 00:37:35,820
changing towards shared infrastructure

00:37:33,150 --> 00:37:38,760
as long as you can prove that there's

00:37:35,820 --> 00:37:41,460
real decent security boundaries around

00:37:38,760 --> 00:37:44,040
the processing of one tenant as well as

00:37:41,460 --> 00:37:46,790
the persistence and we can really

00:37:44,040 --> 00:37:48,750
partition the data per tenant then

00:37:46,790 --> 00:37:51,020
regulated industries are actually fine

00:37:48,750 --> 00:37:51,020
with that

00:38:01,070 --> 00:38:05,610
we don't do anything like that today

00:38:03,270 --> 00:38:07,680
we've thought that that was a good idea

00:38:05,610 --> 00:38:09,390
but our bandwidth hasn't been in place

00:38:07,680 --> 00:38:11,670
to add that into the environment right

00:38:09,390 --> 00:38:13,860
but we have our pre fraud environment

00:38:11,670 --> 00:38:15,530
set up that submit mix of production our

00:38:13,860 --> 00:38:18,060
thought would be that that is a

00:38:15,530 --> 00:38:19,410
playground where we could run stuff like

00:38:18,060 --> 00:38:21,030
that so it wouldn't necessarily affect

00:38:19,410 --> 00:38:22,440
production but we can monitor it just

00:38:21,030 --> 00:38:23,970
like we would monitor production and

00:38:22,440 --> 00:38:25,230
then it's another Cloud Foundry

00:38:23,970 --> 00:38:27,450
environment with all the firewall rules

00:38:25,230 --> 00:38:29,190
and security and stuff built in and so

00:38:27,450 --> 00:38:31,230
we could do whatever we want developers

00:38:29,190 --> 00:38:32,760
could actually log in there and fix

00:38:31,230 --> 00:38:35,370
problems or look at problems in real

00:38:32,760 --> 00:38:37,200
time whereas the production environment

00:38:35,370 --> 00:38:39,510
because of the EMC rules and a couple

00:38:37,200 --> 00:38:41,930
other regulated stuff developers can't

00:38:39,510 --> 00:38:44,550
log into the production environment so

00:38:41,930 --> 00:38:45,340
all right we're out of time right thanks

00:38:44,550 --> 00:38:48,689
a lot

00:38:45,340 --> 00:38:48,689

YouTube URL: https://www.youtube.com/watch?v=4PZwBBr1ZU0


