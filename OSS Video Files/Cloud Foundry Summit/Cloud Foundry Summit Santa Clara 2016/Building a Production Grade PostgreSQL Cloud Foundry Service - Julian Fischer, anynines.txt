Title: Building a Production Grade PostgreSQL Cloud Foundry Service - Julian Fischer, anynines
Publication date: 2016-05-29
Playlist: Cloud Foundry Summit Santa Clara 2016
Description: 
	Julian Fischer, CEO of anynines guides through the journey of building a highly available, self-provisioning PostgreSQL Cloud Foundry service. The talk is meant to share experience collected in the anynines team from years of building production grade backing services. PostgreSQL in particular is a good example as it has never been built to be fully automated. Challenges such as setting up a proper replication, detect failure scenarios and perform automatic failovers are interesting challenges with many pitfalls for beginners. Learn about production requirements, technical challenges and have a look at an exemplary architecture to learn about building failure resilient Cloud Foundry data services. 

Julian Fischer
anynines
CEO
Julian Fischer is CEO of anynines. As an expert for digital transformation he has been attending hundreds of IT conferences and speaking at dozens including several Cloud Foundry events. His passion for Cloud Foundry was the motivation to found anynines, a startup entirely focused on cloud-native software development and operations. anynines has been the first Cloud Foundry based public PaaS and is entirely operated under German jurisdiction. With more than three years of Cloud Foundry operation experience anynines has established in CF related enterprise operations and consulting for central europe. More than that anynines does significant groundwork in the field of highly available, self-provisioning, infrastructure agnostic Cloud Foundry services.
Captions: 
	00:00:00,060 --> 00:00:06,210
welcome everybody to this talk my name

00:00:03,240 --> 00:00:09,050
is Julian Fisher I'm CEO of any nines we

00:00:06,210 --> 00:00:12,540
are a cloud cloud foundry consultancy

00:00:09,050 --> 00:00:17,789
currently focused on class family data

00:00:12,540 --> 00:00:19,439
services located in Germany so the talk

00:00:17,789 --> 00:00:21,210
is going to be about building a

00:00:19,439 --> 00:00:25,859
production trade Postgres cloud foundry

00:00:21,210 --> 00:00:29,910
service which leads us to the question

00:00:25,859 --> 00:00:31,650
what does production grade mean so with

00:00:29,910 --> 00:00:34,170
a variety of customers we've been

00:00:31,650 --> 00:00:37,770
working in the past we've seen various

00:00:34,170 --> 00:00:41,489
definitions of of production grade so

00:00:37,770 --> 00:00:43,980
for this talk we'll actually go with the

00:00:41,489 --> 00:00:47,190
definition we at any nines have been

00:00:43,980 --> 00:00:50,820
using in the past because for historical

00:00:47,190 --> 00:00:53,219
reasons we run a public any nines but a

00:00:50,820 --> 00:00:53,699
public cloud foundry platform called any

00:00:53,219 --> 00:00:57,090
nines

00:00:53,699 --> 00:01:01,649
so back in 2013 we started running cloud

00:00:57,090 --> 00:01:06,090
foundry as a as a public offering

00:01:01,649 --> 00:01:09,270
so when using when offering cloud

00:01:06,090 --> 00:01:11,490
foundry to a public audience you

00:01:09,270 --> 00:01:14,640
actually onboard various kinds of users

00:01:11,490 --> 00:01:18,290
so people you don't know people with you

00:01:14,640 --> 00:01:21,240
know poorly designed apps people with

00:01:18,290 --> 00:01:24,049
with vicious apps people bad people

00:01:21,240 --> 00:01:26,640
trying to break your platform so

00:01:24,049 --> 00:01:30,420
operating a public pass actually turned

00:01:26,640 --> 00:01:33,509
into a learning environment so the

00:01:30,420 --> 00:01:35,670
production readiness litmus test you can

00:01:33,509 --> 00:01:38,130
have is offering a public platform

00:01:35,670 --> 00:01:41,070
basically so I think it can't get any

00:01:38,130 --> 00:01:43,350
worse no in case yeah you will spend the

00:01:41,070 --> 00:01:47,909
public offering you will also be fine in

00:01:43,350 --> 00:01:50,970
a more private environment so actually

00:01:47,909 --> 00:01:53,729
with these experience we've made we

00:01:50,970 --> 00:01:56,490
started to investigate how to create

00:01:53,729 --> 00:01:58,770
data services because they were not you

00:01:56,490 --> 00:02:01,259
know back in 2013 nobody had service

00:01:58,770 --> 00:02:03,750
brokers there were some you know

00:02:01,259 --> 00:02:06,810
shipping with cloud foundry but they've

00:02:03,750 --> 00:02:10,080
been deprecated them so we used them and

00:02:06,810 --> 00:02:12,030
they were well it's say not really

00:02:10,080 --> 00:02:12,930
production ready so we actually started

00:02:12,030 --> 00:02:15,299
investigating what

00:02:12,930 --> 00:02:19,019
doesn't actually mean to have you know a

00:02:15,299 --> 00:02:22,170
database being deployed on a public on a

00:02:19,019 --> 00:02:25,140
cloud foundry so we've been through a

00:02:22,170 --> 00:02:28,079
journey that took us like two years and

00:02:25,140 --> 00:02:32,760
six people working on naanum on a

00:02:28,079 --> 00:02:35,280
service framework to doing taking care

00:02:32,760 --> 00:02:40,409
of data services so this talk will focus

00:02:35,280 --> 00:02:41,760
on Postgres but basically most of of the

00:02:40,409 --> 00:02:44,939
things that can be transferred to other

00:02:41,760 --> 00:02:49,230
data services as well so and as the rest

00:02:44,939 --> 00:02:51,299
of of the conversation we have will be

00:02:49,230 --> 00:02:53,040
around leading through the design

00:02:51,299 --> 00:02:55,500
decisions that are necessary when

00:02:53,040 --> 00:02:59,099
building a data service

00:02:55,500 --> 00:03:01,109
sadly 25 minutes or 30 minutes are not

00:02:59,099 --> 00:03:03,389
enough to cover this this topic I can

00:03:01,109 --> 00:03:05,069
you can you know come to our booth and

00:03:03,389 --> 00:03:07,409
push play I will talk like two days

00:03:05,069 --> 00:03:09,510
straight about this topic so feel free

00:03:07,409 --> 00:03:11,790
to do so

00:03:09,510 --> 00:03:14,250
so one of the most important things when

00:03:11,790 --> 00:03:16,109
building a cloud foundry service is how

00:03:14,250 --> 00:03:18,229
do you actually implement the service

00:03:16,109 --> 00:03:22,620
broker API so there are only a few

00:03:18,229 --> 00:03:25,440
methods to be implemented looks simple

00:03:22,620 --> 00:03:27,569
but isn't so one of the most important

00:03:25,440 --> 00:03:33,359
decisions is actually what is your

00:03:27,569 --> 00:03:36,269
service instance going to be and for

00:03:33,359 --> 00:03:39,750
Postgres there are a number of decisions

00:03:36,269 --> 00:03:42,150
you actually have to make and one of

00:03:39,750 --> 00:03:46,470
which is what you actually want to offer

00:03:42,150 --> 00:03:49,609
as a service instance so we actually

00:03:46,470 --> 00:03:51,989
come back to this question as it will

00:03:49,609 --> 00:03:54,569
require going through some other

00:03:51,989 --> 00:03:58,650
Postgres decisions one of which is

00:03:54,569 --> 00:04:05,659
whether using a Postgres server or a

00:03:58,650 --> 00:04:08,729
single server or a cluster so well

00:04:05,659 --> 00:04:11,099
offering a platform you will have users

00:04:08,729 --> 00:04:13,739
you know playing around with a small toy

00:04:11,099 --> 00:04:16,789
app they won't have a Postgres server

00:04:13,739 --> 00:04:20,820
that's cheap like five years a month and

00:04:16,789 --> 00:04:23,360
some some customers with a production

00:04:20,820 --> 00:04:25,630
create app maybe they move away from

00:04:23,360 --> 00:04:27,430
physical servers they

00:04:25,630 --> 00:04:29,020
to go to your platform they want to have

00:04:27,430 --> 00:04:31,780
a production cray database that's being

00:04:29,020 --> 00:04:34,570
clustered in order to be stable even

00:04:31,780 --> 00:04:38,170
when infrastructure incidents happen so

00:04:34,570 --> 00:04:40,660
the question on whether to deploy a

00:04:38,170 --> 00:04:43,810
server a cluster cannot be answered as a

00:04:40,660 --> 00:04:46,960
as a general this is how to do it right

00:04:43,810 --> 00:04:51,820
answer but it heavily depends on the the

00:04:46,960 --> 00:04:54,880
context of the customer so let's go into

00:04:51,820 --> 00:04:57,190
the cluster topic just for a few minutes

00:04:54,880 --> 00:04:59,290
because once you got the ability to

00:04:57,190 --> 00:05:03,040
deploy a cluster you can deploy a single

00:04:59,290 --> 00:05:05,740
instance anyway so the clustering

00:05:03,040 --> 00:05:08,350
Postgres and making Postgres highly

00:05:05,740 --> 00:05:11,260
available is a little cumbersome because

00:05:08,350 --> 00:05:13,900
it inherently has never been designed to

00:05:11,260 --> 00:05:16,330
do that and with the transaction based

00:05:13,900 --> 00:05:18,400
sequel database you know reminding of

00:05:16,330 --> 00:05:21,070
the cap theorem it's not it's not a

00:05:18,400 --> 00:05:22,870
simple task to do so one of the

00:05:21,070 --> 00:05:28,120
possibilities to actually make a

00:05:22,870 --> 00:05:31,300
database to make a database highly

00:05:28,120 --> 00:05:34,030
available is to introduce replication so

00:05:31,300 --> 00:05:36,790
when thinking of replication you that

00:05:34,030 --> 00:05:40,770
you make think of whether using

00:05:36,790 --> 00:05:43,600
synchronous or asynchronous replication

00:05:40,770 --> 00:05:45,730
where synchronous replication means that

00:05:43,600 --> 00:05:48,190
transaction has to be confirmed by a

00:05:45,730 --> 00:05:49,540
majority of the cluster nodes where a

00:05:48,190 --> 00:05:51,010
synchronous replication you're right

00:05:49,540 --> 00:05:55,690
against the master and those changes

00:05:51,010 --> 00:05:57,580
will be replicated to a slave accepting

00:05:55,690 --> 00:05:59,140
that there's a certain difference

00:05:57,580 --> 00:06:00,940
between the master and the slave called

00:05:59,140 --> 00:06:03,460
the so called replication like so the

00:06:00,940 --> 00:06:05,830
far away your your slaves are the

00:06:03,460 --> 00:06:08,440
greater the replication like and in case

00:06:05,830 --> 00:06:11,140
something goes wrong the greater your

00:06:08,440 --> 00:06:14,170
data loss is because there is a data

00:06:11,140 --> 00:06:17,410
loss with the size of the replication

00:06:14,170 --> 00:06:19,630
lag so in our case we've been looking at

00:06:17,410 --> 00:06:22,150
my sequel and Postgres we decided to go

00:06:19,630 --> 00:06:24,520
with Postgres as the first relational

00:06:22,150 --> 00:06:27,730
database to implement using the service

00:06:24,520 --> 00:06:31,000
framework so we had in mind that there's

00:06:27,730 --> 00:06:34,840
my sequel with Galera very nice

00:06:31,000 --> 00:06:38,290
synchronous database management cluster

00:06:34,840 --> 00:06:41,670
based on synchronous as replication

00:06:38,290 --> 00:06:44,980
so we thought looking at Postgres

00:06:41,670 --> 00:06:47,590
bosphorus has built-in replication since

00:06:44,980 --> 00:06:50,410
Postgres 9 and it's a synchronous

00:06:47,590 --> 00:06:53,290
replication we thought wine fair enough

00:06:50,410 --> 00:06:55,030
having one already BMS with synchronous

00:06:53,290 --> 00:06:56,860
replication and one with a synchronous

00:06:55,030 --> 00:07:00,810
replication we stick with s increments

00:06:56,860 --> 00:07:04,750
for Postgres so that decision been done

00:07:00,810 --> 00:07:06,730
we figured out that not too hard that

00:07:04,750 --> 00:07:08,770
the replication facilities within

00:07:06,730 --> 00:07:11,620
posters are fairly limited and there is

00:07:08,770 --> 00:07:13,720
a variety of tools you can use and to to

00:07:11,620 --> 00:07:15,760
actually overcome that problem and it's

00:07:13,720 --> 00:07:18,640
not really easy to find out which one to

00:07:15,760 --> 00:07:20,590
use but we actually try to build a

00:07:18,640 --> 00:07:24,370
Postgres and make this available to our

00:07:20,590 --> 00:07:27,700
customers which is you know you know

00:07:24,370 --> 00:07:31,480
easy to automate because we we actually

00:07:27,700 --> 00:07:34,480
have to take care of the cluster

00:07:31,480 --> 00:07:35,950
management so we we just looked at the

00:07:34,480 --> 00:07:37,810
replication built into Postgres i'm

00:07:35,950 --> 00:07:39,250
seeing that there is actually no cluster

00:07:37,810 --> 00:07:42,280
management there's replication but don't

00:07:39,250 --> 00:07:45,490
cluster management so what is last time

00:07:42,280 --> 00:07:48,730
management mean is you have let's say

00:07:45,490 --> 00:07:51,700
three databases a master and two slaves

00:07:48,730 --> 00:07:54,520
what happens if your master database

00:07:51,700 --> 00:07:57,400
server goes away so you need something

00:07:54,520 --> 00:08:01,660
that actually recognizes that yes your

00:07:57,400 --> 00:08:05,320
master is gone that seems easy but it is

00:08:01,660 --> 00:08:08,140
not because sometimes your network might

00:08:05,320 --> 00:08:10,180
have trouble so in case of a network

00:08:08,140 --> 00:08:11,920
partitioning you it could be the case

00:08:10,180 --> 00:08:14,020
that the communication between the

00:08:11,920 --> 00:08:16,570
service are disturbed but the masters

00:08:14,020 --> 00:08:19,420
actually there so you have to find a way

00:08:16,570 --> 00:08:20,830
to ensure that you're not performing a

00:08:19,420 --> 00:08:23,530
failover although still you have an

00:08:20,830 --> 00:08:25,840
 so those are the tasks usually

00:08:23,530 --> 00:08:27,370
solved by cluster managers not worth

00:08:25,840 --> 00:08:30,040
talking about that too much here so

00:08:27,370 --> 00:08:32,880
there are solutions to that but we need

00:08:30,040 --> 00:08:36,250
a component taking care of this so a

00:08:32,880 --> 00:08:38,560
small summary of the cluster thing is

00:08:36,250 --> 00:08:40,690
that you want to have three nodes

00:08:38,560 --> 00:08:43,150
instead of two so that you have always a

00:08:40,690 --> 00:08:47,050
majority of service so you can clearly

00:08:43,150 --> 00:08:49,270
judge whether there's well that a

00:08:47,050 --> 00:08:50,100
majority can decide upon whether there's

00:08:49,270 --> 00:08:54,509
you

00:08:50,100 --> 00:08:57,449
and who's it going to be so after

00:08:54,509 --> 00:09:00,029
looking at several solutions his from

00:08:57,449 --> 00:09:03,449
historical reasons we've been operating

00:09:00,029 --> 00:09:05,670
Cloud Foundry and sorry of databases

00:09:03,449 --> 00:09:09,480
post trust databases for seven years now

00:09:05,670 --> 00:09:12,720
we've been starting with physical data

00:09:09,480 --> 00:09:15,360
bases being clustered in such a cluster

00:09:12,720 --> 00:09:18,089
with Massa and slave we've been using

00:09:15,360 --> 00:09:21,959
pacemaker to to do the cluster

00:09:18,089 --> 00:09:23,850
management so the first approach was can

00:09:21,959 --> 00:09:25,649
we do the automation can we take the

00:09:23,850 --> 00:09:28,050
automation we have around pacemaker and

00:09:25,649 --> 00:09:31,440
put it into a Cloud Foundry environment

00:09:28,050 --> 00:09:34,529
and clearly the answer to this is no you

00:09:31,440 --> 00:09:36,480
can't well you can but it's not really

00:09:34,529 --> 00:09:40,050
meaningful because pacemaker is a beast

00:09:36,480 --> 00:09:42,690
it depends on every single Linux library

00:09:40,050 --> 00:09:44,190
ever written so in order to pacify that

00:09:42,690 --> 00:09:46,639
you will actually have to Pasha Phi the

00:09:44,190 --> 00:09:50,490
universe so that's a bad idea

00:09:46,639 --> 00:09:52,920
also it's it's not really nice to

00:09:50,490 --> 00:09:55,290
automate it it's the way pacemaker has

00:09:52,920 --> 00:09:58,680
been done is it's not really something

00:09:55,290 --> 00:10:00,779
you want to put into a Bosch release so

00:09:58,680 --> 00:10:03,360
what we found is that rep manager is a

00:10:00,779 --> 00:10:06,870
good resolution so it's simple it does

00:10:03,360 --> 00:10:08,880
the job and it you know it actually

00:10:06,870 --> 00:10:13,589
based it basically does the job fairly

00:10:08,880 --> 00:10:15,240
very good so it does also monitor your

00:10:13,589 --> 00:10:17,209
application performance but most

00:10:15,240 --> 00:10:20,240
importantly does failure detection and

00:10:17,209 --> 00:10:22,790
helps performing automated failures

00:10:20,240 --> 00:10:26,310
there's a little research to that and

00:10:22,790 --> 00:10:28,439
there are a lot of let's say edge cases

00:10:26,310 --> 00:10:30,779
and when it comes to plow foundry

00:10:28,439 --> 00:10:33,750
because you can't just you know take

00:10:30,779 --> 00:10:36,779
away one server and promote another

00:10:33,750 --> 00:10:39,029
server in in a cloud foundry environment

00:10:36,779 --> 00:10:41,490
because there might be IP address

00:10:39,029 --> 00:10:43,380
changes so how do you actually tell your

00:10:41,490 --> 00:10:45,899
application that the application should

00:10:43,380 --> 00:10:47,430
now write to a different database server

00:10:45,899 --> 00:10:52,290
so that's one of the problems to be

00:10:47,430 --> 00:10:56,639
solved in our case we added a console

00:10:52,290 --> 00:10:58,980
cluster to our service framework so to

00:10:56,639 --> 00:11:02,130
be a little ahead of the talk we we use

00:10:58,980 --> 00:11:03,570
Bosh underneath to deploy database

00:11:02,130 --> 00:11:05,250
clusters so we

00:11:03,570 --> 00:11:09,420
ever there's a change in the cluster

00:11:05,250 --> 00:11:14,340
we'll tell our consul and we use a DNS

00:11:09,420 --> 00:11:16,920
alias to in the credentials so that your

00:11:14,340 --> 00:11:20,100
application will always have a DNS entry

00:11:16,920 --> 00:11:22,110
resolving to the right master and the

00:11:20,100 --> 00:11:24,900
cluster manager the rep manager in this

00:11:22,110 --> 00:11:27,900
case has one of the one of the purposes

00:11:24,900 --> 00:11:30,420
is that when it promotes a new master we

00:11:27,900 --> 00:11:34,140
will talk to the console and update the

00:11:30,420 --> 00:11:36,030
alias pointing to the master so the

00:11:34,140 --> 00:11:38,370
trigger actually comes from the rep

00:11:36,030 --> 00:11:42,360
manager but the execution of the actual

00:11:38,370 --> 00:11:45,930
failover stun using console alright once

00:11:42,360 --> 00:11:49,680
we actually made a decision that we want

00:11:45,930 --> 00:11:52,770
to support a clustered Postgres we also

00:11:49,680 --> 00:11:54,960
have to make the decision what the

00:11:52,770 --> 00:11:58,500
service instance is going to be is it

00:11:54,960 --> 00:12:00,510
going to be a single cluster that will

00:11:58,500 --> 00:12:03,810
be sliced up or is it going to be a

00:12:00,510 --> 00:12:05,850
cluster per service instance so actually

00:12:03,810 --> 00:12:09,630
two different strategies come to mind a

00:12:05,850 --> 00:12:12,030
shared or a dedicated approach with a

00:12:09,630 --> 00:12:14,400
shared approach what you do is basically

00:12:12,030 --> 00:12:17,310
you create a single Postgres server or

00:12:14,400 --> 00:12:19,380
single Postgres cluster and you slice it

00:12:17,310 --> 00:12:22,800
up into different databases and each

00:12:19,380 --> 00:12:27,030
database is going to represent a service

00:12:22,800 --> 00:12:29,340
instance so this is very easy because

00:12:27,030 --> 00:12:32,270
you need to do you need to create a

00:12:29,340 --> 00:12:35,520
bas-reliefs you need to deploy one bosh

00:12:32,270 --> 00:12:37,650
deployment creating your Postgres

00:12:35,520 --> 00:12:40,320
cluster and your service broker will

00:12:37,650 --> 00:12:43,290
then access this cluster and return

00:12:40,320 --> 00:12:45,870
appropriate credentials the drawback of

00:12:43,290 --> 00:12:48,120
this solution however is that the

00:12:45,870 --> 00:12:51,650
isolation between the service instances

00:12:48,120 --> 00:12:53,990
are pretty weak because Postgres has

00:12:51,650 --> 00:12:56,670
isolation built in you know

00:12:53,990 --> 00:12:59,520
multi-tenancy capabilities but they are

00:12:56,670 --> 00:13:02,370
fairly restricted so when exiting a

00:12:59,520 --> 00:13:04,110
database server one app can drag down

00:13:02,370 --> 00:13:06,900
the performance of the entire cluster

00:13:04,110 --> 00:13:11,520
hitting the cache or you know creating

00:13:06,900 --> 00:13:14,130
disk utilization and CPU utilization so

00:13:11,520 --> 00:13:16,320
the contract towards the customer will

00:13:14,130 --> 00:13:17,320
be fairly fuzzy because you never know

00:13:16,320 --> 00:13:20,980
what you

00:13:17,320 --> 00:13:23,019
of a database you actually get another

00:13:20,980 --> 00:13:25,180
major problem with that approach that

00:13:23,019 --> 00:13:26,889
even with a cluster your cluster

00:13:25,180 --> 00:13:30,670
represents a single point of failure

00:13:26,889 --> 00:13:33,610
within your entire architecture so a

00:13:30,670 --> 00:13:35,170
production Cloud Foundry system you have

00:13:33,610 --> 00:13:36,970
a runtime and the runtime is you know

00:13:35,170 --> 00:13:39,550
it's just an awesome piece of technology

00:13:36,970 --> 00:13:41,500
cloud family is awesome right so you'll

00:13:39,550 --> 00:13:44,920
deploy a tremendous amount of apps

00:13:41,500 --> 00:13:47,860
against against the runtime and now you

00:13:44,920 --> 00:13:51,160
have a load of apps and one database

00:13:47,860 --> 00:13:54,190
cluster I mean you know that doesn't

00:13:51,160 --> 00:13:55,990
sound right doesn't appeal right for the

00:13:54,190 --> 00:14:00,370
obvious reason that whenever your

00:13:55,990 --> 00:14:03,040
database cluster goes down and I can

00:14:00,370 --> 00:14:06,009
tell you you have a component in your

00:14:03,040 --> 00:14:08,380
system it's going to fail whatever it is

00:14:06,009 --> 00:14:11,500
it's going to break right so we recently

00:14:08,380 --> 00:14:14,949
melt it down our OpenStack because of a

00:14:11,500 --> 00:14:16,540
kernel driver issue and because all the

00:14:14,949 --> 00:14:19,149
hosts we are the same we have the same

00:14:16,540 --> 00:14:22,300
problem all the hosts so 20 out of 24

00:14:19,149 --> 00:14:24,779
holds died within one hour right so

00:14:22,300 --> 00:14:29,170
availability zones sorry didn't say that

00:14:24,779 --> 00:14:31,029
what's just all gone all right so let's

00:14:29,170 --> 00:14:33,940
say we want we really want to ensure

00:14:31,029 --> 00:14:36,730
that whenever a cluster goes down for

00:14:33,940 --> 00:14:39,160
some reason the situation won't look

00:14:36,730 --> 00:14:42,069
like this because a lot of applications

00:14:39,160 --> 00:14:44,410
will rely on your post press and I can

00:14:42,069 --> 00:14:46,839
tell you how this feels exactly like

00:14:44,410 --> 00:14:48,399
that because your phone will keep on

00:14:46,839 --> 00:14:49,690
ringing and customers will give you a

00:14:48,399 --> 00:14:51,639
bad time because they are so

00:14:49,690 --> 00:14:54,880
disappointed because they said newest

00:14:51,639 --> 00:14:56,410
they expected the platform to work so

00:14:54,880 --> 00:14:59,050
the problem with the shared cluster in

00:14:56,410 --> 00:15:01,480
general and that's true for every data

00:14:59,050 --> 00:15:04,350
service is that once this cluster goes

00:15:01,480 --> 00:15:08,230
down you all your instances are gone and

00:15:04,350 --> 00:15:10,540
you'll have a lot of trouble so what's

00:15:08,230 --> 00:15:14,620
the counter strategy to that obviously

00:15:10,540 --> 00:15:17,259
it's going to be dedicated clusters for

00:15:14,620 --> 00:15:20,230
everybody so instead of having a single

00:15:17,259 --> 00:15:22,800
database blast our single database

00:15:20,230 --> 00:15:27,189
instance you'll have multiple of them

00:15:22,800 --> 00:15:30,459
maybe even both so in the ideal world

00:15:27,189 --> 00:15:31,180
and we've made that happen you can

00:15:30,459 --> 00:15:34,390
actually create

00:15:31,180 --> 00:15:36,310
a single instance or a large single

00:15:34,390 --> 00:15:38,410
instance or a single cluster or a large

00:15:36,310 --> 00:15:43,210
cluster and you can also migrate between

00:15:38,410 --> 00:15:46,030
them and with that you'll have a big

00:15:43,210 --> 00:15:48,820
advantage also when creating a contract

00:15:46,030 --> 00:15:51,310
towards your customer because now when

00:15:48,820 --> 00:15:53,740
you create a dedicated cluster you use

00:15:51,310 --> 00:15:56,260
infrastructure resources for and the

00:15:53,740 --> 00:15:59,530
infrastructure isolation to create thus

00:15:56,260 --> 00:16:02,710
multi-tenancy behavior which means that

00:15:59,530 --> 00:16:05,230
when I provision a Postgres with four

00:16:02,710 --> 00:16:07,660
gigs of ram you're going to get four

00:16:05,230 --> 00:16:10,420
gigs of ram CPU and a certain amount of

00:16:07,660 --> 00:16:12,790
disk and in case your application needs

00:16:10,420 --> 00:16:15,160
more resources will just scale to a

00:16:12,790 --> 00:16:17,590
larger database but it's never going to

00:16:15,160 --> 00:16:20,500
be your neighbor tracking down your

00:16:17,590 --> 00:16:24,310
cluster because his app actually goes

00:16:20,500 --> 00:16:25,660
crazy unless you have a let's say unfair

00:16:24,310 --> 00:16:28,810
amount of over-commitment in your

00:16:25,660 --> 00:16:30,580
infrastructure which is totally up to

00:16:28,810 --> 00:16:33,310
you to decide but the solution is

00:16:30,580 --> 00:16:35,620
actually safe so looking at the same

00:16:33,310 --> 00:16:37,180
scenario you now have a different ratio

00:16:35,620 --> 00:16:39,430
between applications and service

00:16:37,180 --> 00:16:42,670
instances so one of these serves

00:16:39,430 --> 00:16:44,380
instances go down the problem is pretty

00:16:42,670 --> 00:16:46,510
much contained and you have only one

00:16:44,380 --> 00:16:49,180
angry call to answer right saying well I

00:16:46,510 --> 00:16:50,820
excuse me if things went wrong and we're

00:16:49,180 --> 00:16:53,740
going to fix it

00:16:50,820 --> 00:16:56,290
so your problems are going your problem

00:16:53,740 --> 00:17:01,950
with Postgres failures are going to be

00:16:56,290 --> 00:17:04,390
contained right so we've been through

00:17:01,950 --> 00:17:06,280
the question of having a single server

00:17:04,390 --> 00:17:08,560
or a cluster we've been through the

00:17:06,280 --> 00:17:11,860
question of being of having a shared or

00:17:08,560 --> 00:17:14,380
a dedicate our dedicated approach so

00:17:11,860 --> 00:17:16,360
ideally you have a choice between single

00:17:14,380 --> 00:17:17,160
or cluster and it's going to be

00:17:16,360 --> 00:17:20,290
dedicated

00:17:17,160 --> 00:17:23,260
well the drawback obviously is that it

00:17:20,290 --> 00:17:25,000
uses more infrastructure resources but

00:17:23,260 --> 00:17:27,850
then you have a stable contract to the

00:17:25,000 --> 00:17:29,650
customer it leads to the question when

00:17:27,850 --> 00:17:33,130
do I actually provision those virtual

00:17:29,650 --> 00:17:35,230
machines so two strategies again come to

00:17:33,130 --> 00:17:36,850
mind we could actually pre provision

00:17:35,230 --> 00:17:39,070
those virtual machines so that can be

00:17:36,850 --> 00:17:41,110
immediately handed or we'll do this

00:17:39,070 --> 00:17:43,600
later so with the pre-prohibition

00:17:41,110 --> 00:17:44,780
strategy you have a service broker and a

00:17:43,600 --> 00:17:47,450
pool of service instance

00:17:44,780 --> 00:17:50,870
like you know several of each plan you

00:17:47,450 --> 00:17:52,940
offer and whenever somebody performs a

00:17:50,870 --> 00:17:55,160
create service command you'll just

00:17:52,940 --> 00:17:57,860
assign one of the virtual one of the

00:17:55,160 --> 00:18:02,060
service instances out of your virtual

00:17:57,860 --> 00:18:05,170
machine pool same for cluster just that

00:18:02,060 --> 00:18:07,130
you you know assign a cluster instead

00:18:05,170 --> 00:18:10,130
the problem with that approach is

00:18:07,130 --> 00:18:12,230
obvious like you'll have ten of those

00:18:10,130 --> 00:18:14,780
things on hand and there's a hackathon

00:18:12,230 --> 00:18:17,360
going on and people start creating

00:18:14,780 --> 00:18:21,350
service instances like crazy you run out

00:18:17,360 --> 00:18:23,300
of pre provision instances so also these

00:18:21,350 --> 00:18:25,490
pre provision instances will consume

00:18:23,300 --> 00:18:30,080
infrastructure resources even if nobody

00:18:25,490 --> 00:18:32,420
uses the database so it's actually again

00:18:30,080 --> 00:18:34,880
a counter strategy strategy that comes

00:18:32,420 --> 00:18:38,690
to mind which is why don't we provision

00:18:34,880 --> 00:18:42,140
this these service instances once you

00:18:38,690 --> 00:18:44,270
know somebody creates service so in

00:18:42,140 --> 00:18:47,780
order to do that you have to provide

00:18:44,270 --> 00:18:50,920
some automation and whenever you do CF

00:18:47,780 --> 00:18:54,770
create this will actually create them a

00:18:50,920 --> 00:19:00,800
a post press server or a post crest

00:18:54,770 --> 00:19:03,470
cluster so pre provisioning the benefit

00:19:00,800 --> 00:19:06,830
is you will have your service instance

00:19:03,470 --> 00:19:09,170
right away and the on-demand well you

00:19:06,830 --> 00:19:11,270
have the advantage that you don't use

00:19:09,170 --> 00:19:14,390
the resources and once you go down that

00:19:11,270 --> 00:19:17,420
that path you'll be able to serve as

00:19:14,390 --> 00:19:22,190
many instances as your infrastructure

00:19:17,420 --> 00:19:24,410
actually has resources so we actually

00:19:22,190 --> 00:19:27,380
started with the pre provision approach

00:19:24,410 --> 00:19:30,350
because then we could actually fill the

00:19:27,380 --> 00:19:31,910
pool being deployed manually you know

00:19:30,350 --> 00:19:34,450
already giving the customer the

00:19:31,910 --> 00:19:36,920
appearance of having you know dedicated

00:19:34,450 --> 00:19:39,380
instances and then we actually did the

00:19:36,920 --> 00:19:42,140
automation afterwards filling up the

00:19:39,380 --> 00:19:44,630
pool once the automation is ready

00:19:42,140 --> 00:19:46,220
automatically and then of course we can

00:19:44,630 --> 00:19:48,590
actually switch down the pool size

00:19:46,220 --> 00:19:50,420
because you'll actually have to provide

00:19:48,590 --> 00:19:54,400
you know instances from each service

00:19:50,420 --> 00:19:56,660
plan so we can turn our framework into

00:19:54,400 --> 00:19:57,350
deploying those things entirely on

00:19:56,660 --> 00:20:01,400
demand

00:19:57,350 --> 00:20:03,620
a mixture is interesting in cases where

00:20:01,400 --> 00:20:06,470
you have CI pipeline creating service

00:20:03,620 --> 00:20:09,440
certain service instances at a high pace

00:20:06,470 --> 00:20:12,470
so where the provision time does matter

00:20:09,440 --> 00:20:13,700
to you so it's a kind of drawback and a

00:20:12,470 --> 00:20:15,620
balance you have to make you have to

00:20:13,700 --> 00:20:18,590
make those design decisions and you can

00:20:15,620 --> 00:20:21,799
configure it so we'd like to have a pool

00:20:18,590 --> 00:20:23,360
small server for for testing purposes on

00:20:21,799 --> 00:20:27,440
hand and the rest is going to be

00:20:23,360 --> 00:20:30,710
provisioned on-demand so with that being

00:20:27,440 --> 00:20:33,169
said you can't on-demand provision

00:20:30,710 --> 00:20:34,940
something without automation so one of

00:20:33,169 --> 00:20:40,220
the key questions is how do you actually

00:20:34,940 --> 00:20:43,460
do this automation and while containers

00:20:40,220 --> 00:20:47,659
are very modern and fancy and maybe it's

00:20:43,460 --> 00:20:50,480
going to be the future we had the

00:20:47,659 --> 00:20:53,690
impression that having a database a

00:20:50,480 --> 00:20:56,450
database should be close to the metal as

00:20:53,690 --> 00:20:59,179
close as as it is possible because often

00:20:56,450 --> 00:21:01,520
you know performance is is an issue and

00:20:59,179 --> 00:21:04,250
also we would like to have an automation

00:21:01,520 --> 00:21:07,429
technology we can really really rely on

00:21:04,250 --> 00:21:12,140
and after cooperating cloud foundry with

00:21:07,429 --> 00:21:13,820
Bosch for years we really fell in love

00:21:12,140 --> 00:21:16,820
with wash and that's not very obvious

00:21:13,820 --> 00:21:19,820
because our team was using chef for six

00:21:16,820 --> 00:21:21,429
or seven years so for them Bosch was

00:21:19,820 --> 00:21:26,150
really a challenge to what they already

00:21:21,429 --> 00:21:29,240
been using but they learned to fall in

00:21:26,150 --> 00:21:32,090
love with Bosch one of the reason is

00:21:29,240 --> 00:21:34,159
because Bosch gives you infrastructure

00:21:32,090 --> 00:21:36,289
independence and we moved infrastructure

00:21:34,159 --> 00:21:38,929
twice we actually started on VMware

00:21:36,289 --> 00:21:41,090
moved to OpenStack for cost reasons and

00:21:38,929 --> 00:21:44,360
recently move to Amazon for stability

00:21:41,090 --> 00:21:46,940
reasons but that's just because we we

00:21:44,360 --> 00:21:50,049
can't run OpenStack we are a platform

00:21:46,940 --> 00:21:53,000
company another infrastructure company

00:21:50,049 --> 00:21:55,700
also I've not seen many solutions that

00:21:53,000 --> 00:21:57,679
really inherently do do the

00:21:55,700 --> 00:22:00,380
orchestration of entire distributed

00:21:57,679 --> 00:22:02,240
systems so well as bashed us including

00:22:00,380 --> 00:22:05,840
virtual machine and persistent disk

00:22:02,240 --> 00:22:07,760
image while being entirely uncoupled

00:22:05,840 --> 00:22:09,910
from operating system so back in the

00:22:07,760 --> 00:22:11,440
chef's days you have a cookbook with if

00:22:09,910 --> 00:22:14,140
else clauses for different operating

00:22:11,440 --> 00:22:15,780
system also using different package

00:22:14,140 --> 00:22:19,630
managers gives you a very heterogeneous

00:22:15,780 --> 00:22:21,970
system in the end so in this hole in the

00:22:19,630 --> 00:22:25,120
operating system support will will

00:22:21,970 --> 00:22:27,460
actually go through the the cookbook and

00:22:25,120 --> 00:22:30,880
it's not very nice so with boss you have

00:22:27,460 --> 00:22:33,550
a clear contract here also the

00:22:30,880 --> 00:22:37,390
separation of of a blueprint of a

00:22:33,550 --> 00:22:40,060
distributed system let's say yeah the

00:22:37,390 --> 00:22:42,130
blueprint in a bas-reliefs and in

00:22:40,060 --> 00:22:44,140
contradiction to that the the specific

00:22:42,130 --> 00:22:46,690
construction is a very interesting

00:22:44,140 --> 00:22:48,580
approach in Bosch so when it comes to

00:22:46,690 --> 00:22:50,620
deploying data services the advantage

00:22:48,580 --> 00:22:53,260
you get is looking at the Postgres

00:22:50,620 --> 00:22:55,390
cluster example is we have a Bosch

00:22:53,260 --> 00:22:57,700
release that that deploys a Postgres

00:22:55,390 --> 00:22:59,320
cluster but the same Bosch release with

00:22:57,700 --> 00:23:02,590
the different many tasks can just deploy

00:22:59,320 --> 00:23:05,170
a single machine so you actually cover a

00:23:02,590 --> 00:23:11,740
variety of data service plans with just

00:23:05,170 --> 00:23:14,770
a single automation also interesting

00:23:11,740 --> 00:23:16,510
using Bosch is once you use Bosch to

00:23:14,770 --> 00:23:18,400
deploy your Postgres cluster you get the

00:23:16,510 --> 00:23:23,650
monitoring and self-healing capabilities

00:23:18,400 --> 00:23:26,140
of wash for free so as I said the rep

00:23:23,650 --> 00:23:27,210
manager will take care of your data of

00:23:26,140 --> 00:23:30,780
your instance

00:23:27,210 --> 00:23:33,760
so whenever a database server goes down

00:23:30,780 --> 00:23:35,740
the rep manager will talk to console and

00:23:33,760 --> 00:23:38,890
your application will continue to write

00:23:35,740 --> 00:23:40,570
to one to the new database master but

00:23:38,890 --> 00:23:42,340
Bosch will recognize that there's a

00:23:40,570 --> 00:23:44,020
missing virtual machine and will just

00:23:42,340 --> 00:23:46,300
redirect the virtual machine and the

00:23:44,020 --> 00:23:49,450
virtual machine comes up it will

00:23:46,300 --> 00:23:50,950
actually recognize that it's not there's

00:23:49,450 --> 00:23:53,380
a masked anymore it's a new virtual

00:23:50,950 --> 00:23:55,720
machine right so we recognize that it's

00:23:53,380 --> 00:23:58,000
that's now it's a slave and be an

00:23:55,720 --> 00:24:00,160
integrate into the cluster as a new

00:23:58,000 --> 00:24:03,340
slave so we had actually have with Bosch

00:24:00,160 --> 00:24:06,070
a integrated way of recovering from a

00:24:03,340 --> 00:24:08,320
degraded mode after an incident that's a

00:24:06,070 --> 00:24:12,250
very nice thing to do that is also

00:24:08,320 --> 00:24:15,940
topped by the scalability scenario where

00:24:12,250 --> 00:24:18,280
we also want to be able to take a single

00:24:15,940 --> 00:24:20,590
service instance let's say I've created

00:24:18,280 --> 00:24:22,750
a small app and deployed it on the

00:24:20,590 --> 00:24:25,540
platform but now my app need

00:24:22,750 --> 00:24:28,120
to grow so what I can do is see if

00:24:25,540 --> 00:24:30,880
create update and turn this into a large

00:24:28,120 --> 00:24:34,330
cluster so how is that possible it's

00:24:30,880 --> 00:24:37,180
possible because the sows framework

00:24:34,330 --> 00:24:39,580
actually creates a new Bosch deployment

00:24:37,180 --> 00:24:41,200
and you hand over Bosch the Bosch

00:24:39,580 --> 00:24:43,120
deployment and Bosch will actually

00:24:41,200 --> 00:24:45,490
create new virtual machines and you know

00:24:43,120 --> 00:24:49,300
scale the one that's existing and copy

00:24:45,490 --> 00:24:52,030
over the data so you you get that

00:24:49,300 --> 00:24:53,530
behavior fairly at low cost

00:24:52,030 --> 00:24:55,900
it's not for free you have to do some

00:24:53,530 --> 00:24:57,520
management around it but it's it's it's

00:24:55,900 --> 00:25:01,390
so much that's already been done by

00:24:57,520 --> 00:25:03,490
Bosch that it's fantastic so of course

00:25:01,390 --> 00:25:06,880
the same strategy applies when you want

00:25:03,490 --> 00:25:09,190
to create one you scale a small cluster

00:25:06,880 --> 00:25:11,380
like this fellow on the right side to a

00:25:09,190 --> 00:25:14,230
large cluster it's just taking down the

00:25:11,380 --> 00:25:16,600
virtual machines one after the other so

00:25:14,230 --> 00:25:18,700
your service keeps on running scaling

00:25:16,600 --> 00:25:22,630
the virtual machines so you scaled your

00:25:18,700 --> 00:25:24,610
cluster all right so now with that all

00:25:22,630 --> 00:25:27,220
being said well this fancy thing is like

00:25:24,610 --> 00:25:29,860
how does it actually look like in in the

00:25:27,220 --> 00:25:33,280
resulting system the architectural

00:25:29,860 --> 00:25:36,640
overview is looking like that so we

00:25:33,280 --> 00:25:38,230
found out that the service broker

00:25:36,640 --> 00:25:41,590
basically does nothing

00:25:38,230 --> 00:25:43,510
we released data service specific so we

00:25:41,590 --> 00:25:46,270
outsource everything that's specific to

00:25:43,510 --> 00:25:50,080
a data service into a small separate

00:25:46,270 --> 00:25:52,480
micro service called the Postgres SPI

00:25:50,080 --> 00:25:55,030
service provider interface comparable to

00:25:52,480 --> 00:25:56,980
the cloud provider interface of Bosch so

00:25:55,030 --> 00:25:58,990
what this fellow does is offer the meter

00:25:56,980 --> 00:26:01,660
data so telling which service plans are

00:25:58,990 --> 00:26:03,670
offered and also when creating a service

00:26:01,660 --> 00:26:05,860
binding it issues the credentials for

00:26:03,670 --> 00:26:09,280
the initial credentials this includes

00:26:05,860 --> 00:26:11,830
the initial credentials so credential

00:26:09,280 --> 00:26:14,590
management and everything service

00:26:11,830 --> 00:26:17,530
specific is going to be in the SPI so

00:26:14,590 --> 00:26:22,990
these cells broker then triggers the

00:26:17,530 --> 00:26:25,210
creation of Bosch deployment which will

00:26:22,990 --> 00:26:27,130
then talk to Bosch and subsequently of

00:26:25,210 --> 00:26:30,160
course there will be virtual machines

00:26:27,130 --> 00:26:32,380
being deployed by motion so the service

00:26:30,160 --> 00:26:34,540
progress has said it implements the

00:26:32,380 --> 00:26:36,010
Cloud Foundry service broker API is

00:26:34,540 --> 00:26:38,020
generic for all

00:26:36,010 --> 00:26:41,340
services we have we have rabid Redis

00:26:38,020 --> 00:26:45,760
rabbitmq more going to be in Postgres

00:26:41,340 --> 00:26:49,780
and it can be configured to use the spi

00:26:45,760 --> 00:26:52,060
as a remote service the spi itself as a

00:26:49,780 --> 00:26:56,710
certain capsule aids those data service

00:26:52,060 --> 00:27:00,520
pacific logic and among that the service

00:26:56,710 --> 00:27:04,140
catalog and the credential management so

00:27:00,520 --> 00:27:06,340
the deployer is a small abstraction

00:27:04,140 --> 00:27:07,720
abstracting from bosh deployment so it

00:27:06,340 --> 00:27:10,570
actually does two things first it

00:27:07,720 --> 00:27:13,210
manages deployments of course and the

00:27:10,570 --> 00:27:16,120
second is it manages templates which is

00:27:13,210 --> 00:27:22,600
can be you know seen as a borscht

00:27:16,120 --> 00:27:24,610
manifest with placeholders in it so how

00:27:22,600 --> 00:27:27,610
does it actually look and how those how

00:27:24,610 --> 00:27:29,980
do these components interact is whenever

00:27:27,610 --> 00:27:32,890
you can you see that yeah so whenever

00:27:29,980 --> 00:27:35,980
you call a create service you'll

00:27:32,890 --> 00:27:39,100
actually hit the service broker who will

00:27:35,980 --> 00:27:40,630
then talk to the spi because what the

00:27:39,100 --> 00:27:43,690
service broker has to do in the next

00:27:40,630 --> 00:27:46,240
step is to to trigger a deployment using

00:27:43,690 --> 00:27:49,800
the deployer in order to do that it has

00:27:46,240 --> 00:27:53,800
to hand over the name of the template to

00:27:49,800 --> 00:27:57,550
to deploy as well as some deployment

00:27:53,800 --> 00:28:01,000
attributes so one of the information

00:27:57,550 --> 00:28:02,500
that is required to do that is the the

00:28:01,000 --> 00:28:05,130
service plan the customer has chosen

00:28:02,500 --> 00:28:07,360
which maps then to a deployment template

00:28:05,130 --> 00:28:09,640
so the system what the system actually

00:28:07,360 --> 00:28:13,320
does it creates a service proko that

00:28:09,640 --> 00:28:15,400
lets you trigger bosch deployments so

00:28:13,320 --> 00:28:17,230
actually you could also deploy cloud

00:28:15,400 --> 00:28:21,240
families with that solution if you

00:28:17,230 --> 00:28:24,970
create a partial ease for cloud families

00:28:21,240 --> 00:28:28,360
so yeah after you got those information

00:28:24,970 --> 00:28:29,920
you can then pick a deployment against

00:28:28,360 --> 00:28:31,810
the deployer by handing over the

00:28:29,920 --> 00:28:33,460
template and the attributes who will

00:28:31,810 --> 00:28:35,560
then generate the deployment manifest

00:28:33,460 --> 00:28:37,450
and trigger the deployment the cloud

00:28:35,560 --> 00:28:39,910
controller then keeps on palling whether

00:28:37,450 --> 00:28:44,650
the deployment is already done and once

00:28:39,910 --> 00:28:46,570
it's done the the service broker will

00:28:44,650 --> 00:28:49,740
store some metadata about the deployment

00:28:46,570 --> 00:28:51,990
because if you want to create them later

00:28:49,740 --> 00:28:54,390
a service binding you'll have to know

00:28:51,990 --> 00:28:57,840
that there is a data you know dedicated

00:28:54,390 --> 00:29:01,080
instance running somewhere so the SPI is

00:28:57,840 --> 00:29:03,120
able to connect to this database server

00:29:01,080 --> 00:29:06,270
and you know create a new database user

00:29:03,120 --> 00:29:10,200
so you have to store some metadata which

00:29:06,270 --> 00:29:13,890
is again not service specific because

00:29:10,200 --> 00:29:15,720
it's handled by the SPI in the end so

00:29:13,890 --> 00:29:18,150
this works like charm we've been using X

00:29:15,720 --> 00:29:19,620
solution for roughly developing it for

00:29:18,150 --> 00:29:25,410
two years and using it more than a year

00:29:19,620 --> 00:29:28,799
and on our platform and yeah it's proven

00:29:25,410 --> 00:29:31,940
and it works so what can we actually

00:29:28,799 --> 00:29:34,710
learn from that is first of all

00:29:31,940 --> 00:29:37,200
designing a data service go with the

00:29:34,710 --> 00:29:39,740
dedicated service instances anything

00:29:37,200 --> 00:29:42,240
based on shared cluster is dangerous

00:29:39,740 --> 00:29:45,660
might be working if your company is

00:29:42,240 --> 00:29:48,630
small but it's scale would be I would be

00:29:45,660 --> 00:29:50,970
using it so every every shared data

00:29:48,630 --> 00:29:52,880
service we've been offering it exploded

00:29:50,970 --> 00:29:55,559
at some point at time

00:29:52,880 --> 00:29:57,900
so with that on-demand provisioning is

00:29:55,559 --> 00:30:00,419
essential so you have to pick an

00:29:57,900 --> 00:30:03,330
automation tool you are familiar with

00:30:00,419 --> 00:30:05,820
and you'll you know have to find

00:30:03,330 --> 00:30:08,340
something preferably that really takes

00:30:05,820 --> 00:30:10,740
care of the life cycle of a distributed

00:30:08,340 --> 00:30:13,200
system such as a database so because you

00:30:10,740 --> 00:30:18,020
you will have to solve the problems of

00:30:13,200 --> 00:30:21,419
how to update that in the end as well so

00:30:18,020 --> 00:30:23,190
the biggest challenge was not about the

00:30:21,419 --> 00:30:24,900
framework was not about bosh was not

00:30:23,190 --> 00:30:27,419
about everything who actually was about

00:30:24,900 --> 00:30:29,100
Postgres so finding a Postgres

00:30:27,419 --> 00:30:30,690
replication and clustering truths that

00:30:29,100 --> 00:30:32,400
was the things we actually have

00:30:30,690 --> 00:30:34,890
investigate and how to make this

00:30:32,400 --> 00:30:37,950
failover happen on on infrastructure but

00:30:34,890 --> 00:30:39,630
still not being infrastructure specific

00:30:37,950 --> 00:30:41,580
so we can take the service framework and

00:30:39,630 --> 00:30:43,440
we've deployed it on vmware we've

00:30:41,580 --> 00:30:45,630
deployed it on OpenStack we've deployed

00:30:43,440 --> 00:30:48,600
it on Amazon and we didn't have to have

00:30:45,630 --> 00:30:50,760
to change a thing despite of cloud

00:30:48,600 --> 00:30:53,160
configuration of the of the boss

00:30:50,760 --> 00:30:57,240
releases obviously so let's consider

00:30:53,160 --> 00:30:59,640
configuration or change of code so we

00:30:57,240 --> 00:31:01,820
also had to learn a lot of Postgres and

00:30:59,640 --> 00:31:04,190
iterally you know shape the thing

00:31:01,820 --> 00:31:07,519
edge cases have been found so you have

00:31:04,190 --> 00:31:08,659
to do some automation around that but

00:31:07,519 --> 00:31:13,519
yeah that's about it

00:31:08,659 --> 00:31:16,340
it works the strategy works so and we

00:31:13,519 --> 00:31:18,470
also open to conversation on how to to

00:31:16,340 --> 00:31:20,269
share that with you so just approached

00:31:18,470 --> 00:31:21,529
me and asked if you're interested in

00:31:20,269 --> 00:31:24,289
something like that

00:31:21,529 --> 00:31:26,960
will we help you building data servers

00:31:24,289 --> 00:31:36,019
if you want to so feel free to ask any

00:31:26,960 --> 00:31:39,549
question about this questions that they

00:31:36,019 --> 00:31:39,549
handsome fellow in the blue shirt yes

00:31:41,160 --> 00:31:44,859
[Applause]

00:31:41,650 --> 00:31:47,409
[Laughter]

00:31:44,859 --> 00:31:50,960
you all wait now okay so basically

00:31:47,409 --> 00:31:52,999
overall very good thought father there

00:31:50,960 --> 00:31:54,919
good approach Thank You Wayne I have to

00:31:52,999 --> 00:31:57,080
respectfully disagree a bit about the

00:31:54,919 --> 00:31:58,220
dedicated versus shared there are times

00:31:57,080 --> 00:31:59,570
when you want to go share it especially

00:31:58,220 --> 00:32:01,909
if you like you're a service provider

00:31:59,570 --> 00:32:03,649
and you've got just massive amounts so

00:32:01,909 --> 00:32:04,940
the key there is actually investigating

00:32:03,649 --> 00:32:06,499
whether you can actually have a pet

00:32:04,940 --> 00:32:08,929
coffee sorry if you whether you can

00:32:06,499 --> 00:32:10,759
actually have a plan to start somebody

00:32:08,929 --> 00:32:12,859
on shared like the free tier and then

00:32:10,759 --> 00:32:14,210
migrate them very easily to a dedicated

00:32:12,859 --> 00:32:17,119
it's like that's something that could be

00:32:14,210 --> 00:32:19,099
a way to go to yeah wash with as first

00:32:17,119 --> 00:32:21,249
Postgres is concerned oftentimes you can

00:32:19,099 --> 00:32:24,979
actually get a much better performance

00:32:21,249 --> 00:32:27,019
scenario if you have separate discs so

00:32:24,979 --> 00:32:28,879
Bosch currently has I'm saying this for

00:32:27,019 --> 00:32:31,059
the community as a whole Bosch has a

00:32:28,879 --> 00:32:32,210
really nasty limitation of a one disc

00:32:31,059 --> 00:32:34,489
policy

00:32:32,210 --> 00:32:35,720
I'm hoping it's in the roadmap to fix

00:32:34,489 --> 00:32:36,859
that I'd like everybody to apply

00:32:35,720 --> 00:32:39,710
pressure for that because that can

00:32:36,859 --> 00:32:44,539
really help the services stories when to

00:32:39,710 --> 00:32:46,639
playing with Bosch and we just announced

00:32:44,539 --> 00:32:49,729
at the Postgres conf in New York City

00:32:46,639 --> 00:32:52,729
that open sourcing of a similar project

00:32:49,729 --> 00:32:55,159
called the RDP G it's we did it for GE

00:32:52,729 --> 00:32:57,229
they allowed us to open source it a lot

00:32:55,159 --> 00:33:00,169
of the same concepts and approaches were

00:32:57,229 --> 00:33:02,389
done within it so now that that's open

00:33:00,169 --> 00:33:05,720
source what I'm I would literally like

00:33:02,389 --> 00:33:07,549
to see as if you know is this open

00:33:05,720 --> 00:33:09,109
source and can we merge efforts instead

00:33:07,549 --> 00:33:09,950
of having two efforts and like what are

00:33:09,109 --> 00:33:11,869
your thoughts on that

00:33:09,950 --> 00:33:13,279
well my thoughts on open sourcing that

00:33:11,869 --> 00:33:14,360
is that we are actually currently

00:33:13,279 --> 00:33:16,370
investigating opens

00:33:14,360 --> 00:33:19,220
sourcing it so this solution has also

00:33:16,370 --> 00:33:22,310
been this developed over time we've been

00:33:19,220 --> 00:33:25,040
using it at the platform so it could be

00:33:22,310 --> 00:33:26,870
open so soon but this is a discussion

00:33:25,040 --> 00:33:29,540
that's currently ongoing so I can answer

00:33:26,870 --> 00:33:31,760
that - final decree yet but we're

00:33:29,540 --> 00:33:34,340
currently talking about with partners

00:33:31,760 --> 00:33:36,410
and how actually open sourcing could

00:33:34,340 --> 00:33:38,480
look like because we have a development

00:33:36,410 --> 00:33:40,160
team to fund here and if there's no

00:33:38,480 --> 00:33:43,610
license money coming in we have to

00:33:40,160 --> 00:33:45,290
replace that so that is very fair so

00:33:43,610 --> 00:33:47,300
everybody hire them so that they can

00:33:45,290 --> 00:33:49,510
open either one of the models we

00:33:47,300 --> 00:33:52,160
actually could apply is that we'll have

00:33:49,510 --> 00:33:53,840
sponsorships so that people can

00:33:52,160 --> 00:33:56,990
influence the backlog of such a solution

00:33:53,840 --> 00:33:58,750
maybe telling us which data service to

00:33:56,990 --> 00:34:01,240
make next so we opened such as

00:33:58,750 --> 00:34:06,340
suggestions here sounds good thanks

00:34:01,240 --> 00:34:06,340
welcome any other questions

00:34:07,900 --> 00:34:10,900
No

00:34:10,940 --> 00:34:20,139
well then yeah there's a plucking system

00:34:18,020 --> 00:34:23,450
in the framework that allows you to

00:34:20,139 --> 00:34:26,659
create streamed backups so you can

00:34:23,450 --> 00:34:28,970
actually read will say right ahead locks

00:34:26,659 --> 00:34:31,730
and then stream it in chunks to let's

00:34:28,970 --> 00:34:33,829
say OpenStack Swift or Amazon s3 we can

00:34:31,730 --> 00:34:36,230
only have only basic strategies

00:34:33,829 --> 00:34:38,839
implemented like like creating a dump

00:34:36,230 --> 00:34:40,490
instead of write a headlock logging

00:34:38,839 --> 00:34:42,200
I think Stockman Wayne has something

00:34:40,490 --> 00:34:45,770
interesting it could be integrated as

00:34:42,200 --> 00:34:48,200
well so yes there is something foreseen

00:34:45,770 --> 00:34:50,000
in the framework but implementation the

00:34:48,200 --> 00:34:52,730
plugins actually need to make sure a

00:34:50,000 --> 00:34:57,109
little more it's it's that's the thing

00:34:52,730 --> 00:35:01,900
currently under development all right

00:34:57,109 --> 00:35:01,900

YouTube URL: https://www.youtube.com/watch?v=MCGqAq1dm9I


