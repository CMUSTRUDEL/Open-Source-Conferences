Title: Distributed Service Bundles for Cloud Foundry - Krishanu Biswas, SAP
Publication date: 2017-10-18
Playlist: Cloud Foundry Summit Europe 2017
Description: 
	Distributed Service Bundles for Cloud Foundry - Krishanu Biswas, SAP

Distributed Service Bundle is a mechanism to define a group of services whose lifecycle and operations are controlled as a single unit. This means parts of the bundle are deployed, un-deployed, updated, operated and monitored together. Cloud Foundry provides well-defined model for application to service communication via the service binding. While this model works well for majority of the use cases, there are use cases where a managed service needs to communicate with other managed services. For example – connecting the Jobs in a managed Spark to Topics in a managed Kafka. Cloud Foundry doesn’t inherently have a notion of service to service binding. To address the above scenarios we need a way to deploy services together with dependencies injected via the environment variables similar to the way CF applications use the VCAP environment variables. In this session, we will explain the need for distributed service bundles and show how such a distributed bundle can be realised via a generic implementation of Cloud Foundry Service Broker.

About Krishanu Biswas
Krishanu Biswas is a Product & Engineering Manager at SAP building server technologies and distributed systems over a decade. As a lead product expert, he helps define product requirements around key platform core services and backing services in SAP Cloud Platform based on Cloud Foundry. Krishanu comes with over 14 years of industry experience across the technology board of application servers, web development & enterprise software development. Cloud computing, cloud platforms, containers are his key areas of interest. Being an engineering manager, he also drives the execution & delivery of SAP Cloud Platform together with an assembly of highly talented engineers spanning across several teams. He has been a regular speaker at many SAP Internal events like DKOM and external events like SAP TechED, CF Summit.
Captions: 
	00:00:00,060 --> 00:00:07,350
so thank you very much for coming to

00:00:04,350 --> 00:00:09,450
this session we will discuss today about

00:00:07,350 --> 00:00:12,780
distributed service bundles for Cloud

00:00:09,450 --> 00:00:14,250
Foundry my name is Cristiano viswas and

00:00:12,780 --> 00:00:17,039
I am one of the product and engineering

00:00:14,250 --> 00:00:22,260
managers at recipe I work from

00:00:17,039 --> 00:00:24,660
Mississippi labs India so next 30

00:00:22,260 --> 00:00:28,289
minutes we will discuss about the idea

00:00:24,660 --> 00:00:32,070
of distributed service bundles what it

00:00:28,289 --> 00:00:34,399
is and why are we basically talking

00:00:32,070 --> 00:00:36,989
about this topic what's the motivation

00:00:34,399 --> 00:00:41,450
what what's the future of it and where

00:00:36,989 --> 00:00:43,920
we are and what's idea is all about so

00:00:41,450 --> 00:00:47,610
today I think the applications are not

00:00:43,920 --> 00:00:49,950
merely a cloud client server anymore

00:00:47,610 --> 00:00:53,640
right all the applications are kind of

00:00:49,950 --> 00:00:57,030
distributed in nature so Cloud Foundry

00:00:53,640 --> 00:00:59,190
gives us a model wherein if you have an

00:00:57,030 --> 00:01:01,320
application you can bind it to a backing

00:00:59,190 --> 00:01:04,229
service and that's how your application

00:01:01,320 --> 00:01:07,110
can talk to a backing service so this is

00:01:04,229 --> 00:01:09,659
via the service broker concept that

00:01:07,110 --> 00:01:12,750
Cloud Foundry has and this is kind of

00:01:09,659 --> 00:01:15,150
well-established this is working I'll

00:01:12,750 --> 00:01:17,670
final chord but in the enterprise space

00:01:15,150 --> 00:01:20,100
what we see today that we there are more

00:01:17,670 --> 00:01:25,350
and more use cases for service to

00:01:20,100 --> 00:01:28,710
service consumption so think of change

00:01:25,350 --> 00:01:30,180
data capture pipeline for example so

00:01:28,710 --> 00:01:33,869
that the database has the single source

00:01:30,180 --> 00:01:36,180
of truth in this concept and the change

00:01:33,869 --> 00:01:39,479
is extracted from the transaction or the

00:01:36,180 --> 00:01:41,549
commit log so all the data that comes in

00:01:39,479 --> 00:01:45,240
into your database that becomes the

00:01:41,549 --> 00:01:48,960
source of the entire pipeline so to say

00:01:45,240 --> 00:01:52,020
and you extract that information out of

00:01:48,960 --> 00:01:54,380
the database automatically through some

00:01:52,020 --> 00:01:57,210
eventing mechanism and you push those

00:01:54,380 --> 00:01:59,729
even's as a message to some message

00:01:57,210 --> 00:02:02,969
broker in between could be Kafka could

00:01:59,729 --> 00:02:04,979
be something else and then you have a

00:02:02,969 --> 00:02:08,190
set of services which are listening to

00:02:04,979 --> 00:02:10,500
this a particular pipeline and then they

00:02:08,190 --> 00:02:11,770
take these messages out and do the stuff

00:02:10,500 --> 00:02:14,760
that they want to do

00:02:11,770 --> 00:02:17,560
these messages so for example and this

00:02:14,760 --> 00:02:19,060
record is inserted and then you want to

00:02:17,560 --> 00:02:22,480
index that or you want to make it

00:02:19,060 --> 00:02:25,060
searchable maybe all the cells order

00:02:22,480 --> 00:02:27,730
that you want to show to through some

00:02:25,060 --> 00:02:31,240
queries and an OLAP system maybe right

00:02:27,730 --> 00:02:33,010
so you have then an indexing or a

00:02:31,240 --> 00:02:35,530
searching system which will take this

00:02:33,010 --> 00:02:38,170
event out and then this will make it

00:02:35,530 --> 00:02:40,930
searchable or put an index on it so that

00:02:38,170 --> 00:02:42,040
it is easily searchable you can build a

00:02:40,930 --> 00:02:44,320
cache out of it

00:02:42,040 --> 00:02:47,830
you can build some kind of a machine

00:02:44,320 --> 00:02:51,070
learning system around around these

00:02:47,830 --> 00:02:53,170
events and then of course these are

00:02:51,070 --> 00:02:54,910
different services you can also put some

00:02:53,170 --> 00:02:57,100
kind of for monitoring around it so

00:02:54,910 --> 00:02:59,680
these are the different services that

00:02:57,100 --> 00:03:01,900
can actually be hooked into this

00:02:59,680 --> 00:03:04,240
pipeline and they are all different

00:03:01,900 --> 00:03:06,070
services and they hook into the pipeline

00:03:04,240 --> 00:03:09,610
they extract their data and then the

00:03:06,070 --> 00:03:11,680
process on this on this data so this has

00:03:09,610 --> 00:03:14,350
changed data capture a kind of a

00:03:11,680 --> 00:03:16,240
pipeline why are we talking about this

00:03:14,350 --> 00:03:18,730
in the context of distributed service

00:03:16,240 --> 00:03:21,690
bundles the question here then is that

00:03:18,730 --> 00:03:23,980
if I have so many of those services

00:03:21,690 --> 00:03:26,340
which are which are kind of working on

00:03:23,980 --> 00:03:28,930
the same set of data can I just think of

00:03:26,340 --> 00:03:31,150
in Cloud Foundry terms that these

00:03:28,930 --> 00:03:34,180
services are kind of deployed together

00:03:31,150 --> 00:03:37,120
managed together right so that is where

00:03:34,180 --> 00:03:39,340
this idea of distributed service bundles

00:03:37,120 --> 00:03:41,800
comes comes in and that we will talk

00:03:39,340 --> 00:03:43,959
about it in a moment in more detail so

00:03:41,800 --> 00:03:48,120
this is if you're more interested just

00:03:43,959 --> 00:03:51,040
visit this division project from Red Hat

00:03:48,120 --> 00:03:54,280
they are offering a lot of decode buff

00:03:51,040 --> 00:03:55,750
for different databases that you can put

00:03:54,280 --> 00:03:58,090
it along with the database and then it

00:03:55,750 --> 00:04:00,490
can extract this database evens out of

00:03:58,090 --> 00:04:01,810
it and then yeah it can give you the

00:04:00,490 --> 00:04:04,390
information the way in the pipeline in

00:04:01,810 --> 00:04:06,959
the way you want you can also read a

00:04:04,390 --> 00:04:08,890
little more about LinkedIn's de Ribas

00:04:06,959 --> 00:04:10,209
infrastructure they are also kind of

00:04:08,890 --> 00:04:12,400
doing the similar kind of a thing to

00:04:10,209 --> 00:04:15,280
make this concept a little more clear

00:04:12,400 --> 00:04:19,030
let's talk about a simple blogging site

00:04:15,280 --> 00:04:22,960
so what happens is you have a blocking

00:04:19,030 --> 00:04:25,120
site and then you ingest blogs basically

00:04:22,960 --> 00:04:26,590
right so you have the data and they

00:04:25,120 --> 00:04:30,520
you have the metadata that you want to

00:04:26,590 --> 00:04:32,010
store into the database right and from

00:04:30,520 --> 00:04:38,110
this database basically

00:04:32,010 --> 00:04:41,050
this evens gets emitted and then you

00:04:38,110 --> 00:04:43,419
have certain such mechanism implemented

00:04:41,050 --> 00:04:46,419
by means of list let's say elasticsearch

00:04:43,419 --> 00:04:48,250
right so it will index all this blogs

00:04:46,419 --> 00:04:50,530
based on the metadata and the content

00:04:48,250 --> 00:04:52,030
etc so that other users who were

00:04:50,530 --> 00:04:55,660
searching for it can get it easily

00:04:52,030 --> 00:04:58,510
another service that is probably looking

00:04:55,660 --> 00:05:00,010
into this change data pipeline is

00:04:58,510 --> 00:05:02,620
probably the aggregates maybe it's a

00:05:00,010 --> 00:05:05,199
spark service that is hooked into this

00:05:02,620 --> 00:05:06,940
pipeline it takes it out it does a agree

00:05:05,199 --> 00:05:09,039
gate and you know kind of keeping

00:05:06,940 --> 00:05:10,720
information of who is writing blogs how

00:05:09,039 --> 00:05:12,580
many of them and then things like that

00:05:10,720 --> 00:05:14,260
and then maybe you want to apply some

00:05:12,580 --> 00:05:16,449
kind of a classification or clustering

00:05:14,260 --> 00:05:18,789
on top of this blogging information that

00:05:16,449 --> 00:05:20,440
is coming in maybe an ml service of

00:05:18,789 --> 00:05:22,930
machine learning services running there

00:05:20,440 --> 00:05:25,780
so what I mean to say here is that that

00:05:22,930 --> 00:05:28,150
there are many services which are kind

00:05:25,780 --> 00:05:29,830
of hooked into this pipeline and this

00:05:28,150 --> 00:05:32,190
services are all working together

00:05:29,830 --> 00:05:35,080
right so the question is now if we can

00:05:32,190 --> 00:05:37,410
have a system in place where all these

00:05:35,080 --> 00:05:40,450
services are kind of managed together

00:05:37,410 --> 00:05:42,430
yet not sacrificing on on the

00:05:40,450 --> 00:05:43,750
flexibility part of it I should be able

00:05:42,430 --> 00:05:45,729
because tomorrow the technology will

00:05:43,750 --> 00:05:48,039
probably change I might need to bring in

00:05:45,729 --> 00:05:49,960
another service in take some service out

00:05:48,039 --> 00:05:52,960
maybe spark is out there maybe something

00:05:49,960 --> 00:05:55,060
else I should also be able to do that so

00:05:52,960 --> 00:05:57,360
this is the motivation so a set of

00:05:55,060 --> 00:06:01,260
service then what do I do about it

00:05:57,360 --> 00:06:03,910
that's where you know the thinking of

00:06:01,260 --> 00:06:04,419
composite services comes into into the

00:06:03,910 --> 00:06:06,610
picture

00:06:04,419 --> 00:06:08,860
so in Cloud Foundry rearm basically each

00:06:06,610 --> 00:06:13,199
backing service in itself is is kind of

00:06:08,860 --> 00:06:16,180
distributed right and then a set of

00:06:13,199 --> 00:06:18,699
services if we can just bundle them

00:06:16,180 --> 00:06:21,849
together creating a composite service

00:06:18,699 --> 00:06:25,720
out of it that is deployed and deployed

00:06:21,849 --> 00:06:27,729
updated operated and monitored and

00:06:25,720 --> 00:06:29,740
scaled together right and thereby

00:06:27,729 --> 00:06:31,330
enabling inter-service communication the

00:06:29,740 --> 00:06:34,570
services should be able to also talk to

00:06:31,330 --> 00:06:36,430
each other allowing credential and

00:06:34,570 --> 00:06:38,409
message exchange between the constituent

00:06:36,430 --> 00:06:41,529
services so

00:06:38,409 --> 00:06:43,719
cloud foundries a proven model of app

00:06:41,529 --> 00:06:45,939
talking to a service we are now talking

00:06:43,719 --> 00:06:48,789
about service to service communication a

00:06:45,939 --> 00:06:51,099
set of services deployed together be

00:06:48,789 --> 00:06:53,169
able to we should be able to flexibly

00:06:51,099 --> 00:06:56,830
modify this composition over a period of

00:06:53,169 --> 00:06:58,899
time that this bundle then we can bring

00:06:56,830 --> 00:07:01,389
in new services into it and can take

00:06:58,899 --> 00:07:03,550
maybe some existing services out of it

00:07:01,389 --> 00:07:04,989
so it should be flexible in them and the

00:07:03,550 --> 00:07:06,819
composition is the key here

00:07:04,989 --> 00:07:08,619
allowing bundling and wearing of

00:07:06,819 --> 00:07:10,569
individual services so that's the

00:07:08,619 --> 00:07:13,509
central idea around distributed service

00:07:10,569 --> 00:07:16,779
bundles are the composites and there is

00:07:13,509 --> 00:07:18,309
no such mechanism today in in the in the

00:07:16,779 --> 00:07:21,339
in the Cloud Foundry backing services

00:07:18,309 --> 00:07:24,550
world that realizes this we have tried

00:07:21,339 --> 00:07:27,729
to build a proof-of-concept around it

00:07:24,550 --> 00:07:30,789
and that's what I'm here to present you

00:07:27,729 --> 00:07:32,919
so what we need and that is where we

00:07:30,789 --> 00:07:36,129
come in we need an automated way now to

00:07:32,919 --> 00:07:38,949
provision this services as a single unit

00:07:36,129 --> 00:07:40,839
a bundle we're in the wearing of the

00:07:38,949 --> 00:07:44,229
services is automatically taken care of

00:07:40,839 --> 00:07:47,050
and controlled by the service broker so

00:07:44,229 --> 00:07:49,569
we need a service broker now and able a

00:07:47,050 --> 00:07:52,209
capable service broker and that is where

00:07:49,569 --> 00:07:54,610
it comes to service fabric if you are

00:07:52,209 --> 00:07:57,999
not aware of service fabric I've just

00:07:54,610 --> 00:08:00,309
put up a link there have a look so this

00:07:57,999 --> 00:08:02,649
is a proposed the service fabric is a

00:08:00,309 --> 00:08:03,999
proposed project for incubation now I

00:08:02,649 --> 00:08:06,809
think you have heard in the keynote

00:08:03,999 --> 00:08:10,019
about service fabric as a piece for

00:08:06,809 --> 00:08:13,629
moving towards supporting multi-cloud

00:08:10,019 --> 00:08:16,539
including Azure including GC P and W as

00:08:13,629 --> 00:08:18,909
an OpenStack and of course service

00:08:16,539 --> 00:08:22,179
fabric is battle tested and being used

00:08:18,909 --> 00:08:25,269
within SOP and for recipe customers for

00:08:22,179 --> 00:08:30,099
for quite some time now on production

00:08:25,269 --> 00:08:32,649
and service fabric is is able to spin

00:08:30,099 --> 00:08:34,689
Bosch based production ready services as

00:08:32,649 --> 00:08:36,579
well as docker services for developers

00:08:34,689 --> 00:08:38,649
only so what is service fabric it's a

00:08:36,579 --> 00:08:40,809
set of tools which can be used to

00:08:38,649 --> 00:08:43,509
provision manage and operate backing

00:08:40,809 --> 00:08:46,509
services at scale for cloud foundry

00:08:43,509 --> 00:08:48,459
applications and in an automated and

00:08:46,509 --> 00:08:51,410
managed way on a variety of cloud

00:08:48,459 --> 00:08:53,960
infrastructures so we needed this broker

00:08:51,410 --> 00:08:57,470
the we wanted to ask this broker hey hey

00:08:53,960 --> 00:09:00,140
it is no longer a CF create service and

00:08:57,470 --> 00:09:02,390
it's just one service instance that you

00:09:00,140 --> 00:09:04,580
have to create but now it's going to be

00:09:02,390 --> 00:09:06,470
multiple of those services and how do we

00:09:04,580 --> 00:09:08,480
you know kind of do this multiple

00:09:06,470 --> 00:09:11,000
service we manage and operate them is

00:09:08,480 --> 00:09:12,890
something that where we needed service

00:09:11,000 --> 00:09:14,240
fabric just to be a little more on the

00:09:12,890 --> 00:09:18,920
service fabric so that you get the idea

00:09:14,240 --> 00:09:22,340
so service fabric usual you have the CFC

00:09:18,920 --> 00:09:25,820
Li and with that you you say CF create

00:09:22,340 --> 00:09:27,410
service and then the call comes to the

00:09:25,820 --> 00:09:30,080
central component you see there the

00:09:27,410 --> 00:09:33,710
service fabric broker and service fabric

00:09:30,080 --> 00:09:35,900
broker then based on the request that's

00:09:33,710 --> 00:09:38,330
coming from you you're either it's pins

00:09:35,900 --> 00:09:41,050
let's say a Bosch based service maybe

00:09:38,330 --> 00:09:44,240
let's say it's a post Chris or a MongoDB

00:09:41,050 --> 00:09:47,720
or or maybe a radius or a RabbitMQ today

00:09:44,240 --> 00:09:50,420
or it could be one of the docker

00:09:47,720 --> 00:09:52,610
services of this services doctor

00:09:50,420 --> 00:09:55,070
services are like single Nord it just

00:09:52,610 --> 00:09:58,160
comes and go every no SLS nothing but

00:09:55,070 --> 00:10:00,950
the Bosch based services they come in

00:09:58,160 --> 00:10:03,620
clusters so post critical it has masters

00:10:00,950 --> 00:10:06,020
it has the slaves and it's it's a proper

00:10:03,620 --> 00:10:09,010
set up with the failure work even so

00:10:06,020 --> 00:10:11,600
that's what it does same for MongoDB

00:10:09,010 --> 00:10:13,460
based on the request I get either goes

00:10:11,600 --> 00:10:17,870
towards the Bosch or it goes to the

00:10:13,460 --> 00:10:19,550
swamp manager so the advantage that we

00:10:17,870 --> 00:10:21,080
see here with the service fabric that

00:10:19,550 --> 00:10:22,450
and that that's the reason why we used

00:10:21,080 --> 00:10:25,910
we chose service perfect for this

00:10:22,450 --> 00:10:29,480
particular use cases is that that it

00:10:25,910 --> 00:10:32,000
provisions manage and operate such k in

00:10:29,480 --> 00:10:33,710
an automated and managed way and then it

00:10:32,000 --> 00:10:36,290
enables seamless application development

00:10:33,710 --> 00:10:38,510
and deployment experience you start your

00:10:36,290 --> 00:10:40,790
developer in your company it starts with

00:10:38,510 --> 00:10:42,860
a docker service and when he's

00:10:40,790 --> 00:10:44,630
comfortable he has worked his

00:10:42,860 --> 00:10:46,370
application out against this docker

00:10:44,630 --> 00:10:49,640
service let's it suppose Chris service

00:10:46,370 --> 00:10:52,340
the database just came out immediately

00:10:49,640 --> 00:10:54,470
when he started developing his

00:10:52,340 --> 00:10:56,560
application it all works fine now then

00:10:54,470 --> 00:10:58,640
maybe you want to move more towards a

00:10:56,560 --> 00:11:02,030
production grade service then you would

00:10:58,640 --> 00:11:05,059
probably need a Bosch based post Chris

00:11:02,030 --> 00:11:06,979
in that case so it's kind of a gradual

00:11:05,059 --> 00:11:08,599
elevation of a developer developing his

00:11:06,979 --> 00:11:11,629
application from a very innocent state

00:11:08,599 --> 00:11:14,119
to a production set of it supports this

00:11:11,629 --> 00:11:17,989
transformation multi-cloud you don't

00:11:14,119 --> 00:11:20,449
have to care about like underlying which

00:11:17,989 --> 00:11:22,909
particular is you are running on it

00:11:20,449 --> 00:11:26,659
could be it could be I AWS OpenStack

00:11:22,909 --> 00:11:27,289
genre and GCP for the developers it all

00:11:26,659 --> 00:11:29,329
looks the same

00:11:27,289 --> 00:11:32,359
he once suppose Chris it just says that

00:11:29,329 --> 00:11:37,009
I wanted on AWS or next time I want it

00:11:32,359 --> 00:11:39,319
on a jar or GCP that's it right we I

00:11:37,009 --> 00:11:41,529
think service fabric takes that does the

00:11:39,319 --> 00:11:43,039
heavy lifting of managing that

00:11:41,529 --> 00:11:44,899
integrated monitoring

00:11:43,039 --> 00:11:46,999
alerting this is very important when you

00:11:44,899 --> 00:11:49,639
have spawn let's say hundreds of this

00:11:46,999 --> 00:11:51,409
kind of systems and then you have

00:11:49,639 --> 00:11:53,659
hundreds of virtual machines running

00:11:51,409 --> 00:11:55,309
around right now question is how is my

00:11:53,659 --> 00:11:59,539
post Chris doing how is my mango doing

00:11:55,309 --> 00:12:01,699
how do you monitor them audit logging is

00:11:59,539 --> 00:12:03,199
important who is inserting what into

00:12:01,699 --> 00:12:04,609
your database needs to be tracked and

00:12:03,199 --> 00:12:07,429
all this was all integrated within

00:12:04,609 --> 00:12:10,759
service fabric and as I mentioned before

00:12:07,429 --> 00:12:13,069
it's battle tested it's used within ACP

00:12:10,759 --> 00:12:15,049
the security is of utmost important

00:12:13,069 --> 00:12:17,629
update an upgrade of this system so

00:12:15,049 --> 00:12:20,929
important so this is all been taken care

00:12:17,629 --> 00:12:24,439
of within the service fabric hence the

00:12:20,929 --> 00:12:27,619
choice of using service fabric for for

00:12:24,439 --> 00:12:29,689
this scenario now that's that takes me

00:12:27,619 --> 00:12:32,959
do to the demo part I was we have a

00:12:29,689 --> 00:12:36,139
small demo so showing what it means to

00:12:32,959 --> 00:12:39,169
do have something like service bundle on

00:12:36,139 --> 00:12:42,799
this my setup looks like this that I

00:12:39,169 --> 00:12:44,569
have an account on AWS and where you

00:12:42,799 --> 00:12:46,699
will be logging in into the jump box and

00:12:44,569 --> 00:12:49,519
then we need of course the Bosh director

00:12:46,699 --> 00:12:52,009
which will spin the cluster for us we of

00:12:49,519 --> 00:12:53,749
course need Cloud Foundry where in our

00:12:52,009 --> 00:12:56,809
application will be running and of

00:12:53,749 --> 00:12:58,639
course we need service fabric the Bosh

00:12:56,809 --> 00:13:02,089
realizes that we've used in this demo

00:12:58,639 --> 00:13:04,429
was like Kafka zookeeper and Porsche

00:13:02,089 --> 00:13:07,789
Chris service fabric itself is is a

00:13:04,429 --> 00:13:09,409
Bosch release and end radius the demo

00:13:07,789 --> 00:13:11,659
scenario goes something like this that

00:13:09,409 --> 00:13:16,189
we have an app which is basically

00:13:11,659 --> 00:13:18,790
interesting tweets and which is pushing

00:13:16,189 --> 00:13:21,260
this twitch into the Postgres

00:13:18,790 --> 00:13:23,450
and so it generates some kind of a

00:13:21,260 --> 00:13:25,880
transaction log and then there is this

00:13:23,450 --> 00:13:28,310
debate IAM decode buff sitting on the

00:13:25,880 --> 00:13:31,100
post chris node which is kind of pushing

00:13:28,310 --> 00:13:34,190
this decoding this wall messages and

00:13:31,100 --> 00:13:37,030
pushing it into Kafka and then we have a

00:13:34,190 --> 00:13:40,670
sentiment analyzer app sitting there

00:13:37,030 --> 00:13:42,050
hooking into hooked into Kafka and you

00:13:40,670 --> 00:13:44,030
know kind of trying to do some kind of

00:13:42,050 --> 00:13:45,500
very basic sentiment analysis that's

00:13:44,030 --> 00:13:47,720
that's not the problem we are trying to

00:13:45,500 --> 00:13:50,630
solve here just wanted to say that you

00:13:47,720 --> 00:13:52,670
know you can hook up this kind of even

00:13:50,630 --> 00:13:55,220
apps or services as many as you want

00:13:52,670 --> 00:13:57,560
into this queue following this change or

00:13:55,220 --> 00:13:59,450
a capture pipeline that we have seen and

00:13:57,560 --> 00:14:03,710
then you can do some kind of an analysis

00:13:59,450 --> 00:14:08,200
on top so very basic but let's get

00:14:03,710 --> 00:14:08,200
started with the demo let's say

00:14:15,500 --> 00:14:24,230
okay let's make it slightly bigger here

00:14:24,410 --> 00:14:30,810
so if you see now I have already logged

00:14:27,690 --> 00:14:34,079
in the marketplace if you look at you

00:14:30,810 --> 00:14:37,110
see that there is a service called

00:14:34,079 --> 00:14:40,079
distributed service bundle and then it

00:14:37,110 --> 00:14:41,459
has a plan called standard there are

00:14:40,079 --> 00:14:44,670
other services as well for example

00:14:41,459 --> 00:14:47,700
MongoDB rabbitmq they are independent

00:14:44,670 --> 00:14:50,220
and individual services but this one is

00:14:47,700 --> 00:14:53,310
a composite service that we we are

00:14:50,220 --> 00:14:57,029
interested in now in this so what we do

00:14:53,310 --> 00:15:02,760
know is we try to create a service so CF

00:14:57,029 --> 00:15:07,350
create service and then the service name

00:15:02,760 --> 00:15:13,310
we need we need the plan and let's say

00:15:07,350 --> 00:15:16,310
that this is our CF summit bottle 17

00:15:13,310 --> 00:15:16,310
instance

00:15:25,200 --> 00:15:36,839
yes okay so I have to check on this but

00:15:33,779 --> 00:15:39,959
for the sake of time I think we have

00:15:36,839 --> 00:15:41,730
prepared an instance already maybe we

00:15:39,959 --> 00:15:43,829
can go ahead with that

00:15:41,730 --> 00:15:47,550
so this instance is already already

00:15:43,829 --> 00:15:50,639
there and you see that demo instance and

00:15:47,550 --> 00:15:52,260
this is of course you know this

00:15:50,639 --> 00:15:53,550
distributed service bundle this this

00:15:52,260 --> 00:15:56,070
particular service instance is created

00:15:53,550 --> 00:16:01,860
already for us so let's take a look at

00:15:56,070 --> 00:16:03,779
CF service and then maybe this demo

00:16:01,860 --> 00:16:06,720
instance let's see what details it has

00:16:03,779 --> 00:16:08,279
so it says that the service is this and

00:16:06,720 --> 00:16:11,760
from this the service instance is

00:16:08,279 --> 00:16:15,870
created and of course these are the

00:16:11,760 --> 00:16:18,620
services you have in this bundle right

00:16:15,870 --> 00:16:21,480
so this post grace Kafka and Redis and

00:16:18,620 --> 00:16:23,940
if you if you look at the composition

00:16:21,480 --> 00:16:27,320
now in this system we have just the post

00:16:23,940 --> 00:16:30,720
grace and Kafka so what we can do now is

00:16:27,320 --> 00:16:32,790
we can see where our application is and

00:16:30,720 --> 00:16:34,620
this is our application this basically

00:16:32,790 --> 00:16:36,690
this application shows the sentiment as

00:16:34,620 --> 00:16:38,360
I said the sentiment analysis and the

00:16:36,690 --> 00:16:41,790
other application which is interesting

00:16:38,360 --> 00:16:44,070
from the from the Twitter and that is

00:16:41,790 --> 00:16:45,750
already running so let's see that and

00:16:44,070 --> 00:16:48,690
this is looking into hooking into the

00:16:45,750 --> 00:16:52,079
hash the hashtag CF summit so I

00:16:48,690 --> 00:16:53,699
basically you know refresh this we see

00:16:52,079 --> 00:16:56,339
that there are fifteen hundred and forty

00:16:53,699 --> 00:16:59,610
two positive comments about it and there

00:16:56,339 --> 00:17:02,070
are 372 negative comments about coming

00:16:59,610 --> 00:17:04,049
in but this this analysis is not

00:17:02,070 --> 00:17:08,100
important I mean it is also sometimes

00:17:04,049 --> 00:17:09,870
giving you wrong result but we have

00:17:08,100 --> 00:17:11,640
basic sentiment analysis that is

00:17:09,870 --> 00:17:13,709
happening following the same change data

00:17:11,640 --> 00:17:17,220
capture and what is important here is

00:17:13,709 --> 00:17:21,419
that that we have kind of deployed this

00:17:17,220 --> 00:17:25,319
entire application together the services

00:17:21,419 --> 00:17:28,860
together so that it makes a service

00:17:25,319 --> 00:17:30,450
bundle now so let's let's see we said

00:17:28,860 --> 00:17:32,370
that you know this service we should be

00:17:30,450 --> 00:17:34,140
able to manage monitor it together right

00:17:32,370 --> 00:17:38,070
so we should be able to update it

00:17:34,140 --> 00:17:38,850
together right so what we can do is we

00:17:38,070 --> 00:17:41,220
can look at

00:17:38,850 --> 00:17:43,350
monitoring dashboard and you can see

00:17:41,220 --> 00:17:45,480
this is the monitoring dashboard where

00:17:43,350 --> 00:17:48,390
you get to see all the services together

00:17:45,480 --> 00:17:51,360
you see kafka you see post Chris and

00:17:48,390 --> 00:17:53,010
then you see basically so you see this

00:17:51,360 --> 00:17:55,650
is the Kafka system which is running

00:17:53,010 --> 00:17:57,360
inside there's a Kafka system which is

00:17:55,650 --> 00:17:59,190
running inside this is the

00:17:57,360 --> 00:18:00,450
post-christmas the two node post Chris

00:17:59,190 --> 00:18:02,580
server which is running inside so

00:18:00,450 --> 00:18:04,650
basically it's a composite that you are

00:18:02,580 --> 00:18:08,610
trying to look at how my composite is

00:18:04,650 --> 00:18:10,470
doing yeah so and you see the CPU system

00:18:08,610 --> 00:18:14,309
here the CPU usage likewise you can see

00:18:10,470 --> 00:18:17,179
the memory usage of individual systems

00:18:14,309 --> 00:18:20,340
here and yeah so the idea here is that

00:18:17,179 --> 00:18:22,230
can we now since it is in a POC state at

00:18:20,340 --> 00:18:24,240
this point in time we were thinking and

00:18:22,230 --> 00:18:26,940
I was interested in taking your opinion

00:18:24,240 --> 00:18:29,460
and fit back on do you think that it

00:18:26,940 --> 00:18:31,559
makes sense somehow to think in this in

00:18:29,460 --> 00:18:33,809
this line do you see that you have use

00:18:31,559 --> 00:18:35,600
cases around managing this kind of

00:18:33,809 --> 00:18:37,980
services together in your company

00:18:35,600 --> 00:18:41,190
somehow then we can of course take it

00:18:37,980 --> 00:18:43,080
forward you know can think of you know

00:18:41,190 --> 00:18:44,880
collaborate with you writing some kind

00:18:43,080 --> 00:18:48,659
of a standard specification of how the

00:18:44,880 --> 00:18:51,809
services can be can be managed together

00:18:48,659 --> 00:18:57,570
and then things like that so that that

00:18:51,809 --> 00:19:00,210
was basically the idea so yeah I think

00:18:57,570 --> 00:19:03,179
that was pretty much and I have I can

00:19:00,210 --> 00:19:07,159
show you a little more on if your if you

00:19:03,179 --> 00:19:09,390
are interested in for if for example

00:19:07,159 --> 00:19:12,780
let's say that this this particular

00:19:09,390 --> 00:19:14,909
service now has Kafka and post Chris

00:19:12,780 --> 00:19:17,039
running now like let's say that you want

00:19:14,909 --> 00:19:20,309
to bring in a machine-learning service

00:19:17,039 --> 00:19:22,080
which needs Reddy's as well so how do

00:19:20,309 --> 00:19:25,039
you bring it in it follows all the all

00:19:22,080 --> 00:19:29,549
the CF construct so it's say you say CF

00:19:25,039 --> 00:19:32,659
update service and then you you

00:19:29,549 --> 00:19:35,580
basically give the service name which is

00:19:32,659 --> 00:19:39,720
the service instance ID and then you

00:19:35,580 --> 00:19:44,220
pass on basically I I have I'll show you

00:19:39,720 --> 00:19:47,280
the file file here also so mate I pass a

00:19:44,220 --> 00:19:49,820
JSON I pass a JSON file so I think it

00:19:47,280 --> 00:19:56,680
has had radius

00:19:49,820 --> 00:19:56,680
Jason right yes I meant update service

00:19:59,560 --> 00:20:07,880
okay so this is this has started now if

00:20:03,260 --> 00:20:10,370
we say CF CF s and then you see that the

00:20:07,880 --> 00:20:11,720
updated is in progress so in a moment

00:20:10,370 --> 00:20:14,570
what you will see that the service

00:20:11,720 --> 00:20:18,470
composition which was made up of Kafka

00:20:14,570 --> 00:20:21,470
and Postgres before now will soon have a

00:20:18,470 --> 00:20:23,300
radius also inside so this way you can

00:20:21,470 --> 00:20:25,820
also remain flexible in kind of bringing

00:20:23,300 --> 00:20:27,620
in your new service inside and take the

00:20:25,820 --> 00:20:30,200
other one out and and their entire

00:20:27,620 --> 00:20:33,010
wearing and and everything else has been

00:20:30,200 --> 00:20:36,260
taken care of by service fabric

00:20:33,010 --> 00:20:40,250
extension that we have built here yeah

00:20:36,260 --> 00:20:42,020
so this is it pretty much that I wanted

00:20:40,250 --> 00:20:44,720
to show and talk about distributed

00:20:42,020 --> 00:20:47,270
service bundles if you have any question

00:20:44,720 --> 00:20:49,550
or if you have any suggestion on how

00:20:47,270 --> 00:20:51,680
should this be done because this is at a

00:20:49,550 --> 00:20:54,680
prototype stage at this point in time we

00:20:51,680 --> 00:20:56,510
are thinking of maybe it makes sense for

00:20:54,680 --> 00:20:59,570
larger companies who wants to manage a

00:20:56,510 --> 00:21:01,970
lot of let's say big data services for

00:20:59,570 --> 00:21:04,550
example together all of them SPARC maybe

00:21:01,970 --> 00:21:06,290
kiss I'll draw a calf car together maybe

00:21:04,550 --> 00:21:08,330
some other companies have different

00:21:06,290 --> 00:21:09,590
other change data capture scenarios

00:21:08,330 --> 00:21:11,900
variant they want to manage and maintain

00:21:09,590 --> 00:21:13,940
all the services together so just want

00:21:11,900 --> 00:21:15,860
to know from you if you if you have any

00:21:13,940 --> 00:21:18,260
such use case then maybe we can talk

00:21:15,860 --> 00:21:20,650
about it a little bit if this really

00:21:18,260 --> 00:21:20,650
makes sense

00:21:32,580 --> 00:21:37,390
yeah

00:21:33,820 --> 00:21:39,700
so basically this is managed by a

00:21:37,390 --> 00:21:41,830
service fabric and this is all sitting

00:21:39,700 --> 00:21:44,260
on the on the same subnet at this point

00:21:41,830 --> 00:21:47,800
in time so within the sub Lane I think

00:21:44,260 --> 00:21:49,960
the system's can reach each other and of

00:21:47,800 --> 00:21:52,450
course the credible is something that we

00:21:49,960 --> 00:21:53,800
can use going forward to do you know

00:21:52,450 --> 00:21:55,720
kind of keep the credentials together

00:21:53,800 --> 00:21:57,640
and then you know everybody has one

00:21:55,720 --> 00:21:59,230
place where they can go for the

00:21:57,640 --> 00:22:00,820
credentials and can pick up the

00:21:59,230 --> 00:22:17,550
credentials for the service that they

00:22:00,820 --> 00:22:22,030
want to basically consume yes of course

00:22:17,550 --> 00:22:25,180
so L stack means you have this this this

00:22:22,030 --> 00:22:27,190
this cabana and analog stash in all of

00:22:25,180 --> 00:22:29,320
this system so again these three systems

00:22:27,190 --> 00:22:29,980
are again disparate systems but they

00:22:29,320 --> 00:22:32,440
work together

00:22:29,980 --> 00:22:34,510
of course then yes it's it could be a

00:22:32,440 --> 00:22:36,790
nice use case that these three services

00:22:34,510 --> 00:22:38,650
are kind of put together but I guess

00:22:36,790 --> 00:22:42,160
that for elk you already have a Bosch

00:22:38,650 --> 00:22:44,350
release and this this you can then this

00:22:42,160 --> 00:22:46,300
Bosch release can be directly be

00:22:44,350 --> 00:22:47,860
provisioned directly on the on the

00:22:46,300 --> 00:22:48,400
system so you don't have to do any magic

00:22:47,860 --> 00:22:50,650
there

00:22:48,400 --> 00:22:53,020
I think this use case will fit more on a

00:22:50,650 --> 00:22:55,180
set of heterogeneous services that you

00:22:53,020 --> 00:22:56,830
may have in your company right for which

00:22:55,180 --> 00:23:00,130
there is probably no single Bosch

00:22:56,830 --> 00:23:02,020
release exists today right if you have a

00:23:00,130 --> 00:23:04,120
Bosch release take it just deployed

00:23:02,020 --> 00:23:07,720
using service fabric everything will

00:23:04,120 --> 00:23:09,430
work right but if it is kind of a

00:23:07,720 --> 00:23:12,730
heterogeneous a set of services that

00:23:09,430 --> 00:23:13,900
that you need to deploy and that is

00:23:12,730 --> 00:23:15,340
probably the pipeline you want to

00:23:13,900 --> 00:23:18,940
support within your company then bring

00:23:15,340 --> 00:23:20,800
them together as a composite and then

00:23:18,940 --> 00:23:24,010
kind of try to deploy this so they are

00:23:20,800 --> 00:23:26,260
all independent Bosch releases and then

00:23:24,010 --> 00:23:28,510
ya create your pipeline by deploying

00:23:26,260 --> 00:23:30,580
them a fabric will take care of kind of

00:23:28,510 --> 00:23:32,860
deploy them one after the other ensure

00:23:30,580 --> 00:23:35,080
that internally they are able to access

00:23:32,860 --> 00:23:36,240
each other and yeah they are kind of

00:23:35,080 --> 00:23:38,830
managed

00:23:36,240 --> 00:23:41,920
once you run the update then all the

00:23:38,830 --> 00:23:43,660
software's are kind of updated we can

00:23:41,920 --> 00:23:47,410
think of a scaling mechanism also going

00:23:43,660 --> 00:23:49,780
forward monitoring you you saw a very

00:23:47,410 --> 00:23:51,130
basically example we can also see that

00:23:49,780 --> 00:23:52,840
how this composite so we are not talking

00:23:51,130 --> 00:23:55,180
about one particular service now at this

00:23:52,840 --> 00:23:56,950
point in time but as a composite this

00:23:55,180 --> 00:24:00,520
service how is it working now is it

00:23:56,950 --> 00:24:04,030
healthy is the question right so we can

00:24:00,520 --> 00:24:07,480
work more towards such things so any

00:24:04,030 --> 00:24:09,990
opinion any feedback on if we should be

00:24:07,480 --> 00:24:09,990
using this

00:24:30,050 --> 00:24:33,170
[Music]

00:24:43,639 --> 00:24:49,710
yeah so there are different components

00:24:46,860 --> 00:24:51,149
involved one project which is very very

00:24:49,710 --> 00:24:55,499
interesting at this point in time is

00:24:51,149 --> 00:25:00,570
debits 'i'm from reddit which basically

00:24:55,499 --> 00:25:02,999
provides a lot of adapters onto the

00:25:00,570 --> 00:25:04,830
database that basically you can hook

00:25:02,999 --> 00:25:07,919
this up it is also there on my on my on

00:25:04,830 --> 00:25:09,539
my slide and this kind of they use some

00:25:07,919 --> 00:25:11,429
kind of it the code buff that basically

00:25:09,539 --> 00:25:14,009
because the wall files the right i had

00:25:11,429 --> 00:25:15,570
log so the commit logs are our binary

00:25:14,009 --> 00:25:17,429
files and they they are not understood

00:25:15,570 --> 00:25:19,799
outside so each database provider should

00:25:17,429 --> 00:25:21,720
provide some decoder that that would

00:25:19,799 --> 00:25:25,440
basically you know kind of can

00:25:21,720 --> 00:25:27,509
understand this and so this decode buffs

00:25:25,440 --> 00:25:30,840
they basically read this data from the

00:25:27,509 --> 00:25:33,360
wall the the transaction log and then

00:25:30,840 --> 00:25:36,960
kind of decoded and creates evens

00:25:33,360 --> 00:25:39,029
which is pushed into Kafka in this case

00:25:36,960 --> 00:25:40,679
and then all other services that you

00:25:39,029 --> 00:25:43,559
have in your composition kind of hooked

00:25:40,679 --> 00:25:45,840
up into Kafka and they just extract

00:25:43,559 --> 00:25:47,610
information out of Kafka and then do

00:25:45,840 --> 00:25:50,789
whatever they they want to do are they

00:25:47,610 --> 00:25:52,679
are meant for this domain for yeah and

00:25:50,789 --> 00:25:54,450
the rest of the glue lies with with the

00:25:52,679 --> 00:25:56,399
service fabric where you know this

00:25:54,450 --> 00:25:58,409
message passing and the credential

00:25:56,399 --> 00:26:01,909
exchange and all of this is kind of

00:25:58,409 --> 00:26:01,909
standby service fabric

00:26:06,090 --> 00:26:11,650
you can of course do that yes so you we

00:26:09,880 --> 00:26:13,960
don't want to lose that capability right

00:26:11,650 --> 00:26:15,460
because you want to update Kafka to the

00:26:13,960 --> 00:26:18,990
next version but not necessarily

00:26:15,460 --> 00:26:22,380
Postgres right but you should also be

00:26:18,990 --> 00:26:36,960
you know be able to update it together

00:26:22,380 --> 00:26:40,150
yeah right that's it yeah yep thank you

00:26:36,960 --> 00:26:42,370
thank you thanks thanks you thank you

00:26:40,150 --> 00:26:44,670
very much for your finish thank you

00:26:42,370 --> 00:26:44,670

YouTube URL: https://www.youtube.com/watch?v=L7UwtncfE4c


