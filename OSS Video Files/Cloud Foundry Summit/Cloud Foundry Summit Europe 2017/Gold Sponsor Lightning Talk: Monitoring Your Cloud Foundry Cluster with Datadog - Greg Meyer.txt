Title: Gold Sponsor Lightning Talk: Monitoring Your Cloud Foundry Cluster with Datadog - Greg Meyer
Publication date: 2017-10-18
Playlist: Cloud Foundry Summit Europe 2017
Description: 
	Gold Sponsor Lightning Talk: Monitoring Your Cloud Foundry Cluster with Datadog - Greg Meyer, Datadog

We rely on our monitoring to tell us when our services, Applications, or infrastructure diverge from “normal.” However, Orchestrators like Cloud Foundry and cloud providers have created a new world of dynamic infrastructure where normal is changing constantly, making it quite difficult to define. Join us as we share how Datadog's open source integrations with Cloud Foundry and help you see your cluster's health and performance.

About Greg Meyer
Greg is a Software Engineer at Datadog based in Brooklyn, NY. He primarily focuses Datadog's open source agent and its integrations. From orchestrators like Cloud Foundry to databases like Redis, Greg enjoys diving into and instrumenting new technologies.
Captions: 
	00:00:00,030 --> 00:00:05,759
so I'm gonna give you a quick talk on

00:00:02,250 --> 00:00:07,080
how data dog views monitoring and how we

00:00:05,759 --> 00:00:10,380
believe that monitoring should be done

00:00:07,080 --> 00:00:11,969
so just first introduce myself I'm Greg

00:00:10,380 --> 00:00:13,230
I'm an open source software engineer at

00:00:11,969 --> 00:00:15,480
data dog I work on our agent

00:00:13,230 --> 00:00:17,550
integrations team data dogs a SAS

00:00:15,480 --> 00:00:21,090
platform that processes trillions of

00:00:17,550 --> 00:00:24,510
data points per day so first off what is

00:00:21,090 --> 00:00:27,090
DevOps DevOps is about culture it's

00:00:24,510 --> 00:00:29,640
about automation metrics and sharing if

00:00:27,090 --> 00:00:31,410
you're raising a barn it doesn't take

00:00:29,640 --> 00:00:33,780
very much modern infrastructure or

00:00:31,410 --> 00:00:36,120
modern technology but if you approach it

00:00:33,780 --> 00:00:38,219
collaboratively from the culture side

00:00:36,120 --> 00:00:39,870
many hands can get the job done much

00:00:38,219 --> 00:00:41,730
quicker when you have everyone working

00:00:39,870 --> 00:00:44,309
together and once you identify the

00:00:41,730 --> 00:00:45,930
bottlenecks within your culture then you

00:00:44,309 --> 00:00:49,260
can start working as a well-oiled

00:00:45,930 --> 00:00:52,649
machine and as many of you know people

00:00:49,260 --> 00:00:54,750
are really our bottleneck you have to

00:00:52,649 --> 00:00:57,840
start with culture and that's the big

00:00:54,750 --> 00:00:59,850
thing and so first off culture

00:00:57,840 --> 00:01:03,149
automation and sharing are all great but

00:00:59,850 --> 00:01:04,650
what about metrics if you have no

00:01:03,149 --> 00:01:06,360
observability all you can do is wait for

00:01:04,650 --> 00:01:08,549
your users to tell you that something

00:01:06,360 --> 00:01:10,799
has gone wrong I actually I know big

00:01:08,549 --> 00:01:13,020
organizations that operate like this and

00:01:10,799 --> 00:01:14,430
I have friends who tell me that they

00:01:13,020 --> 00:01:15,780
have to wait for someone to tweet at

00:01:14,430 --> 00:01:17,159
them to tell them that something has

00:01:15,780 --> 00:01:20,100
gone horribly wrong and that their

00:01:17,159 --> 00:01:22,110
platform is down collecting data is very

00:01:20,100 --> 00:01:25,170
cheap and having a lot of data is very

00:01:22,110 --> 00:01:26,729
cheap but when you need it and not

00:01:25,170 --> 00:01:28,170
having it when you need it is very

00:01:26,729 --> 00:01:31,040
expensive so you should instrument

00:01:28,170 --> 00:01:34,170
everything and you can see this you know

00:01:31,040 --> 00:01:35,610
be a went down hugely they didn't know

00:01:34,170 --> 00:01:38,009
what was going on they were using weight

00:01:35,610 --> 00:01:39,240
boards to figure everything out and it

00:01:38,009 --> 00:01:40,770
took them a long time to be able to

00:01:39,240 --> 00:01:42,150
restore a service because they didn't

00:01:40,770 --> 00:01:44,820
have observability into their platform

00:01:42,150 --> 00:01:48,000
and so what are the qualities of good

00:01:44,820 --> 00:01:51,780
metrics it must be well understood so

00:01:48,000 --> 00:01:54,659
you can't be mixing say Imperial and

00:01:51,780 --> 00:01:56,460
metric units as they did in the Mars

00:01:54,659 --> 00:01:59,040
climate orbiter which caused it to crash

00:01:56,460 --> 00:02:02,040
down onto Mars it also has to have

00:01:59,040 --> 00:02:04,409
sufficient granularity if you only have

00:02:02,040 --> 00:02:05,939
one second granularity all of these are

00:02:04,409 --> 00:02:07,259
the same so you must have better

00:02:05,939 --> 00:02:09,720
granularity than that in order to

00:02:07,259 --> 00:02:12,690
determine what's going on and so you can

00:02:09,720 --> 00:02:13,890
see here one second peak of five-minute

00:02:12,690 --> 00:02:15,510
peak in a one minute

00:02:13,890 --> 00:02:17,640
all very different when you when you

00:02:15,510 --> 00:02:20,010
have different granularity and so how

00:02:17,640 --> 00:02:21,090
granular is your granularity as you're

00:02:20,010 --> 00:02:23,100
only has one minute

00:02:21,090 --> 00:02:24,990
AWS only has one minute google

00:02:23,100 --> 00:02:28,980
stackdriver only has one minute we offer

00:02:24,990 --> 00:02:31,410
you 20 second granularity up to 15

00:02:28,980 --> 00:02:33,690
months at full granularity and so you

00:02:31,410 --> 00:02:35,610
can really store that data it must also

00:02:33,690 --> 00:02:38,310
be taggable and filterable so you can

00:02:35,610 --> 00:02:40,920
see you can be able to to slice it down

00:02:38,310 --> 00:02:45,120
by region by Bosch job or by Bosch

00:02:40,920 --> 00:02:47,850
deployment so you can be able to

00:02:45,120 --> 00:02:51,300
actually slice the data up by each piece

00:02:47,850 --> 00:02:53,190
of that data and if you can query on a

00:02:51,300 --> 00:02:55,200
different and if you can define a query

00:02:53,190 --> 00:02:58,230
you can monitor based upon those tags

00:02:55,200 --> 00:03:01,050
and based upon that filtering it must

00:02:58,230 --> 00:03:02,459
also be long-lived what happened lat you

00:03:01,050 --> 00:03:04,470
must be able to know what happened last

00:03:02,459 --> 00:03:07,110
week or last month in order to define

00:03:04,470 --> 00:03:08,700
trends and so when you have enough

00:03:07,110 --> 00:03:10,470
metrics for a long enough period of time

00:03:08,700 --> 00:03:12,330
you can be able to define these trends

00:03:10,470 --> 00:03:14,010
and you can see this here I'm at I'm not

00:03:12,330 --> 00:03:16,140
sure if any of you recognize the pattern

00:03:14,010 --> 00:03:18,840
here that's going on this is a weekly

00:03:16,140 --> 00:03:19,709
pattern and so you can see dips on

00:03:18,840 --> 00:03:23,130
weekends

00:03:19,709 --> 00:03:25,980
but on every weekday except for that one

00:03:23,130 --> 00:03:28,110
that Tuesday they have increasing

00:03:25,980 --> 00:03:30,120
numbers of requests and so what happened

00:03:28,110 --> 00:03:31,980
on that Tuesday was it an outage was it

00:03:30,120 --> 00:03:33,239
a holiday you need to be able to figure

00:03:31,980 --> 00:03:34,560
it out you know in order to figure that

00:03:33,239 --> 00:03:36,480
out you need to be able to keep the data

00:03:34,560 --> 00:03:37,620
long enough in order to analyze those

00:03:36,480 --> 00:03:40,980
trends what you're seeing here that

00:03:37,620 --> 00:03:43,530
shaded area is actually we are using

00:03:40,980 --> 00:03:45,600
machine learning to figure out how that

00:03:43,530 --> 00:03:48,450
data fits together and the patterns

00:03:45,600 --> 00:03:50,459
within that data so how long do you keep

00:03:48,450 --> 00:03:53,790
how long is data kept by the big cloud

00:03:50,459 --> 00:03:55,890
providers not very pretty long but not

00:03:53,790 --> 00:03:57,390
very long at a sufficient granularity

00:03:55,890 --> 00:03:59,430
which is why a lot of people turn to

00:03:57,390 --> 00:04:02,730
data dog we keep it up full granularity

00:03:59,430 --> 00:04:04,110
for 15 months what type of metrics is

00:04:02,730 --> 00:04:05,370
important as wells there's a few

00:04:04,110 --> 00:04:07,079
different types of metrics there's work

00:04:05,370 --> 00:04:09,660
metrics there's resource metrics and

00:04:07,079 --> 00:04:11,549
there's events work metrics are things

00:04:09,660 --> 00:04:12,989
like throughput success error or

00:04:11,549 --> 00:04:14,340
performance if you're in a donut shop

00:04:12,989 --> 00:04:16,709
it's the number of donuts that you're

00:04:14,340 --> 00:04:19,410
selling if you're serving metrics it's

00:04:16,709 --> 00:04:21,269
perhaps you know the number of if you're

00:04:19,410 --> 00:04:22,830
serving requests is the number of

00:04:21,269 --> 00:04:24,300
requests that you're serving the number

00:04:22,830 --> 00:04:25,410
of errors that you're serving the

00:04:24,300 --> 00:04:27,360
throughput

00:04:25,410 --> 00:04:29,970
but then there's also resource metrics

00:04:27,360 --> 00:04:31,470
things like utilization if you're doing

00:04:29,970 --> 00:04:33,570
a doughnut if you have a doughnut to

00:04:31,470 --> 00:04:34,860
turn to the donut analogy again it's the

00:04:33,570 --> 00:04:35,850
amount of sugar that you're using in the

00:04:34,860 --> 00:04:37,230
doughnuts and you need to be able to

00:04:35,850 --> 00:04:38,970
figure out how much sugar that is and

00:04:37,230 --> 00:04:40,980
finally there's events there's things

00:04:38,970 --> 00:04:42,720
like national donut day which demands an

00:04:40,980 --> 00:04:44,040
enormous number more donuts are going to

00:04:42,720 --> 00:04:46,140
be sold that day so you need to know

00:04:44,040 --> 00:04:51,540
when those things happen and discrete

00:04:46,140 --> 00:04:53,820
events are important so yeah and so we

00:04:51,540 --> 00:04:56,190
like to overlay our graphs with events

00:04:53,820 --> 00:04:58,830
in order to determine what happens so

00:04:56,190 --> 00:05:02,130
you look at these different types of

00:04:58,830 --> 00:05:03,930
events and you you recurs until you find

00:05:02,130 --> 00:05:05,940
the technical causes and you want to be

00:05:03,930 --> 00:05:07,200
able to put metrics to work we want to

00:05:05,940 --> 00:05:09,000
be able to put metrics to work so we can

00:05:07,200 --> 00:05:11,310
figure out exactly what's going on and

00:05:09,000 --> 00:05:13,050
we figure out what to page on and so you

00:05:11,310 --> 00:05:15,540
know who to alert when different things

00:05:13,050 --> 00:05:16,920
are happening and you want those alerts

00:05:15,540 --> 00:05:18,630
to be actionable you don't want them to

00:05:16,920 --> 00:05:19,950
be cryptic and nonsensical so you want

00:05:18,630 --> 00:05:21,870
to be able to make actionable alerts

00:05:19,950 --> 00:05:25,200
that tell you what's going on and what

00:05:21,870 --> 00:05:26,730
you need to do so we want metrics that

00:05:25,200 --> 00:05:28,050
are well understood that are taggle and

00:05:26,730 --> 00:05:30,180
foldable that are long-lived net of

00:05:28,050 --> 00:05:31,350
sufficient granularity we want metrics

00:05:30,180 --> 00:05:33,240
that are well understood we want to

00:05:31,350 --> 00:05:34,830
alert on the appropriate metrics and

00:05:33,240 --> 00:05:37,440
make your alerts actionable and if it

00:05:34,830 --> 00:05:40,280
has any questions my time is done but

00:05:37,440 --> 00:05:40,280

YouTube URL: https://www.youtube.com/watch?v=JUD2_aHjojc


