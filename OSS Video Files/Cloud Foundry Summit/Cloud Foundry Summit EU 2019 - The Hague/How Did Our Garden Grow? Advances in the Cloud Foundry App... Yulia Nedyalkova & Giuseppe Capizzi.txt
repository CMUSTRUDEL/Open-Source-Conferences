Title: How Did Our Garden Grow? Advances in the Cloud Foundry App... Yulia Nedyalkova & Giuseppe Capizzi
Publication date: 2019-09-13
Playlist: Cloud Foundry Summit EU 2019 - The Hague
Description: 
	How Did Our Garden Grow? Advances in the Cloud Foundry App Runtime's Container Engine - Yulia Nedyalkova, SAP & Giuseppe Capizzi, Pivotal 

Garden (the Cloud Foundry App Runtime’s container engine, optimised for PaaS workloads) saw awesome features bloom over the last year. This talk will help operators understand and leverage new security features such as rootless, and will introduce developers to the new and long-requested CPU Entitlements feature. It will also give a sneak peek into Garden’s latest track of work - productising CF’s multi-tenancy security defaults for K8S users.  Garden is the first production runtime to support rootless mode; a wonderful security benefit. Leveraging containerd, Garden now uses more industry standards than ever, allowing for a much more familiar operator experience. New container CPU metrics enable operators and tooling to make better decisions about scaling. The next step will be to expose our cool features and defaults to K8S so we can all take advantage of them. 

For more info: https://www.cloudfoundry.org/
Captions: 
	00:00:00,500 --> 00:00:08,069
hello everyone thanks for coming

00:00:04,110 --> 00:00:11,099
I am Giuseppe this is this is Julia we

00:00:08,069 --> 00:00:14,280
work on the garden team and we're going

00:00:11,099 --> 00:00:16,199
to be talking about what's new in garden

00:00:14,280 --> 00:00:18,480
land what we've been doing the last few

00:00:16,199 --> 00:00:21,840
months

00:00:18,480 --> 00:00:24,150
first of all let's go through what

00:00:21,840 --> 00:00:26,099
garden is so garden is the containers

00:00:24,150 --> 00:00:29,250
engine that powers Cloud Foundry but

00:00:26,099 --> 00:00:30,980
also concourse and Bosch light so if

00:00:29,250 --> 00:00:34,680
you're wondering what our container is

00:00:30,980 --> 00:00:36,930
then let's look at that so

00:00:34,680 --> 00:00:40,260
containers are effectively just a way to

00:00:36,930 --> 00:00:42,899
distribute applications the reason we

00:00:40,260 --> 00:00:45,480
like them so much is that they offers us

00:00:42,899 --> 00:00:48,899
they offer us encapsulation of our

00:00:45,480 --> 00:00:50,460
dependencies and also isolation from any

00:00:48,899 --> 00:00:54,300
other thing that might be running on the

00:00:50,460 --> 00:00:56,730
same machine how do we do this well

00:00:54,300 --> 00:00:58,469
effectively containers are on Linux at

00:00:56,730 --> 00:01:01,170
least are nothing more than processes

00:00:58,469 --> 00:01:02,850
that have been isolated from all the

00:01:01,170 --> 00:01:05,189
other processes on the system through

00:01:02,850 --> 00:01:07,080
some primitives that the kernel gives us

00:01:05,189 --> 00:01:09,990
and in particular we're talking about

00:01:07,080 --> 00:01:12,659
namespaces which basically allow us to

00:01:09,990 --> 00:01:15,090
limit what a process can see on the

00:01:12,659 --> 00:01:17,130
system and then see groups which allow

00:01:15,090 --> 00:01:20,909
us to limit what the container what the

00:01:17,130 --> 00:01:23,670
process can do on the system and we've

00:01:20,909 --> 00:01:25,710
been leveraging these primitives on

00:01:23,670 --> 00:01:27,930
cloud foundry for for quite a long time

00:01:25,710 --> 00:01:29,630
now since the beginning actually and we

00:01:27,930 --> 00:01:32,070
had this container engine called worden

00:01:29,630 --> 00:01:33,659
which was very teeny Ruby actually and

00:01:32,070 --> 00:01:39,420
then we rewrote it and go we renamed it

00:01:33,659 --> 00:01:43,049
garden and this was was out great until

00:01:39,420 --> 00:01:46,170
in 2015 dr. was released and it was like

00:01:43,049 --> 00:01:48,119
this massive revolution Dokken wasn't

00:01:46,170 --> 00:01:49,619
doing anything special that they were

00:01:48,119 --> 00:01:51,390
using exactly the same kind of features

00:01:49,619 --> 00:01:53,280
that we were using but what I did was

00:01:51,390 --> 00:01:55,170
they provided this amazing user

00:01:53,280 --> 00:01:57,299
experience on top of them which meant

00:01:55,170 --> 00:01:59,820
like every every developer was able to

00:01:57,299 --> 00:02:02,310
just manage their own containers very

00:01:59,820 --> 00:02:04,110
very easily and they got so popular that

00:02:02,310 --> 00:02:06,540
basically they became like a de facto

00:02:04,110 --> 00:02:08,190
standard like people started that it

00:02:06,540 --> 00:02:09,569
kind of it was kind of a synonym of

00:02:08,190 --> 00:02:12,090
containers and when you said docker

00:02:09,569 --> 00:02:15,300
it what most people meant just come

00:02:12,090 --> 00:02:17,069
and it got so popular that we were like

00:02:15,300 --> 00:02:19,830
maybe maybe we should start using this

00:02:17,069 --> 00:02:23,790
in Cloud Foundry as well and we tried to

00:02:19,830 --> 00:02:25,800
integrate it but the truth is doctor was

00:02:23,790 --> 00:02:28,020
never standard dr. Muraki was never

00:02:25,800 --> 00:02:30,330
conceived to be a reusable piece of

00:02:28,020 --> 00:02:34,140
software it was always like this very

00:02:30,330 --> 00:02:36,690
monolithic system and not very modular

00:02:34,140 --> 00:02:37,590
so like in the end when we tried to

00:02:36,690 --> 00:02:41,250
integrate this

00:02:37,590 --> 00:02:44,310
it didn't really work very well luckily

00:02:41,250 --> 00:02:48,720
things changed a couple of years later

00:02:44,310 --> 00:02:51,299
when basically Doc plus another bunch of

00:02:48,720 --> 00:02:54,750
companies joined forces and they created

00:02:51,299 --> 00:02:56,970
the open container initiative and what

00:02:54,750 --> 00:02:59,280
they did was they they basically they

00:02:56,970 --> 00:03:02,220
came up with a standard a really

00:02:59,280 --> 00:03:05,459
standard format for containers and they

00:03:02,220 --> 00:03:07,709
also released a reference implementation

00:03:05,459 --> 00:03:09,810
for that for that format for that

00:03:07,709 --> 00:03:12,090
specification which is called R and C

00:03:09,810 --> 00:03:14,630
and recei is like everything that docker

00:03:12,090 --> 00:03:17,160
is not it's like this very tiny binary

00:03:14,630 --> 00:03:19,890
super reusable it just takes this pill

00:03:17,160 --> 00:03:23,549
once back in anyone's the container

00:03:19,890 --> 00:03:25,859
described by the spec so this was very

00:03:23,549 --> 00:03:29,880
very easy for us to integrate so we did

00:03:25,859 --> 00:03:32,819
it immediately and later on so once he

00:03:29,880 --> 00:03:34,829
was extracted from docker container ID

00:03:32,819 --> 00:03:37,260
was released from dr. as well and the

00:03:34,829 --> 00:03:39,750
continuity is this demon that runs on

00:03:37,260 --> 00:03:41,850
top of run C and had a bunch of features

00:03:39,750 --> 00:03:45,170
on top of it and once that was released

00:03:41,850 --> 00:03:48,959
we also started to integrate with that

00:03:45,170 --> 00:03:51,709
so this means that garden stopped being

00:03:48,959 --> 00:03:54,090
this like full stack containerization

00:03:51,709 --> 00:03:56,639
solution instead it became basically

00:03:54,090 --> 00:03:58,079
just just a layer on top of these

00:03:56,639 --> 00:04:01,380
well-established containerization

00:03:58,079 --> 00:04:03,209
technologies and acts as as an

00:04:01,380 --> 00:04:06,290
abstraction an abstraction that we're

00:04:03,209 --> 00:04:09,150
trying to make like as thin as possible

00:04:06,290 --> 00:04:12,060
but still very valuable because

00:04:09,150 --> 00:04:14,910
basically we are abstract in the rest of

00:04:12,060 --> 00:04:17,100
the platform from all the changes that

00:04:14,910 --> 00:04:20,310
are happening in the world of containers

00:04:17,100 --> 00:04:22,560
so every time we we start supporting a

00:04:20,310 --> 00:04:23,640
new continent technology we are the only

00:04:22,560 --> 00:04:25,350
ones that have to make changes basically

00:04:23,640 --> 00:04:28,530
the rest of Cloud Foundry can

00:04:25,350 --> 00:04:30,120
keep working as usual and another good

00:04:28,530 --> 00:04:32,850
thing is that we also have an

00:04:30,120 --> 00:04:35,400
opportunity at the garden level to apply

00:04:32,850 --> 00:04:38,430
our own security standards our security

00:04:35,400 --> 00:04:40,860
defaults and this is what we do we

00:04:38,430 --> 00:04:42,770
basically try to turn on as many

00:04:40,860 --> 00:04:45,480
security features as we can by default

00:04:42,770 --> 00:04:47,460
this is there's a list of some of the

00:04:45,480 --> 00:04:49,980
ones that we do turn on by default and

00:04:47,460 --> 00:04:52,860
this allows us to get our containers as

00:04:49,980 --> 00:04:57,630
soon as possible regardless of what we

00:04:52,860 --> 00:04:58,830
use to run them all right over to you

00:04:57,630 --> 00:05:01,860
let's talk about what we've actually

00:04:58,830 --> 00:05:04,140
been doing so now you know what garden

00:05:01,860 --> 00:05:06,360
is but how did our garden grow in the

00:05:04,140 --> 00:05:08,160
last few months so we spent most of our

00:05:06,360 --> 00:05:10,290
time working on three main tracks and

00:05:08,160 --> 00:05:11,790
those are CPU metrics container D and

00:05:10,290 --> 00:05:14,190
rootless and I'm gonna go through the

00:05:11,790 --> 00:05:15,870
first one so CPU metrics you're probably

00:05:14,190 --> 00:05:18,480
wondering Cloud Foundry already has a

00:05:15,870 --> 00:05:20,550
CPU metrics so when I do CF app I see

00:05:18,480 --> 00:05:22,290
some numbers why do we need a new one

00:05:20,550 --> 00:05:24,570
and you're kind of right but it has

00:05:22,290 --> 00:05:26,790
three main problems so the first one the

00:05:24,570 --> 00:05:28,380
metric is really confusing so let's say

00:05:26,790 --> 00:05:31,980
you push an application to Cloud Foundry

00:05:28,380 --> 00:05:33,720
and it's using 20% of the CPU then you

00:05:31,980 --> 00:05:35,280
push the exact same instance of the

00:05:33,720 --> 00:05:37,980
application doing the exact same amount

00:05:35,280 --> 00:05:40,260
of work and all of a sudden you see 200%

00:05:37,980 --> 00:05:42,180
so how is that even possible

00:05:40,260 --> 00:05:46,050
so the current metric has a few problems

00:05:42,180 --> 00:05:48,210
and one of the main problem is that it

00:05:46,050 --> 00:05:51,000
depends on factors that are not visible

00:05:48,210 --> 00:05:52,830
by the user for example what how big the

00:05:51,000 --> 00:05:54,900
Diego cell is and are there other

00:05:52,830 --> 00:05:57,870
application running on the same ta go

00:05:54,900 --> 00:05:59,850
cell so that means that it's super hard

00:05:57,870 --> 00:06:01,830
for users to understand how much CPU

00:05:59,850 --> 00:06:04,530
their application actually needs and

00:06:01,830 --> 00:06:06,450
consumes right now and when they need to

00:06:04,530 --> 00:06:09,420
scale and it also makes auto scaling

00:06:06,450 --> 00:06:11,870
just impossible the other problem is

00:06:09,420 --> 00:06:14,550
that high CPU applications can currently

00:06:11,870 --> 00:06:16,620
negatively affect normal applications or

00:06:14,550 --> 00:06:18,180
low CPU applications it's just like an

00:06:16,620 --> 00:06:19,740
example so let's say you have a Diego

00:06:18,180 --> 00:06:22,290
sale and you have five applications

00:06:19,740 --> 00:06:24,990
running on it and they're saying all of

00:06:22,290 --> 00:06:27,180
the same size so all of them will get

00:06:24,990 --> 00:06:29,370
their fair share of one-fifth of the

00:06:27,180 --> 00:06:31,500
whole CPU or around one-fifth of the

00:06:29,370 --> 00:06:33,300
host CPU that doesn't mean that you want

00:06:31,500 --> 00:06:35,280
each of the applications won't be

00:06:33,300 --> 00:06:37,110
allowed to spike over their fair share

00:06:35,280 --> 00:06:38,380
they will if there are some idle

00:06:37,110 --> 00:06:40,510
resources on the machine

00:06:38,380 --> 00:06:42,370
or for example if the other applications

00:06:40,510 --> 00:06:45,160
are just using less than their fare

00:06:42,370 --> 00:06:47,530
sheep CPU but what will happen if you

00:06:45,160 --> 00:06:49,600
end up on a Diego cell where all of the

00:06:47,530 --> 00:06:52,030
applications are consuming as much CPU

00:06:49,600 --> 00:06:54,220
as they could possibly get that means

00:06:52,030 --> 00:06:56,800
that you won't be able to spike even for

00:06:54,220 --> 00:06:59,320
one second over your fair share which

00:06:56,800 --> 00:07:01,030
leads to poor user experience for

00:06:59,320 --> 00:07:03,100
example on startup because a lot of

00:07:01,030 --> 00:07:06,670
applications just need to temporarily

00:07:03,100 --> 00:07:08,890
spike on startup and the third problem

00:07:06,670 --> 00:07:10,750
is how operators actually solve that

00:07:08,890 --> 00:07:13,060
problem so they want to create this

00:07:10,750 --> 00:07:15,730
perfect world where all the applications

00:07:13,060 --> 00:07:18,250
can use their spiked CPU so they just

00:07:15,730 --> 00:07:20,830
over commit a lot of CPU that users

00:07:18,250 --> 00:07:24,190
don't pay for and it just stays idle for

00:07:20,830 --> 00:07:26,170
most of the time so the thing that we

00:07:24,190 --> 00:07:28,570
want to achieve we want to make the

00:07:26,170 --> 00:07:30,370
metric meaningful so when users see the

00:07:28,570 --> 00:07:33,430
number it should actually give them some

00:07:30,370 --> 00:07:35,770
value so if the percentage should stay

00:07:33,430 --> 00:07:38,680
unaffected by outside factors that the

00:07:35,770 --> 00:07:39,940
users has no way of knowing and the

00:07:38,680 --> 00:07:42,520
other thing is the percentage should

00:07:39,940 --> 00:07:44,590
only change if your is an instance it's

00:07:42,520 --> 00:07:46,000
actually using more or less CPU so if

00:07:44,590 --> 00:07:47,860
they use it changes it shouldn't be

00:07:46,000 --> 00:07:49,930
affected by outside factors the other

00:07:47,860 --> 00:07:52,930
thing bad applications should not

00:07:49,930 --> 00:07:54,820
disadvantage normal apps and the third

00:07:52,930 --> 00:07:56,500
thing we should make life easier for

00:07:54,820 --> 00:07:58,480
operators so they should feel

00:07:56,500 --> 00:08:00,430
comfortable to provision enough CPU on

00:07:58,480 --> 00:08:02,170
the machine without having to over

00:08:00,430 --> 00:08:04,180
provision the amount of CPU so that all

00:08:02,170 --> 00:08:07,750
of the applications can use their spike

00:08:04,180 --> 00:08:09,670
CPU all the time so what do we do we

00:08:07,750 --> 00:08:11,890
introduce a thing which is called CPU

00:08:09,670 --> 00:08:14,680
entitlement so that is more or less the

00:08:11,890 --> 00:08:18,490
amount of CPU that your application is

00:08:14,680 --> 00:08:20,320
allowed to get relative to so it's being

00:08:18,490 --> 00:08:22,090
controlled by the instance size so it's

00:08:20,320 --> 00:08:24,190
currently relative to the memory so for

00:08:22,090 --> 00:08:26,620
example a two gigabyte application will

00:08:24,190 --> 00:08:28,930
currently get twice as much CPU time as

00:08:26,620 --> 00:08:32,260
a one gigabyte application and there's

00:08:28,930 --> 00:08:33,280
also a mapping from memory to CPU that's

00:08:32,260 --> 00:08:35,620
currently being controlled through a

00:08:33,280 --> 00:08:38,410
Bosh property we try to write a document

00:08:35,620 --> 00:08:40,599
which describes that in details so that

00:08:38,410 --> 00:08:42,640
operators can easily find the right

00:08:40,599 --> 00:08:45,100
value for them depending on the fact if

00:08:42,640 --> 00:08:47,830
they want to under or over provision CPU

00:08:45,100 --> 00:08:49,900
and we also set a default to that value

00:08:47,830 --> 00:08:51,520
which just creates a system where all of

00:08:49,900 --> 00:08:52,360
the applications are guaranteed to

00:08:51,520 --> 00:08:56,170
always get

00:08:52,360 --> 00:08:58,240
at least their entitlement so we split

00:08:56,170 --> 00:09:00,730
implementation into three main

00:08:58,240 --> 00:09:03,310
milestones in the first one is the

00:09:00,730 --> 00:09:05,980
meaningful metric so we created a new

00:09:03,310 --> 00:09:08,860
CPU metric which is relative to the

00:09:05,980 --> 00:09:11,140
entitlement so that means that if you

00:09:08,860 --> 00:09:13,630
see 100% that means currently you're

00:09:11,140 --> 00:09:16,660
using all of the CPU that your instance

00:09:13,630 --> 00:09:19,120
size allows you to use right now if you

00:09:16,660 --> 00:09:20,950
see 105 that means ok currently there

00:09:19,120 --> 00:09:22,660
are some spare CPU resources on the

00:09:20,950 --> 00:09:24,190
machine and you're allowed to use them

00:09:22,660 --> 00:09:27,040
in this moment but you're not guaranteed

00:09:24,190 --> 00:09:29,529
to get them in future so the reason we

00:09:27,040 --> 00:09:31,269
didn't change the old metric is because

00:09:29,529 --> 00:09:33,519
the behavior is really different and we

00:09:31,269 --> 00:09:36,070
wanted to give users the time to adapt

00:09:33,519 --> 00:09:37,240
to the new metric so we just wrote a

00:09:36,070 --> 00:09:39,339
plug-in which is called the CPU

00:09:37,240 --> 00:09:41,589
entitlement plug-in and that exports the

00:09:39,339 --> 00:09:42,940
new metric so let's just look at an

00:09:41,589 --> 00:09:44,649
example to see the difference between

00:09:42,940 --> 00:09:45,880
the old and the new metric so let's say

00:09:44,649 --> 00:09:47,709
you have an application that's running

00:09:45,880 --> 00:09:50,829
on the diego's oh and it's constantly

00:09:47,709 --> 00:09:52,630
using around 30% of the host CPU so for

00:09:50,829 --> 00:09:54,459
the old metric it doesn't really matter

00:09:52,630 --> 00:09:56,230
what your fair share is or how much CPU

00:09:54,459 --> 00:09:58,839
you kind of allowed to use it will

00:09:56,230 --> 00:10:00,459
always show you 30% for the new metric

00:09:58,839 --> 00:10:02,649
it does make a difference so if you're

00:10:00,459 --> 00:10:05,320
using 30 and you're entitled to use 30

00:10:02,649 --> 00:10:08,290
use you 100% if you're using 30 out of

00:10:05,320 --> 00:10:10,420
60 then it will show you 50% because for

00:10:08,290 --> 00:10:11,410
us we think that it's a big difference

00:10:10,420 --> 00:10:13,870
if you're using all of the resources

00:10:11,410 --> 00:10:16,120
that you're paying for or are like the

00:10:13,870 --> 00:10:18,300
resources just being idle all of the

00:10:16,120 --> 00:10:20,980
time and you're just paying for nothing

00:10:18,300 --> 00:10:23,110
the second milestone was the operators

00:10:20,980 --> 00:10:25,000
tooling so we wanted to create some

00:10:23,110 --> 00:10:28,510
tools that will allow users and

00:10:25,000 --> 00:10:31,930
operators to track the average CPU usage

00:10:28,510 --> 00:10:33,940
compared to the entitlement over time so

00:10:31,930 --> 00:10:36,339
at this step nothing will happen and

00:10:33,940 --> 00:10:39,220
nothing will change regarding the how we

00:10:36,339 --> 00:10:41,230
give CPU time to applications and

00:10:39,220 --> 00:10:43,690
containers we will just observe how the

00:10:41,230 --> 00:10:45,130
system behave and try to get some

00:10:43,690 --> 00:10:47,170
feedback from operators if we're

00:10:45,130 --> 00:10:49,329
actually going to do the right thing so

00:10:47,170 --> 00:10:51,579
we created the CPU entitlement plug-in

00:10:49,329 --> 00:10:52,810
as I said and currently it has a few

00:10:51,579 --> 00:10:54,610
commands so let's say you have an

00:10:52,810 --> 00:10:57,430
application which is called I'm spikey

00:10:54,610 --> 00:10:59,410
if you do CF CPU I'm spikey you see that

00:10:57,430 --> 00:11:01,270
you have one instance and you can see

00:10:59,410 --> 00:11:03,760
the average and the current usage so the

00:11:01,270 --> 00:11:05,829
current usage is in this second what

00:11:03,760 --> 00:11:06,279
amount of CPU you using relative to your

00:11:05,829 --> 00:11:08,949
and

00:11:06,279 --> 00:11:12,430
settlement and the average one is so

00:11:08,949 --> 00:11:14,410
average from the creation of the

00:11:12,430 --> 00:11:17,649
container in our case or in the last

00:11:14,410 --> 00:11:19,660
period so if your behavior changes at

00:11:17,649 --> 00:11:20,709
some point you've ever to see that

00:11:19,660 --> 00:11:22,180
really quickly in the current behavior

00:11:20,709 --> 00:11:25,209
and then it would just start

00:11:22,180 --> 00:11:27,040
accumulating in the average use it the

00:11:25,209 --> 00:11:29,110
thing that matters for us and the future

00:11:27,040 --> 00:11:32,620
throttling so the third milestone is the

00:11:29,110 --> 00:11:34,389
average usage and so we also have a few

00:11:32,620 --> 00:11:36,370
more things that we added to the plugin

00:11:34,389 --> 00:11:39,189
so if you're near the entitlement in our

00:11:36,370 --> 00:11:41,379
case it's like 95 percent that means

00:11:39,189 --> 00:11:44,139
that if you continue using that much CPU

00:11:41,379 --> 00:11:45,430
or a bit more at some point something

00:11:44,139 --> 00:11:47,319
could happen for example you'll get

00:11:45,430 --> 00:11:49,149
throttled the other thing is when you're

00:11:47,319 --> 00:11:51,309
over entitlement because it should be

00:11:49,149 --> 00:11:54,279
visible to users if they're using more

00:11:51,309 --> 00:11:57,459
resources than they depend on and the

00:11:54,279 --> 00:12:00,069
other thing is the yeah the average use

00:11:57,459 --> 00:12:02,410
so it's my average usage over the

00:12:00,069 --> 00:12:04,779
entitlement and this is going to tell

00:12:02,410 --> 00:12:06,790
you if you're inside a virtue which was

00:12:04,779 --> 00:12:08,999
ever over the entitlement so not right

00:12:06,790 --> 00:12:11,379
now but at some point in in the past

00:12:08,999 --> 00:12:13,689
there's also one more command which is

00:12:11,379 --> 00:12:15,970
over entitlement entitled instances

00:12:13,689 --> 00:12:18,040
which for each organization it's gonna

00:12:15,970 --> 00:12:20,559
tell you all of the applications that

00:12:18,040 --> 00:12:22,360
are currently over their entitlement

00:12:20,559 --> 00:12:24,129
that's not full so it's fully

00:12:22,360 --> 00:12:25,839
implemented but we're going to change

00:12:24,129 --> 00:12:27,579
this because we want to we're trying to

00:12:25,839 --> 00:12:30,000
gather feedback from operators right now

00:12:27,579 --> 00:12:32,769
what will be useful for them as behavior

00:12:30,000 --> 00:12:36,399
so if you have any ideas feel free to

00:12:32,769 --> 00:12:39,189
share them with us after the lot

00:12:36,399 --> 00:12:41,470
so the third milestone is the throttling

00:12:39,189 --> 00:12:43,870
one so here things will start to really

00:12:41,470 --> 00:12:45,550
change on the system we haven't started

00:12:43,870 --> 00:12:47,649
implementing this but we spiked it and

00:12:45,550 --> 00:12:49,300
the plan is the following so we'll have

00:12:47,649 --> 00:12:51,759
two groups of applications

00:12:49,300 --> 00:12:53,470
good ones and bad ones the good ones are

00:12:51,759 --> 00:12:55,329
the ones have been that have been below

00:12:53,470 --> 00:12:57,879
their entitlement over the last period

00:12:55,329 --> 00:12:59,800
and the bad ones the one that just used

00:12:57,879 --> 00:13:02,860
more resources and they should have

00:12:59,800 --> 00:13:05,559
should have been used in the last period

00:13:02,860 --> 00:13:08,740
so since creation all applications are

00:13:05,559 --> 00:13:11,860
good by default if your average usage

00:13:08,740 --> 00:13:14,019
goes over 100% you're being moved to the

00:13:11,860 --> 00:13:16,449
bad group and when it goes back under

00:13:14,019 --> 00:13:18,129
100% it will be a good application again

00:13:16,449 --> 00:13:18,930
so the difference between the two groups

00:13:18,129 --> 00:13:21,240
is

00:13:18,930 --> 00:13:23,130
if there are some spare resources on the

00:13:21,240 --> 00:13:24,870
machine cpu resources they were before

00:13:23,130 --> 00:13:27,750
it's given to good applications that

00:13:24,870 --> 00:13:29,220
want to spike over their entitlement if

00:13:27,750 --> 00:13:31,350
there are no good applications that

00:13:29,220 --> 00:13:33,750
actually need those resources it will be

00:13:31,350 --> 00:13:35,220
given to bad ones and if there are a lot

00:13:33,750 --> 00:13:37,680
of bad applications that just want to

00:13:35,220 --> 00:13:39,120
spike over their entitlement they would

00:13:37,680 --> 00:13:40,920
be allowed they will be allowed to do

00:13:39,120 --> 00:13:42,600
that but that won't negatively affect

00:13:40,920 --> 00:13:46,170
the good ones so they should be kind of

00:13:42,600 --> 00:13:47,790
in isolation and there are two key

00:13:46,170 --> 00:13:49,290
points here so the throttling only

00:13:47,790 --> 00:13:51,180
happens if there are not sufficient

00:13:49,290 --> 00:13:52,920
resources for everyone so if there are

00:13:51,180 --> 00:13:54,990
enough resources then both good and bad

00:13:52,920 --> 00:13:57,420
applications will be able to spike over

00:13:54,990 --> 00:13:59,130
there and tighten one and just use

00:13:57,420 --> 00:14:01,140
everything that's idle on the system and

00:13:59,130 --> 00:14:02,670
the other thing you're always guaranteed

00:14:01,140 --> 00:14:04,020
to get your entitlement it doesn't

00:14:02,670 --> 00:14:06,420
really matter if you're a good or a bad

00:14:04,020 --> 00:14:09,180
up your entitlement is guaranteed and

00:14:06,420 --> 00:14:12,450
what we expect to happen we expect that

00:14:09,180 --> 00:14:15,180
good apps won't see worse performance

00:14:12,450 --> 00:14:16,890
than previously they should even start

00:14:15,180 --> 00:14:18,690
behaving better because bad apps won't

00:14:16,890 --> 00:14:20,310
affect them and the so-called bad apps

00:14:18,690 --> 00:14:23,340
that just use more resources than they

00:14:20,310 --> 00:14:25,020
should be using should see that clearly

00:14:23,340 --> 00:14:27,000
so if they actually depend on the

00:14:25,020 --> 00:14:30,360
resources that they're using they should

00:14:27,000 --> 00:14:34,100
just scale their instance and the next

00:14:30,360 --> 00:14:37,890
track that we worked on is container edy

00:14:34,100 --> 00:14:39,570
tell us about all right so I was talking

00:14:37,890 --> 00:14:42,060
about how we're basically trying to

00:14:39,570 --> 00:14:44,850
become this like thinner infinite layer

00:14:42,060 --> 00:14:47,820
on top of existing standard font

00:14:44,850 --> 00:14:49,590
Immunization technology when it comes to

00:14:47,820 --> 00:14:51,450
contain a deed I think last time we were

00:14:49,590 --> 00:14:55,110
we were on this stage we talked about

00:14:51,450 --> 00:14:59,400
how we had all the container related

00:14:55,110 --> 00:15:01,440
operations implemented today we can

00:14:59,400 --> 00:15:04,160
announce that we've pretty much done

00:15:01,440 --> 00:15:07,050
with all the process operations as well

00:15:04,160 --> 00:15:08,670
it's not a hundred percent done but all

00:15:07,050 --> 00:15:11,040
the operations all the process

00:15:08,670 --> 00:15:12,210
operations used by Cloud Foundry are

00:15:11,040 --> 00:15:13,890
implemented are actually going through

00:15:12,210 --> 00:15:15,150
continuity which means that nowadays if

00:15:13,890 --> 00:15:17,960
you deploy called foundry

00:15:15,150 --> 00:15:20,370
with the container defrag enabled

00:15:17,960 --> 00:15:21,540
everything you do in terms of containers

00:15:20,370 --> 00:15:26,130
and processes will be going through

00:15:21,540 --> 00:15:28,380
quantity which is pretty cool so we're

00:15:26,130 --> 00:15:30,630
not done yet as I said we want to finish

00:15:28,380 --> 00:15:31,740
the process lifecycle with adding some

00:15:30,630 --> 00:15:32,910
operations there

00:15:31,740 --> 00:15:36,960
they are used by Congress for example

00:15:32,910 --> 00:15:39,870
and then once that's done we're gonna

00:15:36,960 --> 00:15:41,730
start exploring the image lifecycle so

00:15:39,870 --> 00:15:44,130
at the moment we have this plug-in

00:15:41,730 --> 00:15:47,400
called Goethe fess which we used for our

00:15:44,130 --> 00:15:49,170
image things we want to explore how it

00:15:47,400 --> 00:15:53,820
will look like to delegate those things

00:15:49,170 --> 00:15:56,520
to continue as well next replace mode so

00:15:53,820 --> 00:15:58,860
I talked about security earlier we want

00:15:56,520 --> 00:16:00,720
to we want to make sure that like our

00:15:58,860 --> 00:16:02,550
containers are solid as possible so we

00:16:00,720 --> 00:16:04,800
turn on as many security features as

00:16:02,550 --> 00:16:06,690
possible by default but so there's this

00:16:04,800 --> 00:16:08,190
like elephant in the room with basically

00:16:06,690 --> 00:16:11,250
the garden processes running his route

00:16:08,190 --> 00:16:13,620
which means if anyone by any chance gets

00:16:11,250 --> 00:16:15,510
control of the garden process then they

00:16:13,620 --> 00:16:18,450
will have root privileges on the system

00:16:15,510 --> 00:16:21,090
and of course that's not really nice so

00:16:18,450 --> 00:16:24,540
we've been putting lots of work into

00:16:21,090 --> 00:16:26,100
trying to run the gardens server as just

00:16:24,540 --> 00:16:29,610
a normal process belonging to a normal

00:16:26,100 --> 00:16:30,630
user this work for our run C backend is

00:16:29,610 --> 00:16:32,070
pretty much done there is an

00:16:30,630 --> 00:16:35,340
experimental flag that people can turn

00:16:32,070 --> 00:16:38,280
on there is still a bunch of work to do

00:16:35,340 --> 00:16:42,330
on on the container D back-end we'll

00:16:38,280 --> 00:16:44,220
keep you updated and being able to run

00:16:42,330 --> 00:16:46,110
as a normal route less process also

00:16:44,220 --> 00:16:48,630
means we can run inside our own

00:16:46,110 --> 00:16:50,730
container and this is pretty much

00:16:48,630 --> 00:16:53,880
exactly what we're doing because the new

00:16:50,730 --> 00:16:57,770
Bosch process manager uses runs C to

00:16:53,880 --> 00:17:00,120
containerize the jobs on your VM so when

00:16:57,770 --> 00:17:01,800
when running garden in rootless mode

00:17:00,120 --> 00:17:05,700
you'll be running inside a container

00:17:01,800 --> 00:17:07,230
which is powered by by BPM and of course

00:17:05,700 --> 00:17:10,130
at some point we want to do this flip

00:17:07,230 --> 00:17:12,990
for the continuity back in this form

00:17:10,130 --> 00:17:15,840
okay so this is this is all we've been

00:17:12,990 --> 00:17:17,280
doing in the past few months is that the

00:17:15,840 --> 00:17:20,490
tracks we've been we are working on

00:17:17,280 --> 00:17:22,170
right now when I spend just the last 30

00:17:20,490 --> 00:17:26,790
seconds talking about what we plan to do

00:17:22,170 --> 00:17:28,890
in the future so we all know that the

00:17:26,790 --> 00:17:31,730
future of like container scheduling on

00:17:28,890 --> 00:17:34,770
Cloud Foundry is called kubernetes and

00:17:31,730 --> 00:17:36,960
we all know that that's what that's what

00:17:34,770 --> 00:17:38,600
we are aiming at but at the same time we

00:17:36,960 --> 00:17:41,280
know there are gaps right we know that

00:17:38,600 --> 00:17:43,140
there are things that community still

00:17:41,280 --> 00:17:44,920
does not provide and that we do need to

00:17:43,140 --> 00:17:46,960
run a platform multi

00:17:44,920 --> 00:17:50,200
platform and enterprise level platform

00:17:46,960 --> 00:17:52,510
like Cloud Foundry and as a team that's

00:17:50,200 --> 00:17:55,390
been working on running containers in a

00:17:52,510 --> 00:17:57,990
multi-tenant environment for like so

00:17:55,390 --> 00:18:01,990
many years we think we we can help like

00:17:57,990 --> 00:18:06,730
fill in those gaps so we have identified

00:18:01,990 --> 00:18:08,800
a bunch of things we could help with for

00:18:06,730 --> 00:18:11,740
example a security profile for

00:18:08,800 --> 00:18:15,130
multi-tenant apps the cpu entitlements

00:18:11,740 --> 00:18:19,410
that you guys just talked about XFS disk

00:18:15,130 --> 00:18:21,850
quotas rootless that I've just mentioned

00:18:19,410 --> 00:18:23,470
integration with tools like G visor or

00:18:21,850 --> 00:18:26,830
firecracker which basically make

00:18:23,470 --> 00:18:30,430
containers actual real sign boxes and

00:18:26,830 --> 00:18:32,710
called starting so this is this is still

00:18:30,430 --> 00:18:34,600
very much up in the air but we will

00:18:32,710 --> 00:18:39,790
definitely sit down and start planning

00:18:34,600 --> 00:18:41,170
this work concretely very soon so yeah

00:18:39,790 --> 00:18:43,150
that's it this is the attribution for

00:18:41,170 --> 00:18:47,710
all the amazing artwork you've seen in

00:18:43,150 --> 00:18:51,559
the presentation and that's it thank you

00:18:47,710 --> 00:18:51,559

YouTube URL: https://www.youtube.com/watch?v=tuKefpUVMbE


