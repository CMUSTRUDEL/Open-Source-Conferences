Title: Deep Dive: The Stream, the Cache, and the Store: The History and Development... Johanna Ratliff
Publication date: 2019-09-13
Playlist: Cloud Foundry Summit EU 2019 - The Hague
Description: 
	Deep Dive: The Stream, the Cache, and the Store: The History and Development of Cloud Foundryâ€™s Logging and Observability Platform - Johanna Ratliff, Pivotal 

From app logs to BOSH metrics, walk through the history of the logging and the observability platform. Starting with UDP based Loggregator, Johanna will traverse the pain points and needs that led to improving Loggregator, creating Log Cache, and now Metric Store. All three are unique solutions. Learn from the original use cases which one could be best for you! 

For more info: https://www.cloudfoundry.org/
Captions: 
	00:00:00,030 --> 00:00:06,480
all right hello I am Johanna Ratliff and

00:00:03,750 --> 00:00:08,010
what you have just walked into you were

00:00:06,480 --> 00:00:10,290
hopefully anticipating the stream the

00:00:08,010 --> 00:00:11,639
cash and the store this is the history

00:00:10,290 --> 00:00:14,250
and development of Cloud Foundry is

00:00:11,639 --> 00:00:16,800
logging and observability platform and

00:00:14,250 --> 00:00:19,890
it is a deep dive so we have it listed

00:00:16,800 --> 00:00:22,890
at 70 minutes I really hope it's not 70

00:00:19,890 --> 00:00:28,769
minutes and if it is that'll be fun for

00:00:22,890 --> 00:00:30,449
all of us so hello my name is as I said

00:00:28,769 --> 00:00:32,910
Johanna Ratliff I'm an engineer at

00:00:30,449 --> 00:00:35,040
pivotal that's my twitter handle if

00:00:32,910 --> 00:00:39,690
you're on twitter and you want to know

00:00:35,040 --> 00:00:46,020
more about low reader all right so why

00:00:39,690 --> 00:00:49,820
are we here this is a talk about kind of

00:00:46,020 --> 00:00:52,320
three years of history of the

00:00:49,820 --> 00:00:55,440
observability platform itself I have

00:00:52,320 --> 00:00:57,210
been working on logger Gator log cache

00:00:55,440 --> 00:01:00,930
and metrics tour for the last three

00:00:57,210 --> 00:01:02,730
years so some of the whys behind how

00:01:00,930 --> 00:01:06,420
things ended up the way that they are

00:01:02,730 --> 00:01:08,159
and how we made some of the requests

00:01:06,420 --> 00:01:10,560
that consumers wanted out of this

00:01:08,159 --> 00:01:16,340
observability platform happen for them

00:01:10,560 --> 00:01:19,320
the decisions we made along the way and

00:01:16,340 --> 00:01:21,390
really to sum it up how we ended up with

00:01:19,320 --> 00:01:23,130
the logging and metrics platform

00:01:21,390 --> 00:01:25,049
everything for observability

00:01:23,130 --> 00:01:27,570
invisibility that you have at your

00:01:25,049 --> 00:01:27,950
disposal today I'm gonna dive through

00:01:27,570 --> 00:01:31,140
that

00:01:27,950 --> 00:01:37,320
so before we start a few operational

00:01:31,140 --> 00:01:40,409
definitions so logs versus metrics

00:01:37,320 --> 00:01:43,200
you'll hear me speak about some of our

00:01:40,409 --> 00:01:45,750
products metrics store law and cache

00:01:43,200 --> 00:01:48,149
logger Gator there are some terms in

00:01:45,750 --> 00:01:50,159
there that can often be overloaded so

00:01:48,149 --> 00:01:52,320
logs versus metrics for the purposes of

00:01:50,159 --> 00:01:55,369
this talk and for the purposes of the

00:01:52,320 --> 00:01:59,070
observability platform in general our

00:01:55,369 --> 00:02:01,229
logs can be thought of as a set of

00:01:59,070 --> 00:02:03,659
everything that is flowing through the

00:02:01,229 --> 00:02:05,990
logger Gator pipe just everything that

00:02:03,659 --> 00:02:09,060
you are transmitting that has data

00:02:05,990 --> 00:02:11,550
probably some subset of logs right and

00:02:09,060 --> 00:02:13,180
metrics can be thought of as a smaller

00:02:11,550 --> 00:02:15,430
subset of

00:02:13,180 --> 00:02:20,319
any anything that you could classify as

00:02:15,430 --> 00:02:23,379
a counter or engage something that has a

00:02:20,319 --> 00:02:25,810
value and a listing and you could

00:02:23,379 --> 00:02:27,129
probably do some kind of calculations on

00:02:25,810 --> 00:02:32,730
those those are your standard metrics

00:02:27,129 --> 00:02:34,930
but think of it all as logs collectively

00:02:32,730 --> 00:02:36,819
observability platform I've already said

00:02:34,930 --> 00:02:39,010
that like three or four times this talk

00:02:36,819 --> 00:02:41,290
so the observability platform from the

00:02:39,010 --> 00:02:43,720
perspective of this talk is really

00:02:41,290 --> 00:02:46,989
everything that we've built to enable

00:02:43,720 --> 00:02:49,090
visibility of your platform not only the

00:02:46,989 --> 00:02:51,489
components that are there but also

00:02:49,090 --> 00:02:54,180
what's being run on them your apps your

00:02:51,489 --> 00:02:57,099
services everything that encompasses

00:02:54,180 --> 00:03:00,459
visibility about a Cloud Foundry or what

00:02:57,099 --> 00:03:03,670
is running on it or anything that

00:03:00,459 --> 00:03:05,549
touches it right that is observability a

00:03:03,670 --> 00:03:12,910
platform anything that enables you

00:03:05,549 --> 00:03:15,730
visibility v.24 logger gator you'll hear

00:03:12,910 --> 00:03:19,419
me say v2 in the law greater context a

00:03:15,730 --> 00:03:21,549
lot and just as a fair warning that

00:03:19,419 --> 00:03:25,000
means three different things so let's

00:03:21,549 --> 00:03:28,810
unpack that really quick logger Gator v2

00:03:25,000 --> 00:03:31,900
the first v2 for logger Gator was really

00:03:28,810 --> 00:03:35,380
the conversion from UDP to a transport

00:03:31,900 --> 00:03:36,940
protocol of G RPC to enable reliability

00:03:35,380 --> 00:03:41,440
and we'll talk about that a little bit

00:03:36,940 --> 00:03:45,549
later so when you hear v2 from UDP to G

00:03:41,440 --> 00:03:47,319
RPC that's a reliability v2 upgrade we

00:03:45,549 --> 00:03:50,139
were also in the middle of a couple

00:03:47,319 --> 00:03:52,090
different rewrites simultaneously so

00:03:50,139 --> 00:03:54,459
you're like why why isn't this v3 in a

00:03:52,090 --> 00:03:58,180
v4 they all happen consecutively but

00:03:54,459 --> 00:04:00,099
they're different so um so your

00:03:58,180 --> 00:04:03,579
secondary one that you're going to want

00:04:00,099 --> 00:04:06,040
to know about is the envelope structure

00:04:03,579 --> 00:04:09,129
the things flowing through logger Gator

00:04:06,040 --> 00:04:11,530
changed during our v2 rewrite so the

00:04:09,129 --> 00:04:16,090
first one you might be familiar with

00:04:11,530 --> 00:04:19,090
those value metric envelopes or an HTTP

00:04:16,090 --> 00:04:21,640
start-stop metric envelope those were

00:04:19,090 --> 00:04:24,700
all flowing through later in our v1

00:04:21,640 --> 00:04:26,480
process our v2 process we took it to a

00:04:24,700 --> 00:04:29,990
little bit more of a traditional store

00:04:26,480 --> 00:04:32,180
sure of timers events logs gauges

00:04:29,990 --> 00:04:34,010
counters things that you would see in

00:04:32,180 --> 00:04:35,750
Google's site reliability engineering

00:04:34,010 --> 00:04:39,050
book things that we can as colleagues

00:04:35,750 --> 00:04:40,880
speak about on the same terms together

00:04:39,050 --> 00:04:43,310
instead of being like we made up a thing

00:04:40,880 --> 00:04:44,780
called a value metric that's amazing I

00:04:43,310 --> 00:04:49,460
don't know what that is

00:04:44,780 --> 00:04:51,680
so you're third v.24 law greater that

00:04:49,460 --> 00:04:54,740
you're gonna run into is the reverse log

00:04:51,680 --> 00:04:58,610
proxy so many of you have consumed from

00:04:54,740 --> 00:05:01,280
the firehose stream many of you may be

00:04:58,610 --> 00:05:03,110
familiar with the traffic controller so

00:05:01,280 --> 00:05:05,110
in the la garita architecture your

00:05:03,110 --> 00:05:08,660
traffic controller was your original

00:05:05,110 --> 00:05:11,300
reverse proxy from a bunch of routers

00:05:08,660 --> 00:05:12,950
that were charting your your logs and

00:05:11,300 --> 00:05:15,170
things downstream to give you a

00:05:12,950 --> 00:05:17,780
collective streaming endpoint right

00:05:15,170 --> 00:05:21,200
traffic controller was the v1 we did a

00:05:17,780 --> 00:05:23,330
rewrite to upgrade your traffic

00:05:21,200 --> 00:05:26,000
controller to something called an RL P

00:05:23,330 --> 00:05:27,670
we named it a reverse log proxy that is

00:05:26,000 --> 00:05:33,640
that is what it does we stopped with the

00:05:27,670 --> 00:05:36,500
I think I think it was whether based

00:05:33,640 --> 00:05:38,210
whether based metaphors for all of our

00:05:36,500 --> 00:05:41,960
components and just named them what they

00:05:38,210 --> 00:05:43,640
are so they're your v2 for alligator

00:05:41,960 --> 00:05:45,860
we're gonna run into all of these

00:05:43,640 --> 00:05:49,910
different terms so wanted to set the

00:05:45,860 --> 00:05:56,690
stage for that a little bit okay so

00:05:49,910 --> 00:05:58,910
diving right into logger Gator oh so

00:05:56,690 --> 00:06:02,360
when you think of a logger Gator you

00:05:58,910 --> 00:06:04,550
might be imagining a stream of metrics

00:06:02,360 --> 00:06:07,880
that feel something like this it's all

00:06:04,550 --> 00:06:10,400
coming from your Diego cells from your

00:06:07,880 --> 00:06:12,430
apps and it's trickling down to you and

00:06:10,400 --> 00:06:15,230
you're able to consume them

00:06:12,430 --> 00:06:19,400
unfortunately the log reader experience

00:06:15,230 --> 00:06:21,530
is a little bit more like this it is a

00:06:19,400 --> 00:06:23,600
lot of data coming out and you can

00:06:21,530 --> 00:06:26,630
leverage it in a very productive and

00:06:23,600 --> 00:06:29,690
very unique way but it does take a lot

00:06:26,630 --> 00:06:33,110
of control and it is a little bit

00:06:29,690 --> 00:06:36,020
exciting and unfortunately the larger

00:06:33,110 --> 00:06:40,009
experience used to be something closer

00:06:36,020 --> 00:06:41,869
to this so if you've consumed from

00:06:40,009 --> 00:06:44,779
traffic controller you can relate to

00:06:41,869 --> 00:06:46,909
this gift strongly it used to be this

00:06:44,779 --> 00:06:49,129
experience of like here is all of your

00:06:46,909 --> 00:06:50,749
data you are going to be responsible for

00:06:49,129 --> 00:06:53,360
controlling it and so a lot of the time

00:06:50,749 --> 00:06:55,490
the data is controlling you in instead

00:06:53,360 --> 00:06:58,069
of your ability to leverage that and

00:06:55,490 --> 00:07:02,180
really create some visibility for your

00:06:58,069 --> 00:07:04,490
platform so when you've got the stream

00:07:02,180 --> 00:07:06,529
of data dragging you around instead of

00:07:04,490 --> 00:07:08,599
really enabling you to see things about

00:07:06,529 --> 00:07:09,740
your platform that's what we wanted to

00:07:08,599 --> 00:07:16,339
improve from the logger reader

00:07:09,740 --> 00:07:18,979
experience right so what is it so log

00:07:16,339 --> 00:07:21,139
reader is a back pressure resistant

00:07:18,979 --> 00:07:25,219
logging pipe through which your platform

00:07:21,139 --> 00:07:28,309
and app logs and metrics flow so every

00:07:25,219 --> 00:07:31,909
single component of Cloud Foundry every

00:07:28,309 --> 00:07:35,059
VM that is running Cloud Foundry has a

00:07:31,909 --> 00:07:38,809
logger dater agent on it through which

00:07:35,059 --> 00:07:41,990
all of these metrics and logs are

00:07:38,809 --> 00:07:45,949
getting into this you can think of it as

00:07:41,990 --> 00:07:48,620
one large pipe and then your downstream

00:07:45,949 --> 00:07:52,279
and point through which many components

00:07:48,620 --> 00:07:54,139
and users consume all of this data we

00:07:52,279 --> 00:07:55,969
affectionately refer to as a firehose

00:07:54,139 --> 00:07:59,449
because that is the amount of data

00:07:55,969 --> 00:08:01,279
coming out of it so let's unpack back

00:07:59,449 --> 00:08:03,559
pressure resistant for a little bit

00:08:01,279 --> 00:08:05,419
because that informed so many of the

00:08:03,559 --> 00:08:08,240
decisions that we made as we were

00:08:05,419 --> 00:08:11,449
building logger gator so back pressure

00:08:08,240 --> 00:08:13,610
resistant our goal when building log

00:08:11,449 --> 00:08:18,289
reader every step of the way was to make

00:08:13,610 --> 00:08:21,860
sure that no user downstream and no

00:08:18,289 --> 00:08:24,979
component downstream no piece of lover

00:08:21,860 --> 00:08:29,419
gator even could be causing reliability

00:08:24,979 --> 00:08:31,370
loss farther upstream so whenever I'm

00:08:29,419 --> 00:08:33,260
talking about back pressure that's

00:08:31,370 --> 00:08:36,740
really what was happening is we wanted

00:08:33,260 --> 00:08:39,560
to focus on your traffic controller not

00:08:36,740 --> 00:08:42,310
causing any conflict in your logging

00:08:39,560 --> 00:08:45,139
router you're logging router couldn't be

00:08:42,310 --> 00:08:49,430
making anything from a log reader agent

00:08:45,139 --> 00:08:51,290
drop data once it was on its way down

00:08:49,430 --> 00:08:53,510
that pipe there was no way for it to get

00:08:51,290 --> 00:08:53,900
clogged up it might get lost

00:08:53,510 --> 00:08:58,840
but it

00:08:53,900 --> 00:08:58,840
couldn't cause conflict farther upstream

00:09:01,540 --> 00:09:09,640
so the way that it was I think we saw

00:09:06,260 --> 00:09:16,550
from that gift before it really had

00:09:09,640 --> 00:09:20,600
hardly it really had this reliability

00:09:16,550 --> 00:09:22,730
impact that we used to speak about log

00:09:20,600 --> 00:09:27,200
readers reliability without really

00:09:22,730 --> 00:09:30,080
measuring it so we had some reliability

00:09:27,200 --> 00:09:33,050
problems we had some loss enos that was

00:09:30,080 --> 00:09:38,210
very common in a logger infrastructure

00:09:33,050 --> 00:09:40,700
and we also had kind of unordered chaos

00:09:38,210 --> 00:09:43,190
right it is a firehose of data that a

00:09:40,700 --> 00:09:45,230
lot of you downstream were responsible

00:09:43,190 --> 00:09:47,630
for keeping your own apps up to be like

00:09:45,230 --> 00:09:50,000
oh good I am going to consume all of

00:09:47,630 --> 00:09:53,030
this and then filter it the way I need

00:09:50,000 --> 00:09:54,350
right that wasn't as much enablement for

00:09:53,030 --> 00:09:56,680
you as it was putting the responsibility

00:09:54,350 --> 00:10:03,560
on you

00:09:56,680 --> 00:10:06,650
so what needed fixing so number one

00:10:03,560 --> 00:10:08,570
reliability and we'll speak a little bit

00:10:06,650 --> 00:10:12,350
more about this later but effectively

00:10:08,570 --> 00:10:15,560
our reliability prior to all of the v2

00:10:12,350 --> 00:10:17,600
upgrades so I would say three and a half

00:10:15,560 --> 00:10:21,140
four years ago

00:10:17,600 --> 00:10:23,650
our reliability was at about 75 percent

00:10:21,140 --> 00:10:26,930
but we didn't know that until about

00:10:23,650 --> 00:10:29,660
their three and a quarter three years

00:10:26,930 --> 00:10:32,870
ago because as I previously mentioned

00:10:29,660 --> 00:10:35,360
with Google site reliability we decided

00:10:32,870 --> 00:10:36,770
to dive in and say okay we know that

00:10:35,360 --> 00:10:38,600
people are having problems with loss

00:10:36,770 --> 00:10:41,600
enos the only way we're going to improve

00:10:38,600 --> 00:10:45,230
that for them is to be able to see what

00:10:41,600 --> 00:10:48,170
kind of loss were experiencing so really

00:10:45,230 --> 00:10:52,430
setting those objectives and measuring

00:10:48,170 --> 00:10:55,220
end to end on our log reader throughput

00:10:52,430 --> 00:10:58,610
what was getting through at a consistent

00:10:55,220 --> 00:11:02,780
reliable rate at scale at load in

00:10:58,610 --> 00:11:04,640
production how that was looking in terms

00:11:02,780 --> 00:11:07,550
of reliability and what we thought was

00:11:04,640 --> 00:11:12,680
99 percent was a little closer to 75

00:11:07,550 --> 00:11:14,930
so um that UDP to G RPC protocol upgrade

00:11:12,680 --> 00:11:16,670
is one of the big things that improved

00:11:14,930 --> 00:11:21,170
that and we can actually prove it

00:11:16,670 --> 00:11:23,360
because we have numbers this time so the

00:11:21,170 --> 00:11:29,240
other thing that we got asked for a lot

00:11:23,360 --> 00:11:31,130
was ordering and filtering if you know

00:11:29,240 --> 00:11:33,530
about the architecture of log reader

00:11:31,130 --> 00:11:36,980
what happens is every single agent is

00:11:33,530 --> 00:11:39,980
outputting data and then you've got

00:11:36,980 --> 00:11:42,470
downstream routers that collect and then

00:11:39,980 --> 00:11:44,750
are charted across it and then you've

00:11:42,470 --> 00:11:47,420
got a reverse proxy downstream that

00:11:44,750 --> 00:11:49,640
really brings it all back together but

00:11:47,420 --> 00:11:52,520
reverse proxying things that are coming

00:11:49,640 --> 00:11:56,870
through in a stream unless you are

00:11:52,520 --> 00:12:01,510
willing to sacrifice some reliability in

00:11:56,870 --> 00:12:04,760
that reverse proxy and sacrifice your

00:12:01,510 --> 00:12:08,170
promise to back pressure in that reverse

00:12:04,760 --> 00:12:11,150
proxy you are going to experience some

00:12:08,170 --> 00:12:13,160
higher load some higher computational

00:12:11,150 --> 00:12:14,930
load in order and some higher memory

00:12:13,160 --> 00:12:17,120
load in order to just get these things

00:12:14,930 --> 00:12:18,680
in order and that was one of the biggest

00:12:17,120 --> 00:12:20,690
tasks we always had them really can I

00:12:18,680 --> 00:12:24,740
just I just have things in order you

00:12:20,690 --> 00:12:26,840
have time stamps it's not that hard so

00:12:24,740 --> 00:12:31,880
I'll talk about some of the easy

00:12:26,840 --> 00:12:34,160
approaches in just a second okay noisy

00:12:31,880 --> 00:12:37,780
neighbor can I get a show of hands who

00:12:34,160 --> 00:12:41,300
has heard this terminology before yeah

00:12:37,780 --> 00:12:44,660
so for noisy neighbor what we're

00:12:41,300 --> 00:12:47,330
experiencing is effectively the risk of

00:12:44,660 --> 00:12:51,070
having any neighbor on your platform as

00:12:47,330 --> 00:12:54,830
let's take an application for example

00:12:51,070 --> 00:12:57,890
let's say I'm application a and I'm

00:12:54,830 --> 00:12:59,900
sitting over here and I log a few things

00:12:57,890 --> 00:13:03,050
every 30 seconds I've got some metrics

00:12:59,900 --> 00:13:06,800
going out about myself and there is

00:13:03,050 --> 00:13:10,400
application be scaled to ten instances

00:13:06,800 --> 00:13:14,150
and it is crashing and it's in debug

00:13:10,400 --> 00:13:17,210
mode and it is outputting a large amount

00:13:14,150 --> 00:13:20,120
of data so all of that data coming

00:13:17,210 --> 00:13:23,459
through this one collective pipe

00:13:20,120 --> 00:13:26,459
effectively the risk is depending on how

00:13:23,459 --> 00:13:30,810
your lorry ATAR components have been

00:13:26,459 --> 00:13:33,750
scaled you're risking a throughput that

00:13:30,810 --> 00:13:35,639
becomes dominated by this one noisy

00:13:33,750 --> 00:13:38,399
neighbors logs and metrics and it's

00:13:35,639 --> 00:13:40,259
saying I have decided that most of these

00:13:38,399 --> 00:13:44,910
components are going to be responsible

00:13:40,259 --> 00:13:52,139
for transmitting my stack trace and app

00:13:44,910 --> 00:13:54,690
B or app a me is at risk for having okay

00:13:52,139 --> 00:13:56,310
so a single log got dropped well I don't

00:13:54,690 --> 00:13:58,079
log that frequently so it's a large

00:13:56,310 --> 00:14:01,350
percentage of the logs that I cared

00:13:58,079 --> 00:14:03,600
about as opposed to being in a fight for

00:14:01,350 --> 00:14:06,149
who can be the noisiest and scale up

00:14:03,600 --> 00:14:08,699
your log reader the most to handle

00:14:06,149 --> 00:14:12,470
everyone competing for the most load on

00:14:08,699 --> 00:14:16,680
this system so your noisy neighbors

00:14:12,470 --> 00:14:21,870
situation is really the risk of any one

00:14:16,680 --> 00:14:26,519
app service component anyone having

00:14:21,870 --> 00:14:28,680
their logs and metrics kind of hijacked

00:14:26,519 --> 00:14:30,319
by system under performance under load

00:14:28,680 --> 00:14:33,899
from somebody else if that makes sense

00:14:30,319 --> 00:14:37,529
and look back that was the other main

00:14:33,899 --> 00:14:40,560
thing that we wanted fixed from a low

00:14:37,529 --> 00:14:43,529
reader perspective for consumers they

00:14:40,560 --> 00:14:45,420
said hey you're throwing all of this

00:14:43,529 --> 00:14:48,899
data at me and if I miss it there is

00:14:45,420 --> 00:14:52,350
there is no fix for it it's gone it's

00:14:48,899 --> 00:14:56,089
gone into the ether so solving some way

00:14:52,350 --> 00:15:00,480
that I can look back just five minutes

00:14:56,089 --> 00:15:03,000
okay so we actually ended up with two

00:15:00,480 --> 00:15:04,769
solutions for this collection of needs

00:15:03,000 --> 00:15:07,829
because we looked at log reader and

00:15:04,769 --> 00:15:10,050
we're like this might not all be solved

00:15:07,829 --> 00:15:13,709
in logger Gator so what we're going to

00:15:10,050 --> 00:15:16,199
walk through is a little bit of log

00:15:13,709 --> 00:15:20,100
cache was the solution for half of these

00:15:16,199 --> 00:15:22,709
needs and then our reverse log proxy and

00:15:20,100 --> 00:15:24,959
our v2 upgrades for log Raider were a

00:15:22,709 --> 00:15:28,439
solution for half of these needs so

00:15:24,959 --> 00:15:30,720
let's look at that the reliability was

00:15:28,439 --> 00:15:33,390
improved actually in logger gator itself

00:15:30,720 --> 00:15:34,980
now we kept reliability consistent when

00:15:33,390 --> 00:15:36,780
long cache and I'll speak about that in

00:15:34,980 --> 00:15:39,150
a moment but the reliability was

00:15:36,780 --> 00:15:42,180
actually fixed in log reader so by

00:15:39,150 --> 00:15:44,490
upgrading that UDP protocol which has no

00:15:42,180 --> 00:15:46,680
guarantees to a G RPC protocol you've

00:15:44,490 --> 00:15:50,580
got a big improvement right there

00:15:46,680 --> 00:15:53,310
already and then also from the rewrite

00:15:50,580 --> 00:15:57,450
of our traffic controller to a reverse

00:15:53,310 --> 00:15:59,550
log proxy that V to reverse proxy really

00:15:57,450 --> 00:16:03,810
had a stronger focus around reliability

00:15:59,550 --> 00:16:07,550
too so what we got um we'll get to that

00:16:03,810 --> 00:16:13,200
in a second um so what we got for

00:16:07,550 --> 00:16:16,050
reverse log proxy was a G RPC gateway

00:16:13,200 --> 00:16:18,030
that component authors could use and

00:16:16,050 --> 00:16:20,640
they were getting better reliability but

00:16:18,030 --> 00:16:22,620
downstream a lot of users were still

00:16:20,640 --> 00:16:25,440
having to consume from the firehose

00:16:22,620 --> 00:16:27,420
so what has been built recently keep in

00:16:25,440 --> 00:16:30,420
mind not all of this is chronological

00:16:27,420 --> 00:16:33,660
what has been built recently post log

00:16:30,420 --> 00:16:37,050
cache was an RLP gateway to enable that

00:16:33,660 --> 00:16:39,780
same HTTP API from your RLP instead of

00:16:37,050 --> 00:16:42,330
your traffic controller so you can

00:16:39,780 --> 00:16:46,320
experience all of those v2 upgrades with

00:16:42,330 --> 00:16:50,240
your HTTP API whoever was consuming from

00:16:46,320 --> 00:16:54,300
that stream initially so I get the

00:16:50,240 --> 00:16:55,500
upgraded envelope Styles to say I would

00:16:54,300 --> 00:16:58,020
like to be able to speak with my

00:16:55,500 --> 00:17:00,630
colleagues about counters and gauges and

00:16:58,020 --> 00:17:05,180
not whatever an HDTV start-stop metric

00:17:00,630 --> 00:17:08,820
is we made that up and I would like to

00:17:05,180 --> 00:17:13,220
be able to be consuming from this

00:17:08,820 --> 00:17:17,640
upgraded reverse proxy architecture and

00:17:13,220 --> 00:17:19,350
also getting that G RPC was logger gator

00:17:17,640 --> 00:17:20,880
wide so if you are consuming from

00:17:19,350 --> 00:17:23,880
traffic controller a lot of those

00:17:20,880 --> 00:17:26,280
improvements that's that's still in your

00:17:23,880 --> 00:17:28,770
traffic controller HTTP API because if

00:17:26,280 --> 00:17:30,870
you hit that stream endpoint you are

00:17:28,770 --> 00:17:31,800
getting the benefit of G RPC all the way

00:17:30,870 --> 00:17:37,770
up and down the logger reader

00:17:31,800 --> 00:17:40,680
architecture okay so looking at these

00:17:37,770 --> 00:17:42,960
four things a lot of them as I mentioned

00:17:40,680 --> 00:17:45,000
there's kind of this mindset if you have

00:17:42,960 --> 00:17:46,190
a reverse proxy and you have some

00:17:45,000 --> 00:17:49,370
buffering in there

00:17:46,190 --> 00:17:52,480
can we not create a situation in which I

00:17:49,370 --> 00:17:55,040
can do some ordering do some filtering

00:17:52,480 --> 00:17:59,780
within that reverse proxy and let people

00:17:55,040 --> 00:18:02,600
consume it from the RLP itself by the

00:17:59,780 --> 00:18:05,630
way our LP does have filtering now

00:18:02,600 --> 00:18:08,270
because of the selectors that we created

00:18:05,630 --> 00:18:10,760
in it it's what enables log cache so

00:18:08,270 --> 00:18:14,620
there is half of the fix for filtering

00:18:10,760 --> 00:18:14,620
in our LP half of the fix for

00:18:17,440 --> 00:18:22,520
computation in this very hefty box that

00:18:20,450 --> 00:18:26,000
I've allocated for a reverse proxy right

00:18:22,520 --> 00:18:30,380
and so one of the things that you often

00:18:26,000 --> 00:18:34,220
run into is the idea of latent messages

00:18:30,380 --> 00:18:37,370
so if I'm over on this Diego cell and

00:18:34,220 --> 00:18:40,070
I'm outputting data right what happens

00:18:37,370 --> 00:18:41,930
if my reverse proxy is like oh good I've

00:18:40,070 --> 00:18:43,700
taken this envelope and this envelope

00:18:41,930 --> 00:18:45,530
and this envelope and we've ordered them

00:18:43,700 --> 00:18:47,480
and we're gonna send them out to you and

00:18:45,530 --> 00:18:47,960
maybe we use matching hello this is

00:18:47,480 --> 00:18:51,260
great

00:18:47,960 --> 00:18:53,510
have some ordered data and then read is

00:18:51,260 --> 00:18:55,370
it sending it out it gets a two-second

00:18:53,510 --> 00:18:55,730
old log it's like I didn't know about

00:18:55,370 --> 00:19:00,590
that

00:18:55,730 --> 00:19:02,690
when I was ordering this adjustment for

00:19:00,590 --> 00:19:04,430
we don't really know if something

00:19:02,690 --> 00:19:07,960
slightly older is gonna come down the

00:19:04,430 --> 00:19:11,030
pipe at any given time is what really

00:19:07,960 --> 00:19:18,830
inspired us to build a short term cash

00:19:11,030 --> 00:19:20,450
so the physics behind being sure that

00:19:18,830 --> 00:19:24,260
things have come all the way down the

00:19:20,450 --> 00:19:26,120
pipe and aren't just lost because when

00:19:24,260 --> 00:19:27,020
things travel across wires there's

00:19:26,120 --> 00:19:28,730
there's not a hundred percent

00:19:27,020 --> 00:19:30,610
reliability right that's that's

00:19:28,730 --> 00:19:34,430
fundamentally impossible because physics

00:19:30,610 --> 00:19:36,680
and so when you want to be sure that

00:19:34,430 --> 00:19:38,690
you've gotten most of your things in

00:19:36,680 --> 00:19:41,000
order and most of your things filtered

00:19:38,690 --> 00:19:43,460
you end up having a lot of conversations

00:19:41,000 --> 00:19:46,310
about cap theorem okay let's decide on

00:19:43,460 --> 00:19:49,280
if we're doing eventual consistency we

00:19:46,310 --> 00:19:51,620
need more storage than is available to

00:19:49,280 --> 00:19:54,100
us in a reverse proxy because that's not

00:19:51,620 --> 00:19:54,100
that's trouble

00:19:56,360 --> 00:20:03,750
so diving into the cash that we built to

00:19:59,910 --> 00:20:07,770
solve this what is it

00:20:03,750 --> 00:20:10,080
so log cash is an ephemeral self pruning

00:20:07,770 --> 00:20:13,970
storage for everything from the fire

00:20:10,080 --> 00:20:17,550
hose and what it really is is we it's a

00:20:13,970 --> 00:20:21,630
map of your source ID to a red-black

00:20:17,550 --> 00:20:25,020
tree of every single log and metric that

00:20:21,630 --> 00:20:26,960
pertains to that source ID so that is

00:20:25,020 --> 00:20:29,640
how we built the cache it is

00:20:26,960 --> 00:20:32,970
horizontally scalable you may have seen

00:20:29,640 --> 00:20:35,610
it's often deployed as co-located with

00:20:32,970 --> 00:20:37,770
your Doppler VMs also your your logger

00:20:35,610 --> 00:20:41,310
Gator routers are what those Doppler

00:20:37,770 --> 00:20:44,460
vans are it's co-located because they

00:20:41,310 --> 00:20:47,400
themselves are just routing metrics and

00:20:44,460 --> 00:20:49,290
logs around and back and forth so it's

00:20:47,400 --> 00:20:50,670
computationally intensive but it wasn't

00:20:49,290 --> 00:20:52,380
using a lot of its memory so we were

00:20:50,670 --> 00:20:54,720
like well we'll take that we'll take the

00:20:52,380 --> 00:20:57,630
spare memory so we built a cache

00:20:54,720 --> 00:21:00,030
co-located with Doppler but keep in mind

00:20:57,630 --> 00:21:01,950
it's actually consuming downstream from

00:21:00,030 --> 00:21:03,990
our LP and then its own it's its own

00:21:01,950 --> 00:21:06,710
independent component it just happens to

00:21:03,990 --> 00:21:09,630
be co-located on that logger Gator

00:21:06,710 --> 00:21:11,790
component VM so a lot of time there's

00:21:09,630 --> 00:21:13,530
confusion of like does it dump into log

00:21:11,790 --> 00:21:19,710
cache at the exact same time it dumps

00:21:13,530 --> 00:21:23,520
into Doppler No so downstream from our

00:21:19,710 --> 00:21:28,890
LP yeah it is an ephemeral self pruning

00:21:23,520 --> 00:21:32,580
cache and you get some really good

00:21:28,890 --> 00:21:35,130
experiences too because in law

00:21:32,580 --> 00:21:38,610
aggregator you have that CF logs from

00:21:35,130 --> 00:21:40,920
your CF CLI and you also have the HTTP

00:21:38,610 --> 00:21:43,290
API that you can consume from but with

00:21:40,920 --> 00:21:48,750
log cache we went ahead and wrote a CLI

00:21:43,290 --> 00:21:52,110
plugin to really better enable the usage

00:21:48,750 --> 00:21:54,300
of some of those filters etc so instead

00:21:52,110 --> 00:21:56,280
of kind of writing our own query

00:21:54,300 --> 00:21:58,470
language and or anything like that we

00:21:56,280 --> 00:22:02,160
were like what if we just write a CLI

00:21:58,470 --> 00:22:03,870
plug-in so you can say you know tag tag

00:22:02,160 --> 00:22:07,700
envelope type I would like all of my

00:22:03,870 --> 00:22:10,549
counters from this source ID please

00:22:07,700 --> 00:22:12,799
and that is if you've ever used CF tail

00:22:10,549 --> 00:22:18,919
or CF tail follow we made it a very

00:22:12,799 --> 00:22:22,030
traditional unix experience okay so

00:22:18,919 --> 00:22:25,030
talking about the use case for log cache

00:22:22,030 --> 00:22:28,700
we talked about people wanting look-back

00:22:25,030 --> 00:22:30,410
for debugging honestly and figuring

00:22:28,700 --> 00:22:34,190
what's going on with your apps because

00:22:30,410 --> 00:22:38,870
like I said a lot of people I've seen a

00:22:34,190 --> 00:22:41,120
lot of folks with these apps that are ok

00:22:38,870 --> 00:22:45,679
we're gonna dump a bunch of logs in

00:22:41,120 --> 00:22:49,910
order to see about the reliability on

00:22:45,679 --> 00:22:51,470
log log reader let's see downstream how

00:22:49,910 --> 00:22:53,450
much of these we actually ended up with

00:22:51,470 --> 00:22:55,130
and there are these on-the-fly tests

00:22:53,450 --> 00:22:57,470
instead of being able to look back about

00:22:55,130 --> 00:23:00,500
metrics about how your log readers doing

00:22:57,470 --> 00:23:03,980
how your apps are doing and so when we

00:23:00,500 --> 00:23:06,980
or when the previous PM of Lurie or did

00:23:03,980 --> 00:23:09,860
a lot of this research he was finding

00:23:06,980 --> 00:23:12,169
out that so many people their main

00:23:09,860 --> 00:23:13,940
request was look I I just want to be

00:23:12,169 --> 00:23:16,010
able to look back five minutes and know

00:23:13,940 --> 00:23:20,390
what was going on then so that I can

00:23:16,010 --> 00:23:22,010
look at patterns for myself and so the

00:23:20,390 --> 00:23:26,000
cache is really built for that it's

00:23:22,010 --> 00:23:28,490
built for filtering and it's built for

00:23:26,000 --> 00:23:31,010
ordering because like we said being able

00:23:28,490 --> 00:23:33,140
to dump everything into the same place

00:23:31,010 --> 00:23:34,610
before you order it and have a little

00:23:33,140 --> 00:23:38,240
bit of a delay so that you can be

00:23:34,610 --> 00:23:43,429
assured of eventual in the reverse proxy

00:23:38,240 --> 00:23:46,820
is very important and it was built for

00:23:43,429 --> 00:23:49,820
people who needed that 5 to 15 minute

00:23:46,820 --> 00:23:51,650
window of what just happened so that I

00:23:49,820 --> 00:23:53,750
don't so that I don't have to try and

00:23:51,650 --> 00:24:00,740
push information through again to

00:23:53,750 --> 00:24:03,200
replicate it and mostly enough state

00:24:00,740 --> 00:24:04,460
that downstream folks didn't have to

00:24:03,200 --> 00:24:06,110
store it themselves

00:24:04,460 --> 00:24:08,380
so I don't know if any of you have ever

00:24:06,110 --> 00:24:11,870
been on a team that is doing

00:24:08,380 --> 00:24:14,620
observability work and consumes from the

00:24:11,870 --> 00:24:18,350
firehose but a lot of our colleagues

00:24:14,620 --> 00:24:20,220
would want to see metrics and logs about

00:24:18,350 --> 00:24:22,110
this thing and they would end up role

00:24:20,220 --> 00:24:26,460
their own solution to this cache every

00:24:22,110 --> 00:24:29,130
single time like here's a here's a DB

00:24:26,460 --> 00:24:30,840
that we've just put in the middle we

00:24:29,130 --> 00:24:35,250
don't actually need a DB but we need

00:24:30,840 --> 00:24:40,350
some form of slightly older than right

00:24:35,250 --> 00:24:42,210
now data to be stored and so a lot of

00:24:40,350 --> 00:24:44,490
our colleagues we were noticing ended up

00:24:42,210 --> 00:24:46,500
rolling their own very similar solutions

00:24:44,490 --> 00:24:48,419
time and time again and so when that is

00:24:46,500 --> 00:24:50,610
not the main purpose of your product

00:24:48,419 --> 00:24:52,500
when focusing on the reliability of this

00:24:50,610 --> 00:24:53,100
cache is not the main purpose of your

00:24:52,500 --> 00:24:57,210
product

00:24:53,100 --> 00:24:59,490
it becomes a side focus and it doesn't

00:24:57,210 --> 00:25:02,730
get as much attention so those

00:24:59,490 --> 00:25:04,530
downstream products of saying here I

00:25:02,730 --> 00:25:06,570
would love to graph this I would love to

00:25:04,530 --> 00:25:11,940
show you metrics or old logs about this

00:25:06,570 --> 00:25:13,620
get less reliable data from the firehose

00:25:11,940 --> 00:25:20,850
because everybody is independently

00:25:13,620 --> 00:25:25,679
rolling their own so really what this

00:25:20,850 --> 00:25:29,520
fixed was creating the cache fixed a lot

00:25:25,679 --> 00:25:31,679
of the observability teams need to roll

00:25:29,520 --> 00:25:34,140
their own and it also fixed a lot of

00:25:31,679 --> 00:25:37,860
folks who are like hey I end up needing

00:25:34,140 --> 00:25:40,169
to consume from this firehose and dump

00:25:37,860 --> 00:25:41,730
things back in in order to see what just

00:25:40,169 --> 00:25:43,049
happened because I need to replicate it

00:25:41,730 --> 00:25:46,500
time and time again to see what's

00:25:43,049 --> 00:25:49,140
happening so the log cache experience of

00:25:46,500 --> 00:25:49,500
saying I would like to tail this source

00:25:49,140 --> 00:25:50,880
ID

00:25:49,500 --> 00:25:53,760
I would like to tail and follow this

00:25:50,880 --> 00:25:56,039
source idea I would like to look back to

00:25:53,760 --> 00:25:57,450
a start time five minutes ago and an end

00:25:56,039 --> 00:25:59,039
time three minutes ago because I just

00:25:57,450 --> 00:26:05,929
want to know what happened and having

00:25:59,039 --> 00:26:05,929
that still available after your one

00:26:06,049 --> 00:26:11,330
query for it is is very important for

00:26:09,090 --> 00:26:11,330
folks

00:26:15,179 --> 00:26:22,799
so what else did people want Oh No

00:26:20,219 --> 00:26:25,889
so what else did people want really we

00:26:22,799 --> 00:26:30,979
noticed that even as folks were rolling

00:26:25,889 --> 00:26:33,539
their own cash like intermediate cash I

00:26:30,979 --> 00:26:36,419
mentioned that the cash itself is

00:26:33,539 --> 00:26:37,799
horizontally scalable and we saw folks

00:26:36,419 --> 00:26:41,849
trying to really push it to its limits

00:26:37,799 --> 00:26:45,690
which is awesome and say okay so if I

00:26:41,849 --> 00:26:47,669
have a high memory VM and I scale this

00:26:45,690 --> 00:26:50,700
out to 40 instances because it's already

00:26:47,669 --> 00:26:52,080
co-located on my dopplers and I need $40

00:26:50,700 --> 00:26:58,379
for the amount of logs that are going

00:26:52,080 --> 00:27:01,919
through here then you end up seeing that

00:26:58,379 --> 00:27:06,649
a lot of folks want persistence if you

00:27:01,919 --> 00:27:09,539
want any kind of charting any kind of

00:27:06,649 --> 00:27:11,729
query ability on metrics a week ago

00:27:09,539 --> 00:27:14,269
because this is a cash meant to be five

00:27:11,729 --> 00:27:17,879
fifteen minutes long

00:27:14,269 --> 00:27:21,210
two weeks ago six weeks ago that kind of

00:27:17,879 --> 00:27:23,159
charting requires actual persistence and

00:27:21,210 --> 00:27:25,409
like I said this is an ephemeral self

00:27:23,159 --> 00:27:27,960
printing thing oh one thing I also

00:27:25,409 --> 00:27:29,940
forgot to mention on what did this fix

00:27:27,960 --> 00:27:32,849
we previously spoke about the noisy

00:27:29,940 --> 00:27:36,629
neighbor problem so when you'll remember

00:27:32,849 --> 00:27:39,839
I explained the architecture of log

00:27:36,629 --> 00:27:42,479
cache where you have a map of source IDs

00:27:39,839 --> 00:27:46,320
to red-black trees for each source ID

00:27:42,479 --> 00:27:48,149
because of the way that we store that

00:27:46,320 --> 00:27:50,999
information you're a noisy neighbor

00:27:48,149 --> 00:27:53,999
problem once it's come through logger

00:27:50,999 --> 00:27:57,719
Gator is eliminated because I have my

00:27:53,999 --> 00:28:01,739
own tree for my own logs and I can say

00:27:57,719 --> 00:28:03,599
hey my cache duration from my source ID

00:28:01,739 --> 00:28:05,489
because I don't log that often maybe I

00:28:03,599 --> 00:28:08,669
have a hundred thousand nodes on this

00:28:05,489 --> 00:28:11,549
tree my castration for my source ID is

00:28:08,669 --> 00:28:14,219
actually a couple of days but noisy

00:28:11,549 --> 00:28:15,960
crashing app their cache duration is

00:28:14,219 --> 00:28:19,129
fifteen seconds but they did that to

00:28:15,960 --> 00:28:22,830
themselves and they didn't do it to me

00:28:19,129 --> 00:28:24,599
so that was pretty important fix I just

00:28:22,830 --> 00:28:25,710
kind of like glossed over initially

00:28:24,599 --> 00:28:29,460
might be

00:28:25,710 --> 00:28:32,490
so we talked about the persistence being

00:28:29,460 --> 00:28:36,960
really important to people so we created

00:28:32,490 --> 00:28:41,100
metric store and we initially started

00:28:36,960 --> 00:28:44,460
with those definitions about store or

00:28:41,100 --> 00:28:48,600
about metrics V logs so keep in mind

00:28:44,460 --> 00:28:57,180
this is a storage engine only for

00:28:48,600 --> 00:29:01,170
metrics at this stage so metrics store

00:28:57,180 --> 00:29:03,630
is similar to log cache and by similar I

00:29:01,170 --> 00:29:07,290
mean forked from the architecture of

00:29:03,630 --> 00:29:09,450
long cache except instead of that map of

00:29:07,290 --> 00:29:11,309
trees it's actually backed by a real

00:29:09,450 --> 00:29:14,610
time series database we're using the

00:29:11,309 --> 00:29:17,460
influx engine and then we've got the log

00:29:14,610 --> 00:29:20,130
cache architecture which kind of plugs

00:29:17,460 --> 00:29:23,850
this entire thing into your Cloud

00:29:20,130 --> 00:29:27,390
Foundry authentication and and routing

00:29:23,850 --> 00:29:32,370
experience already so what that looks

00:29:27,390 --> 00:29:35,760
like for us is that I as a user instead

00:29:32,370 --> 00:29:37,980
of saying listen you want app metrics

00:29:35,760 --> 00:29:40,170
and you want query ability you're gonna

00:29:37,980 --> 00:29:42,480
have to basically find someone who's

00:29:40,170 --> 00:29:45,120
admin and and write that plug-in for

00:29:42,480 --> 00:29:47,730
yourself what this enables is because

00:29:45,120 --> 00:29:51,300
we've hooked up this same architecture

00:29:47,730 --> 00:29:54,510
as log cache to a time series dB pardon

00:29:51,300 --> 00:29:57,870
me what this looks like

00:29:54,510 --> 00:30:03,270
is that I can as an app developer come

00:29:57,870 --> 00:30:06,530
in and say hey I would really love this

00:30:03,270 --> 00:30:09,540
query about these two apps and our

00:30:06,530 --> 00:30:12,630
metrics store and it's off proxy will

00:30:09,540 --> 00:30:14,640
look at those check with UA a check with

00:30:12,630 --> 00:30:18,150
Kathy confirm that you do actually have

00:30:14,640 --> 00:30:21,179
access to those things and then respond

00:30:18,150 --> 00:30:23,520
with your query data so what this means

00:30:21,179 --> 00:30:25,170
for operators is that you don't have as

00:30:23,520 --> 00:30:28,590
much responsibility for those

00:30:25,170 --> 00:30:31,950
authentication switches and for saying

00:30:28,590 --> 00:30:34,830
hey who is allowed to see this who

00:30:31,950 --> 00:30:37,309
should I be opening up this stream to

00:30:34,830 --> 00:30:40,070
because metrics store itself can

00:30:37,309 --> 00:30:42,590
leveraged just based off of their UAA

00:30:40,070 --> 00:30:46,600
credentials so that eases up a lot of

00:30:42,590 --> 00:30:46,600
responsibility in the operator's sphere

00:30:48,519 --> 00:30:54,080
so like we talked about people really

00:30:52,159 --> 00:30:58,190
wanted persistence and they wanted query

00:30:54,080 --> 00:31:01,129
ability so log cache was our first pass

00:30:58,190 --> 00:31:03,860
at using Prometheus query language there

00:31:01,129 --> 00:31:05,450
are a couple endpoints on log cache that

00:31:03,860 --> 00:31:09,980
do enable that and we learned from that

00:31:05,450 --> 00:31:13,360
because it was painful when you try and

00:31:09,980 --> 00:31:16,490
use a mature query language like from ql

00:31:13,360 --> 00:31:20,259
against a cache that is indeterminately

00:31:16,490 --> 00:31:22,909
long you get a lot of risk in terms of

00:31:20,259 --> 00:31:25,279
what might be coming out and what might

00:31:22,909 --> 00:31:28,700
look like correct data and is inherently

00:31:25,279 --> 00:31:30,919
misleading so when you're using a mature

00:31:28,700 --> 00:31:33,940
query language like from ql you really

00:31:30,919 --> 00:31:36,470
do need that extensive persistence and

00:31:33,940 --> 00:31:38,840
you don't need things self pruning

00:31:36,470 --> 00:31:41,360
themselves out of your your data

00:31:38,840 --> 00:31:43,879
collection in the background right so

00:31:41,360 --> 00:31:46,100
with metrics tor people really wanted to

00:31:43,879 --> 00:31:48,019
be able to say hey I would love

00:31:46,100 --> 00:31:49,789
information about this source ID I would

00:31:48,019 --> 00:31:53,059
love information about this component

00:31:49,789 --> 00:31:57,139
piece and get those consistent answers

00:31:53,059 --> 00:32:02,480
so metrics tor by default stores 42 days

00:31:57,139 --> 00:32:05,240
and it inherited that self pruning from

00:32:02,480 --> 00:32:08,629
log cache so we still apply that to

00:32:05,240 --> 00:32:10,279
metrics store but it's it's adjustable

00:32:08,629 --> 00:32:13,340
you can say hey I'd like 100 days a

00:32:10,279 --> 00:32:17,059
fifth-year org allows for that kind of

00:32:13,340 --> 00:32:21,110
storage but what it does do is say for

00:32:17,059 --> 00:32:25,999
the look-back of these six weeks you are

00:32:21,110 --> 00:32:29,690
guaranteed that persistence and this

00:32:25,999 --> 00:32:31,639
really fixed people's situation where

00:32:29,690 --> 00:32:34,909
they were like hey I would love a graph

00:32:31,639 --> 00:32:38,210
I would I would love a chart that I

00:32:34,909 --> 00:32:42,289
could roll for myself about any source

00:32:38,210 --> 00:32:44,570
ID focusing on any query that I might

00:32:42,289 --> 00:32:46,789
want to craft so if you look at

00:32:44,570 --> 00:32:48,690
something like Griffin ax right they

00:32:46,789 --> 00:32:52,539
have that

00:32:48,690 --> 00:32:54,099
inherent support for prom ql and if

00:32:52,539 --> 00:32:56,229
you've crafted your own query about

00:32:54,099 --> 00:33:00,729
things that are important to you on your

00:32:56,229 --> 00:33:03,339
particular platform it's it's enabled

00:33:00,729 --> 00:33:07,179
you to create those charts for yourself

00:33:03,339 --> 00:33:09,489
instead of being dependent on what other

00:33:07,179 --> 00:33:15,159
people might think your platform looks

00:33:09,489 --> 00:33:20,799
like so yeah the Prometheus API and the

00:33:15,159 --> 00:33:22,450
Prometheus query language are both what

00:33:20,799 --> 00:33:23,919
metrics drawer was written around so

00:33:22,450 --> 00:33:26,320
instead of crafting our own query

00:33:23,919 --> 00:33:29,619
language instead of giving you specific

00:33:26,320 --> 00:33:36,450
selectors or filters what we've done is

00:33:29,619 --> 00:33:41,349
is given you a healthy time series DB in

00:33:36,450 --> 00:33:44,259
your platform that is already linked up

00:33:41,349 --> 00:33:46,619
with UA a and with Cappy but has this

00:33:44,259 --> 00:33:49,839
very mature and very well-documented

00:33:46,619 --> 00:33:54,219
tool behind it which is prong QL and the

00:33:49,839 --> 00:33:57,629
Prometheus API so we've talked about use

00:33:54,219 --> 00:34:04,690
cases already a bit but yeah really

00:33:57,629 --> 00:34:08,049
charting your trend data and being able

00:34:04,690 --> 00:34:09,579
to pay attention to all of those service

00:34:08,049 --> 00:34:11,289
level indicators and service level

00:34:09,579 --> 00:34:13,779
objectives not having them defined for

00:34:11,289 --> 00:34:15,909
you but empowering you as an operator on

00:34:13,779 --> 00:34:17,740
your platform to say hey I would like to

00:34:15,909 --> 00:34:20,470
be measuring these specific things and

00:34:17,740 --> 00:34:23,260
these are my goals for it and being able

00:34:20,470 --> 00:34:25,089
to see those charts for yourself instead

00:34:23,260 --> 00:34:27,129
of having to work around the platform

00:34:25,089 --> 00:34:36,039
having the platform work for you to give

00:34:27,129 --> 00:34:41,379
you that visibility okay so in summary

00:34:36,039 --> 00:34:46,210
we have created in summary we've created

00:34:41,379 --> 00:34:48,000
a observability platform as a whole that

00:34:46,210 --> 00:34:50,260
hopefully leverages each of these

00:34:48,000 --> 00:34:52,869
hopefully enables each of these use

00:34:50,260 --> 00:34:54,399
cases for you to leverage the kind of

00:34:52,869 --> 00:34:57,220
data that you need for your situation

00:34:54,399 --> 00:35:01,920
and realizing that that's not one size

00:34:57,220 --> 00:35:05,650
fits all and that a streaming endpoint

00:35:01,920 --> 00:35:08,010
does need to be the first step in

00:35:05,650 --> 00:35:10,359
getting all of that data to you but then

00:35:08,010 --> 00:35:12,130
enabling that filtering and enabling

00:35:10,359 --> 00:35:15,130
that visibility for each of your use

00:35:12,130 --> 00:35:17,380
cases and not forcing you to roll your

00:35:15,130 --> 00:35:19,950
own solution which often all of our

00:35:17,380 --> 00:35:24,640
solutions look very very similar

00:35:19,950 --> 00:35:27,240
downstream from the log read or fire

00:35:24,640 --> 00:35:27,240
hose um

00:35:27,720 --> 00:35:34,150
and yeah I think basically enabling that

00:35:31,480 --> 00:35:40,119
experience for all of these folks who

00:35:34,150 --> 00:35:42,339
can say hey I am able to focus on this

00:35:40,119 --> 00:35:44,589
particular piece I really cared about

00:35:42,339 --> 00:35:46,750
charting and data and when you gave me a

00:35:44,589 --> 00:35:49,900
cache it was just not in my set of

00:35:46,750 --> 00:35:52,690
requirements like look back on component

00:35:49,900 --> 00:35:55,059
metrics was not important to me and now

00:35:52,690 --> 00:35:57,579
I can go over here and choose this peer

00:35:55,059 --> 00:36:00,190
of the cache that really does enable me

00:35:57,579 --> 00:36:02,109
for what I need by the way metrics store

00:36:00,190 --> 00:36:04,569
is deployable now it's in the

00:36:02,109 --> 00:36:09,760
experimental directory of CFD if you are

00:36:04,569 --> 00:36:11,670
looking for where that exists and yeah I

00:36:09,760 --> 00:36:15,309
think the last thing is just really a

00:36:11,670 --> 00:36:17,680
call for improvement like we built all

00:36:15,309 --> 00:36:20,589
of these things based off of consumer

00:36:17,680 --> 00:36:22,240
feedback and really strong activity in

00:36:20,589 --> 00:36:24,609
the open source channels of saying hey

00:36:22,240 --> 00:36:26,380
I've been building this and it hurts me

00:36:24,609 --> 00:36:29,410
hey I've been building this similar

00:36:26,380 --> 00:36:31,089
thing and I did it this way and look

00:36:29,410 --> 00:36:33,730
what I really need from you is something

00:36:31,089 --> 00:36:35,770
completely different and so continuing

00:36:33,730 --> 00:36:38,589
that open source community feedback and

00:36:35,770 --> 00:36:40,660
we really are working on improving right

00:36:38,589 --> 00:36:44,369
now there's a big focus on improving the

00:36:40,660 --> 00:36:47,619
logger read or agent itself to become a

00:36:44,369 --> 00:36:49,630
more powerful tool for all kinds of

00:36:47,619 --> 00:36:51,339
different metrics structured logs

00:36:49,630 --> 00:36:54,460
everything that you might be wanting to

00:36:51,339 --> 00:36:59,079
put in instead of pigeonholing you into

00:36:54,460 --> 00:37:00,670
our direction and so just continue

00:36:59,079 --> 00:37:03,579
giving us that feedback and engaging

00:37:00,670 --> 00:37:06,640
with us in in the open source slack

00:37:03,579 --> 00:37:08,859
channels etc and on github because that

00:37:06,640 --> 00:37:11,400
is how we're going to continue to build

00:37:08,859 --> 00:37:14,540
out this observability platform and

00:37:11,400 --> 00:37:16,550
really enable visibility for

00:37:14,540 --> 00:37:23,740
all of you about whatever you'd like to

00:37:16,550 --> 00:37:23,740
see hopefully so thank you any questions

00:37:24,040 --> 00:37:32,750
yes hold on so you mentioned Griffin and

00:37:31,490 --> 00:37:36,440
you mentioned that these are Prometheus

00:37:32,750 --> 00:37:38,510
API compatible has your team or any of

00:37:36,440 --> 00:37:41,000
the folks that have been using it that

00:37:38,510 --> 00:37:44,180
you know of been able to directly

00:37:41,000 --> 00:37:47,630
consumed the RLP gateway as a Prometheus

00:37:44,180 --> 00:37:51,260
or as a data store in Griffin AX so that

00:37:47,630 --> 00:37:52,700
it's talking straight to it then you

00:37:51,260 --> 00:37:55,850
know having a third party like

00:37:52,700 --> 00:37:57,380
Prometheus to collect the data and keep

00:37:55,850 --> 00:38:03,200
it

00:37:57,380 --> 00:38:05,180
I actually I don't think I have any off

00:38:03,200 --> 00:38:07,250
the top of my head because we've been

00:38:05,180 --> 00:38:10,310
focusing a lot on leveraging it from

00:38:07,250 --> 00:38:12,740
metric store so our LP gateway itself is

00:38:10,310 --> 00:38:17,510
again that HTTP API but it is not the

00:38:12,740 --> 00:38:19,430
Prometheus API so I don't have any

00:38:17,510 --> 00:38:26,050
examples of that for right now but I can

00:38:19,430 --> 00:38:26,050
follow up any other questions yeah

00:38:29,960 --> 00:38:34,670
I'm wondering if there's any plan to

00:38:31,970 --> 00:38:36,710
move the CF stale stuff into the core of

00:38:34,670 --> 00:38:38,420
the CF CLI so it becomes like this is

00:38:36,710 --> 00:38:41,030
like now the default way to get looks

00:38:38,420 --> 00:38:44,750
instead of like a separate plug-in like

00:38:41,030 --> 00:38:46,850
you have now yeah so right now the CF

00:38:44,750 --> 00:38:49,280
logs experience is still your standard

00:38:46,850 --> 00:38:51,350
and are still isn't adjacent plug-in I

00:38:49,280 --> 00:38:54,080
actually don't know where we landed on

00:38:51,350 --> 00:38:56,300
that there were talks the CLI is going

00:38:54,080 --> 00:38:58,760
through a pretty massive rewrite right

00:38:56,300 --> 00:39:02,270
now and so I don't know where we landed

00:38:58,760 --> 00:39:09,010
I know it's been discussed any other

00:39:02,270 --> 00:39:09,010
questions yeah thank you

00:39:10,220 --> 00:39:15,380
you said Lokesh introduced persistence

00:39:12,710 --> 00:39:19,310
and if I understand it correctly it's

00:39:15,380 --> 00:39:22,910
not deterministic how long you cash lock

00:39:19,310 --> 00:39:25,160
information I think it's based on a

00:39:22,910 --> 00:39:30,110
certain number how does this comply with

00:39:25,160 --> 00:39:33,740
gdpr yeah so log cash itself actually

00:39:30,110 --> 00:39:35,570
didn't it's ephemeral so what you do

00:39:33,740 --> 00:39:37,640
experience with this cash is that

00:39:35,570 --> 00:39:40,880
anytime you're rolling your boxes they

00:39:37,640 --> 00:39:45,050
actually end up dropping that entire

00:39:40,880 --> 00:39:47,150
history and so with gdpr I don't know I

00:39:45,050 --> 00:39:49,100
think that might be operator specific

00:39:47,150 --> 00:39:52,400
because we just built this ephemeral

00:39:49,100 --> 00:39:57,110
cache that is like hey you you could

00:39:52,400 --> 00:39:59,570
drop the data in me if you needed to but

00:39:57,110 --> 00:40:05,410
in terms of implementation of saying oh

00:39:59,570 --> 00:40:05,410
hey a year ago look back I would be

00:40:07,510 --> 00:40:13,580
hesitant to make any like sweeping

00:40:11,300 --> 00:40:15,650
generalizations about like oh and then

00:40:13,580 --> 00:40:17,470
it's a year ago so people are able to

00:40:15,650 --> 00:40:19,940
request like please get rid of that

00:40:17,470 --> 00:40:22,100
because my only solution for that would

00:40:19,940 --> 00:40:24,520
be to roll the boxes and be like now

00:40:22,100 --> 00:40:30,820
start building your cache again but

00:40:24,520 --> 00:40:34,400
realistically a lot of the log cache

00:40:30,820 --> 00:40:36,500
content and you're correct it is like a

00:40:34,400 --> 00:40:38,600
set number of nodes that can be scaled

00:40:36,500 --> 00:40:40,540
based off of operator preference I could

00:40:38,600 --> 00:40:43,760
get defaults to like a hundred thousand

00:40:40,540 --> 00:40:46,280
so for some folks that that does look

00:40:43,760 --> 00:40:48,020
like five minutes of cache for some

00:40:46,280 --> 00:40:51,400
folks that looks like three to four days

00:40:48,020 --> 00:40:54,859
of cache I don't really know how

00:40:51,400 --> 00:40:57,040
implementation against a please drop

00:40:54,859 --> 00:40:59,960
this old data would look like

00:40:57,040 --> 00:41:03,350
predominantly because it is pretty short

00:40:59,960 --> 00:41:06,050
term so there's no option to limit it to

00:41:03,350 --> 00:41:11,270
a certain period of time there is

00:41:06,050 --> 00:41:13,369
actually so if you go into the log

00:41:11,270 --> 00:41:17,840
Reader configuration or sorry at the log

00:41:13,369 --> 00:41:20,150
cache configuration you can say this is

00:41:17,840 --> 00:41:23,030
my number of nodes but also start

00:41:20,150 --> 00:41:25,130
pruning things that are older than

00:41:23,030 --> 00:41:29,840
and you can say hey I don't I don't want

00:41:25,130 --> 00:41:32,000
things I might be mixing up my metric

00:41:29,840 --> 00:41:33,560
store and log cash configurations you

00:41:32,000 --> 00:41:37,460
can definitely prune based off a memory

00:41:33,560 --> 00:41:39,200
percentage and then the time thing I

00:41:37,460 --> 00:41:41,780
might have to look back into because I

00:41:39,200 --> 00:41:43,520
don't think that's configurable right

00:41:41,780 --> 00:41:45,710
now that might be configurable in metric

00:41:43,520 --> 00:41:48,050
store and not log cash right now but we

00:41:45,710 --> 00:41:49,820
could add it just would be nice because

00:41:48,050 --> 00:41:53,060
then you could be compliant if you say

00:41:49,820 --> 00:41:55,280
we store we limit the storage of logs to

00:41:53,060 --> 00:41:57,320
maybe one day and whatever comes first

00:41:55,280 --> 00:42:00,590
one day or one hundred thousand entries

00:41:57,320 --> 00:42:02,600
it gets ruined yeah that's definitely a

00:42:00,590 --> 00:42:11,300
thing we could add pretty easily

00:42:02,600 --> 00:42:14,720
yeah any other questions

00:42:11,300 --> 00:42:18,020
yes that's the metric store and store

00:42:14,720 --> 00:42:21,740
everything from the local teachers or

00:42:18,020 --> 00:42:25,160
locks and metrics only a subset metric

00:42:21,740 --> 00:42:27,710
store is only metrics yeah just because

00:42:25,160 --> 00:42:30,590
of the way that the cardinality works

00:42:27,710 --> 00:42:34,070
out on any given platform if you are

00:42:30,590 --> 00:42:37,040
trying to store those logs that can look

00:42:34,070 --> 00:42:44,990
way way different like between apps and

00:42:37,040 --> 00:42:46,700
components the query speed was deeply

00:42:44,990 --> 00:42:49,730
sacrificed when you were trying to

00:42:46,700 --> 00:42:52,790
optimize for both like the cardinality

00:42:49,730 --> 00:42:54,320
of the metrics and the logs so there are

00:42:52,790 --> 00:42:56,780
two different responsibilities right now

00:42:54,320 --> 00:43:00,050
but a lot of the benefit that you get

00:42:56,780 --> 00:43:04,160
out of the prom qlm prom ql experience

00:43:00,050 --> 00:43:06,230
is like metrics focused anyway so it's

00:43:04,160 --> 00:43:07,820
there to have something similar for logs

00:43:06,230 --> 00:43:09,500
so have something is that for integrated

00:43:07,820 --> 00:43:12,740
for log story so you don't have to store

00:43:09,500 --> 00:43:14,870
it outside for that history there there

00:43:12,740 --> 00:43:16,700
is a thing in the works that I don't

00:43:14,870 --> 00:43:18,640
know if it's gonna be open source or not

00:43:16,700 --> 00:43:25,930
right

00:43:18,640 --> 00:43:25,930
so maybe any other questions

00:43:27,609 --> 00:43:32,520
all right thank you all so much for

00:43:30,259 --> 00:43:38,219
coming

00:43:32,520 --> 00:43:38,219

YouTube URL: https://www.youtube.com/watch?v=EeXyySencGM


