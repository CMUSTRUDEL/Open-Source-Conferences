Title: Managed CF-as-a-Service: An Operator's Cookbook - Andrea Aymon & Fabio Berchtold
Publication date: 2019-09-13
Playlist: Cloud Foundry Summit EU 2019 - The Hague
Description: 
	Managed CF-as-a-Service: An Operator's Cookbook - Andrea Aymon & Fabio Berchtold, Swisscom Schweiz AG 

Follow us on a journey of a managed CF-as-a-Service offering based on cf-deployment, starting from humble beginnings up until a production-ready platform. With all the work being handled by a single team in a true DevOps way - sharing all duties from development, engineering, deployments, operations to customer support. It highlights many different aspects starting with a future-oriented architecture up to automated testing and continuous deployment. There are many challenges in building such platform up and have it production ready with happy customers running their applications on it. It covers a wide range of interesting topics like CI/CD, release management, marketplace services, operations, security, testing and for sure customer support and incident management. Benefit from the experience of a DevOps team - learn about the past, present and future of a managed Cloud Foundry. 

For more info: https://www.cloudfoundry.org/
Captions: 
	00:00:00,089 --> 00:00:09,780
hello everyone I welcome you at our talk

00:00:05,089 --> 00:00:15,660
about a managed CF as a service it's a

00:00:09,780 --> 00:00:19,230
cookbook for an operator hi everyone my

00:00:15,660 --> 00:00:22,140
name is Fabio bertolt I've been working

00:00:19,230 --> 00:00:25,289
for the past five years as a cloud

00:00:22,140 --> 00:00:28,529
engineer for the Swisscom application

00:00:25,289 --> 00:00:30,869
cloud and mostly we've been working

00:00:28,529 --> 00:00:35,280
extensively with the open source cloud

00:00:30,869 --> 00:00:37,829
foundry there we use concourse ton of

00:00:35,280 --> 00:00:40,860
different Porsche leases and of course

00:00:37,829 --> 00:00:43,800
Porsche itself I've also been a cloud

00:00:40,860 --> 00:00:46,079
foundry ambassador in the past and I'm

00:00:43,800 --> 00:00:48,750
also a very passionate golang developer

00:00:46,079 --> 00:00:50,700
as well and of course contributed to

00:00:48,750 --> 00:00:54,480
various cloud foundry projects during

00:00:50,700 --> 00:00:56,610
that time and I also really like on the

00:00:54,480 --> 00:00:58,620
enjoy building new bosch releases for

00:00:56,610 --> 00:01:04,170
fun experimenting with new kinds of

00:00:58,620 --> 00:01:05,909
software and systems so yeah so yeah as

00:01:04,170 --> 00:01:08,520
I said my name is Andrea

00:01:05,909 --> 00:01:12,299
I stopped in working for Swisscom now

00:01:08,520 --> 00:01:14,100
for three years I started as a cloud

00:01:12,299 --> 00:01:17,549
engineer in the infrastructure team

00:01:14,100 --> 00:01:19,110
which was based on OpenStack but when a

00:01:17,549 --> 00:01:21,600
Swisscom decided to change their

00:01:19,110 --> 00:01:23,759
provider I decided to change to the

00:01:21,600 --> 00:01:27,479
bright side of life and joint compound

00:01:23,759 --> 00:01:30,450
retreat so yeah I love to automate

00:01:27,479 --> 00:01:33,930
things that's why I recently worked hard

00:01:30,450 --> 00:01:36,170
on the self-developed testing suite we

00:01:33,930 --> 00:01:40,729
will talk about that a bit later as well

00:01:36,170 --> 00:01:40,729
yeah so let's start

00:01:44,310 --> 00:01:50,230
so let's have a short view on this which

00:01:48,250 --> 00:01:51,970
Swisscom application cloud to give you

00:01:50,230 --> 00:01:56,200
some necessary background information

00:01:51,970 --> 00:01:59,410
for the upcoming topics in this talk

00:01:56,200 --> 00:02:01,870
the Swisscom application cloud is our

00:01:59,410 --> 00:02:04,210
own custom platform as a service

00:02:01,870 --> 00:02:06,330
offering which is based around the open

00:02:04,210 --> 00:02:09,040
source source version of cloud foundry

00:02:06,330 --> 00:02:10,750
we have a public offering which is

00:02:09,040 --> 00:02:14,710
available to anyone you can just go to

00:02:10,750 --> 00:02:17,500
developer swisco.com register and start

00:02:14,710 --> 00:02:19,120
pushing your apps there and we also have

00:02:17,500 --> 00:02:22,630
the private offering the virtual private

00:02:19,120 --> 00:02:25,240
offering for companies and we

00:02:22,630 --> 00:02:26,560
interconnect we have their own managed

00:02:25,240 --> 00:02:28,960
platform as a service installation

00:02:26,560 --> 00:02:31,090
within our data centers and it's

00:02:28,960 --> 00:02:33,340
actually interconnected within the

00:02:31,090 --> 00:02:37,630
company's datacenter and its private

00:02:33,340 --> 00:02:39,670
internal network so this

00:02:37,630 --> 00:02:43,000
interconnectivity is actually one of

00:02:39,670 --> 00:02:45,090
Swisscom specialities we also have a

00:02:43,000 --> 00:02:48,400
variety of service we offer to our

00:02:45,090 --> 00:02:51,580
marketplace it consists of the usual

00:02:48,400 --> 00:02:55,060
suspects like MongoDB Redis Murray DB

00:02:51,580 --> 00:02:57,010
and so on all of this is basically one

00:02:55,060 --> 00:03:01,060
confounder installation stretched over

00:02:57,010 --> 00:03:03,820
three physically different located data

00:03:01,060 --> 00:03:05,590
centers and those are spread over three

00:03:03,820 --> 00:03:09,959
availability zones as you can see on the

00:03:05,590 --> 00:03:12,340
picture those are all managed by Bosh I

00:03:09,959 --> 00:03:15,750
think with that in your backpack you're

00:03:12,340 --> 00:03:15,750
now ready to join us on our journey

00:03:15,780 --> 00:03:20,470
about experience we made the secure

00:03:18,190 --> 00:03:23,620
DevOps engineer maintaining and shearing

00:03:20,470 --> 00:03:26,380
the whole platform with only a small

00:03:23,620 --> 00:03:29,530
team because we were only three people

00:03:26,380 --> 00:03:32,320
and just lately we were four and just a

00:03:29,530 --> 00:03:33,970
week ago a new team member joined so we

00:03:32,320 --> 00:03:36,000
were managing that for a whole for a

00:03:33,970 --> 00:03:38,620
long time only with three people and

00:03:36,000 --> 00:03:42,630
it's also maybe surprising to know

00:03:38,620 --> 00:03:42,630
that's about 16,000 VMs

00:03:50,450 --> 00:03:57,030
so as I now get a general impression how

00:03:54,180 --> 00:03:59,010
our architecture looks like I will try

00:03:57,030 --> 00:04:02,910
to explain you how our services are

00:03:59,010 --> 00:04:04,860
integrated in this landscape you see on

00:04:02,910 --> 00:04:07,020
the picture that we have three example

00:04:04,860 --> 00:04:09,660
tenants there are different foundations

00:04:07,020 --> 00:04:12,260
and all services are existing within

00:04:09,660 --> 00:04:12,260
this tenant

00:04:12,450 --> 00:04:17,580
there is also a service broker it's

00:04:14,580 --> 00:04:21,750
deployed as an app on top of top foundry

00:04:17,580 --> 00:04:22,950
and as you we have like the exactly have

00:04:21,750 --> 00:04:24,570
forgot about that we have like the

00:04:22,950 --> 00:04:27,450
content based services which are

00:04:24,570 --> 00:04:29,490
deployed within the kubernetes and also

00:04:27,450 --> 00:04:33,930
the bush based services which are

00:04:29,490 --> 00:04:36,570
deployed as VMs as you can see this

00:04:33,930 --> 00:04:39,620
setup is still current still currently

00:04:36,570 --> 00:04:42,390
in use but it brings a few pain points

00:04:39,620 --> 00:04:44,880
for example we produced services many

00:04:42,390 --> 00:04:47,820
many times but we only consume we can't

00:04:44,880 --> 00:04:50,790
consume them from everywhere so they are

00:04:47,820 --> 00:04:55,590
tightly coupled to the CF and to

00:04:50,790 --> 00:04:57,480
overcome this pain points we came up

00:04:55,590 --> 00:05:00,120
with the idea of centralized services

00:04:57,480 --> 00:05:04,710
that means we produce the service once

00:05:00,120 --> 00:05:06,870
and can consume them from everywhere in

00:05:04,710 --> 00:05:09,450
other words that means that the services

00:05:06,870 --> 00:05:11,490
are fully reusable you can imagine this

00:05:09,450 --> 00:05:13,919
in three layers I try to explain it from

00:05:11,490 --> 00:05:16,400
a technical point of view the first

00:05:13,919 --> 00:05:19,430
layer are representing the consumers

00:05:16,400 --> 00:05:24,780
this cannot be anything a cloud provider

00:05:19,430 --> 00:05:27,090
user whatever you can imagine of then

00:05:24,780 --> 00:05:29,190
the second layer represents the

00:05:27,090 --> 00:05:31,020
interconnections the brokering which

00:05:29,190 --> 00:05:34,200
consists of a data plane and the control

00:05:31,020 --> 00:05:35,190
plane there is also the OSP api is part

00:05:34,200 --> 00:05:37,140
of it

00:05:35,190 --> 00:05:40,950
it's responsible for the service

00:05:37,140 --> 00:05:42,780
exposure and the third layer or to

00:05:40,950 --> 00:05:45,270
producers and this can be anything who

00:05:42,780 --> 00:05:47,370
produces a service like RabbitMQ s3

00:05:45,270 --> 00:05:51,360
Oracle databases whatever you can

00:05:47,370 --> 00:05:53,340
imagine and the goal is that in the end

00:05:51,360 --> 00:05:55,260
every produced service can be consumed

00:05:53,340 --> 00:05:57,710
from everywhere and not only within the

00:05:55,260 --> 00:05:57,710
app cloud

00:06:00,750 --> 00:06:12,250
so well okay does it work yeah so a few

00:06:08,560 --> 00:06:15,130
things we learned over that time is for

00:06:12,250 --> 00:06:16,990
example is you should build your Bosch

00:06:15,130 --> 00:06:20,080
releases when you build them to be a

00:06:16,990 --> 00:06:22,150
self-contained as possible you what that

00:06:20,080 --> 00:06:27,300
means is you should not have your Bosch

00:06:22,150 --> 00:06:29,470
release or your deployments rely on any

00:06:27,300 --> 00:06:31,540
components or infrastructure that needs

00:06:29,470 --> 00:06:34,690
to be there beforehand so when you build

00:06:31,540 --> 00:06:36,910
something it should be like as easy as

00:06:34,690 --> 00:06:38,970
possible to reason about this deployment

00:06:36,910 --> 00:06:42,880
and not have any external dependencies

00:06:38,970 --> 00:06:45,820
an example of this is our self developed

00:06:42,880 --> 00:06:48,790
task scheduler a Porsche release it's

00:06:45,820 --> 00:06:51,310
basically one deployment with just one

00:06:48,790 --> 00:06:53,410
single instance group of VMs and

00:06:51,310 --> 00:06:57,150
everything is co-located within it has

00:06:53,410 --> 00:06:59,650
like a service broker on this VM the

00:06:57,150 --> 00:07:01,750
distributed chrome back-end it has like

00:06:59,650 --> 00:07:04,200
the task runner which interfaces with

00:07:01,750 --> 00:07:07,330
Cloud Foundry to run tasks also

00:07:04,200 --> 00:07:10,750
co-located it has VBR scripts for doing

00:07:07,330 --> 00:07:12,910
backups we have a backup manager called

00:07:10,750 --> 00:07:15,520
batman' which is also co-located and

00:07:12,910 --> 00:07:19,330
takes care of managing and running these

00:07:15,520 --> 00:07:21,730
PBR scripts and we also make use of the

00:07:19,330 --> 00:07:24,670
route registrar to register the service

00:07:21,730 --> 00:07:27,460
broker on the system domain of Cloud

00:07:24,670 --> 00:07:29,920
Foundry so this is a everything is

00:07:27,460 --> 00:07:32,290
included within just one deployment and

00:07:29,920 --> 00:07:35,020
it makes it easier for our operators to

00:07:32,290 --> 00:07:37,480
not have to reason about on how this

00:07:35,020 --> 00:07:40,320
works with what other components and so

00:07:37,480 --> 00:07:42,880
on so the lesson learned here was that

00:07:40,320 --> 00:07:45,430
yeah the more complex you make a

00:07:42,880 --> 00:07:49,920
deployment of course the more problems

00:07:45,430 --> 00:07:54,430
you're gonna have or might have and also

00:07:49,920 --> 00:07:57,040
something we learned is that not

00:07:54,430 --> 00:08:00,610
everything needs to be an H a deployment

00:07:57,040 --> 00:08:03,970
actually sometimes it's enough to just

00:08:00,610 --> 00:08:07,090
have a single instance of a proven our

00:08:03,970 --> 00:08:11,620
DBMS you don't always need a consensus

00:08:07,090 --> 00:08:13,540
algorithm powered clustered system yeah

00:08:11,620 --> 00:08:16,060
an example of that would

00:08:13,540 --> 00:08:19,750
we in the past we've had quite a few

00:08:16,060 --> 00:08:24,160
issues with the Galera from ZF my sequel

00:08:19,750 --> 00:08:26,440
actually and over time we've migrated a

00:08:24,160 --> 00:08:29,560
bunch of our back-end systems from this

00:08:26,440 --> 00:08:31,830
Calera to a single Postgres deployment

00:08:29,560 --> 00:08:35,680
and since then it has been much more

00:08:31,830 --> 00:08:37,479
robust and stable actually even if it's

00:08:35,680 --> 00:08:42,310
just a single instance I mean sometimes

00:08:37,479 --> 00:08:44,500
even even a single SQLite file can be

00:08:42,310 --> 00:08:48,040
enough for your needs you don't always

00:08:44,500 --> 00:08:50,110
need an H a solution and we've done that

00:08:48,040 --> 00:08:52,720
also with the Bosh director in the

00:08:50,110 --> 00:08:55,330
beginning we had the Bosh database

00:08:52,720 --> 00:08:58,630
externally hosted also on a cólera but

00:08:55,330 --> 00:09:01,690
this has been quite problematic we had

00:08:58,630 --> 00:09:04,180
issues with connection loss and with

00:09:01,690 --> 00:09:06,190
Galera itself and at some point we

00:09:04,180 --> 00:09:08,620
decide that we need to migrate back to

00:09:06,190 --> 00:09:11,320
the co-located Postgres on the Bosh

00:09:08,620 --> 00:09:15,640
director just with the addition of our

00:09:11,320 --> 00:09:18,970
automated backup tools yeah so the

00:09:15,640 --> 00:09:23,670
digest of this is you should design your

00:09:18,970 --> 00:09:23,670
Bosh deployments to be simple but robust

00:09:25,380 --> 00:09:34,450
it works another example of this is the

00:09:30,730 --> 00:09:37,870
aforementioned pac-man Bosh release that

00:09:34,450 --> 00:09:40,300
we developed this is basically a also I

00:09:37,870 --> 00:09:43,390
say self-contained Bosh release it just

00:09:40,300 --> 00:09:45,550
contains like one chop which is the back

00:09:43,390 --> 00:09:48,790
man shop that you can co-locate on to

00:09:45,550 --> 00:09:51,550
any other Bosh deployment and it's been

00:09:48,790 --> 00:09:54,130
built to take care of and managing the

00:09:51,550 --> 00:09:57,250
bbr script so what this pac-man

00:09:54,130 --> 00:10:00,460
component does is it runs the the bbr

00:09:57,250 --> 00:10:04,000
scripts based on a cron top based

00:10:00,460 --> 00:10:07,030
schedule and it package ups the bbr

00:10:04,000 --> 00:10:09,790
artifacts and it archives them onto s3

00:10:07,030 --> 00:10:11,980
and it also takes care of these backup

00:10:09,790 --> 00:10:18,100
files on s3 based on a predefined

00:10:11,980 --> 00:10:20,560
retention policy yeah so this it's an

00:10:18,100 --> 00:10:22,600
example of like a small but very simple

00:10:20,560 --> 00:10:25,480
component that just does one thing and

00:10:22,600 --> 00:10:27,040
does it well it's a battery's completely

00:10:25,480 --> 00:10:29,800
batteries included

00:10:27,040 --> 00:10:32,080
the solution that has no needs for any

00:10:29,800 --> 00:10:33,490
external interaction whatsoever it

00:10:32,080 --> 00:10:35,830
doesn't need an ancient server based

00:10:33,490 --> 00:10:38,320
system or anything else you simply just

00:10:35,830 --> 00:10:41,050
co-locate it on any deployment that

00:10:38,320 --> 00:10:44,770
provides you with a PBR script and it

00:10:41,050 --> 00:10:47,560
will just work yeah so you should always

00:10:44,770 --> 00:10:49,780
try to take advantage of the

00:10:47,560 --> 00:10:51,880
functionality Bosh actually gives you

00:10:49,780 --> 00:10:55,120
and not be fighting against it so in

00:10:51,880 --> 00:10:57,720
this case taking use of or using the

00:10:55,120 --> 00:11:01,810
simplicity of the BB our concept and

00:10:57,720 --> 00:11:03,490
ingenuity behind it yeah and as I've

00:11:01,810 --> 00:11:05,530
mentioned earlier we've by now also

00:11:03,490 --> 00:11:07,300
co-located this thing on our Bosh

00:11:05,530 --> 00:11:09,910
director to take care of the Bosh

00:11:07,300 --> 00:11:12,400
database we've co-located it to

00:11:09,910 --> 00:11:13,270
basically anything that has PBR scripts

00:11:12,400 --> 00:11:17,020
or Galera

00:11:13,270 --> 00:11:19,120
Postgres come through cred hub UAE

00:11:17,020 --> 00:11:28,500
basically almost anything with the state

00:11:19,120 --> 00:11:31,240
now has this component co-located so

00:11:28,500 --> 00:11:34,690
another thing we learned is pretty early

00:11:31,240 --> 00:11:39,040
on that we need to have some kind of way

00:11:34,690 --> 00:11:41,050
to track releases and their versions we

00:11:39,040 --> 00:11:44,140
needed to have some kind of a proper

00:11:41,050 --> 00:11:46,330
release management system and what we

00:11:44,140 --> 00:11:48,790
did is we invented what we call cloud

00:11:46,330 --> 00:11:51,670
releases it's kind of a you can think of

00:11:48,790 --> 00:11:53,650
it as a metadata repository which tracks

00:11:51,670 --> 00:11:55,780
all these different artifacts like

00:11:53,650 --> 00:11:58,990
Porsche Alisa Center versions docker

00:11:55,780 --> 00:12:03,850
images stem cells build packs anything

00:11:58,990 --> 00:12:05,860
you name it we have like built and test

00:12:03,850 --> 00:12:09,070
pipelines in our lab environment they

00:12:05,860 --> 00:12:12,940
track all these artifacts from upstream

00:12:09,070 --> 00:12:17,230
either from bojayá or github or our own

00:12:12,940 --> 00:12:19,480
internal kit lab and they so they track

00:12:17,230 --> 00:12:21,370
T's they build it they run functional

00:12:19,480 --> 00:12:23,290
and unit tests and if it's successful

00:12:21,370 --> 00:12:25,300
they will add it to this metadata

00:12:23,290 --> 00:12:27,850
repository with the version info and

00:12:25,300 --> 00:12:31,450
everything needed and then at some point

00:12:27,850 --> 00:12:33,550
in time we can decide to cut a app cloud

00:12:31,450 --> 00:12:35,350
cloud release what we call it out of

00:12:33,550 --> 00:12:37,990
these versions where we specify the

00:12:35,350 --> 00:12:39,940
different Porsche releases the versions

00:12:37,990 --> 00:12:40,780
they have and all the artifacts bundled

00:12:39,940 --> 00:12:44,020
and packaged to

00:12:40,780 --> 00:12:46,150
and we promote this to our integration

00:12:44,020 --> 00:12:48,780
test environment and there it will be

00:12:46,150 --> 00:12:51,490
automatically deployed it runs some

00:12:48,780 --> 00:12:54,700
integration tests and to end testing is

00:12:51,490 --> 00:12:56,860
done and if it passes this stage we can

00:12:54,700 --> 00:12:58,810
say yeah this is a valid cloud release

00:12:56,860 --> 00:13:01,180
this is something we can use in

00:12:58,810 --> 00:13:03,360
production if we want to so we could

00:13:01,180 --> 00:13:07,480
promote this cloud release in the end to

00:13:03,360 --> 00:13:09,670
deploy it on production this this step

00:13:07,480 --> 00:13:11,850
is still intentionally manual at the end

00:13:09,670 --> 00:13:14,830
because we want to have control over

00:13:11,850 --> 00:13:16,390
which release gets deployed where but

00:13:14,830 --> 00:13:19,510
basically once it has passed the

00:13:16,390 --> 00:13:21,040
integration testing stage we are pretty

00:13:19,510 --> 00:13:25,450
confident that it will work in

00:13:21,040 --> 00:13:27,550
production it's really important to to

00:13:25,450 --> 00:13:29,610
have some kind of a release management

00:13:27,550 --> 00:13:33,370
system to be able to confidently

00:13:29,610 --> 00:13:35,560
reproduce for like 100% reproduce and

00:13:33,370 --> 00:13:37,540
redeploy always the same versions in

00:13:35,560 --> 00:13:39,100
every environment so you you're

00:13:37,540 --> 00:13:41,920
confident that you can always do the

00:13:39,100 --> 00:13:44,350
same thing everywhere because this is

00:13:41,920 --> 00:13:46,839
one of the things we actually do is all

00:13:44,350 --> 00:13:48,670
our environments that we manage for our

00:13:46,839 --> 00:13:50,920
customers they have exactly the same

00:13:48,670 --> 00:13:52,390
versions of cloud releases and Cloud

00:13:50,920 --> 00:13:55,089
Foundry and all these components

00:13:52,390 --> 00:13:57,310
deployed we try to not deviate anywhere

00:13:55,089 --> 00:13:59,230
it's everything is really the same thing

00:13:57,310 --> 00:14:01,030
we have the same pipeline everywhere

00:13:59,230 --> 00:14:08,370
with the click of one button it just

00:14:01,030 --> 00:14:12,490
gets deployed and that's it yeah so

00:14:08,370 --> 00:14:15,820
since we have these cloud releases this

00:14:12,490 --> 00:14:19,000
has become a pretty integral part for us

00:14:15,820 --> 00:14:20,740
and it's important that in how it helped

00:14:19,000 --> 00:14:23,560
us it cannot really be understated it

00:14:20,740 --> 00:14:25,690
was actually very useful we've we've

00:14:23,560 --> 00:14:27,940
expanded upon these cloud releases with

00:14:25,690 --> 00:14:31,120
basically all our tooling and pipelines

00:14:27,940 --> 00:14:33,280
we've built so we created a dashboard

00:14:31,120 --> 00:14:36,550
for these cloud releases that can show

00:14:33,280 --> 00:14:39,220
which versions of Porsche releases or

00:14:36,550 --> 00:14:42,130
stem cells or whatever they a specific

00:14:39,220 --> 00:14:44,100
cloud release contains all the metadata

00:14:42,130 --> 00:14:46,870
around it it even shows us which

00:14:44,100 --> 00:14:48,640
software versions of any particular

00:14:46,870 --> 00:14:50,620
software is contained within a Bosch

00:14:48,640 --> 00:14:52,810
release we also automate have a kind of

00:14:50,620 --> 00:14:54,460
an automated way to track the software

00:14:52,810 --> 00:14:56,410
inside a Bosch release

00:14:54,460 --> 00:14:59,140
and this is all shown on this dashboard

00:14:56,410 --> 00:15:01,270
and we also see which cloud release has

00:14:59,140 --> 00:15:03,580
been deployed in which environment at

00:15:01,270 --> 00:15:08,160
what point in time so we can consider

00:15:03,580 --> 00:15:12,850
history of everything we've also built a

00:15:08,160 --> 00:15:15,010
ops tool that we call AMC it's kind of a

00:15:12,850 --> 00:15:17,800
Swiss Army knife

00:15:15,010 --> 00:15:19,600
CLI tool that we've made and we've

00:15:17,800 --> 00:15:22,330
basically crammed everything we could

00:15:19,600 --> 00:15:24,700
think of into this one tool so this tool

00:15:22,330 --> 00:15:27,310
has knowledge over the cloud release a

00:15:24,700 --> 00:15:29,350
metadata and it knows how to automate

00:15:27,310 --> 00:15:31,780
everything within our environments

00:15:29,350 --> 00:15:34,750
basically from the deployment process

00:15:31,780 --> 00:15:36,700
itself handling locking zin to system

00:15:34,750 --> 00:15:39,070
any interaction with any system or

00:15:36,700 --> 00:15:41,500
component and basically everything is

00:15:39,070 --> 00:15:43,210
done through this one tool either from

00:15:41,500 --> 00:15:46,750
the pipeline or from an operator

00:15:43,210 --> 00:15:50,740
manually since it's a CLI tool so we've

00:15:46,750 --> 00:15:53,920
abstracted everything into this there is

00:15:50,740 --> 00:15:56,410
actually we had some issues in the past

00:15:53,920 --> 00:15:58,330
for example when we used the CF

00:15:56,410 --> 00:15:59,920
deployment or the bush deployment

00:15:58,330 --> 00:16:01,750
directly from upstream all these

00:15:59,920 --> 00:16:05,050
templates and ops files and we were just

00:16:01,750 --> 00:16:07,990
using normal boss CLI commands directly

00:16:05,050 --> 00:16:10,090
we started messing up things or it has

00:16:07,990 --> 00:16:13,210
changed in the meantime from github and

00:16:10,090 --> 00:16:15,820
so we were like we have to put this into

00:16:13,210 --> 00:16:17,290
this tool because this tool has all the

00:16:15,820 --> 00:16:19,690
knowledge about the releases and their

00:16:17,290 --> 00:16:22,330
versions and it knows exactly what needs

00:16:19,690 --> 00:16:24,340
to be ployed DP deployed in which step

00:16:22,330 --> 00:16:25,750
and where and so on every basically it

00:16:24,340 --> 00:16:30,430
knows everything that needs to be done

00:16:25,750 --> 00:16:33,070
so we tried to take away this mental

00:16:30,430 --> 00:16:34,600
overhead from the operator so you can

00:16:33,070 --> 00:16:37,450
just use the tool and it will do

00:16:34,600 --> 00:16:39,460
everything the same in every environment

00:16:37,450 --> 00:16:42,760
the same this was also important that we

00:16:39,460 --> 00:16:51,160
have like 100% reproducible deployments

00:16:42,760 --> 00:16:55,930
in every environment well I have to warn

00:16:51,160 --> 00:16:59,380
you maybe you do it differently but we

00:16:55,930 --> 00:17:02,410
don't use op ops files or upstream

00:16:59,380 --> 00:17:04,510
templates at all we do really our own

00:17:02,410 --> 00:17:07,089
templates and you may wonder why this

00:17:04,510 --> 00:17:08,500
really makes sense so for us to really

00:17:07,089 --> 00:17:10,540
main point is

00:17:08,500 --> 00:17:12,790
that we have a hundred percent

00:17:10,540 --> 00:17:14,949
flexibility in what we do we can do

00:17:12,790 --> 00:17:16,839
everything we want and yeah we're not

00:17:14,949 --> 00:17:20,199
restricted to anything which is upstream

00:17:16,839 --> 00:17:22,449
and the other point is and maybe you

00:17:20,199 --> 00:17:24,970
also see that different than we do but

00:17:22,449 --> 00:17:26,500
if you do your own templates then you're

00:17:24,970 --> 00:17:28,780
really forced to understand what you're

00:17:26,500 --> 00:17:31,330
doing what's the specs are what

00:17:28,780 --> 00:17:34,780
properties you have and how they gonna

00:17:31,330 --> 00:17:36,430
behave in the end so from our point of

00:17:34,780 --> 00:17:38,680
view it really makes sense to understand

00:17:36,430 --> 00:17:41,920
what you deploy and to understand the

00:17:38,680 --> 00:17:43,540
boss releases you have to also have like

00:17:41,920 --> 00:17:45,040
the hundred percent control over what

00:17:43,540 --> 00:17:48,570
you do this goes through only

00:17:45,040 --> 00:17:48,570
hand-in-hand with the release management

00:17:50,400 --> 00:17:55,120
from from the release to to the

00:17:53,020 --> 00:17:59,370
deployment strategies we also learned a

00:17:55,120 --> 00:18:01,480
lot in our journey the past few years

00:17:59,370 --> 00:18:04,660
when you start to build up such a

00:18:01,480 --> 00:18:07,030
platform it may work out to not have

00:18:04,660 --> 00:18:09,760
like a perfect engineered deployment

00:18:07,030 --> 00:18:11,830
strategy but with the environment growth

00:18:09,760 --> 00:18:14,650
and with the complexity of that the

00:18:11,830 --> 00:18:17,640
components changing continuously I

00:18:14,650 --> 00:18:21,070
really recommend to have like a stable

00:18:17,640 --> 00:18:25,180
stable and flexible deployment process

00:18:21,070 --> 00:18:27,550
in place I think if you do that properly

00:18:25,180 --> 00:18:31,570
then you really reduce operation

00:18:27,550 --> 00:18:33,790
overhead massively in the end so we have

00:18:31,570 --> 00:18:36,700
here a few recommendations from our side

00:18:33,790 --> 00:18:39,430
what we learned and you see the first

00:18:36,700 --> 00:18:41,770
point is a deployment shouldn't contain

00:18:39,430 --> 00:18:46,210
more than hundred VMs but how can you

00:18:41,770 --> 00:18:48,430
leverage that so yeah if an environment

00:18:46,210 --> 00:18:51,940
grows there you need probably more than

00:18:48,430 --> 00:18:54,250
100 VM so what we did in this case we

00:18:51,940 --> 00:18:56,740
choose split it to kubernetes and the

00:18:54,250 --> 00:18:59,770
Cloud Foundry into management and cell

00:18:56,740 --> 00:19:04,720
blocks and the cell box each cell blocks

00:18:59,770 --> 00:19:07,090
then contains a max of hundred VMs the

00:19:04,720 --> 00:19:09,580
other point is use a continuous thing

00:19:07,090 --> 00:19:12,250
dual to orchestrate that whole thing and

00:19:09,580 --> 00:19:14,980
what we do with countdown concourse and

00:19:12,250 --> 00:19:17,410
is at this point and I will show you in

00:19:14,980 --> 00:19:19,950
the next slide how pipeline Frommer's

00:19:17,410 --> 00:19:19,950
looks like

00:19:22,110 --> 00:19:26,350
also make sure and that's a really

00:19:24,340 --> 00:19:28,600
important point make sure that your

00:19:26,350 --> 00:19:31,210
deployment process is fully automated

00:19:28,600 --> 00:19:33,490
because with the growth of an

00:19:31,210 --> 00:19:35,980
environments that duration of a

00:19:33,490 --> 00:19:37,870
deployment increases a lot and at some

00:19:35,980 --> 00:19:40,030
point the deployment will easily take 12

00:19:37,870 --> 00:19:43,090
to 24 hours and you really don't want to

00:19:40,030 --> 00:19:45,820
attend that the whole time so also make

00:19:43,090 --> 00:19:49,360
sure that the alerts are also well

00:19:45,820 --> 00:19:51,780
integrated and are alerting you when you

00:19:49,360 --> 00:19:54,760
are deploying so it only to attend that

00:19:51,780 --> 00:19:57,000
as far we already said make sure that

00:19:54,760 --> 00:19:59,590
your deployments are a hundred percent

00:19:57,000 --> 00:20:01,660
reproducible so make sure that you

00:19:59,590 --> 00:20:03,580
standardize everything if even if you

00:20:01,660 --> 00:20:07,180
have different foundation try to keep

00:20:03,580 --> 00:20:10,300
them the similar really the same try to

00:20:07,180 --> 00:20:12,340
not make them diverge too much because

00:20:10,300 --> 00:20:16,650
you have the same upgrade path you have

00:20:12,340 --> 00:20:20,800
the same testing and hopefully something

00:20:16,650 --> 00:20:22,540
reproducible and yeah everything should

00:20:20,800 --> 00:20:27,670
work offline as well I'm not sure if you

00:20:22,540 --> 00:20:30,490
mentioned that probably yes so this is

00:20:27,670 --> 00:20:33,730
how one of our deployment pipeline looks

00:20:30,490 --> 00:20:35,620
like from a customer you don't see all

00:20:33,730 --> 00:20:38,320
the cellblocks because they didn't fit

00:20:35,620 --> 00:20:40,900
on a screen but you probably get an

00:20:38,320 --> 00:20:43,840
impression on how we do it you see the

00:20:40,900 --> 00:20:45,820
cloud releases there and we mainly

00:20:43,840 --> 00:20:50,500
maintain this Club family pipeline and

00:20:45,820 --> 00:20:51,880
decor on the right wear which it

00:20:50,500 --> 00:20:54,520
consists very much of the base

00:20:51,880 --> 00:20:58,180
components like the push itself console

00:20:54,520 --> 00:21:05,260
dock registry and yeah pretty much those

00:20:58,180 --> 00:21:09,340
things yeah okay

00:21:05,260 --> 00:21:12,640
which slide is that this one huh

00:21:09,340 --> 00:21:16,690
yeah a few things about pipelines how to

00:21:12,640 --> 00:21:19,060
pipeline or how to not pipeline one

00:21:16,690 --> 00:21:21,610
thing we've learned is that it's a good

00:21:19,060 --> 00:21:23,800
practice to have when you have like a

00:21:21,610 --> 00:21:25,960
long pipeline chain of deployments or

00:21:23,800 --> 00:21:28,630
whatever is to have like a trigger

00:21:25,960 --> 00:21:31,120
resource for it right you need some way

00:21:28,630 --> 00:21:32,710
to trigger this pipeline chain and what

00:21:31,120 --> 00:21:35,590
we've learned is that it's actually very

00:21:32,710 --> 00:21:38,200
useful to have a git repository for that

00:21:35,590 --> 00:21:43,120
who's like only purpose is to trigger

00:21:38,200 --> 00:21:46,000
this pipeline don't use releases or

00:21:43,120 --> 00:21:47,799
versions as triggers because if you have

00:21:46,000 --> 00:21:50,679
a deployment pipeline and let's say you

00:21:47,799 --> 00:21:53,679
want to just deploy the last step again

00:21:50,679 --> 00:21:56,409
of this whole deployment chain you would

00:21:53,679 --> 00:21:58,150
need to to go through everything if you

00:21:56,409 --> 00:21:59,890
if you want to change the versions if

00:21:58,150 --> 00:22:02,230
that's like your dependency to go

00:21:59,890 --> 00:22:03,700
through the chain but if you have just a

00:22:02,230 --> 00:22:05,880
trigger resource then you can just

00:22:03,700 --> 00:22:09,100
trigger the last step you can still

00:22:05,880 --> 00:22:11,140
select independently the version of

00:22:09,100 --> 00:22:12,789
whatever all your input is whatever

00:22:11,140 --> 00:22:16,110
Porsche lease or cloud release you want

00:22:12,789 --> 00:22:18,789
to deploy so yeah using a git repo

00:22:16,110 --> 00:22:21,520
resource or repository is actually very

00:22:18,789 --> 00:22:24,909
useful as input for deployment pipelines

00:22:21,520 --> 00:22:27,490
and you can also use the git resource to

00:22:24,909 --> 00:22:29,380
to chain together completely different

00:22:27,490 --> 00:22:31,210
pipelines actually based on a commit

00:22:29,380 --> 00:22:32,890
they do and the other pipeline picks up

00:22:31,210 --> 00:22:34,630
this commit and sees it needs to start

00:22:32,890 --> 00:22:36,720
and so on so you can coordinate

00:22:34,630 --> 00:22:41,409
pipelines even through through git

00:22:36,720 --> 00:22:43,120
repositories on the other hand oh we

00:22:41,409 --> 00:22:45,880
also have of course a lot of build

00:22:43,120 --> 00:22:48,399
pipelines and these are actually kind of

00:22:45,880 --> 00:22:52,210
the opposite here you want to have this

00:22:48,399 --> 00:22:54,730
pipeline as rigid rigid as possible it

00:22:52,210 --> 00:22:56,590
should enforce what version goes through

00:22:54,730 --> 00:22:58,240
the pipeline so you always want to have

00:22:56,590 --> 00:23:00,399
like the same release version or the

00:22:58,240 --> 00:23:01,899
same artifact be the one that goes

00:23:00,399 --> 00:23:03,640
through the pipeline and not change

00:23:01,899 --> 00:23:05,919
in-between obviously since you're like

00:23:03,640 --> 00:23:07,899
building and testing this thing to

00:23:05,919 --> 00:23:11,399
verify everything is fine you don't want

00:23:07,899 --> 00:23:15,159
it to change so here you should use

00:23:11,399 --> 00:23:21,340
versions as input or releases or

00:23:15,159 --> 00:23:25,299
artifacts another point Andrea mentioned

00:23:21,340 --> 00:23:27,820
it briefly just before is you need to

00:23:25,299 --> 00:23:30,370
have everything offline available it's

00:23:27,820 --> 00:23:32,799
also very important for for having a

00:23:30,370 --> 00:23:36,309
release management system like we do so

00:23:32,799 --> 00:23:38,620
you should not rely on Bosh io and

00:23:36,309 --> 00:23:41,649
github or anything up stream directly

00:23:38,620 --> 00:23:44,260
you should mirror and vendor everything

00:23:41,649 --> 00:23:47,289
just like you do in software development

00:23:44,260 --> 00:23:49,420
where you should render your ruby gems

00:23:47,289 --> 00:23:51,310
or your goal and packages you

00:23:49,420 --> 00:23:53,410
would render your borscht releases and

00:23:51,310 --> 00:23:57,280
stem cells so you should have a company

00:23:53,410 --> 00:23:59,680
local s3 for example and have some

00:23:57,280 --> 00:24:01,330
pipelines in a lab or deaf environment

00:23:59,680 --> 00:24:03,610
which tracks everything from upstream

00:24:01,330 --> 00:24:06,370
and then a mirror sit on to your local

00:24:03,610 --> 00:24:07,840
s3 and that's actually what we use as

00:24:06,370 --> 00:24:10,240
artifacts in our cloud release

00:24:07,840 --> 00:24:12,730
repository so we're sure we have

00:24:10,240 --> 00:24:16,150
everything offline available or offline

00:24:12,730 --> 00:24:19,840
in our from our perspective and then

00:24:16,150 --> 00:24:21,520
this way we can also deploy in customer

00:24:19,840 --> 00:24:24,220
environments that don't have internet

00:24:21,520 --> 00:24:30,460
access at all because everything is on

00:24:24,220 --> 00:24:32,860
our internal s3 storage yeah so this

00:24:30,460 --> 00:24:35,500
offline availability really is is one of

00:24:32,860 --> 00:24:37,720
the key points to have a proper release

00:24:35,500 --> 00:24:42,370
management system to be able to build

00:24:37,720 --> 00:24:46,650
this and to be able to be confident that

00:24:42,370 --> 00:24:49,450
you can always redeploy the same thing

00:24:46,650 --> 00:24:50,980
anywhere that nothing changes in between

00:24:49,450 --> 00:24:52,900
and you have all these versions

00:24:50,980 --> 00:24:57,040
available and everything is like really

00:24:52,900 --> 00:24:58,390
100% reproducible and really playable in

00:24:57,040 --> 00:25:01,780
the same way because as I mentioned

00:24:58,390 --> 00:25:03,850
before all our environments we have they

00:25:01,780 --> 00:25:06,370
look exactly the same we have one

00:25:03,850 --> 00:25:08,610
pipeline we just trigger it and take and

00:25:06,370 --> 00:25:13,570
grab all these artifacts from our own

00:25:08,610 --> 00:25:15,610
mirrored s3 storage basically and that's

00:25:13,570 --> 00:25:18,400
the way such a small team as we are can

00:25:15,610 --> 00:25:20,200
actually manage so many multiple dozens

00:25:18,400 --> 00:25:30,130
of full fledged environments with with

00:25:20,200 --> 00:25:32,380
all the stuff that's going on there as

00:25:30,130 --> 00:25:34,780
we continuously integrate the latest

00:25:32,380 --> 00:25:37,840
upstream components it's really

00:25:34,780 --> 00:25:39,730
necessary to have some reliable tests in

00:25:37,840 --> 00:25:42,720
place and what we do in tough

00:25:39,730 --> 00:25:44,620
environments we focus on unit and

00:25:42,720 --> 00:25:46,690
integration tests also part of

00:25:44,620 --> 00:25:48,970
end-to-end tests are taking place there

00:25:46,690 --> 00:25:51,070
but the main focus in the Deaf

00:25:48,970 --> 00:25:54,700
environment really relies on this EF

00:25:51,070 --> 00:25:56,200
acceptance test once we're in the int we

00:25:54,700 --> 00:25:59,350
execute the integration and the

00:25:56,200 --> 00:26:01,240
end-to-end tests for sure and our

00:25:59,350 --> 00:26:03,840
end-to-end test as I mentioned before

00:26:01,240 --> 00:26:07,600
they're really self built

00:26:03,840 --> 00:26:09,490
it's it's built based on the Ruby on the

00:26:07,600 --> 00:26:11,980
ruby r-spec so it's it's everything

00:26:09,490 --> 00:26:15,450
fully self developed and we continuously

00:26:11,980 --> 00:26:18,820
improve and distance those use cases

00:26:15,450 --> 00:26:22,899
based on incidents on regressions on

00:26:18,820 --> 00:26:25,299
edge cases on whatever we have yeah

00:26:22,899 --> 00:26:28,779
really special cases so that's always an

00:26:25,299 --> 00:26:30,490
ongoing story to an extent that it also

00:26:28,779 --> 00:26:34,720
includes the absent services so it

00:26:30,490 --> 00:26:36,909
should be covered everything we made

00:26:34,720 --> 00:26:39,190
that we first had like a dedicated

00:26:36,909 --> 00:26:42,700
testing team which just tested every

00:26:39,190 --> 00:26:44,559
release but we kind of thought like it's

00:26:42,700 --> 00:26:51,010
the better thing to automate that as

00:26:44,559 --> 00:26:53,909
well so we just did it like this so now

00:26:51,010 --> 00:26:56,980
we have prepared like three very

00:26:53,909 --> 00:26:58,539
frequently asked questions to our site

00:26:56,980 --> 00:27:00,399
so we thought like we're gonna bring

00:26:58,539 --> 00:27:03,429
them up here in our presentation already

00:27:00,399 --> 00:27:08,020
I think Bobby was done yeah

00:27:03,429 --> 00:27:10,779
so a common question is how do you

00:27:08,020 --> 00:27:13,510
mitigate a single point of failure of a

00:27:10,779 --> 00:27:15,640
single bush directive managing multiple

00:27:13,510 --> 00:27:17,200
ICS because as you've seen in the

00:27:15,640 --> 00:27:21,520
picture in the beginning we have three

00:27:17,200 --> 00:27:24,850
physical aces basically and as you might

00:27:21,520 --> 00:27:29,409
know Bosh is not a che yet maybe it will

00:27:24,850 --> 00:27:31,990
never be but who knows well one thing is

00:27:29,409 --> 00:27:34,419
what we do is we use a DNS host names

00:27:31,990 --> 00:27:36,880
for our Bosh and in pretty much any

00:27:34,419 --> 00:27:38,860
example you will see online or the Bosh

00:27:36,880 --> 00:27:42,010
deployment or anywhere it's always based

00:27:38,860 --> 00:27:44,020
on IP s but that would be very bad if we

00:27:42,010 --> 00:27:45,850
had to move around our Bosh director and

00:27:44,020 --> 00:27:48,159
if we move our Bosh directive from one

00:27:45,850 --> 00:27:50,200
AC to another one we cannot have the

00:27:48,159 --> 00:27:52,600
same IP because they're like physically

00:27:50,200 --> 00:27:54,460
different locations and we simply

00:27:52,600 --> 00:27:56,500
thought yeah hey there's this thing

00:27:54,460 --> 00:27:57,940
called DNS have you heard you heard of

00:27:56,500 --> 00:28:02,649
it before yeah probably

00:27:57,940 --> 00:28:05,169
so our Bosh propagates it's a DNS name

00:28:02,649 --> 00:28:07,149
to the Bosh agent of every deployed VM

00:28:05,169 --> 00:28:09,850
and then we can just move around posh to

00:28:07,149 --> 00:28:12,190
a different IP and all the deployed VM

00:28:09,850 --> 00:28:16,600
stay we'll just find posh again or our

00:28:12,190 --> 00:28:17,290
certificates are based on this host name

00:28:16,600 --> 00:28:21,370
and not the

00:28:17,290 --> 00:28:23,920
P and that's what we can do and also we

00:28:21,370 --> 00:28:26,080
use an external blobstore of course for

00:28:23,920 --> 00:28:28,840
Bosch which is our company internal s3

00:28:26,080 --> 00:28:31,960
and now here I'm contradicting myself

00:28:28,840 --> 00:28:34,750
actually in the past we also had the

00:28:31,960 --> 00:28:37,780
external Galera cluster as for the Bosch

00:28:34,750 --> 00:28:40,120
database but we've switched to the co

00:28:37,780 --> 00:28:42,130
located within the Bosch VM the Postgres

00:28:40,120 --> 00:28:44,830
we just make sure we have regular

00:28:42,130 --> 00:28:46,810
backups so in case we need to redeploy a

00:28:44,830 --> 00:28:49,450
Bosch director on a different AC we can

00:28:46,810 --> 00:28:55,600
spin it up very fast and that we will

00:28:49,450 --> 00:29:00,220
have almost no data loss yeah well so

00:28:55,600 --> 00:29:02,950
how do we handle multi-cloud closed

00:29:00,220 --> 00:29:04,390
foundry deployments and I'm not sure if

00:29:02,950 --> 00:29:06,700
we all have the same understanding of

00:29:04,390 --> 00:29:08,620
multi-cloud but for us it doesn't

00:29:06,700 --> 00:29:10,720
necessarily mean that you have like

00:29:08,620 --> 00:29:14,560
different cloud provider and anything

00:29:10,720 --> 00:29:16,890
and for us it's also necessary that we

00:29:14,560 --> 00:29:22,360
don't have like multiple installation /

00:29:16,890 --> 00:29:24,810
/ yeah / cloud it's like we have one

00:29:22,360 --> 00:29:27,130
single installation stretched over those

00:29:24,810 --> 00:29:31,000
availability zones or clouds or whatever

00:29:27,130 --> 00:29:35,500
so our recommendation is to just use one

00:29:31,000 --> 00:29:42,490
stretched CF deployment over those data

00:29:35,500 --> 00:29:45,010
centers yeah we've been asked this

00:29:42,490 --> 00:29:50,340
question before how how do we handle

00:29:45,010 --> 00:29:54,070
this multi cloud AEC connectivity and

00:29:50,340 --> 00:29:56,920
well what we have is like so we have

00:29:54,070 --> 00:30:00,040
these three availability zones and we

00:29:56,920 --> 00:30:02,650
just have one flat / 60 network that

00:30:00,040 --> 00:30:05,590
spans over them and each availability

00:30:02,650 --> 00:30:09,340
zone gets assigned I think a / 19 block

00:30:05,590 --> 00:30:12,040
out of it and now the thing is I

00:30:09,340 --> 00:30:13,900
actually don't know how the connectivity

00:30:12,040 --> 00:30:16,150
between the three ICS has been set up

00:30:13,900 --> 00:30:18,520
because I'm not a network expert but

00:30:16,150 --> 00:30:20,980
let's say it certainly helps to be

00:30:18,520 --> 00:30:23,590
working for a telco company because

00:30:20,980 --> 00:30:26,770
apparently we know how we do how to do

00:30:23,590 --> 00:30:28,390
that so yeah but I'm not the expert on

00:30:26,770 --> 00:30:30,429
it

00:30:28,390 --> 00:30:33,520
so with that we want to say thank you

00:30:30,429 --> 00:30:35,710
for your attention and we are now open

00:30:33,520 --> 00:30:38,049
to question I will move around with a

00:30:35,710 --> 00:30:41,080
micro so the answer also get captured on

00:30:38,049 --> 00:30:53,740
the video afterwards so feel free to ask

00:30:41,080 --> 00:30:59,650
if there are questions open there's a

00:30:53,740 --> 00:31:02,020
question you mentioned you had the

00:30:59,650 --> 00:31:07,900
Backman and the AMC developed yourself

00:31:02,020 --> 00:31:10,809
are those also publicly available no not

00:31:07,900 --> 00:31:14,020
yet or not at all the AMC tool probably

00:31:10,809 --> 00:31:16,150
would not be very feasible to put public

00:31:14,020 --> 00:31:18,700
because it's very specific to our

00:31:16,150 --> 00:31:20,830
environments it has it makes a lot of

00:31:18,700 --> 00:31:21,520
assumptions on how something needs to be

00:31:20,830 --> 00:31:23,799
and so on

00:31:21,520 --> 00:31:25,630
but just yesterday we actually had to

00:31:23,799 --> 00:31:27,850
discuss in discussion about open

00:31:25,630 --> 00:31:30,640
sourcing the back man because that one

00:31:27,850 --> 00:31:32,710
is extremely generic and maybe some

00:31:30,640 --> 00:31:34,150
people could benefit from it so we were

00:31:32,710 --> 00:31:43,090
thinking about open sourcing that

00:31:34,150 --> 00:31:45,940
one-year customer why using foundry in a

00:31:43,090 --> 00:31:52,600
cloud sorry can you repeat the typical

00:31:45,940 --> 00:31:55,660
cost of a typical customer well I don't

00:31:52,600 --> 00:31:59,830
know how to we have all kinds of

00:31:55,660 --> 00:32:01,900
customers I would say your average we

00:31:59,830 --> 00:32:05,590
try to cater mostly to the enterprise

00:32:01,900 --> 00:32:07,450
market and our customers in that regard

00:32:05,590 --> 00:32:11,080
are mostly bigger enterprise companies

00:32:07,450 --> 00:32:13,809
that just don't want to to manage this

00:32:11,080 --> 00:32:15,760
to manage this platform themselves so we

00:32:13,809 --> 00:32:20,140
just offer which is offer a managed

00:32:15,760 --> 00:32:21,790
platform as a service and yeah it's that

00:32:20,140 --> 00:32:23,260
they are big companies but they don't

00:32:21,790 --> 00:32:25,299
want to manage it

00:32:23,260 --> 00:32:27,610
this platform themselves that's

00:32:25,299 --> 00:32:30,669
basically our our target market or where

00:32:27,610 --> 00:32:33,610
we have some leeway in - and of course

00:32:30,669 --> 00:32:35,320
we also do the whole Swiss thing where

00:32:33,610 --> 00:32:37,210
we say yeah we're a Swiss company all

00:32:35,320 --> 00:32:40,059
our data is based in Switzerland so we

00:32:37,210 --> 00:32:43,230
can cut the - mainly - Swiss companies

00:32:40,059 --> 00:32:43,230
that's that's our market

00:32:49,790 --> 00:32:54,270
can you give him more insight in the

00:32:52,230 --> 00:32:56,220
moment it get more efficient to write

00:32:54,270 --> 00:33:00,540
your own manifest instead of applying

00:32:56,220 --> 00:33:02,310
yours files it's not just that we think

00:33:00,540 --> 00:33:04,470
it's it's it's better to write

00:33:02,310 --> 00:33:07,140
completely write your own templates it's

00:33:04,470 --> 00:33:09,480
also actually the started for historical

00:33:07,140 --> 00:33:11,640
reasons with us because we were pretty

00:33:09,480 --> 00:33:14,700
early on using the open source Cloud

00:33:11,640 --> 00:33:18,780
Foundry day the CF release back then and

00:33:14,700 --> 00:33:21,840
it was still I think spiff templates and

00:33:18,780 --> 00:33:24,960
spiff really was a pain in the ass to to

00:33:21,840 --> 00:33:26,700
make it work and it was hard to reason

00:33:24,960 --> 00:33:29,130
about how the merge process of these

00:33:26,700 --> 00:33:32,340
different templates work at some point

00:33:29,130 --> 00:33:34,140
Stark and Wayne spruce came along and we

00:33:32,340 --> 00:33:35,780
pretty much from that point on we

00:33:34,140 --> 00:33:39,390
started completely writing our own

00:33:35,780 --> 00:33:42,320
deployment manifest templates from

00:33:39,390 --> 00:33:45,240
scratch using spruce so we have our own

00:33:42,320 --> 00:33:47,520
emerge process everything is automated

00:33:45,240 --> 00:33:49,680
we have like a template format that we

00:33:47,520 --> 00:33:52,770
adhere to when writing this template and

00:33:49,680 --> 00:33:55,230
I think it actually has proven really

00:33:52,770 --> 00:33:57,540
beneficial because we really had to

00:33:55,230 --> 00:34:01,110
reason about every single components

00:33:57,540 --> 00:34:03,450
properties why is it set to this what

00:34:01,110 --> 00:34:05,580
does this property do how do these

00:34:03,450 --> 00:34:07,200
because Club boundary its consists of

00:34:05,580 --> 00:34:09,240
many many different components and we

00:34:07,200 --> 00:34:11,310
had to reason about how they interact

00:34:09,240 --> 00:34:14,160
with each other what does it mean and so

00:34:11,310 --> 00:34:16,590
on for for us it was really in the

00:34:14,160 --> 00:34:18,690
beginning it was quite annoying and it

00:34:16,590 --> 00:34:20,190
took a lot of effort but it turned out

00:34:18,690 --> 00:34:21,780
to be great because we have a better

00:34:20,190 --> 00:34:24,440
understanding of the whole system

00:34:21,780 --> 00:34:24,440
because of that

00:34:32,370 --> 00:34:39,060
can you describe the the workflow when

00:34:35,640 --> 00:34:44,250
1az fails and the boss director of multi

00:34:39,060 --> 00:34:47,220
CPI failure others potentially to the AZ

00:34:44,250 --> 00:34:49,320
which is working and and try to talk to

00:34:47,220 --> 00:34:53,670
the AZ and available

00:34:49,320 --> 00:34:57,000
what does it work for you yeah if one AC

00:34:53,670 --> 00:34:59,460
fails pretty much nothing happens I mean

00:34:57,000 --> 00:35:01,500
the big deployments Bosch goes into

00:34:59,460 --> 00:35:02,520
meltdown mode the resurrector will not

00:35:01,500 --> 00:35:06,470
do anything anymore

00:35:02,520 --> 00:35:09,090
and all our deployments they're like in

00:35:06,470 --> 00:35:11,340
multiples of three so because we use the

00:35:09,090 --> 00:35:14,430
three ACS we always have at least three

00:35:11,340 --> 00:35:17,340
types of VMs for everything and the loss

00:35:14,430 --> 00:35:20,310
of one AC is actually not noticeable at

00:35:17,340 --> 00:35:22,080
all at least for Club foundry there

00:35:20,310 --> 00:35:23,790
there really is no downtime nothing

00:35:22,080 --> 00:35:25,980
happens I mean some apps will may be

00:35:23,790 --> 00:35:27,480
moved around but we always encourage our

00:35:25,980 --> 00:35:30,180
customers to have more than one instance

00:35:27,480 --> 00:35:33,900
anyway and as far as I know the

00:35:30,180 --> 00:35:37,650
scheduler tries to to spread them out

00:35:33,900 --> 00:35:40,650
over ACS so the loss of an AC really

00:35:37,650 --> 00:35:42,270
isn't isn't an issue and usually if it's

00:35:40,650 --> 00:35:44,990
the one AC where the bush director

00:35:42,270 --> 00:35:47,820
itself is located we don't even bother

00:35:44,990 --> 00:35:49,860
moving or migrating the Boche director

00:35:47,820 --> 00:35:51,720
and spinning it up again if it's just

00:35:49,860 --> 00:35:55,020
like for one day or something that's

00:35:51,720 --> 00:35:57,000
completely fine so no impact on that

00:35:55,020 --> 00:36:00,300
airplane and control plane you just wait

00:35:57,000 --> 00:36:03,330
yeah of course I mean if it takes very

00:36:00,300 --> 00:36:05,610
long and if like an AC like a data

00:36:03,330 --> 00:36:07,260
center exploded or whatever then we

00:36:05,610 --> 00:36:09,780
might think about what to do but usually

00:36:07,260 --> 00:36:13,680
if it's just for like 24 hours we really

00:36:09,780 --> 00:36:15,830
we don't do anything we just wait thank

00:36:13,680 --> 00:36:15,830
you

00:36:22,640 --> 00:36:28,579
so you said you will just wait can you

00:36:26,089 --> 00:36:32,210
please elaborate the connection between

00:36:28,579 --> 00:36:35,779
an easy outage and single singleton

00:36:32,210 --> 00:36:38,089
database you mentioned earlier the one

00:36:35,779 --> 00:36:41,989
for the Bosch director or no the my

00:36:38,089 --> 00:36:45,349
scale wasn't it are the Postgres stuff

00:36:41,989 --> 00:36:47,900
yeah we there are some services which

00:36:45,349 --> 00:36:51,700
just like Bosch are single instance but

00:36:47,900 --> 00:36:55,220
they're not that important I would say

00:36:51,700 --> 00:36:57,319
we we would have the issue of not being

00:36:55,220 --> 00:36:58,970
able to provision new Bosch based

00:36:57,319 --> 00:37:01,400
services because we also have multiple

00:36:58,970 --> 00:37:03,829
so what we call service Porsches which

00:37:01,400 --> 00:37:05,480
are also deployed and those in turn they

00:37:03,829 --> 00:37:09,200
are fronted by a service broker and

00:37:05,480 --> 00:37:11,269
deploy databases and we lose the ability

00:37:09,200 --> 00:37:13,819
or our customers would lose the ability

00:37:11,269 --> 00:37:17,480
to provision new types of these services

00:37:13,819 --> 00:37:19,009
so that also would be an issue yes I'm

00:37:17,480 --> 00:37:21,499
not talking about the provisioning I

00:37:19,009 --> 00:37:23,779
mean if you have a single instance for

00:37:21,499 --> 00:37:27,920
sure if the AC is down you have two

00:37:23,779 --> 00:37:29,480
downtime or yeah but what I meant is

00:37:27,920 --> 00:37:32,059
these tea service Porsches they have a

00:37:29,480 --> 00:37:34,700
single instance of a database DT

00:37:32,059 --> 00:37:36,470
database offering we have for the

00:37:34,700 --> 00:37:38,599
customers they're always also they still

00:37:36,470 --> 00:37:41,509
use the Galera and they still have three

00:37:38,599 --> 00:37:43,489
nodes and so on the the single instance

00:37:41,509 --> 00:37:45,579
is really only on components which are

00:37:43,489 --> 00:37:48,829
not so important and they can survive

00:37:45,579 --> 00:37:50,630
being a day down and another thing we

00:37:48,829 --> 00:37:53,029
have is the app autoscaler the open

00:37:50,630 --> 00:37:55,910
source one that one has a single

00:37:53,029 --> 00:37:58,099
instance of a Postgres and it's true if

00:37:55,910 --> 00:38:00,529
the the a see where this Postgres

00:37:58,099 --> 00:38:03,619
database would be the our auto scaling

00:38:00,529 --> 00:38:05,839
service would be down and we would have

00:38:03,619 --> 00:38:07,910
to decide on what to do there if we need

00:38:05,839 --> 00:38:10,579
to spin up this Postgres database in a

00:38:07,910 --> 00:38:13,940
different a see yes that that's true but

00:38:10,579 --> 00:38:16,099
we think like this service can afford to

00:38:13,940 --> 00:38:20,210
be down for one day so so the single

00:38:16,099 --> 00:38:21,890
instance if it goes down it's all used

00:38:20,210 --> 00:38:26,980
in deployments where it's not so

00:38:21,890 --> 00:38:26,980
important to be up all the time

00:38:30,580 --> 00:38:38,030
I don't see any more question so have a

00:38:36,430 --> 00:38:41,849
nice day enjoy it

00:38:38,030 --> 00:38:41,849

YouTube URL: https://www.youtube.com/watch?v=5vGAj-e7nl8


