Title: Introduction to TensorFlow.js - Ember London - April  2018
Publication date: 2018-04-17
Playlist: Ember London
Description: 
	Introduction to TensorFlow.js

Anyone interested in TensorFlow.js, the JavaScript-based deep learning framework. Its history and how it works. Projects and how to get started.
Official site of TensorFlow.js: https://js.tensorflow.org/

Website: https://js.tensorflow.org/
Community Discord: https://discord.gg/64MVzQX

Nikos Katsikanis
Captions: 
	00:00:00,060 --> 00:00:05,910
the sort of performance characteristics

00:00:01,860 --> 00:00:09,990
of tends to four GS is like maybe a

00:00:05,910 --> 00:00:11,490
quarter of what four times slower than

00:00:09,990 --> 00:00:14,400
what you can do with native CUDA and

00:00:11,490 --> 00:00:18,660
Python stuff but still it's good good

00:00:14,400 --> 00:00:20,220
performance so I'm not betting but I'm

00:00:18,660 --> 00:00:22,529
sticking a lot of my energy and effort

00:00:20,220 --> 00:00:24,570
into this future direction I'm see

00:00:22,529 --> 00:00:26,310
eighty percent of my time right now I'm

00:00:24,570 --> 00:00:28,560
actually focusing on learning can't

00:00:26,310 --> 00:00:31,949
answer for also learning machine

00:00:28,560 --> 00:00:33,360
learning and the kinda cool thing is a

00:00:31,949 --> 00:00:36,600
sort of motivation from me there to

00:00:33,360 --> 00:00:37,800
learn this stuff what to interview in at

00:00:36,600 --> 00:00:39,329
school your trainer line up my

00:00:37,800 --> 00:00:40,500
mathematics it's kind of really boring

00:00:39,329 --> 00:00:43,260
and dry because you're not seeing any

00:00:40,500 --> 00:00:45,090
practical use for it so what I'm doing

00:00:43,260 --> 00:00:47,129
is I'm learning this machine learning

00:00:45,090 --> 00:00:49,020
course and I'm also playing around with

00:00:47,129 --> 00:00:50,820
tencel 40s and I saw higher level to see

00:00:49,020 --> 00:00:52,590
what I can do and then that I get that

00:00:50,820 --> 00:00:54,690
concept I don't understand now that more

00:00:52,590 --> 00:00:57,840
it beats me to go on back and look at

00:00:54,690 --> 00:00:59,520
the maths and Wikipedia no I want a

00:00:57,840 --> 00:01:01,260
warning when you start looking at some

00:00:59,520 --> 00:01:03,719
of these terms machine learning and

00:01:01,260 --> 00:01:07,170
you're going to Wikipedia you really

00:01:03,719 --> 00:01:09,659
quickly get into 20 tab grab a hold deep

00:01:07,170 --> 00:01:11,310
of terms and you still that's like it's

00:01:09,659 --> 00:01:12,810
like looking at a fractal all these

00:01:11,310 --> 00:01:14,810
terms you can go down and down down and

00:01:12,810 --> 00:01:17,400
down down

00:01:14,810 --> 00:01:20,280
but it's mummy it's really stimulating

00:01:17,400 --> 00:01:23,580
so there's good news and bad news here

00:01:20,280 --> 00:01:25,110
for you guys you guys are probably a lot

00:01:23,580 --> 00:01:26,250
weaker at maths compared to the people

00:01:25,110 --> 00:01:29,040
that are doing machine learning at the

00:01:26,250 --> 00:01:30,840
moment but you guys JavaScript

00:01:29,040 --> 00:01:32,280
experienced an application development

00:01:30,840 --> 00:01:37,770
give you much big advantage in those

00:01:32,280 --> 00:01:39,420
guys so all those people are moving want

00:01:37,770 --> 00:01:41,670
again to the web from path and I don't

00:01:39,420 --> 00:01:44,100
have to relearn how to do JavaScript

00:01:41,670 --> 00:01:45,450
application development you guys if you

00:01:44,100 --> 00:01:47,399
want to get their stuff you have to look

00:01:45,450 --> 00:01:49,110
at the maths and and there's gonna be

00:01:47,399 --> 00:01:50,759
hard if you guys want to you can't just

00:01:49,110 --> 00:01:54,540
casual again to their stuff you have to

00:01:50,759 --> 00:01:56,939
think okay one night a week or three

00:01:54,540 --> 00:01:58,590
nights a week for an hour yes I saw

00:01:56,939 --> 00:02:00,299
office like through roughcast I walls I

00:01:58,590 --> 00:02:01,740
mistake some doesn't so in order to

00:02:00,299 --> 00:02:06,030
actually help me in my learning process

00:02:01,740 --> 00:02:08,550
are on in this whole deep learning

00:02:06,030 --> 00:02:11,370
journey I create a desk or server for it

00:02:08,550 --> 00:02:12,900
and actually for people in Google about

00:02:11,370 --> 00:02:13,890
you have joined the server this is a

00:02:12,900 --> 00:02:17,300
discourse

00:02:13,890 --> 00:02:19,200
for 10 to the 4 GS and just kind of

00:02:17,300 --> 00:02:22,560
interesting discussions going on because

00:02:19,200 --> 00:02:26,819
a lot of these these concepts and stuff

00:02:22,560 --> 00:02:28,650
that scan our country key but yeah it

00:02:26,819 --> 00:02:33,270
does what about linking me up about this

00:02:28,650 --> 00:02:38,310
desk or server this is a tensor for GS

00:02:33,270 --> 00:02:40,920
website there are some interesting

00:02:38,310 --> 00:02:43,950
examples that there are the Kenard high

00:02:40,920 --> 00:02:46,709
level and there's a lot of advanced

00:02:43,950 --> 00:02:48,480
concepts in them that you know would

00:02:46,709 --> 00:02:50,670
take you a few months to figure out how

00:02:48,480 --> 00:02:52,560
was actually going on lower level but

00:02:50,670 --> 00:02:56,459
the libraries itself is kind of high

00:02:52,560 --> 00:02:59,610
level them a lot of the the algorithms

00:02:56,459 --> 00:03:00,989
and stuff for machine learning and for

00:02:59,610 --> 00:03:02,190
various shapes of artificial

00:03:00,989 --> 00:03:05,100
intelligence applications are built into

00:03:02,190 --> 00:03:07,260
the library and they're becoming more

00:03:05,100 --> 00:03:11,370
and more like the Python version of

00:03:07,260 --> 00:03:14,220
1040s so for example Python has support

00:03:11,370 --> 00:03:15,530
for inverting a matrix whereas Java

00:03:14,220 --> 00:03:21,329
Script one doesn't have that yet and

00:03:15,530 --> 00:03:24,450
what the light actually does it has you

00:03:21,329 --> 00:03:26,370
can do all the stuff in a CPU as well as

00:03:24,450 --> 00:03:29,940
a GPU and never of a library that

00:03:26,370 --> 00:03:33,299
manages all the calculations on the GPU

00:03:29,940 --> 00:03:37,170
and the code is kind of hairy because

00:03:33,299 --> 00:03:40,440
it's the the actual library tends to 4GS

00:03:37,170 --> 00:03:42,200
is having to manage a lot of were level

00:03:40,440 --> 00:03:44,850
details and operations on the GPU

00:03:42,200 --> 00:03:46,260
translated sort of from JavaScript and

00:03:44,850 --> 00:03:48,470
what it does internally is that has a

00:03:46,260 --> 00:03:52,410
big string Boulder and it builds up

00:03:48,470 --> 00:03:55,470
shader programs and make sure things

00:03:52,410 --> 00:03:56,820
like that and gets stuff to run in GPU

00:03:55,470 --> 00:03:59,010
but anyway a lot of stuff is actually

00:03:56,820 --> 00:04:00,780
can be headed for you from you and you

00:03:59,010 --> 00:04:03,030
can do as long as you understand the

00:04:00,780 --> 00:04:04,410
mathematics and the models you can do

00:04:03,030 --> 00:04:08,070
machine learning applications in a

00:04:04,410 --> 00:04:11,160
browser running on the GPU they're also

00:04:08,070 --> 00:04:15,120
working on North GS support so you can

00:04:11,160 --> 00:04:18,120
run the same calculations on GPU you can

00:04:15,120 --> 00:04:20,840
run all the CPU stuff on the nodes but

00:04:18,120 --> 00:04:20,840
not the GPU yet

00:04:24,210 --> 00:04:27,100
so that the tools are actually really

00:04:26,350 --> 00:04:28,449
really well-written

00:04:27,100 --> 00:04:29,860
there used to once in deep plunge es

00:04:28,449 --> 00:04:33,190
we're not quite as good as these ones

00:04:29,860 --> 00:04:34,570
but they've done a really good job here

00:04:33,190 --> 00:04:36,870
there's a lot of there's quite a few

00:04:34,570 --> 00:04:40,690
Google employees are work on us library

00:04:36,870 --> 00:04:41,770
I created one just to just to play

00:04:40,690 --> 00:04:45,910
around for difference different things

00:04:41,770 --> 00:04:47,740
and the one the deep learning examples

00:04:45,910 --> 00:04:49,690
were using rows and rows of Phi or

00:04:47,740 --> 00:04:52,840
something like that I don't like using

00:04:49,690 --> 00:04:56,639
birds by using webpack so I took one of

00:04:52,840 --> 00:04:59,800
their examples to to test for a

00:04:56,639 --> 00:05:03,310
quadratic equation and Android webpack

00:04:59,800 --> 00:05:06,729
way to import it and I used I used

00:05:03,310 --> 00:05:08,650
typescript so before the deep one jet

00:05:06,729 --> 00:05:10,090
library you know in a few months nobody

00:05:08,650 --> 00:05:12,250
asks yeah but no it was actually called

00:05:10,090 --> 00:05:13,270
deep lunge yes nobody look here but just

00:05:12,250 --> 00:05:14,830
to give you guys a lot about history

00:05:13,270 --> 00:05:17,500
because you there's about history with

00:05:14,830 --> 00:05:22,510
me you guys and deep lunge yes before

00:05:17,500 --> 00:05:24,340
the library was called deep learn and

00:05:22,510 --> 00:05:27,160
everything was underlined in space DL

00:05:24,340 --> 00:05:31,630
but whatever it was and then never the

00:05:27,160 --> 00:05:34,539
new library its pencil for and the

00:05:31,630 --> 00:05:40,300
import is no just that's actually split

00:05:34,539 --> 00:05:43,180
onto 7m p.m. modules it's just at tensor

00:05:40,300 --> 00:05:45,070
for I need widest replace to get the the

00:05:43,180 --> 00:05:49,660
new pencil for one versus the deep one

00:05:45,070 --> 00:05:52,380
one I just replaced DL with TF and then

00:05:49,660 --> 00:05:54,820
you can use all of the library features

00:05:52,380 --> 00:05:58,210
so just to give you an example of what a

00:05:54,820 --> 00:06:01,300
typical machine learning thing looks

00:05:58,210 --> 00:06:03,130
like an intensive or I'll just take you

00:06:01,300 --> 00:06:10,840
through the first tutorial here and

00:06:03,130 --> 00:06:12,880
explain what it's doing so I'll skip

00:06:10,840 --> 00:06:14,860
this bit here but the low-level building

00:06:12,880 --> 00:06:18,090
walks what what tensors are these things

00:06:14,860 --> 00:06:18,090
called tensors which are basically

00:06:18,780 --> 00:06:23,800
multi-dimensional arrays and they're

00:06:21,190 --> 00:06:26,500
immutable sort of when immutable

00:06:23,800 --> 00:06:30,340
straight the box you can you know and

00:06:26,500 --> 00:06:33,789
then so a typical problem the machine

00:06:30,340 --> 00:06:34,960
learning is trying to find the patterns

00:06:33,789 --> 00:06:37,900
two things

00:06:34,960 --> 00:06:40,990
whether it's be classifying emojis or in

00:06:37,900 --> 00:06:44,500
this example fitting a quadratic

00:06:40,990 --> 00:06:49,270
equation ax squared plus BX plus C Jen

00:06:44,500 --> 00:06:52,710
remember that from from high school then

00:06:49,270 --> 00:06:52,710
I don't know what quadratic equation is

00:06:55,530 --> 00:07:07,860
CV cigar y-axis in x-axis and you draw a

00:07:00,340 --> 00:07:11,800
line just regular line so for example

00:07:07,860 --> 00:07:13,330
this is this is a what I mean this is

00:07:11,800 --> 00:07:15,280
not a high performance application this

00:07:13,330 --> 00:07:16,720
is just like the CPU could run there's

00:07:15,280 --> 00:07:19,240
no problem this this training example

00:07:16,720 --> 00:07:21,490
but it's just to show you can I however

00:07:19,240 --> 00:07:25,000
what the library library does of this

00:07:21,490 --> 00:07:27,580
course on my Bitcoin share price use up

00:07:25,000 --> 00:07:30,849
here I can't see anyway today was a bit

00:07:27,580 --> 00:07:34,870
of a wild one for a Bitcoin ready so

00:07:30,849 --> 00:07:39,070
here a chart that we we know is an ax

00:07:34,870 --> 00:07:40,449
squared plus BX plus C and that's

00:07:39,070 --> 00:07:41,590
definitely more than you read nor if

00:07:40,449 --> 00:07:43,180
you're looking at me using machine

00:07:41,590 --> 00:07:45,010
learning in our real-world application

00:07:43,180 --> 00:07:47,740
but this is a simplify thing so we've

00:07:45,010 --> 00:07:52,080
got all these points here and what we

00:07:47,740 --> 00:07:55,150
want to do is we wanna train a machine

00:07:52,080 --> 00:07:57,760
learning program to work with the real

00:07:55,150 --> 00:08:00,520
data on what the coincidence are for

00:07:57,760 --> 00:08:05,909
that equation so a B and C for the

00:08:00,520 --> 00:08:07,780
quadratic equation so what you do if

00:08:05,909 --> 00:08:10,270
tensor for here is you create these

00:08:07,780 --> 00:08:12,729
variables and these aren't immutable

00:08:10,270 --> 00:08:17,560
variables and they're sort of magically

00:08:12,729 --> 00:08:23,590
updated well you train the model and and

00:08:17,560 --> 00:08:25,830
you eat yours in JavaScript so these are

00:08:23,590 --> 00:08:31,079
the you create these variables

00:08:25,830 --> 00:08:34,450
JavaScript and and these will be used

00:08:31,079 --> 00:08:38,500
throughout the application and this as

00:08:34,450 --> 00:08:39,729
you train it and the way the e train the

00:08:38,500 --> 00:08:42,490
application should create this thing

00:08:39,729 --> 00:08:47,200
called a production function and a loss

00:08:42,490 --> 00:08:48,410
function so what we do here is some this

00:08:47,200 --> 00:08:52,070
is a prediction

00:08:48,410 --> 00:08:57,260
so it takes an a value and then it

00:08:52,070 --> 00:08:59,510
multiplies it by the constants here and

00:08:57,260 --> 00:09:02,840
there are national Asian initialized at

00:08:59,510 --> 00:09:06,560
random its star so for example this will

00:09:02,840 --> 00:09:08,210
be ABC with a random number and the

00:09:06,560 --> 00:09:10,040
reason that we have to wrap it and say

00:09:08,210 --> 00:09:11,900
this tensor for scalar things that we

00:09:10,040 --> 00:09:14,540
can't connect commits to a tenser and

00:09:11,900 --> 00:09:16,040
tenser is a thing that tends of for

00:09:14,540 --> 00:09:16,670
manatees in the GPU and all that kind of

00:09:16,040 --> 00:09:19,220
stuff for you

00:09:16,670 --> 00:09:22,720
so these all these things that are

00:09:19,220 --> 00:09:28,130
tensors are sort of running on a GPU

00:09:22,720 --> 00:09:31,220
because much quicker and this prediction

00:09:28,130 --> 00:09:34,480
function takes in a value and then

00:09:31,220 --> 00:09:37,910
multiplies this tensor up of operations

00:09:34,480 --> 00:09:40,040
these all the operators upon GPU or CPU

00:09:37,910 --> 00:09:42,290
depending on whether you specify that on

00:09:40,040 --> 00:09:44,450
the CPU or CP isn't available and it

00:09:42,290 --> 00:09:46,670
gives you an output value and then well

00:09:44,450 --> 00:09:50,150
it does in the tutorial if you run on

00:09:46,670 --> 00:09:53,060
your laptop or plot the line so for this

00:09:50,150 --> 00:09:57,550
one it's not being trained I'll give you

00:09:53,060 --> 00:10:00,410
a line that doesn't fit the data that

00:09:57,550 --> 00:10:04,250
then you go into this what's chaining

00:10:00,410 --> 00:10:05,930
the models so you have to create these

00:10:04,250 --> 00:10:09,350
different things a loss function which

00:10:05,930 --> 00:10:13,130
compares the actual value versus predict

00:10:09,350 --> 00:10:15,160
put that leader value and then you

00:10:13,130 --> 00:10:19,370
create an optimizer function which then

00:10:15,160 --> 00:10:22,070
changes but the variables in a certain

00:10:19,370 --> 00:10:24,890
way in this case is called gradient

00:10:22,070 --> 00:10:27,800
descent it's a tournament you'd get onto

00:10:24,890 --> 00:10:30,140
and week two of the machine learning

00:10:27,800 --> 00:10:33,920
course the free one creating the same

00:10:30,140 --> 00:10:39,110
basically what this is a tries to find a

00:10:33,920 --> 00:10:40,640
point where your error function gets a

00:10:39,110 --> 00:10:43,160
lowest point and it says that's that's

00:10:40,640 --> 00:10:48,470
the value and that can work for any as

00:10:43,160 --> 00:10:50,330
much dimensions as you want so they have

00:10:48,470 --> 00:10:52,810
this statistic sort of stuff where you

00:10:50,330 --> 00:10:55,310
find them a lot of click commonly this a

00:10:52,810 --> 00:10:57,470
lot of the stuff intensive for is they

00:10:55,310 --> 00:10:59,839
use normal patterns within machine

00:10:57,470 --> 00:11:01,460
learning and those patterns are the same

00:10:59,839 --> 00:11:02,060
whether it's Python JavaScript or

00:11:01,460 --> 00:11:04,190
whatever

00:11:02,060 --> 00:11:06,440
just there's a lot of research papers

00:11:04,190 --> 00:11:07,910
have been taking you know this decades

00:11:06,440 --> 00:11:09,200
and decades of research that have

00:11:07,910 --> 00:11:16,340
basically come up with a lot of these

00:11:09,200 --> 00:11:20,600
things so yeah there's loss functions

00:11:16,340 --> 00:11:22,580
compares predictions to the values from

00:11:20,600 --> 00:11:26,960
your model and then you have this

00:11:22,580 --> 00:11:28,180
optimizer function in this case for this

00:11:26,960 --> 00:11:30,320
example we're using something called

00:11:28,180 --> 00:11:33,890
using descent or stochastic gradient

00:11:30,320 --> 00:11:35,510
descent and you have a chaining look so

00:11:33,890 --> 00:11:37,670
you have this thing training in the

00:11:35,510 --> 00:11:39,560
browser for like five seconds or

00:11:37,670 --> 00:11:41,270
something like that and it keeps

00:11:39,560 --> 00:11:43,010
adjusting the values of the variables

00:11:41,270 --> 00:11:44,810
until it finds a loss function is

00:11:43,010 --> 00:11:47,150
reached a minimum and it says this is

00:11:44,810 --> 00:11:50,330
the coefficients of the equation that's

00:11:47,150 --> 00:11:53,300
the simplest example decently that ISIF

00:11:50,330 --> 00:11:56,510
are learning the later examples and it's

00:11:53,300 --> 00:12:00,980
gone to images just how to look at a

00:11:56,510 --> 00:12:03,290
number and how to classify a handbrake

00:12:00,980 --> 00:12:05,150
number into actual error I'm still

00:12:03,290 --> 00:12:07,220
learning how that works there's a lot of

00:12:05,150 --> 00:12:08,840
stuff on to do the convolution there's

00:12:07,220 --> 00:12:12,020
some good videos on YouTube explain what

00:12:08,840 --> 00:12:15,860
convolution does and not sure what this

00:12:12,020 --> 00:12:18,920
is it looks for patterns within lighters

00:12:15,860 --> 00:12:22,070
and from what I understand as a sort of

00:12:18,920 --> 00:12:26,330
groups these patterns and gives you a

00:12:22,070 --> 00:12:29,690
probability of what number that hand

00:12:26,330 --> 00:12:33,410
rating was but I think I see em so it's

00:12:29,690 --> 00:12:35,390
gonna new stuff it's been doing for 10

00:12:33,410 --> 00:12:38,330
years it's but it's a fashion you know

00:12:35,390 --> 00:12:40,310
because I want to be you know people

00:12:38,330 --> 00:12:44,840
thought these conferences are not really

00:12:40,310 --> 00:12:46,670
talking too excited about you know way

00:12:44,840 --> 00:12:48,020
Bluebell and stuff like it's all a I so

00:12:46,670 --> 00:12:49,850
I want to be part of that a future

00:12:48,020 --> 00:12:52,760
that's why I'm that's why I'm basing a

00:12:49,850 --> 00:12:57,230
lot of time and stuff so anyway this in

00:12:52,760 --> 00:12:58,760
this case here you have your this is

00:12:57,230 --> 00:13:01,040
what's this training it so you have them

00:12:58,760 --> 00:13:05,150
the real values of x and and and the

00:13:01,040 --> 00:13:08,450
real values of Y and then but you don't

00:13:05,150 --> 00:13:10,310
know what they you don't know what the

00:13:08,450 --> 00:13:11,900
coins that the variables are and what

00:13:10,310 --> 00:13:16,510
what happens here is and it's optimized

00:13:11,900 --> 00:13:16,510
a function you check the loss of

00:13:16,709 --> 00:13:28,899
the equation above and you adjusts the

00:13:20,769 --> 00:13:31,199
variables for this is making sense it's

00:13:28,899 --> 00:13:31,199
not easy

00:13:38,340 --> 00:13:42,390
so any once you tree in this thing you

00:13:40,529 --> 00:13:44,820
end up with coefficients that minimize

00:13:42,390 --> 00:13:47,040
the loss function for the quadratic

00:13:44,820 --> 00:13:53,400
quadratic equation and actually it will

00:13:47,040 --> 00:13:56,260
find out the constants for you that's

00:13:53,400 --> 00:14:01,260
really all they wanted we talked about

00:13:56,260 --> 00:14:01,260

YouTube URL: https://www.youtube.com/watch?v=NE6HixlU-5A


