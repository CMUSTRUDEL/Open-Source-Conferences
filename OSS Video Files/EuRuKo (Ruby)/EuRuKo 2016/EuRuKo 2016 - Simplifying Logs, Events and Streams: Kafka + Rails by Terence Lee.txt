Title: EuRuKo 2016 - Simplifying Logs, Events and Streams: Kafka + Rails by Terence Lee
Publication date: 2016-10-11
Playlist: EuRuKo 2016
Description: 
	
Captions: 
	00:00:04,060 --> 00:00:13,250
and yeah now it's time for Terrance to

00:00:09,230 --> 00:00:15,980
come to the stage he is kuroko's Ruby

00:00:13,250 --> 00:00:20,150
he's the lead of the Roku's to Ruby task

00:00:15,980 --> 00:00:22,999
force he has worked on some OSS projects

00:00:20,150 --> 00:00:25,939
so some open source projects you have my

00:00:22,999 --> 00:00:28,730
heard of like Ruby the language and Ruby

00:00:25,939 --> 00:00:33,829
Ruby see a lie so you know small things

00:00:28,730 --> 00:00:37,940
like bundler rescue and he likes Friday

00:00:33,829 --> 00:00:40,250
hats like all of us I guess and also he

00:00:37,940 --> 00:00:44,060
believes in getting people together for

00:00:40,250 --> 00:00:53,210
Ruby kuroki so let's give a nice welcome

00:00:44,060 --> 00:01:00,800
to Terrance and to you the stage good

00:00:53,210 --> 00:01:03,500
afternoon Oh God turn this on nice happy

00:01:00,800 --> 00:01:04,759
Friday like I said I like Friday hugs if

00:01:03,500 --> 00:01:08,149
you've seen me and talked before you've

00:01:04,759 --> 00:01:11,869
probably seen me do one of these and

00:01:08,149 --> 00:01:13,729
this is actually my second year ago so

00:01:11,869 --> 00:01:16,390
the first one I want to was a while back

00:01:13,729 --> 00:01:20,119
so I'm excited to be back at another one

00:01:16,390 --> 00:01:22,969
and this of photo I took back then and

00:01:20,119 --> 00:01:26,060
so I would like to continue this

00:01:22,969 --> 00:01:27,829
tradition so I know you all just sat in

00:01:26,060 --> 00:01:29,929
your chairs but if you could all stand

00:01:27,829 --> 00:01:31,700
up if you're not familiar with Friday

00:01:29,929 --> 00:01:33,889
hug we essentially take a photo of

00:01:31,700 --> 00:01:37,310
people just like hugging I guess the

00:01:33,889 --> 00:01:40,609
camera so I do you want to come up and

00:01:37,310 --> 00:01:42,889
do the photo so if you could all join me

00:01:40,609 --> 00:01:47,920
and doing a Friday hug I like to collect

00:01:42,889 --> 00:01:47,920
these as a memorabilia of this thing

00:02:08,720 --> 00:02:10,780
you

00:02:29,099 --> 00:02:35,890
thanks for entertaining me with that so

00:02:33,189 --> 00:02:38,859
my name is Terrence I go by hone 02 on

00:02:35,890 --> 00:02:40,810
Twitter I'm mostly known for my blue hat

00:02:38,859 --> 00:02:43,450
not really in the open source stuff but

00:02:40,810 --> 00:02:46,510
I do have some blue hat stickers so if

00:02:43,450 --> 00:02:49,810
you would like some swag definitely come

00:02:46,510 --> 00:02:52,239
by and talk to me I worked at Heroku

00:02:49,810 --> 00:02:54,280
like they said I work on the Ruby

00:02:52,239 --> 00:02:57,189
experience stuff with Richard schneemann

00:02:54,280 --> 00:02:58,989
some of you might know him as well for

00:02:57,189 --> 00:03:02,079
some as rails work but if you deploy

00:02:58,989 --> 00:03:04,030
apps or Ruby Absaroka using stuff that

00:03:02,079 --> 00:03:05,530
we work on so if you have any problems

00:03:04,030 --> 00:03:08,650
with that you should come talk to us

00:03:05,530 --> 00:03:11,049
about that as well I did bring a few

00:03:08,650 --> 00:03:13,480
shirts and some stickers and iron on

00:03:11,049 --> 00:03:16,419
patches and stuff for Heroku but it's

00:03:13,480 --> 00:03:19,689
pretty limited so come say hi collect

00:03:16,419 --> 00:03:21,909
some Heroku swag as well i live in

00:03:19,689 --> 00:03:24,220
austin texas we have really amazing

00:03:21,909 --> 00:03:25,930
tacos if you are ever visiting in town

00:03:24,220 --> 00:03:27,549
definitely reach out to me and I'd be

00:03:25,930 --> 00:03:31,209
more than happy to take you out for some

00:03:27,549 --> 00:03:36,669
tacos I run a conference called keeper

00:03:31,209 --> 00:03:38,139
be weird which is also in Austin and one

00:03:36,669 --> 00:03:40,479
of my favorite things about the Ruby

00:03:38,139 --> 00:03:42,489
community is that we can do like weird

00:03:40,479 --> 00:03:45,159
fun things like Friday hug or Ruby

00:03:42,489 --> 00:03:47,199
karaoke and for those you who are not

00:03:45,159 --> 00:03:49,269
familiar with Ruby Cherokee it's what it

00:03:47,199 --> 00:03:51,819
sounds like you just get together and

00:03:49,269 --> 00:03:53,560
sing with a bunch of rubios some my

00:03:51,819 --> 00:03:55,900
favorite moments on the conference

00:03:53,560 --> 00:03:57,790
circuit has been going around with

00:03:55,900 --> 00:04:01,930
people like Charlie Nutter and watching

00:03:57,790 --> 00:04:05,290
him sing Kira Shakira he's actually a

00:04:01,930 --> 00:04:09,310
really amazing singer I think I was

00:04:05,290 --> 00:04:10,780
talking to someone yesterday at the

00:04:09,310 --> 00:04:13,750
speaker dinner about maybe getting a

00:04:10,780 --> 00:04:15,370
crew together tomorrow after the

00:04:13,750 --> 00:04:18,220
conference ends so if you're still

00:04:15,370 --> 00:04:19,570
around you like to sing some tunes we'll

00:04:18,220 --> 00:04:24,039
try to get a crew together to do some

00:04:19,570 --> 00:04:29,500
singing so now that i'm done with intro

00:04:24,039 --> 00:04:31,960
let's talk about Kafka so the agenda for

00:04:29,500 --> 00:04:34,360
today is we're going to talk in about

00:04:31,960 --> 00:04:34,840
kapha talk about some design constraints

00:04:34,360 --> 00:04:38,050
and some term

00:04:34,840 --> 00:04:41,740
knology there we'll show you how you can

00:04:38,050 --> 00:04:45,850
use Kafka from Ruby itself will take a

00:04:41,740 --> 00:04:47,949
deep dive and a more concrete use case

00:04:45,850 --> 00:04:50,110
scenario with code that goes along with

00:04:47,949 --> 00:04:51,910
that and we'll kind of go through a

00:04:50,110 --> 00:04:53,500
bunch of other patterns of how it's

00:04:51,910 --> 00:04:56,740
being used in the real world for real

00:04:53,500 --> 00:04:58,240
production applications at scale there

00:04:56,740 --> 00:05:00,669
won't be any code samples in that part

00:04:58,240 --> 00:05:04,770
but it'll give you a sense of like how

00:05:00,669 --> 00:05:08,860
and what you can use Kafka for so I

00:05:04,770 --> 00:05:11,260
remember like a year or so ago I've

00:05:08,860 --> 00:05:13,360
heard of Kafka by no idea really what it

00:05:11,260 --> 00:05:17,770
was just that it was a buzz word and

00:05:13,360 --> 00:05:19,000
people talked about it and so one of the

00:05:17,770 --> 00:05:21,370
first things I did was I went to the

00:05:19,000 --> 00:05:23,470
official documentation it's an Apache

00:05:21,370 --> 00:05:26,110
project and these are the first two

00:05:23,470 --> 00:05:28,600
senses that you read an introduction of

00:05:26,110 --> 00:05:30,789
what cough is so it says cava is a

00:05:28,600 --> 00:05:32,410
distributed partition replicated commit

00:05:30,789 --> 00:05:34,330
log service provides the functionality

00:05:32,410 --> 00:05:36,400
of a messaging system but with unique

00:05:34,330 --> 00:05:38,350
design and I remember reading this and

00:05:36,400 --> 00:05:43,710
having no idea what this actually meant

00:05:38,350 --> 00:05:46,990
like some bunch of buzzwords salad and

00:05:43,710 --> 00:05:49,000
so if I was to sum it down into kind of

00:05:46,990 --> 00:05:51,250
four words this is what it really this

00:05:49,000 --> 00:05:53,590
is what I would talk about it it's a

00:05:51,250 --> 00:05:55,720
distributed pub/sub matching system so

00:05:53,590 --> 00:05:57,970
there's a lot of terminology and things

00:05:55,720 --> 00:06:01,300
that are similar to stuff like if you

00:05:57,970 --> 00:06:03,250
use Redis before or rabbit or amqp they

00:06:01,300 --> 00:06:05,919
all have similar things because those

00:06:03,250 --> 00:06:09,669
are all like message systems that also

00:06:05,919 --> 00:06:12,250
do pops up stuff but with the unique

00:06:09,669 --> 00:06:13,900
design Kafka has these kind of design

00:06:12,250 --> 00:06:16,870
constraints that's trying to achieve it

00:06:13,900 --> 00:06:19,090
needs to be fast it needs to do stuff at

00:06:16,870 --> 00:06:21,039
scale and it needs to have it needs to

00:06:19,090 --> 00:06:23,919
be durable with regards to data loss and

00:06:21,039 --> 00:06:26,410
retention and when we're talking about

00:06:23,919 --> 00:06:28,479
performance this is a quote that a

00:06:26,410 --> 00:06:32,650
fellow colleague of mine Tom crayford

00:06:28,479 --> 00:06:34,389
who works on the Heroku Kafka team I was

00:06:32,650 --> 00:06:36,490
asking what the poor performance of like

00:06:34,389 --> 00:06:38,289
a small coffee cluster is and he says

00:06:36,490 --> 00:06:40,030
you can do hundreds of thousands to

00:06:38,289 --> 00:06:42,520
millions of messages second and process

00:06:40,030 --> 00:06:44,140
them on a small cluster and as a point

00:06:42,520 --> 00:06:46,150
of reference when you're talking about

00:06:44,140 --> 00:06:47,260
like mqp you're talking about tens of

00:06:46,150 --> 00:06:51,700
thousands of mess

00:06:47,260 --> 00:06:53,020
a second on a single instance so with

00:06:51,700 --> 00:06:55,480
this small confident cluster we're

00:06:53,020 --> 00:06:58,990
talking about an order or two orders of

00:06:55,480 --> 00:07:00,250
magnitude more throughput with it but

00:06:58,990 --> 00:07:02,050
that comes with certain design

00:07:00,250 --> 00:07:05,710
constraints that we'll see how we can

00:07:02,050 --> 00:07:07,420
achieve that kind of stuff so I don't

00:07:05,710 --> 00:07:10,840
know how many familiar with skylight the

00:07:07,420 --> 00:07:14,980
rails reporting tool done by the total

00:07:10,840 --> 00:07:16,870
guys so this is a rails performance

00:07:14,980 --> 00:07:20,920
monitoring tool for those of you who are

00:07:16,870 --> 00:07:22,600
not familiar and Kafka makes up a core

00:07:20,920 --> 00:07:24,280
part of their architecture for me able

00:07:22,600 --> 00:07:28,620
to deliver the product that they have

00:07:24,280 --> 00:07:30,580
and when you're in this business of

00:07:28,620 --> 00:07:32,560
doing performance monitoring you're

00:07:30,580 --> 00:07:34,720
collecting a ton of data like a ton of

00:07:32,560 --> 00:07:37,750
metrics for the performance of your

00:07:34,720 --> 00:07:39,190
application and they generate histograms

00:07:37,750 --> 00:07:40,540
based off that kind of stuff so you need

00:07:39,190 --> 00:07:42,430
to be able to process all this kind of

00:07:40,540 --> 00:07:45,340
information and so they use Kafka's the

00:07:42,430 --> 00:07:47,890
backbone to handle all this streaming

00:07:45,340 --> 00:07:49,840
data to then put it inside of Cassandra

00:07:47,890 --> 00:07:51,640
for actually those windows of the

00:07:49,840 --> 00:07:54,100
histograms that you're getting and one

00:07:51,640 --> 00:07:56,110
of the other does things that they like

00:07:54,100 --> 00:07:57,610
about cafe is that it's one the only

00:07:56,110 --> 00:08:02,920
pieces their architecture that doesn't

00:07:57,610 --> 00:08:05,980
go down like ever and so you don't

00:08:02,920 --> 00:08:08,920
really want to be using a product where

00:08:05,980 --> 00:08:10,870
if their service goes down you lose data

00:08:08,920 --> 00:08:13,330
or you get some misrepresentation of the

00:08:10,870 --> 00:08:14,890
histogram because it goes down during

00:08:13,330 --> 00:08:16,810
the middle of it calculating that

00:08:14,890 --> 00:08:19,900
histogram window and so one of the nice

00:08:16,810 --> 00:08:21,910
things about Kafka's even if the thing

00:08:19,900 --> 00:08:23,140
that is calculating your histogram goes

00:08:21,910 --> 00:08:26,710
down during the middle of that

00:08:23,140 --> 00:08:29,320
transaction in Kafka they have position

00:08:26,710 --> 00:08:30,820
data so when it goes down and it comes

00:08:29,320 --> 00:08:32,800
back up if it didn't commit that

00:08:30,820 --> 00:08:35,470
position because I didn't finish it will

00:08:32,800 --> 00:08:37,180
just continue from where it started and

00:08:35,470 --> 00:08:40,150
then recalculate that whole thing again

00:08:37,180 --> 00:08:42,130
so when using something like skylight

00:08:40,150 --> 00:08:44,410
even if their service goes down you're

00:08:42,130 --> 00:08:46,780
not going to lose that data that they

00:08:44,410 --> 00:08:49,420
have soared there so that's really a

00:08:46,780 --> 00:08:52,690
great reason for why Papa fits this

00:08:49,420 --> 00:08:54,580
particular use case and if we take a

00:08:52,690 --> 00:08:56,710
step back and start to look at the

00:08:54,580 --> 00:08:58,330
terminology things that will be talking

00:08:56,710 --> 00:09:01,350
about when we're talking about designing

00:08:58,330 --> 00:09:05,800
and building stuff in

00:09:01,350 --> 00:09:07,209
it's a very similar to a lot of

00:09:05,800 --> 00:09:10,180
producer-consumer stuff that we're ready

00:09:07,209 --> 00:09:12,000
used to today so if you sidekick on

00:09:10,180 --> 00:09:14,649
Rails you're probably very familiar with

00:09:12,000 --> 00:09:18,339
this pattern you have a bunch of stuff

00:09:14,649 --> 00:09:20,380
that feeds work into some data store and

00:09:18,339 --> 00:09:22,930
with psychics case like inside of rails

00:09:20,380 --> 00:09:24,329
you're sending the email request for

00:09:22,930 --> 00:09:26,560
someone who's registering as a new user

00:09:24,329 --> 00:09:28,240
so those would be the producers inside

00:09:26,560 --> 00:09:30,190
of rails like creating the work that

00:09:28,240 --> 00:09:31,510
you're putting into it and so reddest

00:09:30,190 --> 00:09:34,450
will be like the coffee cluster that's

00:09:31,510 --> 00:09:35,950
holding all this data and then the

00:09:34,450 --> 00:09:38,260
consumers would be the actual sidekick

00:09:35,950 --> 00:09:40,510
workers actually reading stuff off this

00:09:38,260 --> 00:09:45,730
data store actually doing something with

00:09:40,510 --> 00:09:47,920
that information and so when you're

00:09:45,730 --> 00:09:49,740
actually putting stuff into the coffee a

00:09:47,920 --> 00:09:52,510
cluster these things are called messages

00:09:49,740 --> 00:09:55,209
inside of Kafka and the messager

00:09:52,510 --> 00:09:56,380
actually stored as byte arrays and what

00:09:55,209 --> 00:09:58,600
that means is that you can actually

00:09:56,380 --> 00:10:01,029
store any kind of data inside of the

00:09:58,600 --> 00:10:02,589
message by default you can just use a

00:10:01,029 --> 00:10:05,260
string so you can have some type of

00:10:02,589 --> 00:10:07,899
unstructured data that can be any type

00:10:05,260 --> 00:10:08,980
of representation of anything really but

00:10:07,899 --> 00:10:10,480
most you will probably want to use

00:10:08,980 --> 00:10:12,970
something more structured so I think

00:10:10,480 --> 00:10:14,529
some like JSON or main message pack or

00:10:12,970 --> 00:10:16,870
any real format that you want to be

00:10:14,529 --> 00:10:18,550
using as long as the thing that is

00:10:16,870 --> 00:10:20,980
serializing the thing into the message

00:10:18,550 --> 00:10:25,120
you're able to do deserialize that

00:10:20,980 --> 00:10:26,949
message later after the fact and so we

00:10:25,120 --> 00:10:28,360
have this huge stream of messages

00:10:26,949 --> 00:10:32,230
because we're doing this high throughput

00:10:28,360 --> 00:10:34,540
stuff these are organized into this idea

00:10:32,230 --> 00:10:37,019
and Coffield called topics and so topics

00:10:34,540 --> 00:10:39,760
are just a feed of these messages and

00:10:37,019 --> 00:10:43,870
these messages are split across multiple

00:10:39,760 --> 00:10:46,149
partitions so you can have it replicate

00:10:43,870 --> 00:10:48,220
and distributed across your cluster so a

00:10:46,149 --> 00:10:50,050
single topic can have in this case we

00:10:48,220 --> 00:10:54,519
have three partitions partition 0 1 and

00:10:50,050 --> 00:10:56,230
2 and within each of these individual

00:10:54,519 --> 00:10:58,180
partitions the order that the messages

00:10:56,230 --> 00:11:00,430
comes in is the order that's and it's

00:10:58,180 --> 00:11:03,010
written in so it kapha guarantees the

00:11:00,430 --> 00:11:06,790
order within that partition of the way

00:11:03,010 --> 00:11:09,370
those messages are stored and since it

00:11:06,790 --> 00:11:11,800
is a commit log it doesn't actually

00:11:09,370 --> 00:11:12,680
allow you to reorder any of the messages

00:11:11,800 --> 00:11:14,420
once they've been

00:11:12,680 --> 00:11:16,970
so you can only append them at the end

00:11:14,420 --> 00:11:18,500
but this gives you some nice guarantees

00:11:16,970 --> 00:11:20,149
of what you can actually do with this

00:11:18,500 --> 00:11:24,380
data so you can only write to the end of

00:11:20,149 --> 00:11:27,140
it and you can't reorder it and so if we

00:11:24,380 --> 00:11:29,870
go back to this picture it means that we

00:11:27,140 --> 00:11:31,490
can find a particular message by looking

00:11:29,870 --> 00:11:34,760
at the offset of where it is from the

00:11:31,490 --> 00:11:37,490
start so we can easily read through this

00:11:34,760 --> 00:11:39,350
thing or start or replay any data by

00:11:37,490 --> 00:11:42,020
knowing the offset positions of where

00:11:39,350 --> 00:11:43,399
they are in that particular position so

00:11:42,020 --> 00:11:45,350
this is something that the skylight team

00:11:43,399 --> 00:11:50,360
uses when they're calculating those

00:11:45,350 --> 00:11:51,770
histogram windows so as those messages

00:11:50,360 --> 00:11:54,170
are distributed across the various

00:11:51,770 --> 00:11:56,240
partitions by default or just randomly

00:11:54,170 --> 00:11:58,070
distributed evenly across all the

00:11:56,240 --> 00:12:00,050
partitions that you have you can

00:11:58,070 --> 00:12:01,520
actually set a key for a particular

00:12:00,050 --> 00:12:05,300
message and that will guarantee that

00:12:01,520 --> 00:12:06,560
that message goes to all messages that

00:12:05,300 --> 00:12:09,920
share that key go to a particular

00:12:06,560 --> 00:12:11,750
partition so on the consumer side you

00:12:09,920 --> 00:12:14,270
can actually do some nice optimizations

00:12:11,750 --> 00:12:17,089
because you know that you might get

00:12:14,270 --> 00:12:19,459
multiple keys for that consumer but you

00:12:17,089 --> 00:12:21,740
know that all the keys that are coming

00:12:19,459 --> 00:12:24,170
in for say like user ID 12 are

00:12:21,740 --> 00:12:26,089
definitely going to this consumer and so

00:12:24,170 --> 00:12:30,260
you can do some nice optimizations in

00:12:26,089 --> 00:12:32,690
memory or calculations that way and then

00:12:30,260 --> 00:12:35,270
on the consumer side we actually have

00:12:32,690 --> 00:12:37,190
this concept in cocktail called consumer

00:12:35,270 --> 00:12:39,260
groups so you bunch a bunch of consumers

00:12:37,190 --> 00:12:41,600
together as a consumer group and that

00:12:39,260 --> 00:12:45,470
allows you to scale on the consuming

00:12:41,600 --> 00:12:48,470
side of the work that you're doing so if

00:12:45,470 --> 00:12:50,660
we look at this picture we have to

00:12:48,470 --> 00:12:52,850
consumer groups consumer group a and

00:12:50,660 --> 00:12:56,420
consumer group B and we have this cough

00:12:52,850 --> 00:12:59,660
of cluster of four partitions p0 up to

00:12:56,420 --> 00:13:01,279
p3 and so each consumer group can

00:12:59,660 --> 00:13:03,770
subscribe to topics so let's say

00:13:01,279 --> 00:13:07,459
consumer group and beef both subscribe

00:13:03,770 --> 00:13:09,529
to some topic a first topic X or

00:13:07,459 --> 00:13:11,630
something and so that means that they

00:13:09,529 --> 00:13:14,899
will both receive all the messages from

00:13:11,630 --> 00:13:16,700
that topic as they come in and so the

00:13:14,899 --> 00:13:18,920
nice thing is within a certain consumer

00:13:16,700 --> 00:13:21,500
all the partitions are then split evenly

00:13:18,920 --> 00:13:23,959
across the consumers inside of that

00:13:21,500 --> 00:13:26,209
consumer group so in consumer group AC

00:13:23,959 --> 00:13:29,959
one takes partition 0

00:13:26,209 --> 00:13:31,999
three and then consumer to takes p1 and

00:13:29,959 --> 00:13:35,540
p2 and they're both getting all these

00:13:31,999 --> 00:13:37,339
messages but if we scaled consumer a to

00:13:35,540 --> 00:13:39,980
be like consumer be and we added two

00:13:37,339 --> 00:13:42,589
more consumers Kafka will automatically

00:13:39,980 --> 00:13:44,629
repart redistribute the partitions so

00:13:42,589 --> 00:13:46,279
they're evenly split across all the

00:13:44,629 --> 00:13:48,439
consumers in that consumer group so

00:13:46,279 --> 00:13:51,199
actually look more like consumer group B

00:13:48,439 --> 00:13:53,029
where each of the consumers in it will

00:13:51,199 --> 00:13:55,759
only be handling one partition on time

00:13:53,029 --> 00:13:58,970
so as you can imagine it's pretty easy

00:13:55,759 --> 00:14:00,860
to then scale up a Kafka of cluster to

00:13:58,970 --> 00:14:02,629
be able to consume as you have hired a

00:14:00,860 --> 00:14:06,529
throughput the nice thing is that the

00:14:02,629 --> 00:14:11,119
inverse is also true so if consumers say

00:14:06,529 --> 00:14:12,949
six drops in consumer group B there's

00:14:11,119 --> 00:14:15,740
actually a heartbeat system built into

00:14:12,949 --> 00:14:17,889
Kafka that will detect that hey I can't

00:14:15,740 --> 00:14:22,309
connect to consumer six so i will stop

00:14:17,889 --> 00:14:24,529
actually giving a partition to out to

00:14:22,309 --> 00:14:26,569
consumer six and i will put it on it

00:14:24,529 --> 00:14:29,149
some other consumer so you can only get

00:14:26,569 --> 00:14:31,699
this like scalability elastic effect

00:14:29,149 --> 00:14:35,689
built right into coffee itself which is

00:14:31,699 --> 00:14:37,040
really nice so when we're talking about

00:14:35,689 --> 00:14:39,350
distributed systems we have to talk

00:14:37,040 --> 00:14:41,179
about how messages get delivered because

00:14:39,350 --> 00:14:44,600
of cap theorem and other things like

00:14:41,179 --> 00:14:47,629
that the the ideal case that most people

00:14:44,600 --> 00:14:50,119
want is not be any of these two what you

00:14:47,629 --> 00:14:52,069
actually probably want is that you want

00:14:50,119 --> 00:14:55,999
to guarantee that a message is delivered

00:14:52,069 --> 00:14:57,319
at mote is delivered once and you want

00:14:55,999 --> 00:14:58,850
to be able to guarantee that but that's

00:14:57,319 --> 00:15:02,360
pretty hard to do in a distributed

00:14:58,850 --> 00:15:04,339
system and so what kapha does it gives

00:15:02,360 --> 00:15:07,100
you the option but by default it does at

00:15:04,339 --> 00:15:10,490
least once and so like the name says it

00:15:07,100 --> 00:15:12,889
means that it will just if you subscribe

00:15:10,490 --> 00:15:14,899
to a topic it will deliver messages from

00:15:12,889 --> 00:15:18,350
that topic at least once to that

00:15:14,899 --> 00:15:20,089
consumer group but if there's any faults

00:15:18,350 --> 00:15:21,529
that happen throughout the thing it may

00:15:20,089 --> 00:15:24,199
deliver messages more than once so you

00:15:21,529 --> 00:15:26,059
have to have the ability to do

00:15:24,199 --> 00:15:28,220
duplication detection and not be able to

00:15:26,059 --> 00:15:30,259
do things more than once if that matters

00:15:28,220 --> 00:15:31,790
maybe you're doing some cash calculation

00:15:30,259 --> 00:15:33,439
that doesn't matter but say you're

00:15:31,790 --> 00:15:34,549
handling credit card transactions you

00:15:33,439 --> 00:15:35,929
don't actually want to build someone

00:15:34,549 --> 00:15:39,379
more than once because you haven't get a

00:15:35,929 --> 00:15:40,040
message twice on the other side of the

00:15:39,379 --> 00:15:42,440
spectrum you can

00:15:40,040 --> 00:15:44,810
you at most once which means you will

00:15:42,440 --> 00:15:45,860
only try sending a message once and it

00:15:44,810 --> 00:15:47,660
guarantees that you will not get

00:15:45,860 --> 00:15:49,970
duplicates but it does not guarantee

00:15:47,660 --> 00:15:51,259
that you actually get every message so

00:15:49,970 --> 00:15:55,310
those are kind of the two ends of the

00:15:51,259 --> 00:15:56,839
spectrum and most people I think go with

00:15:55,310 --> 00:16:00,949
at least once and try to do some

00:15:56,839 --> 00:16:03,230
duplication detection and so with that

00:16:00,949 --> 00:16:05,000
with the messages and topics and things

00:16:03,230 --> 00:16:06,800
like that with Kaufman were able to

00:16:05,000 --> 00:16:10,730
within a partition guarantee that the

00:16:06,800 --> 00:16:12,589
messages that are put in order are

00:16:10,730 --> 00:16:14,420
actually written in that order and so

00:16:12,589 --> 00:16:16,819
when we consume those messages we can

00:16:14,420 --> 00:16:18,709
consume them in that order and that the

00:16:16,819 --> 00:16:21,259
topics are distributed and replicated

00:16:18,709 --> 00:16:24,350
across our cluster so if any the nodes

00:16:21,259 --> 00:16:28,880
go down we still have access to those

00:16:24,350 --> 00:16:31,279
messages so now that we have some

00:16:28,880 --> 00:16:33,790
understanding what Kafka is like how do

00:16:31,279 --> 00:16:37,100
we actually use it inside of Ruby itself

00:16:33,790 --> 00:16:41,000
so one of the oldest Kafka libraries out

00:16:37,100 --> 00:16:43,009
there is JRuby Kafka and what this is is

00:16:41,000 --> 00:16:45,800
just a simple wrapper around the

00:16:43,009 --> 00:16:48,380
official Kafka libraries cough is

00:16:45,800 --> 00:16:49,670
written in Scala and so with JRuby Cosmo

00:16:48,380 --> 00:16:50,839
we can actually just leverage the

00:16:49,670 --> 00:16:53,569
official libraries that they're using

00:16:50,839 --> 00:16:55,490
the downside of that is you have to be

00:16:53,569 --> 00:16:58,040
pretty comfortable reading Java API Docs

00:16:55,490 --> 00:17:00,350
which I think given enough time is not

00:16:58,040 --> 00:17:02,120
too hard but probably the bigger

00:17:00,350 --> 00:17:04,429
Steelbreaker for most people is that if

00:17:02,120 --> 00:17:06,169
you have an MRI appt it's potentially a

00:17:04,429 --> 00:17:08,870
lot of work to convert it to JRuby just

00:17:06,169 --> 00:17:10,610
to use something like Kafka or it may

00:17:08,870 --> 00:17:12,319
not even be feasible because you might

00:17:10,610 --> 00:17:15,230
be using some native see extension that

00:17:12,319 --> 00:17:17,750
may not port over super well to jruby so

00:17:15,230 --> 00:17:21,620
I didn't really want to go into too much

00:17:17,750 --> 00:17:25,610
detail with jer root chakra luckily for

00:17:21,620 --> 00:17:27,350
us Zendesk has been working on this Ruby

00:17:25,610 --> 00:17:29,659
Kafka library which allows us to

00:17:27,350 --> 00:17:33,559
actually build stuff both in JRuby or

00:17:29,659 --> 00:17:35,960
MRI and actually leverage Kafka there's

00:17:33,559 --> 00:17:38,720
an older Kafka library called Poseidon

00:17:35,960 --> 00:17:40,730
that only supports Kafka point 8 the

00:17:38,720 --> 00:17:43,040
latest version of cough has point 10 but

00:17:40,730 --> 00:17:45,260
Kafka point 9 introduced a bunch of new

00:17:43,040 --> 00:17:47,210
AP is on how we consume stuff with

00:17:45,260 --> 00:17:48,620
consumer groups that doesn't support it

00:17:47,210 --> 00:17:51,110
doesn't even support consumer groups at

00:17:48,620 --> 00:17:53,370
all you have to use a separate plugin so

00:17:51,110 --> 00:17:55,140
we'll be focusing on Ruby cough

00:17:53,370 --> 00:17:57,570
because it supports the point 9 api's

00:17:55,140 --> 00:18:02,490
which are much nicer and better than the

00:17:57,570 --> 00:18:04,140
point 8 stuff so with Ruby Kafka in

00:18:02,490 --> 00:18:07,370
order to do a simple producer to put

00:18:04,140 --> 00:18:10,620
messages actually in the cockpit cluster

00:18:07,370 --> 00:18:13,320
we need to initialize our connection to

00:18:10,620 --> 00:18:15,000
Kafka and it looks pretty similar to a

00:18:13,320 --> 00:18:17,070
Redis connection or anything else that

00:18:15,000 --> 00:18:18,780
you're probably using a ruby one of the

00:18:17,070 --> 00:18:21,210
things note since we're talking about a

00:18:18,780 --> 00:18:24,480
cluster is that you need to pass it a

00:18:21,210 --> 00:18:25,710
set of seed brokers which are initial

00:18:24,480 --> 00:18:27,090
instances that you want to try to

00:18:25,710 --> 00:18:28,950
connect to and once we've connected to

00:18:27,090 --> 00:18:30,870
one of those it can then give us the

00:18:28,950 --> 00:18:33,300
rest of the network topology of where

00:18:30,870 --> 00:18:35,429
all the machines are so you don't need

00:18:33,300 --> 00:18:36,870
to know what the whole set is but you

00:18:35,429 --> 00:18:38,670
probably want potentially more than one

00:18:36,870 --> 00:18:41,130
in case one of those goes down and you

00:18:38,670 --> 00:18:43,140
need to connect to something else once

00:18:41,130 --> 00:18:44,370
we have that we can actually just call

00:18:43,140 --> 00:18:46,290
producer and this will make a

00:18:44,370 --> 00:18:49,020
synchronous producer that we can then

00:18:46,290 --> 00:18:51,179
use to send messages into the cluster so

00:18:49,020 --> 00:18:52,530
we just call produce pass it whatever

00:18:51,179 --> 00:18:54,420
you want for the body in this case it's

00:18:52,530 --> 00:18:56,220
just a simple string and then we need to

00:18:54,420 --> 00:18:58,920
specify what the topic of how we

00:18:56,220 --> 00:19:01,230
organize this message we talked about

00:18:58,920 --> 00:19:03,990
keys earlier so this is how simple it is

00:19:01,230 --> 00:19:06,179
to then key a message if we really want

00:19:03,990 --> 00:19:07,770
we can manually decide where what

00:19:06,179 --> 00:19:10,920
partition that this particular message

00:19:07,770 --> 00:19:14,480
goes to so we can specify that here as

00:19:10,920 --> 00:19:16,650
well with all that we can call this a

00:19:14,480 --> 00:19:19,440
synchronous blocking deliver messages

00:19:16,650 --> 00:19:20,610
call and what that does is it will take

00:19:19,440 --> 00:19:22,380
all these messages that have been

00:19:20,610 --> 00:19:24,870
buffered and actually send them out and

00:19:22,380 --> 00:19:27,540
since this is blocking it will only

00:19:24,870 --> 00:19:30,540
return once it is either failed or been

00:19:27,540 --> 00:19:32,400
successful if it is failed it actually

00:19:30,540 --> 00:19:34,200
raises an exception and so with that

00:19:32,400 --> 00:19:36,540
exception we can choose if we want to do

00:19:34,200 --> 00:19:38,520
the at least once or at most once

00:19:36,540 --> 00:19:40,230
delivery mechanism so we can catch the

00:19:38,520 --> 00:19:42,000
exception and then deliver all the

00:19:40,230 --> 00:19:45,900
messages again or we can just ignore it

00:19:42,000 --> 00:19:47,370
and then keep moving on but you probably

00:19:45,900 --> 00:19:49,410
don't actually want to be doing

00:19:47,370 --> 00:19:51,380
synchronous work inside of rails process

00:19:49,410 --> 00:19:53,580
if you're producing this kind of stuff

00:19:51,380 --> 00:19:54,990
that's going to block that web request

00:19:53,580 --> 00:19:59,309
and it's going to take a long time for

00:19:54,990 --> 00:20:02,130
to return so the library supports an

00:19:59,309 --> 00:20:04,560
asynchronous producer and we're able to

00:20:02,130 --> 00:20:06,540
pass in basically various thresholds of

00:20:04,560 --> 00:20:06,900
how long we want to wait until we

00:20:06,540 --> 00:20:10,170
actually

00:20:06,900 --> 00:20:11,790
send the messages in order for coffee to

00:20:10,170 --> 00:20:13,470
get the performance levels it does it

00:20:11,790 --> 00:20:15,440
needs to buffer the messages so it's not

00:20:13,470 --> 00:20:19,020
continually hitting the coffee cluster

00:20:15,440 --> 00:20:20,220
and so we can specify either a threshold

00:20:19,020 --> 00:20:22,560
of how many messages we want in the

00:20:20,220 --> 00:20:24,900
buffer or how long to wait and if we

00:20:22,560 --> 00:20:29,970
specify both it will hold trigger when

00:20:24,900 --> 00:20:32,430
the first condition gets hit here's an

00:20:29,970 --> 00:20:34,980
example of how you can very easily see

00:20:32,430 --> 00:20:36,600
relies data into the cockpit cluster so

00:20:34,980 --> 00:20:39,600
this is example of taking a simple Ruby

00:20:36,600 --> 00:20:41,880
hash dumping into JSON and then putting

00:20:39,600 --> 00:20:43,350
that into the coffee cluster and they're

00:20:41,880 --> 00:20:45,480
not a consumer side we just need to be

00:20:43,350 --> 00:20:49,950
able to call JSON not parse on the

00:20:45,480 --> 00:20:52,110
message value so in order to use this

00:20:49,950 --> 00:20:53,760
inside a rails it's pretty simple you

00:20:52,110 --> 00:20:56,930
just have to do all this stuff inside of

00:20:53,760 --> 00:20:59,100
initializer so let's instantiate

00:20:56,930 --> 00:21:01,890
connection to Kafka passed and see

00:20:59,100 --> 00:21:05,340
brokers we can pass in the rails logger

00:21:01,890 --> 00:21:06,660
so all the logs from the kafka producer

00:21:05,340 --> 00:21:09,750
actually gets sent to where all our

00:21:06,660 --> 00:21:12,720
standard rails logs go from there we

00:21:09,750 --> 00:21:14,340
want to make an async producer and this

00:21:12,720 --> 00:21:16,170
is probably the important bit that I

00:21:14,340 --> 00:21:18,090
didn't talk about is that you want to

00:21:16,170 --> 00:21:20,400
make sure you shut down your Kafka class

00:21:18,090 --> 00:21:22,200
your coffee cup producer at the end so

00:21:20,400 --> 00:21:24,000
you're not leaking connections and it in

00:21:22,200 --> 00:21:27,480
the coffee cluster doesn't think you're

00:21:24,000 --> 00:21:29,610
holding open connections to it so make

00:21:27,480 --> 00:21:32,880
sure to add this at exit call inside the

00:21:29,610 --> 00:21:35,280
initializer and from there we can start

00:21:32,880 --> 00:21:37,890
sending messages inside rails so here's

00:21:35,280 --> 00:21:41,010
an example of in orders controller of

00:21:37,890 --> 00:21:44,520
how we're going to create orders inside

00:21:41,010 --> 00:21:46,530
of rails and then send them into of

00:21:44,520 --> 00:21:49,380
crow so we take an active record object

00:21:46,530 --> 00:21:53,430
convert to JSON and then a scientist

00:21:49,380 --> 00:21:54,540
some topic and send that in if you're

00:21:53,430 --> 00:21:56,820
doing this you probably would

00:21:54,540 --> 00:21:58,560
potentially also have some form key from

00:21:56,820 --> 00:22:01,460
order that associate with them user ID

00:21:58,560 --> 00:22:04,050
and with that we can actually key the

00:22:01,460 --> 00:22:05,970
kafka message with that user ID if we

00:22:04,050 --> 00:22:08,220
potentially wanted to make sure all the

00:22:05,970 --> 00:22:11,600
messages for that particular use ID for

00:22:08,220 --> 00:22:14,010
that order end up on the same partition

00:22:11,600 --> 00:22:18,110
so pretty easy to be able to use this

00:22:14,010 --> 00:22:18,110
stuff inside rails with this producer

00:22:18,890 --> 00:22:23,960
so the next part is you need to be able

00:22:22,760 --> 00:22:26,480
to actually do something with the

00:22:23,960 --> 00:22:29,540
messages inside of Kafka and this is the

00:22:26,480 --> 00:22:34,160
part that is where the new API is from

00:22:29,540 --> 00:22:37,970
point nine are and the zendesk library

00:22:34,160 --> 00:22:40,610
Marxist s alpha so they make the claim

00:22:37,970 --> 00:22:42,500
that they might change the API sometime

00:22:40,610 --> 00:22:44,780
in the future so this is something to be

00:22:42,500 --> 00:22:49,010
wary of but the API is actually pretty

00:22:44,780 --> 00:22:50,750
good for doing real work so there's a

00:22:49,010 --> 00:22:52,580
bunch of different api's that you can

00:22:50,750 --> 00:22:54,050
use that they have but the main one that

00:22:52,580 --> 00:22:55,700
I'll be talking about is the one with

00:22:54,050 --> 00:23:00,320
consumer groups so I think that's the

00:22:55,700 --> 00:23:01,850
best way to consume stuff inside a kafka

00:23:00,320 --> 00:23:03,590
at scale because you want to be a bad

00:23:01,850 --> 00:23:04,970
stuff to consumer groups and be able to

00:23:03,590 --> 00:23:07,510
process those messages and be able to

00:23:04,970 --> 00:23:10,400
scale up those processes if you need it

00:23:07,510 --> 00:23:12,260
so we instantiate cough feel like before

00:23:10,400 --> 00:23:14,480
with that we just call consumer and

00:23:12,260 --> 00:23:16,100
passive group ID and the group ID is

00:23:14,480 --> 00:23:18,380
just some name that we're going to

00:23:16,100 --> 00:23:19,790
associate with it that usually describes

00:23:18,380 --> 00:23:24,200
the type of work we're doing so that

00:23:19,790 --> 00:23:27,160
could be maybe log archiving or metric

00:23:24,200 --> 00:23:30,350
analyzing or some billing thing right

00:23:27,160 --> 00:23:32,810
and then we can subscribe it to as many

00:23:30,350 --> 00:23:34,970
topics as we want to get messages for so

00:23:32,810 --> 00:23:36,230
if you have two topics you subscribe it

00:23:34,970 --> 00:23:38,270
to those two topics that you can get the

00:23:36,230 --> 00:23:40,900
messages for and then that consumer

00:23:38,270 --> 00:23:44,240
group will get all those messages and

00:23:40,900 --> 00:23:47,150
each message here is actually a blocking

00:23:44,240 --> 00:23:49,190
call that will go through and it will

00:23:47,150 --> 00:23:51,410
block and pull four messages and as it

00:23:49,190 --> 00:23:53,330
gets them it will then process the code

00:23:51,410 --> 00:23:55,580
in the block and we can access the

00:23:53,330 --> 00:23:58,100
various things like the value which is

00:23:55,580 --> 00:24:00,200
the body of the message the key or

00:23:58,100 --> 00:24:02,110
partition and things like that so that's

00:24:00,200 --> 00:24:04,910
pretty simple on how you can create a

00:24:02,110 --> 00:24:08,660
consumer and actually use the messages

00:24:04,910 --> 00:24:10,220
that you're putting inside of kafka one

00:24:08,660 --> 00:24:11,840
of the new features inside of Kafka

00:24:10,220 --> 00:24:14,630
point nine is that it supports us to

00:24:11,840 --> 00:24:17,450
sell now point eight did not so this

00:24:14,630 --> 00:24:20,210
makes it pretty easy to actually secure

00:24:17,450 --> 00:24:21,770
the connection if you were not if you

00:24:20,210 --> 00:24:23,720
don't have cocktail on the same machine

00:24:21,770 --> 00:24:25,610
as say your rails app that you're using

00:24:23,720 --> 00:24:27,320
this with allows you to secure those

00:24:25,610 --> 00:24:30,760
connections so it's nice that these NS

00:24:27,320 --> 00:24:30,760
liar who supports that out of the box

00:24:31,360 --> 00:24:40,690
so the the demo app that I will be

00:24:35,080 --> 00:24:44,770
walking through is a metrics one and

00:24:40,690 --> 00:24:47,559
what this is is will be building metric

00:24:44,770 --> 00:24:50,380
space off of the web traffic so per path

00:24:47,559 --> 00:24:52,120
will be calculating like the service

00:24:50,380 --> 00:24:53,559
time Helen takes service requests what

00:24:52,120 --> 00:24:56,320
the status code is and things like that

00:24:53,559 --> 00:24:58,030
and we'll be using Kafka to base you

00:24:56,320 --> 00:25:00,130
hold that stream of data in case it's

00:24:58,030 --> 00:25:03,429
high throughput and then storing it in

00:25:00,130 --> 00:25:05,530
Simon some type of data store so here's

00:25:03,429 --> 00:25:08,410
that architecture we have some may nap

00:25:05,530 --> 00:25:11,500
it gets a web request we take those logs

00:25:08,410 --> 00:25:14,140
of the routing data that we care about

00:25:11,500 --> 00:25:17,140
we pass it to the metrics app and then

00:25:14,140 --> 00:25:19,990
we store that data like I said and if

00:25:17,140 --> 00:25:22,720
we're to drill in a little bit more the

00:25:19,990 --> 00:25:25,059
example app that i will be using is co

00:25:22,720 --> 00:25:27,370
triage I don't know people are familiar

00:25:25,059 --> 00:25:29,169
with it but Richard built this app as a

00:25:27,370 --> 00:25:31,330
way to get involved open source if you

00:25:29,169 --> 00:25:33,250
sign up for it you can sign up for

00:25:31,330 --> 00:25:35,380
particular repositories and it will base

00:25:33,250 --> 00:25:37,770
the message you like an issue a day to

00:25:35,380 --> 00:25:40,540
basically checked in on and respond to

00:25:37,770 --> 00:25:43,150
to help maintain errs maintain open

00:25:40,540 --> 00:25:47,320
source so this app is a rails app that's

00:25:43,150 --> 00:25:49,150
hosted on Heroku open sores and with

00:25:47,320 --> 00:25:52,090
that we're able to take those router

00:25:49,150 --> 00:25:54,220
logs and build a ruby app that is then

00:25:52,090 --> 00:25:57,160
able to produce the messages that get

00:25:54,220 --> 00:25:59,590
sent to kaka and once the messages in

00:25:57,160 --> 00:26:01,679
Kafka we can then build another Ruby app

00:25:59,590 --> 00:26:03,850
that will consume those messages and

00:26:01,679 --> 00:26:05,890
aggregate the data and store them inside

00:26:03,850 --> 00:26:07,510
a Redis and then once it's in Redis we

00:26:05,890 --> 00:26:09,130
can kind of do whatever we want with it

00:26:07,510 --> 00:26:11,200
right like it's pretty easy to access

00:26:09,130 --> 00:26:13,900
Redis inside of rails you could build

00:26:11,200 --> 00:26:16,750
even like a node order or whatever app

00:26:13,900 --> 00:26:19,600
that interfaces with that Retta status

00:26:16,750 --> 00:26:21,580
or we're not going to walk through the

00:26:19,600 --> 00:26:22,720
rails part of that because it really

00:26:21,580 --> 00:26:24,630
doesn't have anything to do with kaka

00:26:22,720 --> 00:26:27,070
but I think you'd be pretty simple for

00:26:24,630 --> 00:26:29,140
anyone who's dealt with retta's to be

00:26:27,070 --> 00:26:31,000
able to do stuff with that data maybe

00:26:29,140 --> 00:26:34,179
build like a dashboard some

00:26:31,000 --> 00:26:36,970
visualization layer or what have you so

00:26:34,179 --> 00:26:39,400
first looking at the router logging how

00:26:36,970 --> 00:26:41,169
we're going to get this information if

00:26:39,400 --> 00:26:42,940
you used Roku before you've probably

00:26:41,169 --> 00:26:44,580
seen a bunch of these but these are

00:26:42,940 --> 00:26:47,340
basically

00:26:44,580 --> 00:26:49,529
here's a single router log that we get

00:26:47,340 --> 00:26:53,730
when we get a request into a Roku app

00:26:49,529 --> 00:26:56,460
and there's a few pieces information on

00:26:53,730 --> 00:26:59,429
here that make it useful for us so the

00:26:56,460 --> 00:27:01,980
first one is we can find the path that

00:26:59,429 --> 00:27:05,309
is being requested so we have that path

00:27:01,980 --> 00:27:07,730
to kind of use as a key for associating

00:27:05,309 --> 00:27:10,980
metrics with right like we want to know

00:27:07,730 --> 00:27:14,429
the metrics a / path and not necessarily

00:27:10,980 --> 00:27:16,679
across the entire app generically we

00:27:14,429 --> 00:27:18,809
want to know the connect time which is

00:27:16,679 --> 00:27:20,820
the time it takes for the router to

00:27:18,809 --> 00:27:22,799
connect to your Dino for this request

00:27:20,820 --> 00:27:24,299
and then the service time which is the

00:27:22,799 --> 00:27:26,730
time the Dino takes actually service

00:27:24,299 --> 00:27:28,380
that requests and then we also even get

00:27:26,730 --> 00:27:29,880
the status code which is did this

00:27:28,380 --> 00:27:32,549
actually return successfully is it at

00:27:29,880 --> 00:27:34,409
200 did we get a 500 was it a 404 things

00:27:32,549 --> 00:27:38,039
like that so with that we can actually

00:27:34,409 --> 00:27:40,529
calculate the count of water the number

00:27:38,039 --> 00:27:45,360
of successful requests we're getting for

00:27:40,529 --> 00:27:48,929
this actual rap with that on roku we can

00:27:45,360 --> 00:27:52,860
actually just add a https drain which

00:27:48,929 --> 00:27:55,380
basically just sends all this logging

00:27:52,860 --> 00:27:57,929
data and in both the router and

00:27:55,380 --> 00:28:01,110
everything else to some type of endpoint

00:27:57,929 --> 00:28:02,730
we have out there in the universe and so

00:28:01,110 --> 00:28:04,980
in our case we're going to host another

00:28:02,730 --> 00:28:08,870
app on roku that's just going to take

00:28:04,980 --> 00:28:11,309
that data and then put it into Kafka and

00:28:08,870 --> 00:28:14,039
this is what the post requests body

00:28:11,309 --> 00:28:16,679
looks like so it's just a collection of

00:28:14,039 --> 00:28:18,120
these log messages and as one the

00:28:16,679 --> 00:28:19,919
headers it will actually tell you how

00:28:18,120 --> 00:28:21,929
many of these messages are in a single

00:28:19,919 --> 00:28:24,750
body so you can clearly parse that

00:28:21,929 --> 00:28:28,799
information one thing to note is that

00:28:24,750 --> 00:28:33,360
this so RFC 5424 is the syslog RFC and

00:28:28,799 --> 00:28:34,919
that the Heroku syslog is not actually

00:28:33,360 --> 00:28:37,320
completely compliant because we leave

00:28:34,919 --> 00:28:40,169
out this structured data thing but

00:28:37,320 --> 00:28:42,630
luckily inside of the Ruby world we have

00:28:40,169 --> 00:28:44,399
libraries that can take that into

00:28:42,630 --> 00:28:48,539
account and still parse our stuff fairly

00:28:44,399 --> 00:28:50,760
easily as we'll see so now that we

00:28:48,539 --> 00:28:53,010
understand like what kind of data we're

00:28:50,760 --> 00:28:55,440
dealing with from the requests like

00:28:53,010 --> 00:28:57,730
let's take a look at the an app that

00:28:55,440 --> 00:28:59,680
actually takes that data and processes

00:28:57,730 --> 00:29:03,460
it so this is the producer part of the

00:28:59,680 --> 00:29:05,770
kafka architecture so we can have a

00:29:03,460 --> 00:29:07,990
simple Sinatra app that takes a post on

00:29:05,770 --> 00:29:10,630
process if you've done snack before this

00:29:07,990 --> 00:29:11,650
is probably pretty simple and what

00:29:10,630 --> 00:29:14,830
you've seen before we're going to return

00:29:11,650 --> 00:29:16,540
a 202 because the messages may not be

00:29:14,830 --> 00:29:18,880
delivered immediately so we want to just

00:29:16,540 --> 00:29:20,800
tell the client that we've gotten the

00:29:18,880 --> 00:29:24,310
message and we've accepted it but may or

00:29:20,800 --> 00:29:26,590
may not have finished the request one

00:29:24,310 --> 00:29:29,290
thing to note is that the kafka producer

00:29:26,590 --> 00:29:31,450
is not thread-safe so that means that

00:29:29,290 --> 00:29:33,250
you can't actually share a kafka

00:29:31,450 --> 00:29:35,380
producer at the same time across

00:29:33,250 --> 00:29:37,210
multiple threads so we're going to use

00:29:35,380 --> 00:29:40,120
my parents connection pool to ensure

00:29:37,210 --> 00:29:45,160
that a producer can only be used by

00:29:40,120 --> 00:29:46,870
single thread at the same time so inside

00:29:45,160 --> 00:29:50,950
that process messages method we can

00:29:46,870 --> 00:29:53,920
basically take that body tax chunk it

00:29:50,950 --> 00:29:58,210
into a basically array of messages that

00:29:53,920 --> 00:29:59,800
we can then send into Kafka and so we

00:29:58,210 --> 00:30:02,320
check out a producer from the connection

00:29:59,800 --> 00:30:04,840
pool and then this should seem pretty

00:30:02,320 --> 00:30:06,760
close to the really early example code

00:30:04,840 --> 00:30:08,980
that i showed of how you send messages

00:30:06,760 --> 00:30:10,240
kapha so since we have an async producer

00:30:08,980 --> 00:30:11,740
we don't actually need to call deliver

00:30:10,240 --> 00:30:13,180
messages and then when it hits that

00:30:11,740 --> 00:30:18,190
threshold it will actually go ahead and

00:30:13,180 --> 00:30:21,220
deliver those messages for us so let's

00:30:18,190 --> 00:30:25,840
talk about the like kothrud cluster

00:30:21,220 --> 00:30:28,540
itself so one of the I think Kafka is

00:30:25,840 --> 00:30:30,280
great and that's why I'm talking to you

00:30:28,540 --> 00:30:31,690
about it but I think one of the most

00:30:30,280 --> 00:30:34,930
annoying things about kapha is actually

00:30:31,690 --> 00:30:36,250
running it as a service but you know

00:30:34,930 --> 00:30:37,960
it's an open source product so you can

00:30:36,250 --> 00:30:40,950
go ahead and run it you have to run a

00:30:37,960 --> 00:30:43,150
cluster uses the zookeeper for

00:30:40,950 --> 00:30:45,760
configuration so it can sync all this

00:30:43,150 --> 00:30:48,610
information across all the nodes that

00:30:45,760 --> 00:30:50,560
it's running but if you're in the Ruby

00:30:48,610 --> 00:30:52,750
ecosystem you may or may not be super

00:30:50,560 --> 00:30:55,870
familiar with the JVM technology and how

00:30:52,750 --> 00:30:58,600
to operate and run those services but

00:30:55,870 --> 00:31:01,420
you can go ahead and go do that so

00:30:58,600 --> 00:31:02,980
Heroku is actually we're starting to

00:31:01,420 --> 00:31:05,860
launch a coffee service it's an early

00:31:02,980 --> 00:31:07,210
access i think it's june pretty soon and

00:31:05,860 --> 00:31:08,530
we want to basically take the pain out

00:31:07,210 --> 00:31:11,350
of running if you want to use something

00:31:08,530 --> 00:31:13,330
like kaka like going ahead and running

00:31:11,350 --> 00:31:15,309
all that stuff like the entire JVM stack

00:31:13,330 --> 00:31:19,960
running zookeeper making sure it doesn't

00:31:15,309 --> 00:31:21,370
go down and since it's just say Heroku

00:31:19,960 --> 00:31:23,890
add-on you can go through and just

00:31:21,370 --> 00:31:25,900
create it like any other add-on and

00:31:23,890 --> 00:31:27,039
since it's creating a cluster actually

00:31:25,900 --> 00:31:29,740
takes a while to sit up all these

00:31:27,039 --> 00:31:31,929
machines so we have the cockpit wait

00:31:29,740 --> 00:31:35,590
command so when that returns you know

00:31:31,929 --> 00:31:37,179
your + ur is actually up and running and

00:31:35,590 --> 00:31:38,890
then we provide you some n bars like

00:31:37,179 --> 00:31:40,480
every other add-on that allows you to

00:31:38,890 --> 00:31:42,820
easily connect to your Catholic cluster

00:31:40,480 --> 00:31:45,070
with ssl because you want to ensure that

00:31:42,820 --> 00:31:47,890
that data is encrypted across the

00:31:45,070 --> 00:31:51,220
network with that you'll need to install

00:31:47,890 --> 00:31:52,990
the kafka CLI plugin which gives you

00:31:51,220 --> 00:31:56,380
access to the coffin names based on the

00:31:52,990 --> 00:31:58,830
baroque of CLI and this allows you to

00:31:56,380 --> 00:32:01,390
actually do things with Kafka itself so

00:31:58,830 --> 00:32:04,630
by default when you're developing with

00:32:01,390 --> 00:32:07,000
cough go locally for stuff you it

00:32:04,630 --> 00:32:09,880
configures it so it auto creates topics

00:32:07,000 --> 00:32:11,470
for you as you use them but you probably

00:32:09,880 --> 00:32:13,030
don't actually want this setting for

00:32:11,470 --> 00:32:16,030
production so at brookwood we turn this

00:32:13,030 --> 00:32:18,100
off so you need to use this cop Kafka

00:32:16,030 --> 00:32:19,330
topic create to actually create this

00:32:18,100 --> 00:32:21,490
router topic that we're going to be

00:32:19,330 --> 00:32:23,740
using but if you're setting this up

00:32:21,490 --> 00:32:26,380
yourself you could just make sure to

00:32:23,740 --> 00:32:28,840
either not set that thing off or to make

00:32:26,380 --> 00:32:31,480
sure to actually create it so its coffin

00:32:28,840 --> 00:32:34,090
knows about the topic and the reason for

00:32:31,480 --> 00:32:35,350
this is like if you are doing something

00:32:34,090 --> 00:32:37,120
in production you probably know all the

00:32:35,350 --> 00:32:38,530
topics that you're going to be using and

00:32:37,120 --> 00:32:41,169
you don't want some typos or something

00:32:38,530 --> 00:32:43,600
else to actually insert stuff into a

00:32:41,169 --> 00:32:47,020
topic that you may not even realize as

00:32:43,600 --> 00:32:48,789
an actual topic you can do things with

00:32:47,020 --> 00:32:50,650
the cough plugin like inspect your

00:32:48,789 --> 00:32:53,440
coffee cluster see what version you're

00:32:50,650 --> 00:32:55,090
running see how many topics you have see

00:32:53,440 --> 00:32:57,820
like the network throughput how many

00:32:55,090 --> 00:32:59,799
consumers you have attached to it you

00:32:57,820 --> 00:33:01,090
can even inspect a individual topic so

00:32:59,799 --> 00:33:03,100
you can see how many partitions are

00:33:01,090 --> 00:33:06,549
actually created so this kind of limits

00:33:03,100 --> 00:33:08,020
you on how many feasible consumers you

00:33:06,549 --> 00:33:10,030
can have in a single consumer group so

00:33:08,020 --> 00:33:11,730
by default you get 32 but you can

00:33:10,030 --> 00:33:16,450
increase that to even more if you wanted

00:33:11,730 --> 00:33:18,669
and then similarly you also have this

00:33:16,450 --> 00:33:20,350
tail command like blogs tail command so

00:33:18,669 --> 00:33:23,049
you can actually see the messages and

00:33:20,350 --> 00:33:24,999
tail a particular topic and see them

00:33:23,049 --> 00:33:26,649
come through there's a bunch of other

00:33:24,999 --> 00:33:29,649
stuff inside the coffee namespace

00:33:26,649 --> 00:33:31,450
plug-in like you can write to a

00:33:29,649 --> 00:33:34,480
particular topic like you can do test

00:33:31,450 --> 00:33:35,889
messages or actual messages by hand

00:33:34,480 --> 00:33:38,350
without having to programmatically do

00:33:35,889 --> 00:33:42,940
that as well as configure a bunch of

00:33:38,350 --> 00:33:44,350
stuff that fit your specific needs now

00:33:42,940 --> 00:33:45,879
that we actually have a running cafe

00:33:44,350 --> 00:33:48,039
cluster whether it's on roku or you've

00:33:45,879 --> 00:33:52,619
done it yourself we can look at the

00:33:48,039 --> 00:33:55,720
consumer side of that application so

00:33:52,619 --> 00:33:57,749
with that we have we're cutting these

00:33:55,720 --> 00:34:00,070
metrics so we have this metric consumer

00:33:57,749 --> 00:34:02,529
we're going to call the group ID metrics

00:34:00,070 --> 00:34:04,869
not very original and then we need to

00:34:02,529 --> 00:34:07,899
subscribe to that router topic that we

00:34:04,869 --> 00:34:09,520
created earlier and with that we can

00:34:07,899 --> 00:34:11,770
actually pass in where we want it to

00:34:09,520 --> 00:34:15,819
start when we initially connect so this

00:34:11,770 --> 00:34:17,440
is the default offset so by default it

00:34:15,819 --> 00:34:18,909
actually sets it to latest so it means

00:34:17,440 --> 00:34:21,220
that it will just pull the latest

00:34:18,909 --> 00:34:24,399
position and use that of whatever was

00:34:21,220 --> 00:34:26,139
last committed and if you don't have an

00:34:24,399 --> 00:34:28,240
offset that has been committed it will

00:34:26,139 --> 00:34:30,540
start with whatever the latest actual

00:34:28,240 --> 00:34:32,470
absolute position has been in the queue

00:34:30,540 --> 00:34:34,929
but since we're kind of dealing with

00:34:32,470 --> 00:34:36,700
cache data here like I didn't think it

00:34:34,929 --> 00:34:38,440
mattered that much but you can obviously

00:34:36,700 --> 00:34:41,470
fine-tune this depending on your use

00:34:38,440 --> 00:34:42,549
case like the skylight people have we

00:34:41,470 --> 00:34:43,659
need the connection a Redis because

00:34:42,549 --> 00:34:45,099
that's where we're storing stuff and

00:34:43,659 --> 00:34:48,750
we're basically going to go through each

00:34:45,099 --> 00:34:51,010
of these messages and then parse

00:34:48,750 --> 00:34:53,379
deserialize that JSON that we put in

00:34:51,010 --> 00:34:55,690
initially and then with that data we can

00:34:53,379 --> 00:34:58,380
insert the stuff that we actually want

00:34:55,690 --> 00:35:01,180
to inside a brightest so here's that

00:34:58,380 --> 00:35:03,309
insert code it's not very important but

00:35:01,180 --> 00:35:06,190
basically we're calculating the average

00:35:03,309 --> 00:35:08,710
of the service and connect time or

00:35:06,190 --> 00:35:11,260
keeping track of the number of status

00:35:08,710 --> 00:35:13,150
codes we're getting / route and so since

00:35:11,260 --> 00:35:16,599
we have all this stuff segregated to

00:35:13,150 --> 00:35:18,190
make it very easy for a simple sinatra

00:35:16,599 --> 00:35:20,049
rails app to pull this information at

00:35:18,190 --> 00:35:26,380
arrests actually do something very

00:35:20,049 --> 00:35:27,609
useful with it so that's a single

00:35:26,380 --> 00:35:29,859
consumer group but one of the nice

00:35:27,609 --> 00:35:30,910
things about Kafka is that you don't

00:35:29,859 --> 00:35:33,940
really need a lot of coordination

00:35:30,910 --> 00:35:36,069
between the producer and the consumer

00:35:33,940 --> 00:35:36,550
besides kind of the serialization and

00:35:36,069 --> 00:35:38,470
knowing

00:35:36,550 --> 00:35:41,590
what's being put into it so if you had

00:35:38,470 --> 00:35:43,450
say separate team that want to do

00:35:41,590 --> 00:35:45,700
something else with it in this case it's

00:35:43,450 --> 00:35:48,190
probably the same team here but I have

00:35:45,700 --> 00:35:50,140
this replay consumer here and what this

00:35:48,190 --> 00:35:51,820
does is actually takes those router logs

00:35:50,140 --> 00:35:53,500
and does something completely different

00:35:51,820 --> 00:35:55,270
with it so I'm going to get all the same

00:35:53,500 --> 00:35:57,400
messages that the metrics aggregator is

00:35:55,270 --> 00:36:00,910
getting bug and do something else with

00:35:57,400 --> 00:36:03,250
it so I'm going to parse out the path

00:36:00,910 --> 00:36:05,800
that I have and I'm going to basically

00:36:03,250 --> 00:36:07,660
replay this traffic against a staging

00:36:05,800 --> 00:36:09,460
app that I have up and running and so

00:36:07,660 --> 00:36:11,170
that's pretty easy to set up with a

00:36:09,460 --> 00:36:13,510
complete separate consumer group inside

00:36:11,170 --> 00:36:16,480
of Kafka so here's the code that does

00:36:13,510 --> 00:36:17,830
that you know we give it a different

00:36:16,480 --> 00:36:19,930
group ID because we don't want to be

00:36:17,830 --> 00:36:22,420
part of the same group we have to tell

00:36:19,930 --> 00:36:25,630
that consumer group to subscribe to this

00:36:22,420 --> 00:36:28,870
topic and then we were just making HP

00:36:25,630 --> 00:36:30,670
calls based off of the path inside that

00:36:28,870 --> 00:36:33,120
message from the router log and then

00:36:30,670 --> 00:36:36,610
we're going through and doing that stuff

00:36:33,120 --> 00:36:38,650
this was built in JRuby to basically be

00:36:36,610 --> 00:36:40,330
able to get um some really nice

00:36:38,650 --> 00:36:42,550
concurrency with that but you could

00:36:40,330 --> 00:36:43,930
obviously build an MRI or something else

00:36:42,550 --> 00:36:46,390
you could probably even do it not in

00:36:43,930 --> 00:36:51,280
Ruby if performance was a particular

00:36:46,390 --> 00:36:55,540
concern so all this code is up on github

00:36:51,280 --> 00:36:58,450
you can go here to this repo download

00:36:55,540 --> 00:37:03,040
the code and play with it and examine it

00:36:58,450 --> 00:37:04,990
in more depth and I said all the stuff

00:37:03,040 --> 00:37:08,500
up with docker so you get Redis you get

00:37:04,990 --> 00:37:10,720
the entire coffee cluster you get all of

00:37:08,500 --> 00:37:12,130
the JRuby stuff all set up so you don't

00:37:10,720 --> 00:37:14,290
even need to having that stuff locally

00:37:12,130 --> 00:37:15,640
you just need to have the docker tool

00:37:14,290 --> 00:37:19,690
set up and you can get all that stuff

00:37:15,640 --> 00:37:21,190
spun up with this single command so

00:37:19,690 --> 00:37:25,810
let's talk about some other patterns

00:37:21,190 --> 00:37:27,190
that we can use with Kafka itself we're

00:37:25,810 --> 00:37:28,840
talking about a mess immune system so

00:37:27,190 --> 00:37:31,960
using it for messaging is probably an

00:37:28,840 --> 00:37:34,180
obvious use case a particularly good

00:37:31,960 --> 00:37:36,310
example for this is any type of high

00:37:34,180 --> 00:37:38,830
trade financial transaction system would

00:37:36,310 --> 00:37:40,420
use something like this if you're

00:37:38,830 --> 00:37:41,830
talking about stock trades and things

00:37:40,420 --> 00:37:43,870
like that you probably don't want to be

00:37:41,830 --> 00:37:45,610
losing that data so the durability

00:37:43,870 --> 00:37:47,760
guarantees that coffee gives are really

00:37:45,610 --> 00:37:47,760
nice

00:37:47,810 --> 00:37:52,200
trades happen at pretty fast rates and

00:37:50,850 --> 00:37:54,180
you want to be able to have that low

00:37:52,200 --> 00:37:56,580
latency to be able to do stuff very

00:37:54,180 --> 00:37:58,980
easily with that data and quickly and

00:37:56,580 --> 00:38:00,330
provide that to customers and you're

00:37:58,980 --> 00:38:02,010
having a lot of it so you need that

00:38:00,330 --> 00:38:03,690
through put that big pipe of being on to

00:38:02,010 --> 00:38:06,210
handle that kind of traffic and not have

00:38:03,690 --> 00:38:09,540
it fall over the original use case for

00:38:06,210 --> 00:38:11,880
was activity tracking and so costco was

00:38:09,540 --> 00:38:14,220
created originally by linkedin to solve

00:38:11,880 --> 00:38:16,200
this problem where you know you go

00:38:14,220 --> 00:38:19,470
around doing stuff on the LinkedIn page

00:38:16,200 --> 00:38:21,480
and you want to be able to see what

00:38:19,470 --> 00:38:24,330
people are doing so you can create a

00:38:21,480 --> 00:38:26,160
topic / activity that you care about and

00:38:24,330 --> 00:38:27,630
then track that through Kafka and so you

00:38:26,160 --> 00:38:29,580
can have various groups within your

00:38:27,630 --> 00:38:32,160
organization like saying a recruitment

00:38:29,580 --> 00:38:33,840
team that goes and picks subscribe to

00:38:32,160 --> 00:38:36,410
the topics that they care about and do

00:38:33,840 --> 00:38:38,640
stuff there or you could have say the

00:38:36,410 --> 00:38:41,850
personalization or recommendation engine

00:38:38,640 --> 00:38:43,440
probably do something similar and I

00:38:41,850 --> 00:38:45,150
don't exactly know what Shopify uses

00:38:43,440 --> 00:38:46,950
copper for but I imagine for some

00:38:45,150 --> 00:38:48,119
ecommerce type of site like activity

00:38:46,950 --> 00:38:50,310
tracking probably makes a lot of sense

00:38:48,119 --> 00:38:53,940
to be doing stuff with kaka and I know

00:38:50,310 --> 00:38:56,550
they're using it in house there we use

00:38:53,940 --> 00:38:59,430
it heavily in house at Heroku as well so

00:38:56,550 --> 00:39:01,650
if you've use our if you've into the

00:38:59,430 --> 00:39:03,480
dashboard and use our metrics we have

00:39:01,650 --> 00:39:05,730
all these like the memory uses the

00:39:03,480 --> 00:39:07,530
response time and things like that and

00:39:05,730 --> 00:39:10,020
that's actually all powered by both

00:39:07,530 --> 00:39:12,150
cough ground postgres so if you imagine

00:39:10,020 --> 00:39:13,770
like running millions of apps on all

00:39:12,150 --> 00:39:16,290
these dinos and collecting all this kind

00:39:13,770 --> 00:39:18,150
of information is a ton of data to deal

00:39:16,290 --> 00:39:21,090
with and postgres really just can't

00:39:18,150 --> 00:39:26,000
handle that pure throughput of data so

00:39:21,090 --> 00:39:29,070
we use Kafka as this stream to basically

00:39:26,000 --> 00:39:32,510
handle that kind of load and then

00:39:29,070 --> 00:39:35,010
basically we take windows of time and

00:39:32,510 --> 00:39:36,450
calculate the information that we need

00:39:35,010 --> 00:39:38,730
and put them inside a post rests and

00:39:36,450 --> 00:39:41,130
then from within dashboard or metrics

00:39:38,730 --> 00:39:43,380
team is able to then take that data out

00:39:41,130 --> 00:39:45,750
and display useful information to you so

00:39:43,380 --> 00:39:48,990
similar to the this is what I based the

00:39:45,750 --> 00:39:50,700
example a pond but this is a more actual

00:39:48,990 --> 00:39:54,330
complete use case and this was built by

00:39:50,700 --> 00:39:58,170
only two people originally and it works

00:39:54,330 --> 00:39:59,550
pretty well the other final example that

00:39:58,170 --> 00:40:02,610
I have is the

00:39:59,550 --> 00:40:04,410
heroku api eventbus and so as you're

00:40:02,610 --> 00:40:06,240
doing stuff in Heroku you're interacting

00:40:04,410 --> 00:40:08,610
with API whether it's the tool belt or

00:40:06,240 --> 00:40:11,250
calling it directly you're doing actions

00:40:08,610 --> 00:40:13,500
on her if you like scaling dinos up or

00:40:11,250 --> 00:40:14,850
creating ad on scaling dinos down now as

00:40:13,500 --> 00:40:17,010
you can imagine the ordering of those

00:40:14,850 --> 00:40:18,480
events are pretty important if you are

00:40:17,010 --> 00:40:21,060
say the billing team because you want to

00:40:18,480 --> 00:40:23,370
build people properly and the order that

00:40:21,060 --> 00:40:26,640
those messages come in but what's nice

00:40:23,370 --> 00:40:28,230
about the event bus here is that many

00:40:26,640 --> 00:40:31,380
teams that broke we want access to this

00:40:28,230 --> 00:40:32,700
kind of information so both the billing

00:40:31,380 --> 00:40:35,130
team want to be able to know when you

00:40:32,700 --> 00:40:37,380
created that on but the data team that

00:40:35,130 --> 00:40:39,030
we have actually runs Kafka on postgres

00:40:37,380 --> 00:40:41,400
and all those services probably also

00:40:39,030 --> 00:40:44,040
cares specifically if you create a data

00:40:41,400 --> 00:40:46,230
specific ad on so without coordinating

00:40:44,040 --> 00:40:48,330
with each other they're able to consume

00:40:46,230 --> 00:40:50,610
this event bus and actually do what they

00:40:48,330 --> 00:40:52,830
need to and get their job done and I

00:40:50,610 --> 00:40:55,680
think currently today we have 13 various

00:40:52,830 --> 00:40:57,600
teams using different parts of the event

00:40:55,680 --> 00:41:02,310
bus and they're not all consuming all

00:40:57,600 --> 00:41:05,520
the messages so i hope this talk has

00:41:02,310 --> 00:41:07,350
given you some insight and what you can

00:41:05,520 --> 00:41:10,050
do the kind of applications you can

00:41:07,350 --> 00:41:13,140
build with Kafka and I think rails is in

00:41:10,050 --> 00:41:15,570
a place today where it's part of a

00:41:13,140 --> 00:41:18,210
bigger critical path for a lot of

00:41:15,570 --> 00:41:21,480
businesses where it started maybe like

00:41:18,210 --> 00:41:24,420
five years ago with a rails app and I

00:41:21,480 --> 00:41:27,630
think things like durability scalability

00:41:24,420 --> 00:41:31,020
and speed matter now more for these kind

00:41:27,630 --> 00:41:33,750
of apps and I believe technologies like

00:41:31,020 --> 00:41:35,250
Kafka that have been kind of started in

00:41:33,750 --> 00:41:36,960
the even though it's starting the JVM

00:41:35,250 --> 00:41:39,060
ecosystem can be really helpful for us

00:41:36,960 --> 00:41:41,370
today where you may have had problems

00:41:39,060 --> 00:41:43,920
doing stuff and scaling things with

00:41:41,370 --> 00:41:46,770
Redis but maybe Kafka is able to solve

00:41:43,920 --> 00:41:48,420
some of those problems for you so that's

00:41:46,770 --> 00:41:51,390
kind of all I had and the organizers are

00:41:48,420 --> 00:41:54,540
telling me I'm out of time so thank you

00:41:51,390 --> 00:41:56,430
I will be around till Sunday so if you

00:41:54,540 --> 00:41:57,360
have any questions please come talk to

00:41:56,430 --> 00:42:03,450
me and then

00:41:57,360 --> 00:42:07,050
that's why Thank You Terrance so yeah

00:42:03,450 --> 00:42:10,310
you can grab him here at the stage if

00:42:07,050 --> 00:42:10,310
you have some questions for him

00:42:16,369 --> 00:42:18,430

YouTube URL: https://www.youtube.com/watch?v=yl3JmF3n2bQ


