Title: EuRuKo 2019 Keynote: The Miseducation of This Machine by Laura Linda Laugwitz
Publication date: 2021-01-11
Playlist: EuRuKo 2019
Description: 
	Closing keynote: The Miseducation of This Machine 

While machines continue to learn with more sophisticated algorithms and larger amounts of data, humans need to understand how such learning works in order to take its results with the proper grain of salt. Let's make ML tangible and thus help you become a better machine teacher!

Laura Linda Laugwitz - https://twitter.com/lauralindal
EuRuKo 2019
Captions: 
	00:00:06,080 --> 00:00:09,200
so

00:00:06,480 --> 00:00:09,840
uh laura is going to come up um she

00:00:09,200 --> 00:00:13,040
likes

00:00:09,840 --> 00:00:14,160
the 1990 television shows i know that

00:00:13,040 --> 00:00:16,400
for sure

00:00:14,160 --> 00:00:17,359
because she was wearing a 1990s t-shirt

00:00:16,400 --> 00:00:18,720
yesterday so

00:00:17,359 --> 00:00:21,119
well potentially just means she likes

00:00:18,720 --> 00:00:31,840
those t-shirts buffy the vampire slayer

00:00:21,119 --> 00:00:31,840
feel free to come up yes hello laura hey

00:00:32,640 --> 00:00:36,800
fun fact about laura she started out

00:00:35,200 --> 00:00:38,960
studying uh social and cultural

00:00:36,800 --> 00:00:42,000
anthropology and as you do you ended up

00:00:38,960 --> 00:00:42,000
and she ended up in tech

00:00:42,079 --> 00:00:46,719
that's what you do yeah because you need

00:00:43,680 --> 00:00:46,719
to pay rent somehow

00:00:47,760 --> 00:00:51,600
and rent is getting expensive even in

00:00:49,680 --> 00:00:54,320
berlin unfortunately

00:00:51,600 --> 00:00:56,160
well so let's give it up for our final

00:00:54,320 --> 00:00:59,840
talk of the day

00:00:56,160 --> 00:00:59,840
laura thank you

00:01:04,799 --> 00:01:09,119
thanks i haven't even said anything of

00:01:06,479 --> 00:01:09,119
importance yet

00:01:09,200 --> 00:01:13,360
so my name is laura and uh i've been

00:01:12,560 --> 00:01:15,759
studying

00:01:13,360 --> 00:01:18,479
various things for the past 11 years

00:01:15,759 --> 00:01:20,880
actually um and i'm still not done

00:01:18,479 --> 00:01:22,640
um but i've also helped organize rails

00:01:20,880 --> 00:01:25,119
girls berlin for quite a while

00:01:22,640 --> 00:01:27,040
um and i would like to see everyone

00:01:25,119 --> 00:01:29,680
who's involved in rails girls or

00:01:27,040 --> 00:01:30,560
rails girls like projects either as a

00:01:29,680 --> 00:01:32,960
learner or

00:01:30,560 --> 00:01:34,560
as a coach or organizer to raise their

00:01:32,960 --> 00:01:36,479
hands awesome

00:01:34,560 --> 00:01:37,840
that's a lot of people and thank you

00:01:36,479 --> 00:01:39,840
because you're kind of

00:01:37,840 --> 00:01:41,200
you know making sure this community

00:01:39,840 --> 00:01:42,720
keeps growing and that's really

00:01:41,200 --> 00:01:43,830
important so give yourself a round of

00:01:42,720 --> 00:01:50,399
applause

00:01:43,830 --> 00:01:55,840
[Applause]

00:01:50,399 --> 00:01:55,840
okay so now let's get this ship started

00:01:56,079 --> 00:01:59,399
my references are not as good um okay so

00:01:58,960 --> 00:02:03,200
in

00:01:59,399 --> 00:02:03,680
1998 lauren hill released her debut

00:02:03,200 --> 00:02:05,439
album

00:02:03,680 --> 00:02:07,040
the miseducation of lauren hill and you

00:02:05,439 --> 00:02:07,680
might notice that this has something to

00:02:07,040 --> 00:02:10,479
do with

00:02:07,680 --> 00:02:12,640
the title today so the title of her

00:02:10,479 --> 00:02:15,200
album is a reference to several other

00:02:12,640 --> 00:02:16,319
works and they all have make this point

00:02:15,200 --> 00:02:19,760
of how

00:02:16,319 --> 00:02:21,520
the u.s american school education system

00:02:19,760 --> 00:02:24,239
has been indoctrinating black

00:02:21,520 --> 00:02:25,920
communities with white supremacy instead

00:02:24,239 --> 00:02:26,800
of teaching those communities and

00:02:25,920 --> 00:02:30,239
everyone else

00:02:26,800 --> 00:02:31,120
black history and black present um so

00:02:30,239 --> 00:02:34,000
the question of

00:02:31,120 --> 00:02:35,120
who educates and who produces knowledge

00:02:34,000 --> 00:02:38,400
is a question of

00:02:35,120 --> 00:02:41,519
who holds power over a seemingly

00:02:38,400 --> 00:02:43,200
universal truth um the miseducation of

00:02:41,519 --> 00:02:46,000
lauryn hill is still one of my favorite

00:02:43,200 --> 00:02:49,120
albums today

00:02:46,000 --> 00:02:50,800
and kaias right

00:02:49,120 --> 00:02:52,239
and this title will guide us through the

00:02:50,800 --> 00:02:54,720
next 30 minutes maybe

00:02:52,239 --> 00:02:56,560
also just 25 if i forget to breathe in

00:02:54,720 --> 00:02:58,959
between

00:02:56,560 --> 00:02:59,760
so i want you to keep this in mind as we

00:02:58,959 --> 00:03:02,800
explore today

00:02:59,760 --> 00:03:05,040
how not just humans but also machines

00:03:02,800 --> 00:03:06,239
gain and produce knowledge meaning how

00:03:05,040 --> 00:03:08,159
machines learn

00:03:06,239 --> 00:03:10,159
and i promise that i will leave out all

00:03:08,159 --> 00:03:11,599
the math so you should all be able to

00:03:10,159 --> 00:03:15,360
follow

00:03:11,599 --> 00:03:18,239
so let's take one more step back in time

00:03:15,360 --> 00:03:18,959
into 1950 where alan turing published a

00:03:18,239 --> 00:03:20,720
paper

00:03:18,959 --> 00:03:23,280
and one of the questions he asked

00:03:20,720 --> 00:03:25,519
basically was whether machines can think

00:03:23,280 --> 00:03:26,480
now thinking was really hard to define

00:03:25,519 --> 00:03:29,440
philosophically

00:03:26,480 --> 00:03:30,840
for alan turing so what he did was not

00:03:29,440 --> 00:03:34,239
answer it

00:03:30,840 --> 00:03:36,879
and just tweak the question a little bit

00:03:34,239 --> 00:03:38,159
and instead ask whether machines can

00:03:36,879 --> 00:03:40,080
imitate thinking

00:03:38,159 --> 00:03:42,400
and this is something that you might

00:03:40,080 --> 00:03:46,239
have heard of as the imitation game

00:03:42,400 --> 00:03:49,120
where we try to figure out if a human

00:03:46,239 --> 00:03:50,400
can actually make a difference or

00:03:49,120 --> 00:03:52,239
recognize a difference between

00:03:50,400 --> 00:03:55,760
interacting with another human

00:03:52,239 --> 00:03:57,599
or a machine now for thinking

00:03:55,760 --> 00:03:59,280
it seems to be hard but for machine

00:03:57,599 --> 00:04:01,360
learning we kind of

00:03:59,280 --> 00:04:04,640
have accepted this term and i would like

00:04:01,360 --> 00:04:04,640
to question it obviously

00:04:05,519 --> 00:04:09,280
so spoilers i will argue today that

00:04:07,680 --> 00:04:10,239
machines are not fully capable of

00:04:09,280 --> 00:04:11,920
learning

00:04:10,239 --> 00:04:14,000
but really good at making us think that

00:04:11,920 --> 00:04:16,079
they are learning and while it was hard

00:04:14,000 --> 00:04:17,519
for chewing to define what thinking

00:04:16,079 --> 00:04:18,400
means i think it's a bit easier to

00:04:17,519 --> 00:04:20,400
define

00:04:18,400 --> 00:04:23,600
what learning means and so i can

00:04:20,400 --> 00:04:23,600
actually prove my argument

00:04:23,919 --> 00:04:27,120
for the purpose of this talk i will

00:04:25,520 --> 00:04:28,560
understand learning as a process in

00:04:27,120 --> 00:04:30,160
three steps

00:04:28,560 --> 00:04:31,840
and you might have noticed them in

00:04:30,160 --> 00:04:34,080
yourself but

00:04:31,840 --> 00:04:36,080
i think also melanie mentioned at least

00:04:34,080 --> 00:04:38,000
two of these steps today

00:04:36,080 --> 00:04:39,600
and the first one is where you just

00:04:38,000 --> 00:04:41,120
learn to reproduce knowledge it's like

00:04:39,600 --> 00:04:42,560
learning vocab when you're learning a

00:04:41,120 --> 00:04:45,919
new language you need to get

00:04:42,560 --> 00:04:47,600
the terms right same as in programming

00:04:45,919 --> 00:04:49,680
and then you do the remixing part and

00:04:47,600 --> 00:04:51,759
that was actually where

00:04:49,680 --> 00:04:53,759
we had the the pokemon characters

00:04:51,759 --> 00:04:54,880
created that's remixing knowledge taking

00:04:53,759 --> 00:04:56,800
knowledge from one

00:04:54,880 --> 00:04:58,560
domain and putting it onto another

00:04:56,800 --> 00:05:01,759
domain and then the

00:04:58,560 --> 00:05:03,680
well final or or most interesting

00:05:01,759 --> 00:05:06,240
level of acquiring knowledge actually is

00:05:03,680 --> 00:05:08,320
the reflection part where you

00:05:06,240 --> 00:05:09,360
learn to understand where the limits of

00:05:08,320 --> 00:05:10,960
your knowledge are

00:05:09,360 --> 00:05:12,960
and what the knowledge can do and what

00:05:10,960 --> 00:05:14,320
it cannot do and what it should be doing

00:05:12,960 --> 00:05:15,759
and all of those things we'll get into

00:05:14,320 --> 00:05:18,639
that later

00:05:15,759 --> 00:05:19,520
um so we will now look at three key

00:05:18,639 --> 00:05:21,919
elements that are

00:05:19,520 --> 00:05:22,880
involved in machine learning and this is

00:05:21,919 --> 00:05:26,320
data

00:05:22,880 --> 00:05:28,320
obviously algorithms obviously and

00:05:26,320 --> 00:05:31,440
maybe as a surprise to people who don't

00:05:28,320 --> 00:05:31,440
know me humans

00:05:31,520 --> 00:05:35,520
and i will show you that data on its own

00:05:34,240 --> 00:05:36,720
cannot go beyond the level of

00:05:35,520 --> 00:05:39,280
reproduction

00:05:36,720 --> 00:05:40,720
algorithms on their own cannot go beyond

00:05:39,280 --> 00:05:43,039
the level of remixing

00:05:40,720 --> 00:05:45,199
and then humans cannot well we need the

00:05:43,039 --> 00:05:48,080
humans to do the reflection part

00:05:45,199 --> 00:05:48,560
so now you know what this is all about

00:05:48,080 --> 00:05:52,479
you may

00:05:48,560 --> 00:05:53,280
enjoy the sun first a quick content

00:05:52,479 --> 00:05:55,440
warning though

00:05:53,280 --> 00:05:56,560
um when we get to the last part of this

00:05:55,440 --> 00:05:58,800
talk i will

00:05:56,560 --> 00:06:00,400
um i want to give you an example of

00:05:58,800 --> 00:06:03,840
something that is hate speech

00:06:00,400 --> 00:06:04,800
um and it contains racist and racism and

00:06:03,840 --> 00:06:07,520
sexual violence

00:06:04,800 --> 00:06:08,160
so in writing so i will not read it out

00:06:07,520 --> 00:06:11,199
loud

00:06:08,160 --> 00:06:12,400
you can look at it or not look at it but

00:06:11,199 --> 00:06:15,680
i will also let you know

00:06:12,400 --> 00:06:16,400
right before it comes up so let's get

00:06:15,680 --> 00:06:18,840
started with

00:06:16,400 --> 00:06:20,240
the reproduction part of machine

00:06:18,840 --> 00:06:22,639
learning or

00:06:20,240 --> 00:06:23,600
of knowledge more like that so machine

00:06:22,639 --> 00:06:25,919
learning is

00:06:23,600 --> 00:06:27,840
quite good at reproducing what we as a

00:06:25,919 --> 00:06:30,080
human collective

00:06:27,840 --> 00:06:31,520
already know it can identify spam

00:06:30,080 --> 00:06:32,639
messages it can make music

00:06:31,520 --> 00:06:34,639
recommendations

00:06:32,639 --> 00:06:35,680
or translate from one language to

00:06:34,639 --> 00:06:37,759
another

00:06:35,680 --> 00:06:39,520
and machine learning can also take that

00:06:37,759 --> 00:06:40,240
knowledge that we as a human collective

00:06:39,520 --> 00:06:42,800
have

00:06:40,240 --> 00:06:46,400
and make it available to a large much

00:06:42,800 --> 00:06:48,800
larger set of individuals right

00:06:46,400 --> 00:06:50,080
however machines can also learn the

00:06:48,800 --> 00:06:51,840
wrong thing

00:06:50,080 --> 00:06:53,680
this is caroline cinders i don't know if

00:06:51,840 --> 00:06:54,720
you've seen her talk at republica this

00:06:53,680 --> 00:06:57,120
year

00:06:54,720 --> 00:06:58,479
but she mentioned uh her music

00:06:57,120 --> 00:07:00,720
recommendation machine

00:06:58,479 --> 00:07:01,680
that put way too much focus on a few

00:07:00,720 --> 00:07:03,599
weeks of like

00:07:01,680 --> 00:07:04,720
breakup music that she was listening to

00:07:03,599 --> 00:07:06,720
very intensely

00:07:04,720 --> 00:07:09,280
and that has screwed up her algorithm

00:07:06,720 --> 00:07:13,440
ever since and spotify is just

00:07:09,280 --> 00:07:15,520
suggesting mumford and sons

00:07:13,440 --> 00:07:17,280
all the time now i don't have a great

00:07:15,520 --> 00:07:18,080
taste of music so i'm not judging that

00:07:17,280 --> 00:07:21,039
at all

00:07:18,080 --> 00:07:22,880
um but the only option for her is

00:07:21,039 --> 00:07:24,720
actually to delete her account create a

00:07:22,880 --> 00:07:25,440
new one and stay away from that band

00:07:24,720 --> 00:07:26,800
forever

00:07:25,440 --> 00:07:29,280
because she cannot get rid of them

00:07:26,800 --> 00:07:29,280
otherwise

00:07:29,599 --> 00:07:33,120
that was the fun part but there are also

00:07:31,199 --> 00:07:35,759
plenty more serious examples

00:07:33,120 --> 00:07:37,039
where reproducing human behavior is not

00:07:35,759 --> 00:07:39,280
really a good idea

00:07:37,039 --> 00:07:41,039
so you might have heard of some of these

00:07:39,280 --> 00:07:42,479
cases

00:07:41,039 --> 00:07:45,199
because machines are really good at

00:07:42,479 --> 00:07:46,400
learning bias they apply bias in

00:07:45,199 --> 00:07:50,560
recruiting tools

00:07:46,400 --> 00:07:50,560
such as one that amazon once built

00:07:51,039 --> 00:07:54,080
they apply bias in the justice or well

00:07:53,599 --> 00:07:57,680
rather

00:07:54,080 --> 00:07:59,280
the injustice system

00:07:57,680 --> 00:08:02,160
and they apply bias in image

00:07:59,280 --> 00:08:02,160
classification

00:08:02,400 --> 00:08:08,560
because humans do bias quite well

00:08:05,840 --> 00:08:09,520
it will be found in most data sets and

00:08:08,560 --> 00:08:12,240
thus machines

00:08:09,520 --> 00:08:14,240
learn to reproduce it but it's also more

00:08:12,240 --> 00:08:16,479
than that

00:08:14,240 --> 00:08:17,759
so the artist hito style puts it this

00:08:16,479 --> 00:08:20,319
way

00:08:17,759 --> 00:08:21,599
those who own past data colonize the

00:08:20,319 --> 00:08:23,440
future

00:08:21,599 --> 00:08:24,639
and what that means is that machine

00:08:23,440 --> 00:08:27,919
learning predicts

00:08:24,639 --> 00:08:29,280
the future based on past data so we

00:08:27,919 --> 00:08:32,640
always use the past

00:08:29,280 --> 00:08:35,120
in order to learn or predict something

00:08:32,640 --> 00:08:36,479
in the future so whatever has been will

00:08:35,120 --> 00:08:40,880
always been taken

00:08:36,479 --> 00:08:42,880
into the future so that's hopefully our

00:08:40,880 --> 00:08:44,880
first learning for today

00:08:42,880 --> 00:08:46,240
data as one element of machine learning

00:08:44,880 --> 00:08:49,920
will not learn

00:08:46,240 --> 00:08:49,920
beyond that level of reproduction

00:08:50,800 --> 00:08:54,080
how about the next step of learning

00:08:52,839 --> 00:08:56,080
remixing

00:08:54,080 --> 00:08:58,080
and i don't mean like the scratching dj

00:08:56,080 --> 00:09:00,320
thing

00:08:58,080 --> 00:09:01,680
well before we can continue this talk i

00:09:00,320 --> 00:09:06,839
need to know whether you

00:09:01,680 --> 00:09:10,640
are human and i'm asking you to please

00:09:06,839 --> 00:09:12,880
select all the images with a bike

00:09:10,640 --> 00:09:15,040
and since i have no proper interface for

00:09:12,880 --> 00:09:16,640
this we have to agree on something and i

00:09:15,040 --> 00:09:19,600
have a suggestion and i hope you're

00:09:16,640 --> 00:09:23,360
all down for this so does everyone know

00:09:19,600 --> 00:09:24,880
what jazz hands are it's like the

00:09:23,360 --> 00:09:26,480
you're wiggling your fingers and you can

00:09:24,880 --> 00:09:27,200
do it up here or you can do something

00:09:26,480 --> 00:09:29,519
similar

00:09:27,200 --> 00:09:31,279
whatever feels okay for you so i'm going

00:09:29,519 --> 00:09:33,279
to show you four images and when you

00:09:31,279 --> 00:09:36,720
identify a bike please give me your best

00:09:33,279 --> 00:09:40,480
js hands or spirit fingers and if not

00:09:36,720 --> 00:09:43,279
just don't move okay so does this image

00:09:40,480 --> 00:09:43,279
contain a bike

00:09:45,920 --> 00:09:52,720
does this image contain a bike

00:09:51,120 --> 00:09:55,920
did someone bring their bike and it's

00:09:52,720 --> 00:09:55,920
secretly hidden in there

00:09:57,120 --> 00:10:00,640
does this image contain a bike

00:10:01,680 --> 00:10:09,200
and does this image contain a bike

00:10:05,440 --> 00:10:09,920
okay this also contains it contains some

00:10:09,200 --> 00:10:11,839
rotterdam

00:10:09,920 --> 00:10:13,600
architecture that you might to one want

00:10:11,839 --> 00:10:15,360
to check out after this talk because

00:10:13,600 --> 00:10:20,000
it's really awesome those cube

00:10:15,360 --> 00:10:24,000
houses um so congratulations

00:10:20,000 --> 00:10:24,000
you're not a robot however

00:10:27,040 --> 00:10:31,600
or you just tricked us really well

00:10:30,079 --> 00:10:34,640
however i was the one asking the

00:10:31,600 --> 00:10:37,120
questions so we still don't know if i am

00:10:34,640 --> 00:10:38,320
so let's pretend that i actually am a

00:10:37,120 --> 00:10:40,000
machine

00:10:38,320 --> 00:10:42,000
and you classified some bikes for me and

00:10:40,000 --> 00:10:43,200
so obviously now i'm going to learn how

00:10:42,000 --> 00:10:46,079
to classify bikes

00:10:43,200 --> 00:10:46,800
because that's what i was built for and

00:10:46,079 --> 00:10:49,360
i'm gonna

00:10:46,800 --> 00:10:50,320
start doing so by first simplifying this

00:10:49,360 --> 00:10:52,079
image a little bit

00:10:50,320 --> 00:10:53,680
because i am not the most resourceful

00:10:52,079 --> 00:10:55,200
machine and this has a really good

00:10:53,680 --> 00:10:58,640
resolution

00:10:55,200 --> 00:11:00,320
um and i'm gonna so i'm gonna lower the

00:10:58,640 --> 00:11:02,240
resolution and drop the colors and this

00:11:00,320 --> 00:11:03,440
has now like the size of my pants for

00:11:02,240 --> 00:11:06,399
resolution it's still

00:11:03,440 --> 00:11:07,760
way too way too much i'm gonna lower the

00:11:06,399 --> 00:11:10,959
resolution even more

00:11:07,760 --> 00:11:12,880
can you still identify something

00:11:10,959 --> 00:11:14,399
you know what it should be so you can

00:11:12,880 --> 00:11:16,800
right okay

00:11:14,399 --> 00:11:18,880
so i will start comparing this image

00:11:16,800 --> 00:11:23,519
with very basic shapes

00:11:18,880 --> 00:11:25,760
like horizontal lines

00:11:23,519 --> 00:11:28,560
and vertical lines you've seen these in

00:11:25,760 --> 00:11:30,880
your life before

00:11:28,560 --> 00:11:32,720
so i'm going to use these in order to

00:11:30,880 --> 00:11:35,360
identify parts of the bike

00:11:32,720 --> 00:11:36,560
that are standing out and i'm going to

00:11:35,360 --> 00:11:37,920
start small with just

00:11:36,560 --> 00:11:41,440
these shapes and it's going to get more

00:11:37,920 --> 00:11:43,360
complex but i didn't draw more of these

00:11:41,440 --> 00:11:45,200
so you as humans you can already

00:11:43,360 --> 00:11:46,000
identify some of these shapes in the

00:11:45,200 --> 00:11:48,160
image right

00:11:46,000 --> 00:11:49,279
if i walk around here and you can like

00:11:48,160 --> 00:11:52,240
tell me

00:11:49,279 --> 00:11:53,360
which shape is where does someone have

00:11:52,240 --> 00:11:57,839
an idea

00:11:53,360 --> 00:11:57,839
you can just yell at me

00:11:58,480 --> 00:12:05,760
this one on the bottom left this one oh

00:12:02,639 --> 00:12:11,839
yes and look i even measured and it's

00:12:05,760 --> 00:12:11,839
the same pixel size

00:12:12,399 --> 00:12:16,880
also i think thanks to melanie because

00:12:14,320 --> 00:12:19,279
she gave me a ruler

00:12:16,880 --> 00:12:21,519
uh right so you as humans you can like

00:12:19,279 --> 00:12:23,920
identify that this is a horizontal line

00:12:21,519 --> 00:12:25,279
you might also be able to identify that

00:12:23,920 --> 00:12:28,399
over here

00:12:25,279 --> 00:12:30,639
is a vertical line right yeah but me

00:12:28,399 --> 00:12:32,800
as a machine i cannot what do i need to

00:12:30,639 --> 00:12:32,800
do

00:12:33,120 --> 00:12:37,200
i need to do numbers yes but i promise

00:12:34,800 --> 00:12:40,880
to leave out all the math

00:12:37,200 --> 00:12:42,800
so what i would do is i would take

00:12:40,880 --> 00:12:43,920
one shape and i would start at the very

00:12:42,800 --> 00:12:46,959
top

00:12:43,920 --> 00:12:47,279
and put it on there and see if what i

00:12:46,959 --> 00:12:50,000
have

00:12:47,279 --> 00:12:52,000
is similar to that part of the image and

00:12:50,000 --> 00:12:54,000
then i would move on

00:12:52,000 --> 00:12:55,200
one or two pixel and compare again and

00:12:54,000 --> 00:12:56,720
then i would move on

00:12:55,200 --> 00:13:00,560
one or two pixel and you know how

00:12:56,720 --> 00:13:00,560
machines work what would i do then

00:13:01,680 --> 00:13:05,519
exactly and i would do this for the

00:13:03,519 --> 00:13:06,880
entire image and then i would use this

00:13:05,519 --> 00:13:07,600
other one and do that for the entire

00:13:06,880 --> 00:13:09,680
image

00:13:07,600 --> 00:13:12,079
and while i'm doing this i'm actually

00:13:09,680 --> 00:13:14,480
still not really identifying

00:13:12,079 --> 00:13:16,079
shapes i'm just redrawing the image and

00:13:14,480 --> 00:13:19,040
creating a new one

00:13:16,079 --> 00:13:20,000
so i create a new image that's based on

00:13:19,040 --> 00:13:22,320
the old image

00:13:20,000 --> 00:13:23,680
and the shapes that i have and so if i

00:13:22,320 --> 00:13:26,880
apply

00:13:23,680 --> 00:13:30,639
this one my final image

00:13:26,880 --> 00:13:32,639
no sorry if i apply this one

00:13:30,639 --> 00:13:34,079
my image that comes out of the

00:13:32,639 --> 00:13:35,600
comparison is going to look something

00:13:34,079 --> 00:13:37,760
like this

00:13:35,600 --> 00:13:39,279
you as a human can maybe not see much

00:13:37,760 --> 00:13:41,519
more in it

00:13:39,279 --> 00:13:44,800
and if i apply the horizontal filter

00:13:41,519 --> 00:13:44,800
it's going to look something like that

00:13:45,920 --> 00:13:49,519
and so i'm going to do this with other

00:13:47,920 --> 00:13:50,639
shapes as well and i'm going to take

00:13:49,519 --> 00:13:52,480
these new images

00:13:50,639 --> 00:13:54,720
and apply combinations of shapes and so

00:13:52,480 --> 00:13:55,839
on and this will continue to reduce the

00:13:54,720 --> 00:13:57,360
size as well

00:13:55,839 --> 00:13:59,440
and the images will get more and more

00:13:57,360 --> 00:14:01,440
abstract and smaller and you as a human

00:13:59,440 --> 00:14:04,000
will no longer be able to recognize

00:14:01,440 --> 00:14:05,279
and once i'm done with one image i'm

00:14:04,000 --> 00:14:07,199
going to do that with all the other

00:14:05,279 --> 00:14:09,279
images that you identified

00:14:07,199 --> 00:14:11,199
that you classified for me so these are

00:14:09,279 --> 00:14:14,480
all from the ones that you

00:14:11,199 --> 00:14:14,480
just handed out for me

00:14:14,560 --> 00:14:18,240
and then i will remember the

00:14:16,240 --> 00:14:20,079
quintessence of them

00:14:18,240 --> 00:14:21,839
um and i will end up with some blobs

00:14:20,079 --> 00:14:22,639
that you definitely can no longer

00:14:21,839 --> 00:14:25,920
recognize

00:14:22,639 --> 00:14:28,079
but i as a machine can now use them

00:14:25,920 --> 00:14:28,959
and this is basically the intuition for

00:14:28,079 --> 00:14:31,040
convolutional

00:14:28,959 --> 00:14:32,399
neural network if you've never heard of

00:14:31,040 --> 00:14:34,880
it

00:14:32,399 --> 00:14:36,320
there you go so what i do is i extract

00:14:34,880 --> 00:14:38,160
different shapes from images

00:14:36,320 --> 00:14:40,240
that are defining for the shape of a

00:14:38,160 --> 00:14:41,600
bike and then i end up with knowledge

00:14:40,240 --> 00:14:43,360
and this can now help me

00:14:41,600 --> 00:14:45,120
to determine which shape is how

00:14:43,360 --> 00:14:47,279
important in a bike

00:14:45,120 --> 00:14:49,680
and once i have these abstract ideas you

00:14:47,279 --> 00:14:51,839
can hand me a new image

00:14:49,680 --> 00:14:54,160
and i can tell you whether this contains

00:14:51,839 --> 00:14:56,240
a bike or not

00:14:54,160 --> 00:14:57,199
this one does not contain a bike but it

00:14:56,240 --> 00:15:00,399
is matt's

00:14:57,199 --> 00:15:03,680
holding like my name at the airport so i

00:15:00,399 --> 00:15:03,680
needed to include this somehow

00:15:05,519 --> 00:15:09,360
but you could also hand me this image

00:15:07,440 --> 00:15:11,920
and i could probably say this

00:15:09,360 --> 00:15:13,680
this might very well include the image

00:15:11,920 --> 00:15:15,440
of a bike

00:15:13,680 --> 00:15:16,800
however there are some problems to this

00:15:15,440 --> 00:15:18,160
example

00:15:16,800 --> 00:15:20,399
to this approach and i want to show you

00:15:18,160 --> 00:15:21,199
one example and there was a study last

00:15:20,399 --> 00:15:24,079
year actually

00:15:21,199 --> 00:15:25,760
that tried to trick machines like me and

00:15:24,079 --> 00:15:26,399
i believe that those researchers had a

00:15:25,760 --> 00:15:31,279
lot of fun

00:15:26,399 --> 00:15:31,279
doing so because what is this

00:15:36,399 --> 00:15:41,519
yes so

00:15:39,519 --> 00:15:43,279
their classifier actually classified

00:15:41,519 --> 00:15:46,959
this as a bike

00:15:43,279 --> 00:15:48,320
um because the elements or from a

00:15:46,959 --> 00:15:51,680
machine perspective

00:15:48,320 --> 00:15:54,720
rather the basic shapes of a bike

00:15:51,680 --> 00:15:57,759
exist right they're present

00:15:54,720 --> 00:15:59,440
um and it doesn't matter that in this um

00:15:57,759 --> 00:16:02,399
setup the image makes no sense to a

00:15:59,440 --> 00:16:04,800
human and the bible is kind of broken

00:16:02,399 --> 00:16:06,320
um but this shows us how machines tend

00:16:04,800 --> 00:16:08,160
to overlook the bigger picture

00:16:06,320 --> 00:16:09,839
they can mainly just look at images as a

00:16:08,160 --> 00:16:11,759
combination of shapes

00:16:09,839 --> 00:16:13,759
but they fail to see them as a whole

00:16:11,759 --> 00:16:17,360
most of the time

00:16:13,759 --> 00:16:19,440
so second learning for today algorithms

00:16:17,360 --> 00:16:21,199
as another element of machine learning

00:16:19,440 --> 00:16:23,519
do quite all right at reproducing

00:16:21,199 --> 00:16:25,519
they can extract and reproduce shapes

00:16:23,519 --> 00:16:27,680
and see if other images are similar

00:16:25,519 --> 00:16:31,279
in that respect and quite literally this

00:16:27,680 --> 00:16:32,959
machine does well with remixing images

00:16:31,279 --> 00:16:34,959
but remixing of course does have a

00:16:32,959 --> 00:16:36,560
different meaning too so when we come to

00:16:34,959 --> 00:16:38,880
the context of so-called

00:16:36,560 --> 00:16:39,759
transfer learning algorithms can use

00:16:38,880 --> 00:16:41,600
knowledge

00:16:39,759 --> 00:16:42,800
from one context that they have learned

00:16:41,600 --> 00:16:44,560
in one context and

00:16:42,800 --> 00:16:46,320
apply it to a different context but this

00:16:44,560 --> 00:16:49,360
is still in the making

00:16:46,320 --> 00:16:50,880
kind of so however algorithms do

00:16:49,360 --> 00:16:53,360
struggle with like the bigger picture

00:16:50,880 --> 00:16:56,560
and this gets us to the third part

00:16:53,360 --> 00:16:56,560
and a drink of water

00:16:58,800 --> 00:17:05,839
so reflection imagine that i had asked

00:17:02,480 --> 00:17:05,839
you to do a very different task

00:17:07,360 --> 00:17:11,280
imagine i had asked you to identify hate

00:17:09,760 --> 00:17:13,199
speech

00:17:11,280 --> 00:17:14,880
so while detecting objects such as

00:17:13,199 --> 00:17:17,199
spikes is rather easy

00:17:14,880 --> 00:17:19,280
speech is much more complex as we've

00:17:17,199 --> 00:17:21,199
also heard today

00:17:19,280 --> 00:17:23,439
and within speech hate speech is even

00:17:21,199 --> 00:17:25,520
more complex

00:17:23,439 --> 00:17:28,480
so i'm currently assisting in a research

00:17:25,520 --> 00:17:31,120
project that aims to analyze

00:17:28,480 --> 00:17:32,960
hateful communication on social media

00:17:31,120 --> 00:17:34,720
and commentary sections of german use

00:17:32,960 --> 00:17:37,840
media in order to identify causes and

00:17:34,720 --> 00:17:37,840
dynamics of hate speech

00:17:38,559 --> 00:17:42,960
so we're looking at common sections of

00:17:40,960 --> 00:17:45,280
german news media

00:17:42,960 --> 00:17:48,160
on the content on the topic of migration

00:17:45,280 --> 00:17:50,960
so you can imagine that there's a lot of

00:17:48,160 --> 00:17:52,720
terrible things going on and we're

00:17:50,960 --> 00:17:54,320
trying to actually develop methods and

00:17:52,720 --> 00:17:56,000
software that can recognize

00:17:54,320 --> 00:17:58,799
hate speech early and that can also

00:17:56,000 --> 00:18:00,880
suggest strategies for de-escalation

00:17:58,799 --> 00:18:02,640
so everyone who was going like freedom

00:18:00,880 --> 00:18:05,120
of speech it's about de-escalation it's

00:18:02,640 --> 00:18:06,720
not about deleting

00:18:05,120 --> 00:18:10,160
so let me tell you finding hate speech

00:18:06,720 --> 00:18:12,400
is a lot harder than finding bikes

00:18:10,160 --> 00:18:14,160
and unlike many other projects that

00:18:12,400 --> 00:18:16,960
projects that try to do that

00:18:14,160 --> 00:18:18,400
we involve both communication science

00:18:16,960 --> 00:18:20,240
and computer science and they kind of

00:18:18,400 --> 00:18:22,320
work together in this project

00:18:20,240 --> 00:18:26,000
so i will show you how each of these

00:18:22,320 --> 00:18:27,520
perspectives try to learn hate speech

00:18:26,000 --> 00:18:29,679
let's get started with the computer

00:18:27,520 --> 00:18:32,480
science part because this might be close

00:18:29,679 --> 00:18:34,000
to what you already know um so to

00:18:32,480 --> 00:18:36,720
identify hate speech

00:18:34,000 --> 00:18:37,440
machines do what we call supervised

00:18:36,720 --> 00:18:39,600
learning

00:18:37,440 --> 00:18:40,640
right some of you know this i've heard

00:18:39,600 --> 00:18:42,960
of the word

00:18:40,640 --> 00:18:45,280
okay so what happens in supervised

00:18:42,960 --> 00:18:47,120
learning is that we take a set of data

00:18:45,280 --> 00:18:49,120
that already has the information on

00:18:47,120 --> 00:18:50,640
whether a particular statement is hate

00:18:49,120 --> 00:18:52,880
speech or not

00:18:50,640 --> 00:18:54,320
and then we split it into two sets and

00:18:52,880 --> 00:18:56,960
you can only see the train

00:18:54,320 --> 00:18:58,080
set here right now um one is for

00:18:56,960 --> 00:18:59,360
training and the other one is for

00:18:58,080 --> 00:19:02,640
testing and so

00:18:59,360 --> 00:19:05,679
the training begins and this is

00:19:02,640 --> 00:19:07,919
actually where

00:19:05,679 --> 00:19:09,440
that training part can also be done with

00:19:07,919 --> 00:19:11,120
convolutional neural networks

00:19:09,440 --> 00:19:13,600
so we can use our knowledge from the

00:19:11,120 --> 00:19:15,840
bike part for identifying hate speech as

00:19:13,600 --> 00:19:15,840
well

00:19:15,919 --> 00:19:20,880
so unfortunately text only works in one

00:19:18,559 --> 00:19:23,919
dimension and

00:19:20,880 --> 00:19:26,160
this is two dimensions

00:19:23,919 --> 00:19:28,080
so it either text either goes horizontal

00:19:26,160 --> 00:19:30,000
right or it goes down

00:19:28,080 --> 00:19:31,760
and this english text only goes from

00:19:30,000 --> 00:19:33,520
left to right

00:19:31,760 --> 00:19:35,520
so we're not comparing our text to

00:19:33,520 --> 00:19:36,480
shapes particularly but rather to

00:19:35,520 --> 00:19:38,640
positions

00:19:36,480 --> 00:19:39,679
so i promise to leave out all the math

00:19:38,640 --> 00:19:42,320
but you can imagine

00:19:39,679 --> 00:19:43,039
that each word in our entire data set

00:19:42,320 --> 00:19:45,679
which is

00:19:43,039 --> 00:19:48,799
bigger than this one has some kind of

00:19:45,679 --> 00:19:50,400
relation or position within the text

00:19:48,799 --> 00:19:52,160
and now instead of looking for

00:19:50,400 --> 00:19:52,799
horizontal or vertical lines like we had

00:19:52,160 --> 00:19:54,880
before

00:19:52,799 --> 00:19:55,840
we will look at how words or even

00:19:54,880 --> 00:19:58,720
characters

00:19:55,840 --> 00:19:58,720
relate to each other

00:19:59,200 --> 00:20:04,080
so the size of these relations can vary

00:20:02,480 --> 00:20:06,000
we can look at the character level we

00:20:04,080 --> 00:20:08,159
can look at how one or two

00:20:06,000 --> 00:20:09,200
how two or three characters relate to

00:20:08,159 --> 00:20:11,840
each other or

00:20:09,200 --> 00:20:14,400
as you can see here how two words can be

00:20:11,840 --> 00:20:17,679
related to each other

00:20:14,400 --> 00:20:22,000
and then we move our two word filter

00:20:17,679 --> 00:20:23,520
over our text like this and that

00:20:22,000 --> 00:20:25,360
and next we could choose a differently

00:20:23,520 --> 00:20:27,760
sized filter that for example looks at

00:20:25,360 --> 00:20:30,720
three words

00:20:27,760 --> 00:20:31,840
and so on and eventually we will have

00:20:30,720 --> 00:20:34,400
numbers

00:20:31,840 --> 00:20:35,360
that i did not include that show what

00:20:34,400 --> 00:20:37,360
kind of words

00:20:35,360 --> 00:20:39,280
have what kind of relation within hate

00:20:37,360 --> 00:20:40,720
speech statements as opposed to what

00:20:39,280 --> 00:20:41,679
kind of words have what kinds of

00:20:40,720 --> 00:20:45,120
relations in

00:20:41,679 --> 00:20:45,679
non-hate speech statements according to

00:20:45,120 --> 00:20:48,159
the different

00:20:45,679 --> 00:20:50,320
filters that we apply and so instead of

00:20:48,159 --> 00:20:51,039
having a bike classifier we might now

00:20:50,320 --> 00:20:54,240
have

00:20:51,039 --> 00:20:55,760
a classifier for hate speech

00:20:54,240 --> 00:20:57,120
and in order to check how good the

00:20:55,760 --> 00:20:58,640
classifier actually is we're going to

00:20:57,120 --> 00:21:01,120
use the second

00:20:58,640 --> 00:21:02,320
part of our data set the test data set

00:21:01,120 --> 00:21:03,440
and we will check how well our

00:21:02,320 --> 00:21:06,960
classifier can tell

00:21:03,440 --> 00:21:09,840
hate speech from non-hate speech

00:21:06,960 --> 00:21:11,039
and since we humans already know from

00:21:09,840 --> 00:21:13,440
our data set whether

00:21:11,039 --> 00:21:14,240
a statement is hate speech or not we can

00:21:13,440 --> 00:21:16,159
like

00:21:14,240 --> 00:21:17,360
we know what the machine should decide

00:21:16,159 --> 00:21:19,600
so we can

00:21:17,360 --> 00:21:21,200
compare what it should decide to what it

00:21:19,600 --> 00:21:24,159
actually decided

00:21:21,200 --> 00:21:25,600
um but usually usually the result is not

00:21:24,159 --> 00:21:26,480
very satisfying especially not in the

00:21:25,600 --> 00:21:28,240
beginning

00:21:26,480 --> 00:21:30,080
so we need to go back to the print

00:21:28,240 --> 00:21:34,159
training part and

00:21:30,080 --> 00:21:35,600
we'll actually adjust numbers

00:21:34,159 --> 00:21:37,600
we tweak some numbers here and there and

00:21:35,600 --> 00:21:39,679
if you're looking for a scientific

00:21:37,600 --> 00:21:41,600
explanation i cannot give you one

00:21:39,679 --> 00:21:44,960
because most of the articles just say

00:21:41,600 --> 00:21:46,960
adjust param meter x or y until it looks

00:21:44,960 --> 00:21:50,400
good

00:21:46,960 --> 00:21:52,320
thanks science and so we do this until

00:21:50,400 --> 00:21:54,000
we're satisfied with our results

00:21:52,320 --> 00:21:55,360
and then we can calculate some measures

00:21:54,000 --> 00:21:57,280
for quality and end

00:21:55,360 --> 00:21:59,440
up for example with a classifier that

00:21:57,280 --> 00:22:02,320
will give us a green light for

00:21:59,440 --> 00:22:03,120
probably not hate speech or yellow light

00:22:02,320 --> 00:22:06,159
for

00:22:03,120 --> 00:22:07,840
i don't know could be could be not

00:22:06,159 --> 00:22:10,880
and then a red light for yeah this is

00:22:07,840 --> 00:22:10,880
probably hate speech

00:22:11,360 --> 00:22:14,640
sounds good so far but where do machines

00:22:13,440 --> 00:22:16,400
fail well

00:22:14,640 --> 00:22:18,240
unfortunately machines cannot tell us

00:22:16,400 --> 00:22:21,600
themselves where they fail

00:22:18,240 --> 00:22:23,840
um not in this case at least

00:22:21,600 --> 00:22:25,440
so luckily there are humans who don't

00:22:23,840 --> 00:22:27,840
take machine results

00:22:25,440 --> 00:22:29,200
for the truth and they question them

00:22:27,840 --> 00:22:30,960
thoroughly

00:22:29,200 --> 00:22:33,039
and this is a paper that was published

00:22:30,960 --> 00:22:35,440
within the project that i work for

00:22:33,039 --> 00:22:38,240
and to test the waters they took a huge

00:22:35,440 --> 00:22:40,159
wikipedia data set and one from twitter

00:22:38,240 --> 00:22:42,400
and both of them were tagged not for

00:22:40,159 --> 00:22:44,960
hate speech but for toxicity

00:22:42,400 --> 00:22:46,400
um but i want to give you one example

00:22:44,960 --> 00:22:49,440
this is not the content warning

00:22:46,400 --> 00:22:50,960
this is just an example oh i feel like

00:22:49,440 --> 00:22:54,000
such an now

00:22:50,960 --> 00:22:57,679
sorry bud this was tagged by

00:22:54,000 --> 00:22:59,600
the algorithm as toxic well

00:22:57,679 --> 00:23:01,039
because based on the training data that

00:22:59,600 --> 00:23:02,640
the classifier received

00:23:01,039 --> 00:23:05,120
it very strongly learned that swear

00:23:02,640 --> 00:23:06,960
words are an indicator for hate speech

00:23:05,120 --> 00:23:09,280
well in this case it's actually someone

00:23:06,960 --> 00:23:11,919
apologizing for their behavior

00:23:09,280 --> 00:23:11,919
i would guess

00:23:12,880 --> 00:23:17,200
so statements that include swear words

00:23:15,120 --> 00:23:20,000
but do not actually attack someone

00:23:17,200 --> 00:23:21,679
are inherently hard for machines there

00:23:20,000 --> 00:23:23,840
are other things as well

00:23:21,679 --> 00:23:25,600
hate speech that on the other hand does

00:23:23,840 --> 00:23:27,120
not contain swear words is difficult for

00:23:25,600 --> 00:23:29,600
machines to identify

00:23:27,120 --> 00:23:30,799
rhetorical questions are hard metaphors

00:23:29,600 --> 00:23:33,600
and comparisons

00:23:30,799 --> 00:23:34,480
idiosyncratic and we're words sarcasm

00:23:33,600 --> 00:23:36,640
and irony

00:23:34,480 --> 00:23:38,480
quotations or references so if you

00:23:36,640 --> 00:23:39,200
relate to someone else's statement and

00:23:38,480 --> 00:23:41,279
point out that

00:23:39,200 --> 00:23:42,799
their statement was not not okay but

00:23:41,279 --> 00:23:44,240
you're using similar language

00:23:42,799 --> 00:23:47,039
that might also be flagged as hate

00:23:44,240 --> 00:23:49,120
speech so not really optimal because

00:23:47,039 --> 00:23:51,840
all of these things are used a lot in

00:23:49,120 --> 00:23:51,840
those statements

00:23:52,960 --> 00:23:56,799
and that's because the algorithm is so

00:23:54,559 --> 00:23:58,880
focused on the words themselves

00:23:56,799 --> 00:24:00,640
and it cannot look beyond the text that

00:23:58,880 --> 00:24:03,039
it has been given right

00:24:00,640 --> 00:24:03,760
it has no context for the actual meaning

00:24:03,039 --> 00:24:05,679
of words

00:24:03,760 --> 00:24:08,320
and how this changes within different

00:24:05,679 --> 00:24:08,320
contexts

00:24:08,480 --> 00:24:12,799
but we kind of skip the part about the

00:24:11,120 --> 00:24:15,120
underlying data set

00:24:12,799 --> 00:24:16,159
on which machines learn those data sets

00:24:15,120 --> 00:24:19,120
don't just appear

00:24:16,159 --> 00:24:21,120
obviously they're created by humans so

00:24:19,120 --> 00:24:23,520
how do humans learn to identify hate

00:24:21,120 --> 00:24:23,520
speech

00:24:23,760 --> 00:24:26,880
very often these data sets are created

00:24:25,520 --> 00:24:29,200
by crowd workers

00:24:26,880 --> 00:24:32,320
has anyone ever been a crowd worker for

00:24:29,200 --> 00:24:35,600
any task

00:24:32,320 --> 00:24:38,000
okay one person two people okay um

00:24:35,600 --> 00:24:40,400
thanks for doing that uh so for the

00:24:38,000 --> 00:24:44,000
google perspective api for example

00:24:40,400 --> 00:24:46,240
um rather random people were simply

00:24:44,000 --> 00:24:48,400
asked to write an internet comment

00:24:46,240 --> 00:24:49,600
on a scale from very toxic to very

00:24:48,400 --> 00:24:52,159
healthy

00:24:49,600 --> 00:24:54,240
now unfortunately results from crowd

00:24:52,159 --> 00:24:56,080
workers are kind of unreliable

00:24:54,240 --> 00:24:58,400
and their results also won't really

00:24:56,080 --> 00:24:59,039
explain to us why they made a specific

00:24:58,400 --> 00:25:01,039
decision

00:24:59,039 --> 00:25:04,480
right they just click and then we have

00:25:01,039 --> 00:25:06,320
the result but no context

00:25:04,480 --> 00:25:08,000
so it's okay to do that for bikes

00:25:06,320 --> 00:25:09,520
because most of the time it's not so bad

00:25:08,000 --> 00:25:11,120
if you misidentify a bike

00:25:09,520 --> 00:25:13,440
but for hate speech it's a different

00:25:11,120 --> 00:25:14,720
thing so social sciences and

00:25:13,440 --> 00:25:16,559
communication studies

00:25:14,720 --> 00:25:18,799
in particular they go about this a bit

00:25:16,559 --> 00:25:20,880
differently um our strategy

00:25:18,799 --> 00:25:21,840
is usually to start with well some

00:25:20,880 --> 00:25:23,440
theory

00:25:21,840 --> 00:25:25,279
and let's define what hate speech

00:25:23,440 --> 00:25:27,679
actually is

00:25:25,279 --> 00:25:28,559
i have brought you one example of a

00:25:27,679 --> 00:25:30,640
definition

00:25:28,559 --> 00:25:32,080
and goes like this hate speech is

00:25:30,640 --> 00:25:34,799
understood as public

00:25:32,080 --> 00:25:35,360
communication that contains deliberately

00:25:34,799 --> 00:25:38,400
and or

00:25:35,360 --> 00:25:41,600
intentionally discriminatory messages

00:25:38,400 --> 00:25:45,039
thus it is not a question of hate

00:25:41,600 --> 00:25:45,039
nor a question of speech

00:25:45,919 --> 00:25:52,240
are you confused a little bit

00:25:49,840 --> 00:25:54,559
i think this is a very good definition

00:25:52,240 --> 00:25:56,320
because it points out

00:25:54,559 --> 00:25:58,640
that hate speech is not about an

00:25:56,320 --> 00:26:01,360
individual individual's emotion

00:25:58,640 --> 00:26:03,360
or about emotions at all really and it's

00:26:01,360 --> 00:26:05,360
also not limited to speech

00:26:03,360 --> 00:26:07,679
because hate can be expressed in many

00:26:05,360 --> 00:26:11,760
ways and it can appear in rather

00:26:07,679 --> 00:26:13,760
rational ways but we still need a way to

00:26:11,760 --> 00:26:15,200
identify this

00:26:13,760 --> 00:26:17,520
so we're going to agree on this

00:26:15,200 --> 00:26:20,000
definition for now and once we have it

00:26:17,520 --> 00:26:23,120
we have to actually see how it applies

00:26:20,000 --> 00:26:25,360
on specific statements so we

00:26:23,120 --> 00:26:28,000
need to take the theory into like the

00:26:25,360 --> 00:26:29,840
methodology

00:26:28,000 --> 00:26:32,080
so according to our definition we could

00:26:29,840 --> 00:26:34,000
take public user comments then we have

00:26:32,080 --> 00:26:36,960
the public communication

00:26:34,000 --> 00:26:38,000
and say that verbal aggression so things

00:26:36,960 --> 00:26:40,240
like insults

00:26:38,000 --> 00:26:41,840
negative comparisons or metaphors

00:26:40,240 --> 00:26:45,039
negative circumscriptions

00:26:41,840 --> 00:26:48,000
are hate speech dehumanization

00:26:45,039 --> 00:26:48,960
meaning turning humans into objects or

00:26:48,000 --> 00:26:52,880
creatures

00:26:48,960 --> 00:26:55,440
is hate speech calling for

00:26:52,880 --> 00:26:56,480
threatening or legitimizing violence

00:26:55,440 --> 00:26:58,640
against people

00:26:56,480 --> 00:26:59,760
is hate speech and negative

00:26:58,640 --> 00:27:02,880
generalizations

00:26:59,760 --> 00:27:06,320
such as all acts are something terrible

00:27:02,880 --> 00:27:08,320
is hate speech

00:27:06,320 --> 00:27:10,320
now we will take these practical

00:27:08,320 --> 00:27:12,480
understandings and put them into a code

00:27:10,320 --> 00:27:14,559
book

00:27:12,480 --> 00:27:15,679
it's kind of a tutorial on how to find

00:27:14,559 --> 00:27:17,840
hate speech

00:27:15,679 --> 00:27:18,720
and it contains the definition that

00:27:17,840 --> 00:27:20,559
you've just seen

00:27:18,720 --> 00:27:22,240
and the more practical understandings

00:27:20,559 --> 00:27:25,679
but also lots and lots of

00:27:22,240 --> 00:27:28,880
examples in context and so what we do

00:27:25,679 --> 00:27:31,039
in our project is we give this code book

00:27:28,880 --> 00:27:32,159
to humans but not as many as the crowd

00:27:31,039 --> 00:27:35,360
workers

00:27:32,159 --> 00:27:37,840
rather to so a smaller number like

00:27:35,360 --> 00:27:38,880
three to seven people maybe and they're

00:27:37,840 --> 00:27:40,399
called coders

00:27:38,880 --> 00:27:42,720
and they read this book and then in a

00:27:40,399 --> 00:27:43,200
workshop together with the researchers

00:27:42,720 --> 00:27:45,200
they

00:27:43,200 --> 00:27:47,039
discuss a lot of examples and they work

00:27:45,200 --> 00:27:48,960
together on them

00:27:47,039 --> 00:27:50,399
and then they take even more examples

00:27:48,960 --> 00:27:52,640
and the code book home

00:27:50,399 --> 00:27:54,000
and like wreck their brain on these

00:27:52,640 --> 00:27:55,840
examples

00:27:54,000 --> 00:27:57,440
and meet again to discuss all of their

00:27:55,840 --> 00:27:59,210
questions because this is where a lot of

00:27:57,440 --> 00:28:00,640
questions come up

00:27:59,210 --> 00:28:03,120
[Music]

00:28:00,640 --> 00:28:05,120
and i want to now show you an example

00:28:03,120 --> 00:28:07,039
why it's important to discuss all of the

00:28:05,120 --> 00:28:08,240
questions to first of all have questions

00:28:07,039 --> 00:28:11,760
and then discuss them

00:28:08,240 --> 00:28:14,559
and come to a decision on them and so

00:28:11,760 --> 00:28:16,000
now this is the content warning for

00:28:14,559 --> 00:28:18,320
racism and sexual violence

00:28:16,000 --> 00:28:19,520
in words it's going to be on the next

00:28:18,320 --> 00:28:21,919
two slides i'll let you know when

00:28:19,520 --> 00:28:21,919
they're over

00:28:22,960 --> 00:28:26,880
this is a translation of a comment that

00:28:25,279 --> 00:28:30,080
was originally in german

00:28:26,880 --> 00:28:31,120
and i translated it by words basically

00:28:30,080 --> 00:28:33,200
for now

00:28:31,120 --> 00:28:34,559
so you may not understand what this is

00:28:33,200 --> 00:28:38,080
supposed to mean does anyone

00:28:34,559 --> 00:28:40,880
like kind of understand what it means

00:28:38,080 --> 00:28:41,679
okay um because the language is really

00:28:40,880 --> 00:28:43,679
coded

00:28:41,679 --> 00:28:44,960
it's using references to words or

00:28:43,679 --> 00:28:47,120
phrases that well

00:28:44,960 --> 00:28:49,120
german politicians have made in the

00:28:47,120 --> 00:28:51,600
context of migration

00:28:49,120 --> 00:28:52,880
but it's also turning them around in its

00:28:51,600 --> 00:28:54,480
really weird sentence

00:28:52,880 --> 00:28:57,120
so unless you're quite familiar with

00:28:54,480 --> 00:28:59,600
this topic within the german context

00:28:57,120 --> 00:29:01,200
you'd probably be rather unsure whether

00:28:59,600 --> 00:29:02,559
this is hate speech or not or you might

00:29:01,200 --> 00:29:04,799
just dismiss the comment

00:29:02,559 --> 00:29:04,799
right

00:29:07,120 --> 00:29:11,279
i can tell you that we actually find

00:29:09,360 --> 00:29:12,000
three indicators of hate speech within

00:29:11,279 --> 00:29:15,279
this very short

00:29:12,000 --> 00:29:19,679
statement it's legitimizing violence

00:29:15,279 --> 00:29:21,919
that's this part um it's also making

00:29:19,679 --> 00:29:22,960
making an ironic and negative

00:29:21,919 --> 00:29:25,120
generalization

00:29:22,960 --> 00:29:26,799
that's the second part and then there's

00:29:25,120 --> 00:29:28,799
a third part

00:29:26,799 --> 00:29:32,080
that is turning humans into objects

00:29:28,799 --> 00:29:33,600
again with a node of sarcasm

00:29:32,080 --> 00:29:37,520
now keep this in mind and i will show

00:29:33,600 --> 00:29:40,399
you the translation by idea

00:29:37,520 --> 00:29:43,039
so this is translated again but what it

00:29:40,399 --> 00:29:43,039
actually means

00:29:43,120 --> 00:29:49,039
so i think you now understand

00:29:46,559 --> 00:29:50,399
why this is hate speech after all that

00:29:49,039 --> 00:29:54,320
we've been through

00:29:50,399 --> 00:29:56,880
and that's it for the content warning

00:29:54,320 --> 00:29:58,640
so once the coders have asked and

00:29:56,880 --> 00:30:00,320
discussed all their questions we usually

00:29:58,640 --> 00:30:02,559
need to add better descriptions and

00:30:00,320 --> 00:30:03,840
examples into the code book and this is

00:30:02,559 --> 00:30:05,600
such an important step

00:30:03,840 --> 00:30:06,960
that this could actually go on for one

00:30:05,600 --> 00:30:09,120
or two more rounds

00:30:06,960 --> 00:30:11,520
right and then eventually the

00:30:09,120 --> 00:30:13,279
researchers are confident in the coders

00:30:11,520 --> 00:30:14,960
and their calculated agreement is high

00:30:13,279 --> 00:30:16,880
enough and they will actually code a

00:30:14,960 --> 00:30:19,600
larger data set and that is then where

00:30:16,880 --> 00:30:22,559
the algorithm can start to learn

00:30:19,600 --> 00:30:24,080
but where do humans fail well no matter

00:30:22,559 --> 00:30:24,640
how good the discussions and the

00:30:24,080 --> 00:30:28,000
trainings

00:30:24,640 --> 00:30:29,520
are data set will never be perfect

00:30:28,000 --> 00:30:31,520
because humans too have trouble

00:30:29,520 --> 00:30:35,200
understanding irony or sarcasm

00:30:31,520 --> 00:30:37,360
duh but more importantly

00:30:35,200 --> 00:30:38,720
language and specific words can change

00:30:37,360 --> 00:30:41,919
over time and sometimes

00:30:38,720 --> 00:30:45,760
their meaning can change quite quickly

00:30:41,919 --> 00:30:48,640
however we as humans have the capability

00:30:45,760 --> 00:30:51,520
to pay attention to this and that is now

00:30:48,640 --> 00:30:54,320
of the uttermost importance

00:30:51,520 --> 00:30:55,919
because humans are the ones who can stop

00:30:54,320 --> 00:30:56,960
take a step back and look at the bigger

00:30:55,919 --> 00:30:58,640
picture

00:30:56,960 --> 00:31:00,640
humans are the ones who actually

00:30:58,640 --> 00:31:02,640
complete the entire process

00:31:00,640 --> 00:31:04,559
in machine learning we are the ones who

00:31:02,640 --> 00:31:07,120
educate or miseducate

00:31:04,559 --> 00:31:08,880
the machine so i would like to finish up

00:31:07,120 --> 00:31:10,960
with a quote from lauren hill's album

00:31:08,880 --> 00:31:14,559
now

00:31:10,960 --> 00:31:15,840
consequence is no coincidence

00:31:14,559 --> 00:31:18,799
because the knowledge that machine

00:31:15,840 --> 00:31:21,360
learning produces is no coincidence

00:31:18,799 --> 00:31:22,240
it is a consequence of what data we as

00:31:21,360 --> 00:31:24,399
humans

00:31:22,240 --> 00:31:25,600
choose to feed to a machine it is a

00:31:24,399 --> 00:31:28,640
consequence of how

00:31:25,600 --> 00:31:30,320
we as humans build an algorithm

00:31:28,640 --> 00:31:32,799
and finally machine learning is a

00:31:30,320 --> 00:31:35,840
consequence of what we as humans

00:31:32,799 --> 00:31:37,760
decide to question reflect upon or take

00:31:35,840 --> 00:31:40,240
with a grain of salt

00:31:37,760 --> 00:31:42,399
so all that i'm asking you now whether

00:31:40,240 --> 00:31:46,159
you program machines that learn

00:31:42,399 --> 00:31:48,159
learn or you use them is to please be

00:31:46,159 --> 00:32:01,840
the human that reflects

00:31:48,159 --> 00:32:01,840
thank you

00:32:05,440 --> 00:32:08,480
thank you so much laura a little

00:32:07,600 --> 00:32:10,880
something from us

00:32:08,480 --> 00:32:15,519
thank you thank you that was an

00:32:10,880 --> 00:32:15,519

YouTube URL: https://www.youtube.com/watch?v=1-8J0wfvhrU


