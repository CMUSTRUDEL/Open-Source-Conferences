Title: Kerstin Puschke - Scaling a monolith isn't scaling microservices
Publication date: 2018-08-22
Playlist: EuRuKo 2018
Description: 
	Up Next: Amr Abdelwahab - An empathy exercise: contextualising the question of privilege https://www.youtube.com/watch?v=6CqmGYvFwAQ

See below for all videos!

######################
### \o/ EuRuKo 2018 \o/ ###
######################

###   Day 1  ###

Yukihiro Matsumoto - Keynote https://www.youtube.com/watch?v=cs0s5lZAUwc
Chris Salzberg - Metaprogramming for generalists https://www.youtube.com/watch?v=1fIlcnrJHxs
Joannah Nanjekye - Ruby in containers https://www.youtube.com/watch?v=qPNkOPvjecs
Damir Zekić - Tool belt of a seasoned bug hunter 
https://www.youtube.com/watch?v=ObB0dzX_rBs
Igor Morozov - Ducks and monads: wonders of Ruby types https://www.youtube.com/watch?v=v-H9nK8hqfE
Brad Urani - Rails anti-patterns: how not to design your database https://www.youtube.com/watch?v=zo3iRBPzscU
Coraline Ada Ehmke - The broken promise of Open Source https://www.youtube.com/watch?v=5ByUPh_uPqQ
Louisa Barrett - Ruby not red: color theory for the rest of us https://www.youtube.com/watch?v=OgO1EIFDgPU

###  Day 2  ###

Welcome https://www.youtube.com/watch?v=OlOA0aGxud0
Nadia Odunayo - The case of the missing method — a Ruby mystery story https://www.youtube.com/watch?v=OlOA0aGxud0
Pitch the next EuRuKo's location https://www.youtube.com/watch?v=YXe9OoQW8lc
Ana María Martínez Gómez - Let’s refactor some Ruby code https://www.youtube.com/watch?v=jUc8InwoA-E
Pan Thomakos - Debugging adventures in Rack-land https://www.youtube.com/watch?v=5o4krwjJbOI
Lightning talks https://www.youtube.com/watch?v=zSeaNPjwnnA
Kerstin Puschke - Scaling a monolith isn't scaling microservices https://www.youtube.com/watch?v=tA8gGd_Rl7E
Amr Abdelwahab - An empathy exercise: contextualising the question of privilege https://www.youtube.com/watch?v=6CqmGYvFwAQ
Wrap up & announcing the next EuRuKo's location https://www.youtube.com/watch?v=wMggsShGTzk
Captions: 
	00:00:00,030 --> 00:00:05,819
okay and I totally and I totally killed

00:00:02,550 --> 00:00:08,040
it okay and never mind so um a reminder

00:00:05,819 --> 00:00:09,809
now the person that I'm talking about

00:00:08,040 --> 00:00:12,509
all the time is actually in the room so

00:00:09,809 --> 00:00:15,990
we can we can do this thing tomorrow at

00:00:12,509 --> 00:00:19,470
11 a.m. in front of the rouse house this

00:00:15,990 --> 00:00:22,949
person come on a stage maybe we'll be

00:00:19,470 --> 00:00:25,410
waiting for you for a tour so if you're

00:00:22,949 --> 00:00:26,160
interested in a non-sexual tour of

00:00:25,410 --> 00:00:29,580
Vienna

00:00:26,160 --> 00:00:31,859
he's very faction yeah very fictional

00:00:29,580 --> 00:00:34,700
tour apparently I'm very good at tours

00:00:31,859 --> 00:00:34,700
that's

00:00:35,740 --> 00:00:38,889
[Applause]

00:00:40,010 --> 00:00:47,120
I can actually pouch for that yeah okay

00:00:42,920 --> 00:00:50,059
so now skadoodle tomorrow at 11:00

00:00:47,120 --> 00:00:53,030
that's house if you don't know where the

00:00:50,059 --> 00:00:56,780
cathouse is ask us later so next up is

00:00:53,030 --> 00:00:58,129
Kirsten she is a developer at Shopify in

00:00:56,780 --> 00:01:01,250
Canada

00:00:58,129 --> 00:01:04,879
she is transforming a massive rails code

00:01:01,250 --> 00:01:06,140
base into a more modular monolith and

00:01:04,879 --> 00:01:09,740
before moving to Canada

00:01:06,140 --> 00:01:12,170
she ran the Hamburg uber Ruby user group

00:01:09,740 --> 00:01:13,940
and a local real skills chapter so

00:01:12,170 --> 00:01:16,980
Kirsten

00:01:13,940 --> 00:01:20,379
[Applause]

00:01:16,980 --> 00:01:20,379
[Music]

00:01:21,810 --> 00:01:27,549
thank you so I hope you're not going to

00:01:25,390 --> 00:01:29,380
be too disappointed I'm not going to

00:01:27,549 --> 00:01:35,350
talk about transforming our massive

00:01:29,380 --> 00:01:38,680
codebase sorry for that so when a new

00:01:35,350 --> 00:01:41,259
user signs up for any kind of web

00:01:38,680 --> 00:01:43,299
application the application creates an

00:01:41,259 --> 00:01:45,610
account for them but usually it also

00:01:43,299 --> 00:01:48,100
sends some kind of welcome email and

00:01:45,610 --> 00:01:50,500
sending emails is slow the user should

00:01:48,100 --> 00:01:52,869
not have to wait for that email to be

00:01:50,500 --> 00:01:56,289
sent they should be able to use the

00:01:52,869 --> 00:01:58,030
service right away and luckily sending

00:01:56,289 --> 00:02:00,729
welcome emails is not essential for

00:01:58,030 --> 00:02:03,280
account creation so the application can

00:02:00,729 --> 00:02:05,710
return to the user right after creating

00:02:03,280 --> 00:02:08,440
the account and send the welcome email

00:02:05,710 --> 00:02:11,200
later outside of the main request

00:02:08,440 --> 00:02:14,019
response cycle by offloading it into a

00:02:11,200 --> 00:02:16,750
background job and background jobs are

00:02:14,019 --> 00:02:19,060
great for monolithic applications but

00:02:16,750 --> 00:02:22,300
they're not that useful in a distributed

00:02:19,060 --> 00:02:24,069
architecture so today I'm going to talk

00:02:22,300 --> 00:02:26,430
about how to choose the right tool for

00:02:24,069 --> 00:02:29,860
workload distribution in different

00:02:26,430 --> 00:02:32,049
architectures and I'm going to start

00:02:29,860 --> 00:02:34,180
with background jobs how they work the

00:02:32,049 --> 00:02:36,910
features they provide and also some of

00:02:34,180 --> 00:02:39,340
the challenges they might pose and when

00:02:36,910 --> 00:02:41,500
to choose them and I'm also going to

00:02:39,340 --> 00:02:44,019
tell you how Shopify uses background

00:02:41,500 --> 00:02:47,530
jobs to scale one of the largest rails

00:02:44,019 --> 00:02:49,120
applications in existence I'm gonna

00:02:47,530 --> 00:02:52,440
compare background jobs to message

00:02:49,120 --> 00:02:55,840
oriented middleware like rabbitmq and

00:02:52,440 --> 00:02:58,269
I'm briefly touching on event looks like

00:02:55,840 --> 00:03:01,570
Capcom and how they are different from

00:02:58,269 --> 00:03:03,850
more traditional messaging before

00:03:01,570 --> 00:03:07,450
summarizing the comparison of the three

00:03:03,850 --> 00:03:10,750
technologies so let's start with

00:03:07,450 --> 00:03:12,519
background jobs in the Ruby world rescue

00:03:10,750 --> 00:03:14,230
and sidekick are pretty popular

00:03:12,519 --> 00:03:17,769
background job backends but there is

00:03:14,230 --> 00:03:20,109
also delayed job and some others and the

00:03:17,769 --> 00:03:22,780
background job is unit of work to be

00:03:20,109 --> 00:03:25,180
done in the future the application

00:03:22,780 --> 00:03:27,190
server can fork Rocka process for

00:03:25,180 --> 00:03:29,709
running the job and it shouldn't have to

00:03:27,190 --> 00:03:31,480
wait for the job to finish the worker on

00:03:29,709 --> 00:03:33,040
the other hand should be able to

00:03:31,480 --> 00:03:35,469
communicate any error

00:03:33,040 --> 00:03:38,760
so we need some kind of asynchronous

00:03:35,469 --> 00:03:41,650
communication between our two processes

00:03:38,760 --> 00:03:43,840
we can facilitate that by placing a

00:03:41,650 --> 00:03:45,879
message queue in between them sender and

00:03:43,840 --> 00:03:48,579
receiver of messages can interact with a

00:03:45,879 --> 00:03:50,829
queue at different times and the message

00:03:48,579 --> 00:03:53,799
is the job the tasks to be done by the

00:03:50,829 --> 00:03:57,370
worker which is why the queue is often

00:03:53,799 --> 00:03:59,230
also called a task queue the application

00:03:57,370 --> 00:04:01,359
server doesn't have to wait for the

00:03:59,230 --> 00:04:03,010
worker to process a message it drops a

00:04:01,359 --> 00:04:05,530
message into the queue and it can

00:04:03,010 --> 00:04:09,280
immediately move on for example to serve

00:04:05,530 --> 00:04:11,620
the user facing requests we can set up

00:04:09,280 --> 00:04:16,329
more than one worker in order to work on

00:04:11,620 --> 00:04:18,190
more than one task at the same time so a

00:04:16,329 --> 00:04:20,440
background job back-end basically is a

00:04:18,190 --> 00:04:23,050
task queue along with some so-called

00:04:20,440 --> 00:04:26,380
broker code for routing messages and

00:04:23,050 --> 00:04:28,449
managing the workers and it encapsulates

00:04:26,380 --> 00:04:31,750
the asynchronous communication between

00:04:28,449 --> 00:04:34,539
our processes which allows us to keep

00:04:31,750 --> 00:04:37,270
our own background job code very simple

00:04:34,539 --> 00:04:39,880
because our background jobs are my

00:04:37,270 --> 00:04:42,880
current job classes are not aware of

00:04:39,880 --> 00:04:48,909
this code being run asynchronously in

00:04:42,880 --> 00:04:53,169
the future let's have a look at

00:04:48,909 --> 00:04:55,360
background job features background jobs

00:04:53,169 --> 00:04:58,030
allow us to decouple the user facing

00:04:55,360 --> 00:05:01,330
requests from any kind of back-end tasks

00:04:58,030 --> 00:05:04,360
and in particular if those tasks are

00:05:01,330 --> 00:05:06,610
time consuming this helps ask to improve

00:05:04,360 --> 00:05:11,400
the response times and provide a better

00:05:06,610 --> 00:05:14,199
user experience if the application

00:05:11,400 --> 00:05:16,270
experiences a sudden spike for example

00:05:14,199 --> 00:05:18,039
in image uploads that's not going to

00:05:16,270 --> 00:05:20,070
hurt performance if the actual image

00:05:18,039 --> 00:05:22,690
processing is done as a background job

00:05:20,070 --> 00:05:24,760
the queue might grow but the queue is a

00:05:22,690 --> 00:05:27,099
buffer between application server and

00:05:24,760 --> 00:05:29,349
worker and the speed of queueing more

00:05:27,099 --> 00:05:33,580
jobs is not constrained by the speed of

00:05:29,349 --> 00:05:38,830
processing them so the users can still

00:05:33,580 --> 00:05:41,200
upload more images as fast as usual if

00:05:38,830 --> 00:05:43,360
we're facing a very large task we can

00:05:41,200 --> 00:05:47,020
split it up into multiple subtasks and

00:05:43,360 --> 00:05:49,120
kill them as individual background jobs

00:05:47,020 --> 00:05:51,580
when we set up more than one worker they

00:05:49,120 --> 00:05:54,940
can work on those subtasks in parallel

00:05:51,580 --> 00:05:56,950
but our code is again very very simple

00:05:54,940 --> 00:06:00,160
because our background jobs are not

00:05:56,950 --> 00:06:02,650
aware of being run in parallel all this

00:06:00,160 --> 00:06:08,230
code needs to know is how to handle one

00:06:02,650 --> 00:06:10,510
of these subtasks at a time if a worker

00:06:08,230 --> 00:06:13,840
encounters any kind of error running a

00:06:10,510 --> 00:06:19,960
job the job can be recued and retried

00:06:13,840 --> 00:06:21,400
again later a Java file we have some

00:06:19,960 --> 00:06:25,870
background jobs that are very closely

00:06:21,400 --> 00:06:28,150
related to the buyers checkout and those

00:06:25,870 --> 00:06:30,090
jobs are certainly more urgent than for

00:06:28,150 --> 00:06:33,040
example firing webhooks

00:06:30,090 --> 00:06:36,400
back ends like rescue and sidekick allow

00:06:33,040 --> 00:06:39,520
us to prioritize jobs by having multiple

00:06:36,400 --> 00:06:43,000
named queues and jobs queued into a high

00:06:39,520 --> 00:06:45,400
priority queue a processed first but if

00:06:43,000 --> 00:06:48,310
this queue is empty our workers will not

00:06:45,400 --> 00:06:52,350
idle and waste resources they are going

00:06:48,310 --> 00:06:52,350
to work on lower priority jobs

00:06:54,180 --> 00:06:59,470
the main application powering Shopify

00:06:57,310 --> 00:07:02,680
queues tens of thousands of jobs per

00:06:59,470 --> 00:07:06,640
second so we rely heavily on background

00:07:02,680 --> 00:07:09,160
jobs and the features they provide the

00:07:06,640 --> 00:07:11,950
background jobs encapsulate the

00:07:09,160 --> 00:07:15,640
asynchronous communication between the

00:07:11,950 --> 00:07:16,800
two processes but they cannot change the

00:07:15,640 --> 00:07:19,930
fact that we're dealing with

00:07:16,800 --> 00:07:26,080
asynchronous communication and that is

00:07:19,930 --> 00:07:28,690
posing some challenges if a job succeeds

00:07:26,080 --> 00:07:30,610
the worker confirms that it is safe to

00:07:28,690 --> 00:07:33,220
remove this job from the message queue

00:07:30,610 --> 00:07:36,640
but what do we do is if the job is not

00:07:33,220 --> 00:07:39,400
confirmed we can allow another worker to

00:07:36,640 --> 00:07:41,830
pick up that job but we might end up

00:07:39,400 --> 00:07:44,380
running our job twice in case the first

00:07:41,830 --> 00:07:47,770
worker did not crash and it's just a

00:07:44,380 --> 00:07:50,500
little bit slow on the other hand if we

00:07:47,770 --> 00:07:53,290
do not allow a second worker to pick up

00:07:50,500 --> 00:07:56,200
the unconfirmed job there is a risk that

00:07:53,290 --> 00:08:00,430
our job is not running at all in case

00:07:56,200 --> 00:08:01,220
the first worker actually did crash so

00:08:00,430 --> 00:08:04,070
we need to

00:08:01,220 --> 00:08:07,820
choose between at least and at most once

00:08:04,070 --> 00:08:09,800
delivery and most once delivery sounds

00:08:07,820 --> 00:08:12,650
like a great choice of running the job

00:08:09,800 --> 00:08:16,790
twice it's worse than not running it at

00:08:12,650 --> 00:08:20,570
all but usually that points to a flaw in

00:08:16,790 --> 00:08:22,880
the code for example the background job

00:08:20,570 --> 00:08:24,650
might charge some customer and we

00:08:22,880 --> 00:08:27,260
totally want to avoid charging the

00:08:24,650 --> 00:08:30,830
customer twice so we attempted to use

00:08:27,260 --> 00:08:33,620
and most wants delivery but a better

00:08:30,830 --> 00:08:36,860
approach might be to carefully track all

00:08:33,620 --> 00:08:38,719
the charges we are making and make sure

00:08:36,860 --> 00:08:41,750
that the background job code is checking

00:08:38,719 --> 00:08:45,110
this state before making any kind of

00:08:41,750 --> 00:08:48,260
church this is making our job item

00:08:45,110 --> 00:08:51,230
pretend running it a second time is not

00:08:48,260 --> 00:08:53,570
going to create a second charge and it

00:08:51,230 --> 00:09:01,160
allows us to opt for at least once

00:08:53,570 --> 00:09:03,170
delivery safely our jobs are leaving the

00:09:01,160 --> 00:09:05,150
queue in a predefined order but that

00:09:03,170 --> 00:09:08,120
does not necessarily mean they are

00:09:05,150 --> 00:09:10,130
processed in this order we never know

00:09:08,120 --> 00:09:13,460
which worker is faster if we have more

00:09:10,130 --> 00:09:16,550
than one and also if the job encounters

00:09:13,460 --> 00:09:19,940
an error it is recued and retried again

00:09:16,550 --> 00:09:24,740
later so our jobs might run out of order

00:09:19,940 --> 00:09:26,870
and if the order of job matters then we

00:09:24,740 --> 00:09:29,060
should not queue all of them right away

00:09:26,870 --> 00:09:32,180
because there is a risk of them running

00:09:29,060 --> 00:09:35,360
in the wrong order but the first job

00:09:32,180 --> 00:09:38,810
after doing its tasks can queue the

00:09:35,360 --> 00:09:41,060
second job as a follow-up making sure

00:09:38,810 --> 00:09:48,650
that they are going to run sequentially

00:09:41,060 --> 00:09:52,130
a background job does not have to be as

00:09:48,650 --> 00:09:54,880
fast as user-facing requests but long

00:09:52,130 --> 00:09:58,610
running jobs can create some problems

00:09:54,880 --> 00:10:00,770
for example if you're using rescue it is

00:09:58,610 --> 00:10:02,960
preventing the worker from being shut

00:10:00,770 --> 00:10:06,200
down while a job is running so this

00:10:02,960 --> 00:10:08,270
worker cannot be deployed and it is also

00:10:06,200 --> 00:10:10,730
not very cloud friendly in a cloud

00:10:08,270 --> 00:10:12,800
infrastructure we cannot rely on the

00:10:10,730 --> 00:10:15,730
same resources being around for many

00:10:12,800 --> 00:10:15,730
hours in our own

00:10:16,269 --> 00:10:21,769
sidekick is doing a little bit better if

00:10:19,279 --> 00:10:25,190
a worker shutdown is requested sidekick

00:10:21,769 --> 00:10:28,060
aborts a running job and recuse it but

00:10:25,190 --> 00:10:31,399
even that is not necessarily good enough

00:10:28,060 --> 00:10:34,160
the job may actually be aborted again

00:10:31,399 --> 00:10:37,579
before finishing so if you're deploying

00:10:34,160 --> 00:10:41,209
faster than the job can run this job is

00:10:37,579 --> 00:10:44,060
never going to succeed and with a core

00:10:41,209 --> 00:10:45,800
of Shopify deploying about 60 times a

00:10:44,060 --> 00:10:48,320
day this is not a theoretical

00:10:45,800 --> 00:10:51,410
possibility but a real issue that we

00:10:48,320 --> 00:10:56,510
need to address so a sidekick would not

00:10:51,410 --> 00:10:59,779
solve this problem for us when many

00:10:56,510 --> 00:11:03,829
long-running jobs are actually iterating

00:10:59,779 --> 00:11:07,040
over a large collection of data and we

00:11:03,829 --> 00:11:09,970
split those jobs up into the collection

00:11:07,040 --> 00:11:12,440
to iterate over and the tasks to be done

00:11:09,970 --> 00:11:18,290
we're setting a check point after each

00:11:12,440 --> 00:11:20,540
iteration and riku the job this makes

00:11:18,290 --> 00:11:23,570
our jobs interruptible without a medic

00:11:20,540 --> 00:11:26,089
resuming and it allows us to shut down

00:11:23,570 --> 00:11:28,730
our workers anytime we like so that we

00:11:26,089 --> 00:11:32,810
can deploy them and we also have cloud

00:11:28,730 --> 00:11:36,440
friendly workers it also helps with

00:11:32,810 --> 00:11:40,399
disaster prevention for example if our

00:11:36,440 --> 00:11:42,019
database is experiencing any problems we

00:11:40,399 --> 00:11:44,600
can throttle the job or back it off

00:11:42,019 --> 00:11:47,149
completely preventing it from making any

00:11:44,600 --> 00:11:52,550
ongoing incident worse or causing one in

00:11:47,149 --> 00:11:55,550
the first place it's also great for data

00:11:52,550 --> 00:11:58,640
integrity if we need to move data around

00:11:55,550 --> 00:12:00,920
between different database charts we can

00:11:58,640 --> 00:12:06,140
interrupt the job in order to do so

00:12:00,920 --> 00:12:08,720
safely you can reach some of these

00:12:06,140 --> 00:12:10,880
results by splitting the job into

00:12:08,720 --> 00:12:13,760
smaller sub tasks and queuing them

00:12:10,880 --> 00:12:16,279
independently but that makes it pretty

00:12:13,760 --> 00:12:21,310
hard to track retries and failures for

00:12:16,279 --> 00:12:24,319
the overall task a resume is not a retry

00:12:21,310 --> 00:12:26,990
by encapsulating these scaling issues

00:12:24,319 --> 00:12:28,720
into a separate abstraction layer we are

00:12:26,990 --> 00:12:31,660
able to keep our

00:12:28,720 --> 00:12:35,020
creepy background job classes very very

00:12:31,660 --> 00:12:39,670
simple because they are not aware of any

00:12:35,020 --> 00:12:42,220
of these scaling issues this abstraction

00:12:39,670 --> 00:12:44,080
layer was recently open-source so if you

00:12:42,220 --> 00:12:51,160
would like to take a closer look you can

00:12:44,080 --> 00:12:55,240
find it on github when should you choose

00:12:51,160 --> 00:12:58,000
background jobs the background job

00:12:55,240 --> 00:12:59,950
backend code is a ruby gem it's running

00:12:58,000 --> 00:13:04,240
on both application server and worker

00:12:59,950 --> 00:13:06,790
and the tasks pull into our task queue

00:13:04,240 --> 00:13:09,640
is an instance of a job class it's a

00:13:06,790 --> 00:13:11,950
ruby object an application server and

00:13:09,640 --> 00:13:13,960
worker both need to understand this

00:13:11,950 --> 00:13:17,170
object they need to implement that class

00:13:13,960 --> 00:13:20,140
and that makes background jobs a great

00:13:17,170 --> 00:13:22,930
fit for a monolithic application where

00:13:20,140 --> 00:13:26,250
our application server and worker are

00:13:22,930 --> 00:13:30,040
running instances of the same codebase

00:13:26,250 --> 00:13:31,510
the background job backend facilitates

00:13:30,040 --> 00:13:35,800
communication between different

00:13:31,510 --> 00:13:39,280
processes if our background job for

00:13:35,800 --> 00:13:41,200
example is doing image processing and we

00:13:39,280 --> 00:13:43,780
would like to extract that into a

00:13:41,200 --> 00:13:47,350
separate service we would need

00:13:43,780 --> 00:13:50,770
communication not between processes but

00:13:47,350 --> 00:13:55,450
between services and this is something

00:13:50,770 --> 00:13:57,160
background jobs are not that great at so

00:13:55,450 --> 00:13:59,170
to summarize background jobs they're

00:13:57,160 --> 00:14:02,050
great features are mostly based on task

00:13:59,170 --> 00:14:03,700
queues and they abstract away the fact

00:14:02,050 --> 00:14:07,330
that the jobs are running asynchronously

00:14:03,700 --> 00:14:11,160
in the future or in parallel which

00:14:07,330 --> 00:14:14,080
allows us to keep our jobs very simple

00:14:11,160 --> 00:14:16,510
but the overall system might be a little

00:14:14,080 --> 00:14:18,640
bit more complex we have to watch out

00:14:16,510 --> 00:14:22,270
for delivery and order guarantees for

00:14:18,640 --> 00:14:26,430
example and background jobs work great

00:14:22,270 --> 00:14:26,430
in monolithic architectures

00:14:28,350 --> 00:14:32,920
message-oriented middleware

00:14:30,450 --> 00:14:35,320
it's different it facilitates

00:14:32,920 --> 00:14:40,089
communication between different services

00:14:35,320 --> 00:14:41,750
a very widely used implementation is

00:14:40,089 --> 00:14:44,060
RabbitMQ

00:14:41,750 --> 00:14:46,400
and protocols spoken by a message

00:14:44,060 --> 00:14:50,890
oriented middleware include AMQP and

00:14:46,400 --> 00:14:50,890
MQTT but there is some others as well

00:14:52,330 --> 00:14:58,760
this kind of middleware is also working

00:14:55,850 --> 00:15:00,680
based on task queues but the logic for

00:14:58,760 --> 00:15:04,070
message routing and managing workers is

00:15:00,680 --> 00:15:06,500
now inside the middleware completely

00:15:04,070 --> 00:15:09,470
separate from our own code base from our

00:15:06,500 --> 00:15:12,140
services for this Orion consumer of

00:15:09,470 --> 00:15:15,620
messages only need to speak the Midwest

00:15:12,140 --> 00:15:18,230
protocol and instead of passing Ruby

00:15:15,620 --> 00:15:20,840
objects around they agree on a purely

00:15:18,230 --> 00:15:25,090
data based interface for example

00:15:20,840 --> 00:15:25,090
adjacent payload with certain attributes

00:15:27,160 --> 00:15:31,670
since the middleware is based on task

00:15:29,600 --> 00:15:34,970
queues it's bringing the features

00:15:31,670 --> 00:15:40,310
background jobs provide to a distributed

00:15:34,970 --> 00:15:42,410
architecture what they're doing more by

00:15:40,310 --> 00:15:44,870
placing a database interface between our

00:15:42,410 --> 00:15:47,720
services we are effectively decoupling

00:15:44,870 --> 00:15:49,610
them making it pretty easy to replace

00:15:47,720 --> 00:15:53,990
the service with a different

00:15:49,610 --> 00:15:56,780
implementation later on if the

00:15:53,990 --> 00:15:58,930
middleware uses a wire level protocol

00:15:56,780 --> 00:16:01,010
like AMQP we are also gaining

00:15:58,930 --> 00:16:03,350
interoperability and we can glue

00:16:01,010 --> 00:16:06,880
together services that are based on

00:16:03,350 --> 00:16:06,880
totally different technologies

00:16:08,830 --> 00:16:13,310
background jobs mostly our commands send

00:16:12,650 --> 00:16:15,470
this email

00:16:13,310 --> 00:16:19,130
process this image fire this way back

00:16:15,470 --> 00:16:21,050
and we can use command messages for a

00:16:19,130 --> 00:16:24,140
similar purpose in a distributed

00:16:21,050 --> 00:16:26,180
architecture for example send a message

00:16:24,140 --> 00:16:29,150
to some image processing service

00:16:26,180 --> 00:16:33,320
ordering it to process a particular

00:16:29,150 --> 00:16:35,450
image but we can also send event

00:16:33,320 --> 00:16:38,630
messages where the producer of the

00:16:35,450 --> 00:16:40,339
message doesn't know or care what the

00:16:38,630 --> 00:16:44,060
consumer is doing with the information

00:16:40,339 --> 00:16:48,380
it receives and a common use case for

00:16:44,060 --> 00:16:50,480
that is propagating updates let's take a

00:16:48,380 --> 00:16:53,570
look at this made-up example of a

00:16:50,480 --> 00:16:55,400
distributed architecture if my business

00:16:53,570 --> 00:16:57,860
partner goes out of business

00:16:55,400 --> 00:17:01,010
I'm going to update their records in the

00:16:57,860 --> 00:17:02,930
business partner service but I also want

00:17:01,010 --> 00:17:06,200
to cancel their pending orders and put

00:17:02,930 --> 00:17:09,380
their support contracts on hold so some

00:17:06,200 --> 00:17:14,000
client services need to know about this

00:17:09,380 --> 00:17:15,560
update it is also common in a

00:17:14,000 --> 00:17:17,390
distributed architecture that the

00:17:15,560 --> 00:17:20,810
service needs to work with the data that

00:17:17,390 --> 00:17:23,300
it doesn't own and to avoid the overhead

00:17:20,810 --> 00:17:25,640
of requesting this data over and over

00:17:23,300 --> 00:17:29,270
again the service might keep a local

00:17:25,640 --> 00:17:31,430
copy then also means the service needs

00:17:29,270 --> 00:17:38,270
to learn about any updates to this data

00:17:31,430 --> 00:17:41,210
to do a proper cache invalidation a very

00:17:38,270 --> 00:17:43,340
common approach for propagating updates

00:17:41,210 --> 00:17:46,430
is to fire webhooks or make rest

00:17:43,340 --> 00:17:48,920
requests but if a client service is

00:17:46,430 --> 00:17:51,110
temporarily down it's going to miss a

00:17:48,920 --> 00:17:54,260
request and it's never gonna learn about

00:17:51,110 --> 00:17:56,420
the update it also means we need

00:17:54,260 --> 00:17:59,990
separate requests for every client

00:17:56,420 --> 00:18:03,470
service since HTTP is routing one-to-one

00:17:59,990 --> 00:18:06,530
and if we're adding a new service we

00:18:03,470 --> 00:18:10,160
need to make an additional request so

00:18:06,530 --> 00:18:14,800
the service firing the web hooks needs

00:18:10,160 --> 00:18:14,800
to be aware of all the client services

00:18:16,030 --> 00:18:22,250
instead of using HTTP for propagating

00:18:19,130 --> 00:18:27,170
updates we can use our middleware and

00:18:22,250 --> 00:18:29,390
send a message about the update and if

00:18:27,170 --> 00:18:31,070
the message consumer is temporarily down

00:18:29,390 --> 00:18:33,740
the message is going to stay in the

00:18:31,070 --> 00:18:35,750
message queue until it's back and no

00:18:33,740 --> 00:18:42,530
information is getting lost over this

00:18:35,750 --> 00:18:45,950
temporary downtime messaging middleware

00:18:42,530 --> 00:18:49,160
usually is not a single point to point Q

00:18:45,950 --> 00:18:51,860
it is a topic and you can picture that

00:18:49,160 --> 00:18:54,200
as every consumer having its own message

00:18:51,860 --> 00:18:57,110
queues and the middle we are being smart

00:18:54,200 --> 00:19:00,830
enough to duplicate the messages into

00:18:57,110 --> 00:19:03,680
the right queues so we're no longer

00:19:00,830 --> 00:19:06,110
restricted to one-to-one routing and the

00:19:03,680 --> 00:19:08,909
topic is giving us publish/subscribe for

00:19:06,110 --> 00:19:11,489
our capabilities

00:19:08,909 --> 00:19:13,710
a new service can subscribe itself to

00:19:11,489 --> 00:19:15,899
the messages it's interested in by

00:19:13,710 --> 00:19:17,700
creating its own cues and the message

00:19:15,899 --> 00:19:22,769
producer doesn't have to know anything

00:19:17,700 --> 00:19:24,779
about it the consumers stay anonymous we

00:19:22,769 --> 00:19:27,629
can also add a new message producer

00:19:24,779 --> 00:19:29,789
which can start sending messages without

00:19:27,629 --> 00:19:34,019
the consumer being aware of the new

00:19:29,789 --> 00:19:37,049
producer if every service reports some

00:19:34,019 --> 00:19:39,200
suspicious activities to a centralized

00:19:37,049 --> 00:19:42,450
fraud score tracker by sending a message

00:19:39,200 --> 00:19:45,379
the tracker itself does not need to know

00:19:42,450 --> 00:19:52,859
anything about the other services

00:19:45,379 --> 00:19:55,229
message producers stay anonymous similar

00:19:52,859 --> 00:19:57,659
to background jobs we are facing some

00:19:55,229 --> 00:20:04,889
challenges due to the a synchronicity of

00:19:57,659 --> 00:20:07,289
the communication our messages are a

00:20:04,889 --> 00:20:09,419
data based interface and sooner or later

00:20:07,289 --> 00:20:12,570
we need to make some breaking changes to

00:20:09,419 --> 00:20:14,999
that interface that is really hard to

00:20:12,570 --> 00:20:18,379
coordinate if the message is routed from

00:20:14,999 --> 00:20:21,629
multiple producers to multiple consumers

00:20:18,379 --> 00:20:23,609
which is why n2m routing of messages

00:20:21,629 --> 00:20:29,759
even though it's technically possible is

00:20:23,609 --> 00:20:31,919
usually not a great idea also the domain

00:20:29,759 --> 00:20:33,720
the business logic is usually

00:20:31,919 --> 00:20:36,450
represented much better by having

00:20:33,720 --> 00:20:39,989
multiple different messages some going

00:20:36,450 --> 00:20:42,299
and one others going 1 to N instead of

00:20:39,989 --> 00:20:47,549
having this one huge message covering

00:20:42,299 --> 00:20:49,200
everything messages are kept in the

00:20:47,549 --> 00:20:51,450
queue if they are not consumed and

00:20:49,200 --> 00:20:54,539
that's a great feature if the consumer

00:20:51,450 --> 00:20:55,729
is temporarily down but if the consumer

00:20:54,539 --> 00:20:58,320
is gone for good

00:20:55,729 --> 00:21:00,239
messages are piling up in the queue up

00:20:58,320 --> 00:21:02,999
to the point that the whole broker can

00:21:00,239 --> 00:21:05,159
get into trouble so a well behaved

00:21:02,999 --> 00:21:10,070
system should make sure that it removes

00:21:05,159 --> 00:21:10,070
all the queues that are no longer needed

00:21:10,099 --> 00:21:18,379
as with background jobs we usually have

00:21:15,090 --> 00:21:21,179
no guarantees exactly once delivery and

00:21:18,379 --> 00:21:22,830
writing idempotent message consumers

00:21:21,179 --> 00:21:27,769
allows us to

00:21:22,830 --> 00:21:27,769
choose at least once delivery safely

00:21:29,179 --> 00:21:35,130
also my messages might be processed out

00:21:32,130 --> 00:21:38,130
of order so again if the order matters I

00:21:35,130 --> 00:21:41,850
should not kill them all at once but

00:21:38,130 --> 00:21:44,100
have the first message after doing the

00:21:41,850 --> 00:21:49,679
task at hand queue the next message as a

00:21:44,100 --> 00:21:52,169
follow-up when it comes to propagating

00:21:49,679 --> 00:21:55,440
updates we need to watch out for the

00:21:52,169 --> 00:21:57,840
possibility of multiple updates to the

00:21:55,440 --> 00:22:01,139
same data within a short timeframe so

00:21:57,840 --> 00:22:03,210
the last message we receive is not

00:22:01,139 --> 00:22:07,320
necessarily the one from the most recent

00:22:03,210 --> 00:22:09,570
update the two different approaches to

00:22:07,320 --> 00:22:12,779
propagating updates with event messages

00:22:09,570 --> 00:22:16,559
and one is sometimes referred to as

00:22:12,779 --> 00:22:19,529
event carried State transfer in this

00:22:16,559 --> 00:22:23,220
case the payload of the message contains

00:22:19,529 --> 00:22:25,200
the updated data the service receiving

00:22:23,220 --> 00:22:27,299
the message doesn't have to make any

00:22:25,200 --> 00:22:30,590
request to the service owning the

00:22:27,299 --> 00:22:33,440
message but we need to include some

00:22:30,590 --> 00:22:36,740
information that allows the consumer to

00:22:33,440 --> 00:22:39,840
figure out the correct order of events

00:22:36,740 --> 00:22:43,830
without relying on the order of messages

00:22:39,840 --> 00:22:46,019
that can be a simple timestamp but the

00:22:43,830 --> 00:22:49,610
messages might be processed out of order

00:22:46,019 --> 00:22:51,899
so we cannot rely on the order alone a

00:22:49,610 --> 00:22:54,210
different approach is to send very

00:22:51,899 --> 00:22:57,510
simple event notifications where the

00:22:54,210 --> 00:23:01,019
payload is only the ID of the resource

00:22:57,510 --> 00:23:03,059
that has changed these messages are

00:23:01,019 --> 00:23:05,880
commutatively the order doesn't matter

00:23:03,059 --> 00:23:09,870
at all but the message consumer has to

00:23:05,880 --> 00:23:14,370
make a follow-up request to fetch the

00:23:09,870 --> 00:23:15,929
recent changes it also means that the

00:23:14,370 --> 00:23:18,570
consumer doesn't learn every

00:23:15,929 --> 00:23:21,600
intermediate state of the resource it

00:23:18,570 --> 00:23:23,820
only learns the state at the time of the

00:23:21,600 --> 00:23:26,190
follow-up request so this approach does

00:23:23,820 --> 00:23:28,529
not work very well if the consumer is

00:23:26,190 --> 00:23:31,399
building some kind of history or a

00:23:28,529 --> 00:23:31,399
timeline

00:23:33,399 --> 00:23:39,770
messages are removed from the queue

00:23:35,480 --> 00:23:43,940
after processing so I cannot replay the

00:23:39,770 --> 00:23:46,010
stream of messages replayability is a

00:23:43,940 --> 00:23:49,580
feature that's very useful when building

00:23:46,010 --> 00:23:52,130
events or systems in an event source

00:23:49,580 --> 00:23:54,950
system I am NOT storing the current

00:23:52,130 --> 00:23:58,700
state of the resource I'm storing a lot

00:23:54,950 --> 00:24:03,169
of events that I can replay in order to

00:23:58,700 --> 00:24:05,630
recalculate the current state if I want

00:24:03,169 --> 00:24:08,750
to do this based on my messaging system

00:24:05,630 --> 00:24:12,590
the message consumer needs to persist

00:24:08,750 --> 00:24:15,710
the messages it's processing to make

00:24:12,590 --> 00:24:18,710
sure I can replay them but that also

00:24:15,710 --> 00:24:21,740
means every consumer builds its own

00:24:18,710 --> 00:24:24,830
event lock so there is no single source

00:24:21,740 --> 00:24:28,330
of truth and my consumers might end up

00:24:24,830 --> 00:24:28,330
living in different realities

00:24:31,149 --> 00:24:36,169
message-oriented middleware is a better

00:24:34,039 --> 00:24:37,970
fit for distributed architecture than

00:24:36,169 --> 00:24:42,159
background jobs because it's actually

00:24:37,970 --> 00:24:46,299
built for communication between services

00:24:42,159 --> 00:24:49,250
but it works best with mostly stateful

00:24:46,299 --> 00:24:56,809
architecture not such a great fit for

00:24:49,250 --> 00:24:58,159
the event sourcing so to summarize the

00:24:56,809 --> 00:25:00,770
features of message oriented middleware

00:24:58,159 --> 00:25:02,980
are based on cues and topics and the

00:25:00,770 --> 00:25:05,809
topics abstract away the complexity of

00:25:02,980 --> 00:25:08,419
publish/subscribe while the cues

00:25:05,809 --> 00:25:10,549
abstract away the fact that messages are

00:25:08,419 --> 00:25:13,580
processed asynchronously in the future

00:25:10,549 --> 00:25:16,299
or in parallel that allows us to write

00:25:13,580 --> 00:25:19,130
very simple concrete message consumers

00:25:16,299 --> 00:25:23,059
but again our overall system might

00:25:19,130 --> 00:25:26,779
become a bit more complex think about

00:25:23,059 --> 00:25:28,640
delivery and order guarantees and this

00:25:26,779 --> 00:25:32,710
kind of messaging is a great fit for

00:25:28,640 --> 00:25:32,710
stateful distributed architectures

00:25:35,630 --> 00:25:41,880
event locks also known as commit locks

00:25:38,930 --> 00:25:45,660
have a totally different approach to

00:25:41,880 --> 00:25:50,700
events and the most famous one around

00:25:45,660 --> 00:25:53,520
right now is probably Kafka an event

00:25:50,700 --> 00:25:56,520
lock persists the sequence of events

00:25:53,520 --> 00:25:58,680
into an append-only lock and the

00:25:56,520 --> 00:26:02,040
consumers can reach from that lock

00:25:58,680 --> 00:26:06,990
whenever they want and as often as they

00:26:02,040 --> 00:26:09,780
want the broker itself is stateless

00:26:06,990 --> 00:26:11,940
there is no queues the broker does not

00:26:09,780 --> 00:26:15,270
track consumption it's up to the

00:26:11,940 --> 00:26:17,780
consumer to know where to continue

00:26:15,270 --> 00:26:21,780
reading the lock

00:26:17,780 --> 00:26:26,720
Kafka has topics for publish/subscribe

00:26:21,780 --> 00:26:26,720
but these topics are not based on cues

00:26:27,800 --> 00:26:33,030
since a broker keeps all events around

00:26:30,660 --> 00:26:35,640
anyways it doesn't care if a consumer

00:26:33,030 --> 00:26:37,350
disappears there is no queue that could

00:26:35,640 --> 00:26:41,190
overflow and brings a broker into

00:26:37,350 --> 00:26:44,160
trouble and a stateless broker is a very

00:26:41,190 --> 00:26:46,380
fast broker giving us high throughput

00:26:44,160 --> 00:26:49,020
data streaming which is great for

00:26:46,380 --> 00:26:51,210
real-time applications and this is

00:26:49,020 --> 00:26:55,560
particularly true for Kafka which has a

00:26:51,210 --> 00:26:57,990
really amazing throughput we are also

00:26:55,560 --> 00:27:00,990
gaining replayability events are not

00:26:57,990 --> 00:27:03,570
removed after processing so the consumer

00:27:00,990 --> 00:27:07,110
can replay them as often as it wants and

00:27:03,570 --> 00:27:09,780
all consumers share the same lock so we

00:27:07,110 --> 00:27:15,960
get a single source of truth if we want

00:27:09,780 --> 00:27:18,810
to do event sourcing so this makes event

00:27:15,960 --> 00:27:25,740
locks a great fit for event sourcing but

00:27:18,810 --> 00:27:27,510
also for real-time applications to sum

00:27:25,740 --> 00:27:30,060
it up background jobs are based on

00:27:27,510 --> 00:27:32,430
queues and the abstraction allows us to

00:27:30,060 --> 00:27:34,890
write very simple concrete job classes

00:27:32,430 --> 00:27:38,400
and they're a great fit for monolithic

00:27:34,890 --> 00:27:42,180
architecture message-oriented middleware

00:27:38,400 --> 00:27:44,040
is based on topics and queues and it's

00:27:42,180 --> 00:27:46,620
abstraction allows us to write very

00:27:44,040 --> 00:27:49,110
simple message consumers which is

00:27:46,620 --> 00:27:51,690
stateless as far as messages

00:27:49,110 --> 00:27:54,870
concerns and they're a great fit for

00:27:51,690 --> 00:27:59,670
distributed architecture if it's a

00:27:54,870 --> 00:28:02,640
mostly state for one event lots do not

00:27:59,670 --> 00:28:05,790
use cues they have a shared lock shared

00:28:02,640 --> 00:28:07,500
by all consumers but the consumers are

00:28:05,790 --> 00:28:10,920
stateful they need to know where to

00:28:07,500 --> 00:28:13,590
continue reading that lock an event

00:28:10,920 --> 00:28:18,900
locks are a great fit for event sourcing

00:28:13,590 --> 00:28:21,299
and real time applications the

00:28:18,900 --> 00:28:25,190
Thanksgiving weekend also known as Black

00:28:21,299 --> 00:28:29,370
Friday Cyber Monday is the largest

00:28:25,190 --> 00:28:32,070
shopping weekend in the US and in 2017

00:28:29,370 --> 00:28:36,270
Shopify build a Kafka power dashboard

00:28:32,070 --> 00:28:39,270
that visualizes in real time how Shopify

00:28:36,270 --> 00:28:45,140
merchants made sales for over 1 billion

00:28:39,270 --> 00:28:48,120
US dollar over this one long weekend

00:28:45,140 --> 00:28:51,179
Black Friday Cyber Monday 2018 is

00:28:48,120 --> 00:28:52,890
expected to be even bigger and if you

00:28:51,179 --> 00:28:54,690
would like to help us take these things

00:28:52,890 --> 00:28:59,060
to the next level come and talk to me

00:28:54,690 --> 00:29:02,530
after we're hiring thank you so much

00:28:59,060 --> 00:29:05,700
[Applause]

00:29:02,530 --> 00:29:05,700
[Music]

00:29:08,650 --> 00:29:16,300
it is break time and I want you all back

00:29:11,750 --> 00:29:19,840
here at half past four for the next talk

00:29:16,300 --> 00:29:19,840

YouTube URL: https://www.youtube.com/watch?v=tA8gGd_Rl7E


