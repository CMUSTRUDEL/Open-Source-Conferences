Title: Flock 2016 - Fedora with Amazon EC2 Container Service, David Duncan
Publication date: 2016-08-21
Playlist: Flock 2016
Description: 
	Amazon EC2 Container Service (ECS) is a highly scalable, high performance container management service that supports Docker containers and allows you to easily run applications on a managed cluster of Amazon EC2 instances. This discussion covers best practices and basic Implementation using Fedora Cloud.

https://flock2016.sched.org/event/76ng/fedora-with-amazon-ec2-container-service

This video was recorded at the annual Fedora contributor conference, Flock, in KrakÃ³w, Poland, from August 2-5, 2016. You can learn more about Flock at https://flocktofedora.org.

#FlockToFedora

Transcription: https://meetbot.fedoraproject.org/fedora-flock-matejko/2016-08-02/flock2016.2016-08-02-08.02.log.html
Captions: 
	00:00:05,089 --> 00:00:08,089
g

00:00:10,870 --> 00:00:16,610
alright so welcome everyone my name is

00:00:13,969 --> 00:00:19,810
david duncan I'm the open source Linux

00:00:16,610 --> 00:00:24,890
lead for solutions architects at amazon

00:00:19,810 --> 00:00:26,539
in the Amazon Web Services Group and I

00:00:24,890 --> 00:00:31,550
want to talk to you today a little bit

00:00:26,539 --> 00:00:36,140
about running the Amazon ec2 container

00:00:31,550 --> 00:00:43,489
service and specifically running that

00:00:36,140 --> 00:00:51,559
with the door atomic this pause for a

00:00:43,489 --> 00:00:56,449
second nobody will get in so we'll talk

00:00:51,559 --> 00:00:58,579
a little bit about just cluster

00:00:56,449 --> 00:01:01,399
management in general scheduling a

00:00:58,579 --> 00:01:04,759
little bit running the services then

00:01:01,399 --> 00:01:09,140
I'll walk through adding the ecs agent

00:01:04,759 --> 00:01:10,430
to fedora tonic and that should get you

00:01:09,140 --> 00:01:12,649
up to the point that you can actually

00:01:10,430 --> 00:01:14,479
start working with a cluster so I'm not

00:01:12,649 --> 00:01:17,660
going to go into a lot of the detail

00:01:14,479 --> 00:01:20,179
around containers and the container

00:01:17,660 --> 00:01:22,250
building itself because I'm assuming

00:01:20,179 --> 00:01:24,410
that almost everyone in here is pretty

00:01:22,250 --> 00:01:26,179
comfortable with that side of the

00:01:24,410 --> 00:01:28,819
business will stick to what's happening

00:01:26,179 --> 00:01:31,209
on the AWS side and lower that barrier

00:01:28,819 --> 00:01:31,209
to entry

00:01:32,360 --> 00:01:39,360
so why containers easy for micro

00:01:37,230 --> 00:01:43,080
services this is sort of a natural

00:01:39,360 --> 00:01:44,700
natural platform for microservices one

00:01:43,080 --> 00:01:46,920
of the things that happens in the public

00:01:44,700 --> 00:01:51,180
cloud obviously is that a lot of the

00:01:46,920 --> 00:01:54,420
standard applications and and some of

00:01:51,180 --> 00:01:56,280
the simpler processes are abstract away

00:01:54,420 --> 00:01:59,600
and makes it very easy to work with for

00:01:56,280 --> 00:02:03,320
a lot of people who are looking for a

00:01:59,600 --> 00:02:03,320
public cloud process

00:02:05,870 --> 00:02:13,610
so just building containers obviously is

00:02:09,020 --> 00:02:15,200
fairly straightforward and that part we

00:02:13,610 --> 00:02:17,630
don't like I said I'm not going to

00:02:15,200 --> 00:02:21,050
emphasize today but what is hard is

00:02:17,630 --> 00:02:24,350
getting them scheduled and this is a

00:02:21,050 --> 00:02:27,110
challenge to a lot of businesses that

00:02:24,350 --> 00:02:30,920
come to us and they want to know how

00:02:27,110 --> 00:02:32,360
they're going to handle that we want to

00:02:30,920 --> 00:02:35,030
make sure that you have sort of an

00:02:32,360 --> 00:02:40,010
intelligent way to to manage to those

00:02:35,030 --> 00:02:44,440
containers figure out what instances you

00:02:40,010 --> 00:02:44,440
have available etc

00:02:46,390 --> 00:02:52,000
so we start with the ec2 instance the

00:02:49,930 --> 00:02:54,480
ec2 instance itself is going to be the

00:02:52,000 --> 00:02:58,270
base for what you're running darker on

00:02:54,480 --> 00:03:00,700
inside of the docker we're going to

00:02:58,270 --> 00:03:02,709
separate out the specific containers and

00:03:00,700 --> 00:03:05,680
groups of containers and groups of and

00:03:02,709 --> 00:03:08,410
add storage into what we call a task

00:03:05,680 --> 00:03:10,720
definition so when you run a task

00:03:08,410 --> 00:03:17,050
definition that is basically what's

00:03:10,720 --> 00:03:20,140
running in the as a as a solid group for

00:03:17,050 --> 00:03:22,720
container management so we're using this

00:03:20,140 --> 00:03:26,110
to basically track just the cpu memory

00:03:22,720 --> 00:03:32,950
and networking resources that are

00:03:26,110 --> 00:03:37,120
available for containers so the

00:03:32,950 --> 00:03:42,630
scheduler is responsible for the tasks

00:03:37,120 --> 00:03:46,660
and they're in their execution once we

00:03:42,630 --> 00:03:48,370
have the task to find we'll use the we

00:03:46,660 --> 00:03:53,140
use a scheduler actually use a couple

00:03:48,370 --> 00:03:56,880
schedulers to to manage those and place

00:03:53,140 --> 00:03:56,880
them in exactly where you want

00:04:03,660 --> 00:04:08,940
so we have a cluster management engine

00:04:06,330 --> 00:04:10,140
that underlines the service alright so

00:04:08,940 --> 00:04:12,720
there's no requirement for you to

00:04:10,140 --> 00:04:17,180
actually touch that you're just going to

00:04:12,720 --> 00:04:17,180
leverage it for our utilization

00:04:23,480 --> 00:04:28,700
so what we're going to talk about

00:04:25,640 --> 00:04:31,130
ultimately is the ecs agent and the ecs

00:04:28,700 --> 00:04:33,940
agent is what's running on each one of

00:04:31,130 --> 00:04:38,060
the individual container instances so

00:04:33,940 --> 00:04:41,480
what you would refer to as a standard

00:04:38,060 --> 00:04:45,040
instance in this case running the ecs

00:04:41,480 --> 00:04:48,590
agent becomes the container instance

00:04:45,040 --> 00:04:52,310
there is a agent communication service

00:04:48,590 --> 00:04:54,680
and an API that's available here that's

00:04:52,310 --> 00:04:57,110
going to talk directly to the ecs agent

00:04:54,680 --> 00:05:01,270
so the ecs agent actually wraps a lot of

00:04:57,110 --> 00:05:07,010
the docker commands so that they have

00:05:01,270 --> 00:05:10,720
almost native access to the ec2

00:05:07,010 --> 00:05:10,720
container service API

00:05:15,240 --> 00:05:22,050
so basically what we do in order to

00:05:18,000 --> 00:05:26,039
coordinate this is we provide a key

00:05:22,050 --> 00:05:28,530
value store underneath although this is

00:05:26,039 --> 00:05:34,080
kind of the heart of the process and

00:05:28,530 --> 00:05:39,889
keeps the the cluster state available

00:05:34,080 --> 00:05:39,889
across all of the container instances so

00:05:40,220 --> 00:05:45,539
maintaining that at scale is a really

00:05:43,380 --> 00:05:53,810
big deal so we're going to take a look

00:05:45,539 --> 00:05:53,810
at exactly how that we maintain that so

00:05:53,870 --> 00:06:00,539
this is sort of a description of a

00:05:57,710 --> 00:06:06,690
fairly simple transactional algorithm

00:06:00,539 --> 00:06:10,380
right but what happens in the in the key

00:06:06,690 --> 00:06:14,250
value store is we're storing only rights

00:06:10,380 --> 00:06:21,620
that are handled after the last read

00:06:14,250 --> 00:06:24,509
right so if if your if your system will

00:06:21,620 --> 00:06:28,849
read a key value right key value pair

00:06:24,509 --> 00:06:32,580
then that key value pair becomes the

00:06:28,849 --> 00:06:35,840
this snapshot that we we base our next

00:06:32,580 --> 00:06:37,560
right on so you never get out of water

00:06:35,840 --> 00:06:40,080
okay

00:06:37,560 --> 00:06:42,760
so if you have multiple clients and

00:06:40,080 --> 00:06:47,710
they're all doing rights and they're all

00:06:42,760 --> 00:06:50,230
doing reads you actually end up in

00:06:47,710 --> 00:06:54,490
positions where if I have a system

00:06:50,230 --> 00:06:56,590
that's read that say in plus two tries

00:06:54,490 --> 00:07:00,040
to write it in plus three but I have

00:06:56,590 --> 00:07:02,860
another scheduler writes a transactional

00:07:00,040 --> 00:07:04,120
event that occurs that n plus five then

00:07:02,860 --> 00:07:06,970
the only place that that will actually

00:07:04,120 --> 00:07:12,310
be available to write is at the n plus 6

00:07:06,970 --> 00:07:15,160
step so this allows us to have a sort of

00:07:12,310 --> 00:07:17,230
a combination of events occurring from

00:07:15,160 --> 00:07:21,610
different locations all talking to the

00:07:17,230 --> 00:07:23,140
same API that API then goes back to the

00:07:21,610 --> 00:07:27,810
cluster manager and ensures that

00:07:23,140 --> 00:07:27,810
everything stays saying

00:07:30,760 --> 00:07:35,950
so looking at it neck in action here we

00:07:33,370 --> 00:07:38,230
have the API the agent communication

00:07:35,950 --> 00:07:42,400
service all talking to the cluster

00:07:38,230 --> 00:07:45,490
management engine I'll all say that you

00:07:42,400 --> 00:07:48,580
check and done by p value store and this

00:07:45,490 --> 00:07:52,720
is done across a range of availability

00:07:48,580 --> 00:07:56,800
zones and so you can actually you can

00:07:52,720 --> 00:07:59,530
maintain an entire cluster across a span

00:07:56,800 --> 00:08:02,560
of high latency communication data

00:07:59,530 --> 00:08:04,480
centers single region single region yeah

00:08:02,560 --> 00:08:07,150
that's always single region there are

00:08:04,480 --> 00:08:08,860
ways of spanning it but when you define

00:08:07,150 --> 00:08:12,580
a cluster you're always defining it in a

00:08:08,860 --> 00:08:17,160
specific region and that's by design so

00:08:12,580 --> 00:08:19,810
one of the principles of the AWS

00:08:17,160 --> 00:08:23,700
configuration is that every every region

00:08:19,810 --> 00:08:23,700
is independently active

00:08:25,950 --> 00:08:30,320
so I was at work if you want a service

00:08:28,530 --> 00:08:33,960
that is multi

00:08:30,320 --> 00:08:37,020
so so your if you want to service its

00:08:33,960 --> 00:08:39,780
multi-region we tend to suggest that you

00:08:37,020 --> 00:08:41,790
actually create a system that is

00:08:39,780 --> 00:08:44,610
singularly regional right so it's in it

00:08:41,790 --> 00:08:48,810
which is across one single region but

00:08:44,610 --> 00:08:50,460
then make those latency based make

00:08:48,810 --> 00:08:54,060
latency based communications across

00:08:50,460 --> 00:08:55,650
across the four clients so that if you

00:08:54,060 --> 00:08:57,150
have clients who are if you are hitting

00:08:55,650 --> 00:09:00,560
systems they're actually hitting what's

00:08:57,150 --> 00:09:05,120
what's most local to them you can also

00:09:00,560 --> 00:09:09,200
create restrictions based on what what

00:09:05,120 --> 00:09:14,270
geographic locations you have if that is

00:09:09,200 --> 00:09:14,270
critical to your to your business case

00:09:15,410 --> 00:09:20,209
okay so with the API you can choose

00:09:18,560 --> 00:09:23,149
whatever scheduler it is that you want

00:09:20,209 --> 00:09:26,629
want to use if you have your own

00:09:23,149 --> 00:09:29,060
scheduler you can actually leverage this

00:09:26,629 --> 00:09:30,800
API and this is all open source but I'll

00:09:29,060 --> 00:09:35,930
show you where it is on github you just

00:09:30,800 --> 00:09:41,209
a second to actually make make requests

00:09:35,930 --> 00:09:44,779
or for resources and then run those

00:09:41,209 --> 00:09:47,449
tasks or run tasks accordingly test of

00:09:44,779 --> 00:09:50,019
course being combinations of containers

00:09:47,449 --> 00:09:50,019
and storage

00:09:55,840 --> 00:10:01,180
so here i'm just showing multiple

00:09:59,140 --> 00:10:06,790
schedulers right so you have multiple

00:10:01,180 --> 00:10:09,490
schedulers multiple resources you can

00:10:06,790 --> 00:10:11,200
schedule a task by any one of these you

00:10:09,490 --> 00:10:14,410
know this could be this could be a

00:10:11,200 --> 00:10:17,290
long-running service application test

00:10:14,410 --> 00:10:19,510
scheduler this would be for batch

00:10:17,290 --> 00:10:22,830
operations each one of them could be

00:10:19,510 --> 00:10:27,250
working in vastly different rates and

00:10:22,830 --> 00:10:30,520
the cluster management and scheduler is

00:10:27,250 --> 00:10:32,050
going to keep keep that in check right

00:10:30,520 --> 00:10:33,370
so your cluster management is always

00:10:32,050 --> 00:10:35,650
going to look back to the key value

00:10:33,370 --> 00:10:37,300
store verify that these resources have

00:10:35,650 --> 00:10:39,670
not already been allocated or they've

00:10:37,300 --> 00:10:43,030
been freed prior to the actual

00:10:39,670 --> 00:10:45,840
deployment and then your schedulers for

00:10:43,030 --> 00:10:45,840
your work as it chooses

00:10:51,120 --> 00:10:57,300
so this is just demonstrating the same

00:10:54,420 --> 00:11:01,580
thing we had in the algorithm review

00:10:57,300 --> 00:11:05,070
there that if if our if scheduler yellow

00:11:01,580 --> 00:11:07,680
is trying to access resources that are

00:11:05,070 --> 00:11:12,180
already being provided from scheduler

00:11:07,680 --> 00:11:14,720
blue then there's there's always going

00:11:12,180 --> 00:11:23,160
to be a prevention than for that

00:11:14,720 --> 00:11:26,970
occurring so basically we get a full

00:11:23,160 --> 00:11:29,850
scale shared state system that you can

00:11:26,970 --> 00:11:32,610
provide your own schedule or for you can

00:11:29,850 --> 00:11:35,850
allocate resources as you see necessary

00:11:32,610 --> 00:11:37,430
you have your instances that container

00:11:35,850 --> 00:11:40,560
instances that you are actually

00:11:37,430 --> 00:11:42,600
providing all of these resources can can

00:11:40,560 --> 00:11:44,279
then be Auto scaled so you don't

00:11:42,600 --> 00:11:46,529
necessarily have a single group of

00:11:44,279 --> 00:11:49,170
container in instances that are

00:11:46,529 --> 00:11:51,270
associated with your cluster resources

00:11:49,170 --> 00:11:57,480
that can literally scale up the scale

00:11:51,270 --> 00:11:58,980
down all of your central control and

00:11:57,480 --> 00:12:00,839
monitoring is happening through the

00:11:58,980 --> 00:12:03,300
cluster scheduler or through the cluster

00:12:00,839 --> 00:12:05,370
management system and so the scheduler

00:12:03,300 --> 00:12:09,230
can function independently of that and

00:12:05,370 --> 00:12:09,230
just provide information back and forth

00:12:11,580 --> 00:12:19,870
so these are some of the scale out

00:12:17,260 --> 00:12:23,890
numbers that we that we've we've

00:12:19,870 --> 00:12:25,779
actually pushed from our own cluster so

00:12:23,890 --> 00:12:31,470
looking at this you can see that we can

00:12:25,779 --> 00:12:31,470
scale up very quickly a number of nodes

00:12:34,410 --> 00:12:40,959
where you know sort of a singularly

00:12:38,290 --> 00:12:47,500
allocated system would provide you sort

00:12:40,959 --> 00:12:50,290
of a flat number just reiterate that you

00:12:47,500 --> 00:12:52,060
know with the schedulers we have to to

00:12:50,290 --> 00:12:54,670
specific schedulers that are they are by

00:12:52,060 --> 00:12:55,750
default one is for long-running services

00:12:54,670 --> 00:12:57,190
so you're going to actually have a

00:12:55,750 --> 00:12:59,529
service definition that's associated

00:12:57,190 --> 00:13:01,630
with that and then the other is just for

00:12:59,529 --> 00:13:03,640
batch jobs so if you want to run a

00:13:01,630 --> 00:13:06,310
single job that's just going to take

00:13:03,640 --> 00:13:09,550
resources up for for some period of time

00:13:06,310 --> 00:13:13,330
like something thats related to clean up

00:13:09,550 --> 00:13:14,860
for after a backup process then there's

00:13:13,330 --> 00:13:18,149
a scheduler that actually handles that

00:13:14,860 --> 00:13:18,149
those resources is world

00:13:20,020 --> 00:13:24,470
obligatory slide there's all sorts of

00:13:22,760 --> 00:13:26,060
other services out there that this

00:13:24,470 --> 00:13:28,040
touches and makes it really easy to

00:13:26,060 --> 00:13:30,380
integrate those back into whatever it is

00:13:28,040 --> 00:13:33,830
that you're working on so you do want to

00:13:30,380 --> 00:13:36,380
pull that out so the elastic load

00:13:33,830 --> 00:13:38,810
balancing is a really easy one to point

00:13:36,380 --> 00:13:40,850
out how that inter interfaces with this

00:13:38,810 --> 00:13:44,440
because each one of your containers is

00:13:40,850 --> 00:13:47,180
going to present either TCP or UDP from

00:13:44,440 --> 00:13:50,750
from this from the container instance

00:13:47,180 --> 00:13:53,510
each one of those ports can then be

00:13:50,750 --> 00:13:56,029
attached to a single elastic load

00:13:53,510 --> 00:13:57,950
balancer and you can actually scale out

00:13:56,029 --> 00:14:00,890
your service whatever your task

00:13:57,950 --> 00:14:02,750
definition is you can actually scale

00:14:00,890 --> 00:14:05,720
that out behind a single load balancer

00:14:02,750 --> 00:14:07,100
so then you're not just provisioning an

00:14:05,720 --> 00:14:10,010
instance you're provisioning their

00:14:07,100 --> 00:14:12,220
container behind a load balancer so the

00:14:10,010 --> 00:14:14,300
load balancer itself is just making

00:14:12,220 --> 00:14:16,430
communications with the single container

00:14:14,300 --> 00:14:18,100
that's that's driving whatever service

00:14:16,430 --> 00:14:21,490
it is that you're accessing from there

00:14:18,100 --> 00:14:24,140
that service then if it becomes

00:14:21,490 --> 00:14:28,160
non-responsive the containers at this

00:14:24,140 --> 00:14:30,560
the task itself is actually destroyed

00:14:28,160 --> 00:14:32,690
not the container instance if a

00:14:30,560 --> 00:14:35,060
container instance is destroyed the

00:14:32,690 --> 00:14:37,240
tasks are then actually reattached to

00:14:35,060 --> 00:14:39,440
the ALB when they're when they're

00:14:37,240 --> 00:14:41,980
expanded they're scaled out based on the

00:14:39,440 --> 00:14:41,980
scaling group

00:14:43,220 --> 00:14:46,220
so

00:14:46,720 --> 00:14:53,250
this is where you go to see the open

00:14:48,819 --> 00:14:56,589
source side of this the CLI here is

00:14:53,250 --> 00:14:58,240
completely open source the ecs agent is

00:14:56,589 --> 00:15:00,100
completely available and I'm really

00:14:58,240 --> 00:15:05,110
looking forward to hearing your feedback

00:15:00,100 --> 00:15:08,399
and finding out how we can better serve

00:15:05,110 --> 00:15:08,399
the atomic community

00:15:11,630 --> 00:15:14,870
deposit like

00:15:36,440 --> 00:15:40,340
so the other thing about having those is

00:15:38,960 --> 00:15:42,620
open source and makes it very easy to

00:15:40,340 --> 00:15:44,300
sort of pull them into a standard CI CD

00:15:42,620 --> 00:15:45,770
process if you have something that's

00:15:44,300 --> 00:15:48,800
already going with your doctor files

00:15:45,770 --> 00:15:51,350
through github whatever can actually

00:15:48,800 --> 00:15:55,360
roll that right into your process and

00:15:51,350 --> 00:15:55,360
have those instances just turn

00:15:58,760 --> 00:16:03,900
actually you can actually provision

00:16:01,200 --> 00:16:05,220
based upon your process and you can have

00:16:03,900 --> 00:16:08,040
multiple clusters that do different

00:16:05,220 --> 00:16:10,170
things so if you had like a test versus

00:16:08,040 --> 00:16:12,600
a prog cluster you could have one that

00:16:10,170 --> 00:16:16,700
was that had a scaling where that stayed

00:16:12,600 --> 00:16:18,900
at one container instance versus a

00:16:16,700 --> 00:16:22,850
production Windridge scaled out to a

00:16:18,900 --> 00:16:22,850
much larger audience

00:16:26,339 --> 00:16:33,259
okay so now we'll look a little bit

00:16:29,249 --> 00:16:36,269
closer at what the task definition is so

00:16:33,259 --> 00:16:37,620
like I said a task is going to be

00:16:36,269 --> 00:16:40,829
something that's actually handled by a

00:16:37,620 --> 00:16:44,399
container so if you're looking at how

00:16:40,829 --> 00:16:45,990
that how that your your task is going to

00:16:44,399 --> 00:16:47,370
run that that's going to define it

00:16:45,990 --> 00:16:50,759
you're going to find that in the task

00:16:47,370 --> 00:16:54,600
definition the task definition also

00:16:50,759 --> 00:16:57,360
includes storage so you can you can

00:16:54,600 --> 00:16:58,680
actually identify how your your storage

00:16:57,360 --> 00:17:05,839
is associated with this particular

00:16:58,680 --> 00:17:10,350
container and then your your run command

00:17:05,839 --> 00:17:12,750
in that definition altogether will will

00:17:10,350 --> 00:17:16,309
then define what resources will be

00:17:12,750 --> 00:17:20,189
allocated by the scheduler for any given

00:17:16,309 --> 00:17:25,319
instance of that task yes the stories

00:17:20,189 --> 00:17:29,309
can be that he is EBS or elastic file

00:17:25,319 --> 00:17:31,679
system yeah no s3 I mean you can access

00:17:29,309 --> 00:17:35,460
s3 programmatically but then that's not

00:17:31,679 --> 00:17:37,850
not the same as having the storage block

00:17:35,460 --> 00:17:39,740
storage cuz someone

00:17:37,850 --> 00:17:46,929
our own so like for example somebody

00:17:39,740 --> 00:17:49,220
wanted to run lesser absolutely yeah so

00:17:46,929 --> 00:17:51,980
kind of technically you could run staff

00:17:49,220 --> 00:17:54,169
I don't know why you perform want to in

00:17:51,980 --> 00:17:57,020
this context but but you but you could

00:17:54,169 --> 00:17:59,260
do it and gloss black luster is a great

00:17:57,020 --> 00:17:59,260
fit

00:18:02,340 --> 00:18:07,600
so just looking in the graphic interface

00:18:04,990 --> 00:18:09,789
at what you're actually defining for

00:18:07,600 --> 00:18:14,669
each one of these individual tasks you

00:18:09,789 --> 00:18:17,080
can see the just the defining metadata

00:18:14,669 --> 00:18:18,640
the port mappings which are really

00:18:17,080 --> 00:18:23,529
critical and then down here you are

00:18:18,640 --> 00:18:29,590
actually associating a specific number

00:18:23,529 --> 00:18:31,720
of CPU units there are 1024 22 to start

00:18:29,590 --> 00:18:36,750
with and and you're basically going to

00:18:31,720 --> 00:18:36,750
going to divide that up accordingly and

00:18:42,230 --> 00:18:48,490
so it can be a little bit more granular

00:18:45,080 --> 00:18:51,160
you can actually make some more definite

00:18:48,490 --> 00:18:53,950
stronger definitions around the

00:18:51,160 --> 00:18:56,960
environment and your doctor run commands

00:18:53,950 --> 00:18:59,419
but that's typically not necessary as

00:18:56,960 --> 00:19:03,049
long as you allocate what resources you

00:18:59,419 --> 00:19:04,970
need if you've got a defined container

00:19:03,049 --> 00:19:08,750
with with your with your standard

00:19:04,970 --> 00:19:12,510
commands in there you get a name with

00:19:08,750 --> 00:19:14,669
if you want to override your run you can

00:19:12,510 --> 00:19:19,980
do that in your name configuration and

00:19:14,669 --> 00:19:22,880
yeah it's comes out in JSON you can

00:19:19,980 --> 00:19:22,880
define it that way too

00:19:24,820 --> 00:19:30,809
okay so each one of these tasks

00:19:27,130 --> 00:19:36,070
scheduled on to a container instance and

00:19:30,809 --> 00:19:37,600
that is essentially what happens here so

00:19:36,070 --> 00:19:39,519
container instance itself doesn't

00:19:37,600 --> 00:19:42,130
necessarily isn't necessarily consumed

00:19:39,519 --> 00:19:45,610
by a single task right you've got 10 24

00:19:42,130 --> 00:19:47,889
units to to divide up but you can

00:19:45,610 --> 00:19:49,480
identify how many how many units you're

00:19:47,889 --> 00:19:53,519
going to associate with any given task

00:19:49,480 --> 00:19:53,519
and then autoscale accordingly

00:19:58,460 --> 00:20:03,160
so

00:20:00,880 --> 00:20:06,700
we're looking at it from this from that

00:20:03,160 --> 00:20:08,950
perspective right so a task actually

00:20:06,700 --> 00:20:11,830
defines a unit of work this is going to

00:20:08,950 --> 00:20:18,160
be associated with specific containers

00:20:11,830 --> 00:20:20,710
one or more and then the the actual

00:20:18,160 --> 00:20:22,900
resources that you're expecting to to

00:20:20,710 --> 00:20:25,560
associate with that particular target

00:20:22,900 --> 00:20:25,560
right

00:20:35,879 --> 00:20:41,099
okay so now here's what I said for a

00:20:39,149 --> 00:20:44,190
long running application you're going to

00:20:41,099 --> 00:20:46,409
create specifically a service and the

00:20:44,190 --> 00:20:48,929
service is going to refer to something

00:20:46,409 --> 00:20:52,499
that you want running at all times right

00:20:48,929 --> 00:20:54,989
so now we can associate that service

00:20:52,499 --> 00:20:57,209
then with a specific load balancer and

00:20:54,989 --> 00:21:00,179
that load balancer then it will scale

00:20:57,209 --> 00:21:03,419
out the back end to make sure that it's

00:21:00,179 --> 00:21:05,279
actually providing sufficient workload

00:21:03,419 --> 00:21:07,799
right so you can having a load balancers

00:21:05,279 --> 00:21:10,889
auto scaling group and that load

00:21:07,799 --> 00:21:13,949
balancer then we'll have whatever

00:21:10,889 --> 00:21:15,869
whatever triggers you actually identify

00:21:13,949 --> 00:21:19,079
are going to make it possible for you to

00:21:15,869 --> 00:21:23,839
maintain the serviceability of the this

00:21:19,079 --> 00:21:23,839
application or this task group

00:21:26,610 --> 00:21:34,140
I just reiterates exactly what I just

00:21:29,910 --> 00:21:35,400
said so another thing is you can

00:21:34,140 --> 00:21:38,100
actually have a container instance

00:21:35,400 --> 00:21:40,890
configuration that you've defined then

00:21:38,100 --> 00:21:43,380
you reiterate that this is kind of very

00:21:40,890 --> 00:21:46,260
helpful in the context of atomic right

00:21:43,380 --> 00:21:51,420
so we're we're actually going in with

00:21:46,260 --> 00:21:53,640
the OS tray and and and literally moving

00:21:51,420 --> 00:21:57,510
to a different in different version of

00:21:53,640 --> 00:21:59,730
the environment right so we can take a

00:21:57,510 --> 00:22:05,070
machine image that we've created for our

00:21:59,730 --> 00:22:08,610
old or new environment and add that to

00:22:05,070 --> 00:22:15,570
our stack so now we actually have sort

00:22:08,610 --> 00:22:18,030
of a standard kind of red light green

00:22:15,570 --> 00:22:20,090
light deployment here because we've got

00:22:18,030 --> 00:22:22,650
multiple multiple auto scaling groups

00:22:20,090 --> 00:22:25,620
each one of those auto scaling groups is

00:22:22,650 --> 00:22:28,290
associated with a particular version of

00:22:25,620 --> 00:22:33,330
the OS we can drain connections out of

00:22:28,290 --> 00:22:37,350
the old systems and once we have those

00:22:33,330 --> 00:22:42,809
completely drained we can actually go

00:22:37,350 --> 00:22:44,990
back to a full full new deployment so

00:22:42,809 --> 00:22:44,990
now

00:22:45,120 --> 00:22:48,750
all of this is the new infrastructure

00:22:47,550 --> 00:22:52,110
nothing is left to the old

00:22:48,750 --> 00:22:53,580
infrastructure it's all scaled down none

00:22:52,110 --> 00:22:55,380
of the connections were actually dropped

00:22:53,580 --> 00:22:58,110
right because the old old infrastructure

00:22:55,380 --> 00:23:00,330
was there at the same time we just

00:22:58,110 --> 00:23:02,760
remove the dns entries that are

00:23:00,330 --> 00:23:04,940
associated with the older instances so

00:23:02,760 --> 00:23:08,400
anything that's that's still running

00:23:04,940 --> 00:23:10,170
still service once there's no service on

00:23:08,400 --> 00:23:14,690
those no service load on the old

00:23:10,170 --> 00:23:14,690
instances they can just go away

00:23:18,390 --> 00:23:21,260
that make sense

00:23:22,050 --> 00:23:30,450
alright so now we're just going to look

00:23:24,000 --> 00:23:32,640
at code it's real effects so for

00:23:30,450 --> 00:23:35,580
grabbing the most recent atomic instance

00:23:32,640 --> 00:23:40,830
i use the james path query that's

00:23:35,580 --> 00:23:43,110
associated with the AWS CLI so grabbing

00:23:40,830 --> 00:23:45,630
minus one is the easiest of the fastest

00:23:43,110 --> 00:23:48,290
and easiest way to get it if you sort by

00:23:45,630 --> 00:23:48,290
creation date

00:23:51,280 --> 00:23:57,850
so once I have that machine imaged I'm

00:23:55,420 --> 00:23:59,410
going to do a run instance command the

00:23:57,850 --> 00:24:01,780
run instance command i'm going to

00:23:59,410 --> 00:24:05,410
associate with it with whatever size

00:24:01,780 --> 00:24:09,010
instance i think is representative of my

00:24:05,410 --> 00:24:14,680
test environment so it could be just a

00:24:09,010 --> 00:24:18,310
teaching micro but at you I generally

00:24:14,680 --> 00:24:21,640
choose a small because they're a small

00:24:18,310 --> 00:24:26,320
or a large gives you a sustained

00:24:21,640 --> 00:24:27,790
bandwidth and I prefer that I prefer to

00:24:26,320 --> 00:24:30,280
have the sustained bandwidth or

00:24:27,790 --> 00:24:36,610
something measurable rather than having

00:24:30,280 --> 00:24:38,710
something that's just subject to steal

00:24:36,610 --> 00:24:41,320
time

00:24:38,710 --> 00:24:43,120
how they stopped you as an Amazon

00:24:41,320 --> 00:24:46,570
employee from always grabbing the

00:24:43,120 --> 00:24:48,759
largest instance that you possibly want

00:24:46,570 --> 00:24:52,990
where is it up to your well so

00:24:48,759 --> 00:24:56,009
discretion yeah well so it's yeah so one

00:24:52,990 --> 00:24:59,799
of our basic principles would be okay oh

00:24:56,009 --> 00:25:01,269
so so how did how do they stop but just

00:24:59,799 --> 00:25:02,919
an Amazon employee from grabbing the

00:25:01,269 --> 00:25:04,779
biggest instance they possibly can yeah

00:25:02,919 --> 00:25:07,000
so why are we why am I not running

00:25:04,779 --> 00:25:11,710
everything on a GP to and ni ni two

00:25:07,000 --> 00:25:14,320
instances so frugality yes those are

00:25:11,710 --> 00:25:21,759
good yeah it was on value that I would

00:25:14,320 --> 00:25:27,129
say so skates mostly of that don't want

00:25:21,759 --> 00:25:28,960
the phone call from Joe we see well I

00:25:27,129 --> 00:25:32,049
have my own billing alert set up so that

00:25:28,960 --> 00:25:34,360
I know what I am spinning yeah so

00:25:32,049 --> 00:25:36,299
certainly don't want to don't want to

00:25:34,360 --> 00:25:39,639
spend much time and you know I literally

00:25:36,299 --> 00:25:41,919
in so you can you can if you want to

00:25:39,639 --> 00:25:44,320
schedule it so that all your instances

00:25:41,919 --> 00:25:48,129
just are turned off at a specific time

00:25:44,320 --> 00:25:51,279
every day that's you know totally

00:25:48,129 --> 00:25:52,990
possible not that I do that but I do

00:25:51,279 --> 00:26:02,259
find myself occasionally on my phone

00:25:52,990 --> 00:26:05,919
looking and thinking gosh ok so once i

00:26:02,259 --> 00:26:08,049
have that once i have a running instance

00:26:05,919 --> 00:26:12,429
i'm going to attach to that instance and

00:26:08,049 --> 00:26:14,470
just updated right so just pull the just

00:26:12,429 --> 00:26:18,059
update the OS tree i'm going to i'm

00:26:14,470 --> 00:26:22,299
going to go to latest then i'm going to

00:26:18,059 --> 00:26:28,210
grab the ecs agent from the docker

00:26:22,299 --> 00:26:32,980
registry easy enough to do once i have

00:26:28,210 --> 00:26:37,840
that now

00:26:32,980 --> 00:26:40,530
I want to I can create the EC the VCS

00:26:37,840 --> 00:26:40,530
optimism

00:26:42,880 --> 00:26:52,000
I see that command there but there's you

00:26:47,350 --> 00:26:53,890
in the next one no ok so I used a very

00:26:52,000 --> 00:26:56,620
like a couple of different directions

00:26:53,890 --> 00:26:59,860
with the CLI so you can see a couple

00:26:56,620 --> 00:27:02,200
things here there's a create image call

00:26:59,860 --> 00:27:03,940
that I made that I don't I didn't don't

00:27:02,200 --> 00:27:08,620
think I actually recorded on this this

00:27:03,940 --> 00:27:13,860
slide so at this point you would want to

00:27:08,620 --> 00:27:13,860
run a create image so that you have the

00:27:14,580 --> 00:27:22,180
current version of atomic the current

00:27:19,840 --> 00:27:25,450
version of the ecs agent all together so

00:27:22,180 --> 00:27:27,070
we'll call that one our machine image is

00:27:25,450 --> 00:27:28,540
going to keep is going to maintain that

00:27:27,070 --> 00:27:30,220
right so there's a couple of other

00:27:28,540 --> 00:27:33,700
commands that are associated there in

00:27:30,220 --> 00:27:35,530
the on the github repo if you if you

00:27:33,700 --> 00:27:39,190
read the notes there's a couple of

00:27:35,530 --> 00:27:44,580
directories that have to be created for

00:27:39,190 --> 00:27:44,580
logging in the config

00:27:46,010 --> 00:27:50,390
but once you have that once you have

00:27:48,920 --> 00:27:55,310
those created you want to create the

00:27:50,390 --> 00:27:56,660
image so pull the pull of the new create

00:27:55,310 --> 00:28:01,250
the two directories that are necessary

00:27:56,660 --> 00:28:04,310
and then run a create image for this

00:28:01,250 --> 00:28:13,940
particular this particular instance

00:28:04,310 --> 00:28:16,130
right so actually here when I did a run

00:28:13,940 --> 00:28:18,980
instance I made sure to change my

00:28:16,130 --> 00:28:22,160
instance initiating shutdown behavior to

00:28:18,980 --> 00:28:23,690
stop instead of terminate so that in the

00:28:22,160 --> 00:28:25,040
event that something went wrong I'd

00:28:23,690 --> 00:28:31,910
still have the instance they are

00:28:25,040 --> 00:28:34,120
available for me for free for creating

00:28:31,910 --> 00:28:34,120
the image

00:28:35,900 --> 00:28:42,770
okay so here I'm actually generated a

00:28:39,020 --> 00:28:47,810
small CLI skeleton and that's just a

00:28:42,770 --> 00:28:49,130
jace JSON structure for defining any of

00:28:47,810 --> 00:28:52,190
the things that are associated with a

00:28:49,130 --> 00:28:54,020
particular command so any of the

00:28:52,190 --> 00:28:55,700
parameters that are associated can be

00:28:54,020 --> 00:28:56,960
collected into one of those skeletons

00:28:55,700 --> 00:29:00,620
and then you can place that in a

00:28:56,960 --> 00:29:03,200
template that template then can be used

00:29:00,620 --> 00:29:05,360
to run the command consistently and you

00:29:03,200 --> 00:29:10,700
can just make updates to that template

00:29:05,360 --> 00:29:15,880
rather than actually having to type this

00:29:10,700 --> 00:29:18,230
out and those work great with photo so

00:29:15,880 --> 00:29:19,850
you don't necessarily you can you can

00:29:18,230 --> 00:29:22,310
manage them programmatically after that

00:29:19,850 --> 00:29:25,340
once you have a skeleton or a CLI

00:29:22,310 --> 00:29:26,690
configuration that you've generated you

00:29:25,340 --> 00:29:33,410
can make modifications to that

00:29:26,690 --> 00:29:37,280
programmatically ok so now using that in

00:29:33,410 --> 00:29:40,070
the CLI input JSON parameter I've

00:29:37,280 --> 00:29:41,570
actually created a cluster and I've

00:29:40,070 --> 00:29:45,940
given this bus or a name of proto

00:29:41,570 --> 00:29:45,940
duction because that's what we all do

00:29:51,090 --> 00:29:58,770
ah okay there we go yeah so taking that

00:29:55,560 --> 00:30:01,110
instance that we actually updated we're

00:29:58,770 --> 00:30:05,040
going to run this create image to get

00:30:01,110 --> 00:30:09,510
that image via the IDE over there that's

00:30:05,040 --> 00:30:15,210
associated with the the volume and the

00:30:09,510 --> 00:30:16,950
in the instance definition if you have

00:30:15,210 --> 00:30:18,480
so the great thing about this is if you

00:30:16,950 --> 00:30:20,880
decide to allocate more storage right

00:30:18,480 --> 00:30:24,030
let you add more devices into your

00:30:20,880 --> 00:30:26,160
configuration then those devices will be

00:30:24,030 --> 00:30:29,610
recreated with each one of those images

00:30:26,160 --> 00:30:32,040
so if you're using let's say let's say

00:30:29,610 --> 00:30:34,380
you've identified devs that you put in

00:30:32,040 --> 00:30:40,620
your doctor storage set up and then you

00:30:34,380 --> 00:30:44,190
add like sdbs DC whatever that

00:30:40,620 --> 00:30:47,700
configuration can remain each time you

00:30:44,190 --> 00:30:49,680
create an instance of this machine image

00:30:47,700 --> 00:30:53,340
you actually end up with the same

00:30:49,680 --> 00:30:55,680
storage configuration so you're keeping

00:30:53,340 --> 00:30:58,040
consistency consistency with your own

00:30:55,680 --> 00:30:58,040
image

00:31:02,940 --> 00:31:10,650
so I'm creating the image or I'm

00:31:08,310 --> 00:31:12,090
creating an actual an instance here I

00:31:10,650 --> 00:31:15,210
could have done this through an auto

00:31:12,090 --> 00:31:19,080
scaling group but I chose to do to do it

00:31:15,210 --> 00:31:26,730
as a single instance because it's easier

00:31:19,080 --> 00:31:29,940
more clear so now I have one one

00:31:26,730 --> 00:31:37,280
instance with the default configuration

00:31:29,940 --> 00:31:37,280
for this particular region

00:31:41,080 --> 00:31:48,220
so a shell in start the container so do

00:31:46,180 --> 00:31:52,780
a docker run on the on the actual

00:31:48,220 --> 00:31:57,130
configuration now this is the important

00:31:52,780 --> 00:32:00,730
part there's a there is a connection to

00:31:57,130 --> 00:32:03,370
the to the unix socket there and that

00:32:00,730 --> 00:32:07,120
requires you to run privileged so thanks

00:32:03,370 --> 00:32:08,440
Dan Walsh fixing that so that we can

00:32:07,120 --> 00:32:12,460
actually do it this is not a solution

00:32:08,440 --> 00:32:15,550
this is a workaround but the dash dash

00:32:12,460 --> 00:32:18,750
privileged dash dash net equals host is

00:32:15,550 --> 00:32:20,920
a requirement for running the actual

00:32:18,750 --> 00:32:23,290
agent if you don't have that in place

00:32:20,920 --> 00:32:25,870
then the agent will just fail and it

00:32:23,290 --> 00:32:28,800
will fail repeatedly and we won't have

00:32:25,870 --> 00:32:28,800
an attached instance

00:32:31,140 --> 00:32:37,680
once that ecs agent is running its

00:32:36,060 --> 00:32:39,090
associated with the clothes I'm sorry

00:32:37,680 --> 00:32:45,930
there's one more thing that's associated

00:32:39,090 --> 00:32:47,490
here the ecs underscore cluster

00:32:45,930 --> 00:32:50,760
environment variable there's a time in

00:32:47,490 --> 00:32:53,100
our variables in that in the reading for

00:32:50,760 --> 00:32:56,100
the for the actual agent but this one in

00:32:53,100 --> 00:33:01,320
particular associates with a defined

00:32:56,100 --> 00:33:02,910
cluster so if you have you always have a

00:33:01,320 --> 00:33:04,620
default cluster if you just start

00:33:02,910 --> 00:33:08,280
instances that are associated with in

00:33:04,620 --> 00:33:11,760
with the ec2 container service then they

00:33:08,280 --> 00:33:14,190
will by definition associate with a the

00:33:11,760 --> 00:33:19,190
default cluster and that's what its name

00:33:14,190 --> 00:33:21,900
is so so if you just start up and and

00:33:19,190 --> 00:33:23,370
run the ACS agent you're running on the

00:33:21,900 --> 00:33:25,080
default cluster you can actually define

00:33:23,370 --> 00:33:27,600
services that are associated with that

00:33:25,080 --> 00:33:30,480
but if you do like I did improve your

00:33:27,600 --> 00:33:32,760
own cluster then you need to actually

00:33:30,480 --> 00:33:35,160
specify in the run command what cluster

00:33:32,760 --> 00:33:38,270
they should attach to what what cluster

00:33:35,160 --> 00:33:38,270
those instances should attach

00:33:44,340 --> 00:33:50,039
so here you see a specific container

00:33:47,580 --> 00:33:53,010
instance where this is really where

00:33:50,039 --> 00:33:55,409
we're ready to define our tasks

00:33:53,010 --> 00:33:58,289
associate those with services and then

00:33:55,409 --> 00:34:00,740
provide metrics for which they can they

00:33:58,289 --> 00:34:00,740
can scale

00:34:09,840 --> 00:34:14,899
that's all I am

00:34:12,349 --> 00:34:17,440
any questions

00:34:14,899 --> 00:34:20,530
so

00:34:17,440 --> 00:34:23,619
the way you walk through look fairly

00:34:20,530 --> 00:34:25,560
manual Atticus is a SS agent to the

00:34:23,619 --> 00:34:28,359
things that everything you can do that

00:34:25,560 --> 00:34:29,889
so refer back I could have done yeah so

00:34:28,359 --> 00:34:32,829
there's two ways there's two ways to

00:34:29,889 --> 00:34:36,220
handle that the first way in the most

00:34:32,829 --> 00:34:39,730
standard way the atomic way is via the

00:34:36,220 --> 00:34:41,679
cloud and yet okay right so configuring

00:34:39,730 --> 00:34:45,579
configuring the actual doctor command I

00:34:41,679 --> 00:34:47,500
expect to be done in flat again yeah the

00:34:45,579 --> 00:34:49,030
other the second way and the way that I

00:34:47,500 --> 00:34:51,460
feel like is gaining traction and I'm

00:34:49,030 --> 00:34:53,740
really excited about is the simple

00:34:51,460 --> 00:34:57,569
services manager so simple service

00:34:53,740 --> 00:35:00,790
management actually allows you to make

00:34:57,569 --> 00:35:02,950
programmatic arts game programmatic

00:35:00,790 --> 00:35:07,240
access to the to the cert to the

00:35:02,950 --> 00:35:10,060
instance without having to provide an

00:35:07,240 --> 00:35:14,319
ssh login so it's done through it's done

00:35:10,060 --> 00:35:16,270
through the metadata that you'll be

00:35:14,319 --> 00:35:18,640
configuration you instead that you

00:35:16,270 --> 00:35:21,280
associate out of the service it's

00:35:18,640 --> 00:35:22,930
running on fedora atomic instances so

00:35:21,280 --> 00:35:24,970
the service would be defining the task

00:35:22,930 --> 00:35:28,000
those tasks would be running on the

00:35:24,970 --> 00:35:29,050
Fedora Thomas's skills the service is it

00:35:28,000 --> 00:35:32,140
also tag

00:35:29,050 --> 00:35:33,580
in food or atomic instances for scale so

00:35:32,140 --> 00:35:35,470
the cluster would actually have its own

00:35:33,580 --> 00:35:36,700
cloud watch metrics that were associated

00:35:35,470 --> 00:35:38,560
with that so if you were running low on

00:35:36,700 --> 00:35:41,110
resources you could actually scale this

00:35:38,560 --> 00:35:43,240
scaling group that's so when you define

00:35:41,110 --> 00:35:47,080
instances you can do you can define

00:35:43,240 --> 00:35:49,180
instances one of two ways for the easy

00:35:47,080 --> 00:35:51,490
to container service you either define

00:35:49,180 --> 00:35:53,140
them individually or you create an auto

00:35:51,490 --> 00:35:54,700
scaling group that has the scaling

00:35:53,140 --> 00:35:58,570
configuration that includes that in

00:35:54,700 --> 00:36:00,400
which time then you provide the so the

00:35:58,570 --> 00:36:04,210
launch configuration has the instance in

00:36:00,400 --> 00:36:06,220
the associated only and then you're the

00:36:04,210 --> 00:36:08,140
auto scaling group actually has the

00:36:06,220 --> 00:36:12,210
metrics and missions and there's just

00:36:08,140 --> 00:36:12,210
some coordination happening

00:36:14,250 --> 00:36:19,760
yes it's also our schedules

00:36:21,509 --> 00:36:25,380
yeah that's the special that's the

00:36:23,399 --> 00:36:28,069
abstracted part that's that sort of the

00:36:25,380 --> 00:36:31,380
public cloud service right is the end

00:36:28,069 --> 00:36:34,499
while the API set itself is open and

00:36:31,380 --> 00:36:39,380
available free for use the key value

00:36:34,499 --> 00:36:39,380
store is is the secret sauce

00:36:42,280 --> 00:36:49,210
so you're presented running fedora

00:36:45,700 --> 00:36:51,930
atomic cost yes with your agent so my

00:36:49,210 --> 00:36:57,820
question is it's possible to do a

00:36:51,930 --> 00:37:00,760
favorite drug cluster with your Amazon

00:36:57,820 --> 00:37:03,130
container service because you're

00:37:00,760 --> 00:37:08,860
providing just contain images in a

00:37:03,130 --> 00:37:13,750
momentum or I know we provide Zen

00:37:08,860 --> 00:37:19,290
virtual machines it's a NEC to an easy

00:37:13,750 --> 00:37:25,410
to yeah yeah not not strictly containers

00:37:19,290 --> 00:37:30,020
yeah so

00:37:25,410 --> 00:37:33,780
your scheduler and key value store are

00:37:30,020 --> 00:37:40,650
so the scheduler we so we have the other

00:37:33,780 --> 00:37:43,230
scheduler itself is it's not disclosed

00:37:40,650 --> 00:37:45,330
right in itself for the for the service

00:37:43,230 --> 00:37:46,830
the associated services but the API is

00:37:45,330 --> 00:37:49,170
available to actually put whatever

00:37:46,830 --> 00:37:51,120
scheduler you want to put in place so if

00:37:49,170 --> 00:37:55,920
you had something that was given been

00:37:51,120 --> 00:37:59,160
packing right mezzo striking the day at

00:37:55,920 --> 00:38:00,609
that on top or even if it was gonna say

00:37:59,160 --> 00:38:03,499
like

00:38:00,609 --> 00:38:06,439
what if I'm using Amazon along with

00:38:03,499 --> 00:38:14,140
something else and my environments based

00:38:06,439 --> 00:38:16,279
on ec2 sorry it's these way together so

00:38:14,140 --> 00:38:21,709
they kind of play in the same space

00:38:16,279 --> 00:38:24,249
right so this is the truck so don't

00:38:21,709 --> 00:38:27,289
without getting too much into the

00:38:24,249 --> 00:38:29,449
comparison what I will say is that when

00:38:27,289 --> 00:38:31,459
the ec2 container service was being

00:38:29,449 --> 00:38:33,410
developed we had a lot of customers that

00:38:31,459 --> 00:38:35,089
we were talking to who we're saying okay

00:38:33,410 --> 00:38:37,699
so we want to get into the container

00:38:35,089 --> 00:38:39,439
space but we don't want to be in the in

00:38:37,699 --> 00:38:45,049
the business of managing our cluster

00:38:39,439 --> 00:38:48,229
right right so this was a decided move

00:38:45,049 --> 00:38:50,299
to create an abstraction so that so that

00:38:48,229 --> 00:38:54,949
we could eliminate the requirement to do

00:38:50,299 --> 00:38:56,929
both but provide an open API so that if

00:38:54,949 --> 00:38:57,979
you decided you wanted to do both you

00:38:56,929 --> 00:39:01,069
would have the ability to actually

00:38:57,979 --> 00:39:03,949
provide back some numbers right so some

00:39:01,069 --> 00:39:06,589
some details in scheduling now as far as

00:39:03,949 --> 00:39:09,890
the sed goes there's already a key value

00:39:06,589 --> 00:39:11,569
store here you could leverage that and

00:39:09,890 --> 00:39:13,640
you wouldn't wouldn't necessarily

00:39:11,569 --> 00:39:15,880
require it there you see buddy will

00:39:13,640 --> 00:39:15,880
float

00:39:15,900 --> 00:39:21,570
personally I had not but I'm looking

00:39:18,970 --> 00:39:21,570
forward to it

00:39:27,190 --> 00:39:32,170
it's just a general observation I'm not

00:39:30,310 --> 00:39:34,930
a ser assigning anyone for work but I

00:39:32,170 --> 00:39:38,560
would be interested speaker laughing all

00:39:34,930 --> 00:39:40,930
right it would be interesting for me

00:39:38,560 --> 00:39:42,339
from atomic perspective to help

00:39:40,930 --> 00:39:44,560
illuminate the stuff that you had to go

00:39:42,339 --> 00:39:46,690
through where you dr. pull the latest

00:39:44,560 --> 00:39:49,150
version of your agent and then I have to

00:39:46,690 --> 00:39:50,650
save an image I think okay you're

00:39:49,150 --> 00:39:52,060
nodding as well yes it would be nice it

00:39:50,650 --> 00:39:54,670
would be nice if there was an army that

00:39:52,060 --> 00:39:56,349
was just as a hunch Tomic with that and

00:39:54,670 --> 00:39:58,210
we have ways to have attention cashing

00:39:56,349 --> 00:40:01,000
those that's fantastic yeah so we have

00:39:58,210 --> 00:40:02,050
oh yeah that's great and that's actually

00:40:01,000 --> 00:40:04,000
something that i was really i mean

00:40:02,050 --> 00:40:05,500
that's really what what i wanted to why

00:40:04,000 --> 00:40:07,420
I wanted to talk about this because I

00:40:05,500 --> 00:40:11,319
think there's a great opportunity here

00:40:07,420 --> 00:40:17,829
for some for some strong momentum in bcs

00:40:11,319 --> 00:40:18,970
optimized an ecs optimized damage we

00:40:17,829 --> 00:40:20,349
have we have that in a couple of

00:40:18,970 --> 00:40:24,730
different places right so the way that

00:40:20,349 --> 00:40:27,579
the so obviously there's a lot of strong

00:40:24,730 --> 00:40:29,950
time between shorty I'm surprised but

00:40:27,579 --> 00:40:32,380
says so you promote open source yeah

00:40:29,950 --> 00:40:34,480
exactly and that's exactly where I'm

00:40:32,380 --> 00:40:36,520
coming from as well right is that I want

00:40:34,480 --> 00:40:38,470
to see I want to see this strong

00:40:36,520 --> 00:40:40,780
adoption of fedora atomic in the spaces

00:40:38,470 --> 00:40:44,349
we were weird people who remind us that

00:40:40,780 --> 00:40:45,980
every every additional step to get on

00:40:44,349 --> 00:40:48,650
board

00:40:45,980 --> 00:40:50,450
ninety percent of people precisely and

00:40:48,650 --> 00:40:53,030
that's exactly what I want to do is take

00:40:50,450 --> 00:40:56,690
take this opportunity to provide people

00:40:53,030 --> 00:41:02,530
with a very simple level playing field

00:40:56,690 --> 00:41:02,530
for for defining dr. related tasks

00:41:06,080 --> 00:41:11,430
open source project

00:41:09,400 --> 00:41:11,430
Oh

00:41:11,820 --> 00:41:21,090
so obviously we maintain the repo but

00:41:15,960 --> 00:41:22,650
pull requests are accepted yeah now very

00:41:21,090 --> 00:41:27,000
happy to have purchased a strong

00:41:22,650 --> 00:41:28,830
participation we have probably about 15

00:41:27,000 --> 00:41:32,130
strong contributors right everything

00:41:28,830 --> 00:41:34,820
from the outside and then continued a

00:41:32,130 --> 00:41:34,820
dedicated team

00:41:38,930 --> 00:41:43,990
and we also tried their son

00:41:41,619 --> 00:41:46,600
stock to the world

00:41:43,990 --> 00:41:48,670
yes so stock trade or was the first

00:41:46,600 --> 00:41:52,090
thing that I actually deployed easy to

00:41:48,670 --> 00:41:56,020
container service with and that would

00:41:52,090 --> 00:41:58,240
that was just a super easy obviously

00:41:56,020 --> 00:41:59,710
same problem exists right which is that

00:41:58,240 --> 00:42:02,160
you have multiple steps you have to go

00:41:59,710 --> 00:42:04,540
through in order to to get there and

00:42:02,160 --> 00:42:08,170
ultimately I would like to see any CS

00:42:04,540 --> 00:42:10,810
optimized image we love there somebody

00:42:08,170 --> 00:42:15,940
from amazon participate I'm right here

00:42:10,810 --> 00:42:18,040
I'm matches accepting who they say by

00:42:15,940 --> 00:42:23,250
the way in five minutes statement 61

00:42:18,040 --> 00:42:34,450
anybody else any general questions I

00:42:23,250 --> 00:42:36,550
mean while I'm standing up so then we'll

00:42:34,450 --> 00:42:39,520
run it'll run both so it will run both

00:42:36,550 --> 00:42:44,740
ways and it could be it could literally

00:42:39,520 --> 00:42:46,930
be included in the base LX there is a

00:42:44,740 --> 00:42:52,390
there is a configuration for that it's

00:42:46,930 --> 00:42:53,890
it's just a go package right so it lends

00:42:52,390 --> 00:42:56,350
itself to being run as a container

00:42:53,890 --> 00:43:01,150
because this is fedora atomic and not

00:42:56,350 --> 00:43:03,609
fedora proper right the the the idea was

00:43:01,150 --> 00:43:05,500
to follow Collins tenants right and

00:43:03,609 --> 00:43:11,020
ensure that it's that it's running in

00:43:05,500 --> 00:43:13,450
the appropriate package model

00:43:11,020 --> 00:43:16,180
but it's it's all open source so we

00:43:13,450 --> 00:43:18,460
wanted to roll that you have any idea my

00:43:16,180 --> 00:43:21,090
company how much that would add

00:43:18,460 --> 00:43:21,090
size-wise

00:43:25,000 --> 00:43:36,430
I think an 11-man so not inconsiderable

00:43:31,710 --> 00:43:39,670
emacs yeah so that's your guards there

00:43:36,430 --> 00:43:42,340
yes I'm just thinking I'm just basing

00:43:39,670 --> 00:43:45,580
this on a container size at the top of

00:43:42,340 --> 00:43:50,070
my head but if it might be smaller if

00:43:45,580 --> 00:43:50,070
you've got all of your libraries now

00:43:51,570 --> 00:43:54,870
that's a guess

00:43:55,099 --> 00:43:57,999
anything else

00:43:59,050 --> 00:44:02,310

YouTube URL: https://www.youtube.com/watch?v=2odhrtnazQU


