Title: Auto-updates by design: porting Container Linux model to Fedora CoreOS - Luca Bruno - Flock 2019
Publication date: 2019-08-28
Playlist: Flock 2019
Description: 
	Container Linux distribution brought the "auto-updates by design" model to the Linux server ecosystem. The upcoming Fedora CoreOS auto-update architecture is designed and built based on that experience and on the lessons learned from it. This walk will introduce both architectures, covering their technical details and comparing their differences as part of a common evolutionary journey.

--
Recordings of talks at Flock are a community effort. Unfortunately not everything works perfectly every time. If you're interested in helping us improve, let us know.
Captions: 
	00:00:01,750 --> 00:00:07,999
okay so my name is Luca Bruno I'm gonna

00:00:05,569 --> 00:00:10,519
talk about out of days I'm gonna do a

00:00:07,999 --> 00:00:11,750
kind of like mix talk it's mostly

00:00:10,519 --> 00:00:15,259
technical but there is also a bit of

00:00:11,750 --> 00:00:18,350
like history so the idea is in container

00:00:15,259 --> 00:00:20,930
linux as a different distribution from

00:00:18,350 --> 00:00:24,340
fedora we were used to have out updates

00:00:20,930 --> 00:00:26,840
that were automatically going to nodes

00:00:24,340 --> 00:00:28,460
after chorus as a company has been

00:00:26,840 --> 00:00:31,190
acquired by reddit we are trying to port

00:00:28,460 --> 00:00:33,320
the same model to fedora cross and we're

00:00:31,190 --> 00:00:35,089
kind of like redesigning part of it

00:00:33,320 --> 00:00:37,370
learning a few lessons and kind of like

00:00:35,089 --> 00:00:39,170
you're implementing things because they

00:00:37,370 --> 00:00:41,780
didn't we couldn't manage to port them

00:00:39,170 --> 00:00:44,539
as they were from container linux or 2d

00:00:41,780 --> 00:00:49,129
Fedora core s1 and this is more or less

00:00:44,539 --> 00:00:52,609
the whole talk I am here this is my

00:00:49,129 --> 00:00:55,219
second vlog it's pretty nice I'm mostly

00:00:52,609 --> 00:00:58,089
a single developer I take care of a lot

00:00:55,219 --> 00:01:01,039
of things at the u.s. level itself I

00:00:58,089 --> 00:01:04,730
used to work as I say that Korres as a

00:01:01,039 --> 00:01:06,350
company and basing Burdine after the

00:01:04,730 --> 00:01:07,910
acquisition now I'm working a trade that

00:01:06,350 --> 00:01:10,790
after the other position now I'm working

00:01:07,910 --> 00:01:13,160
at IBM after the next acquisition I

00:01:10,790 --> 00:01:14,810
don't know I still plan to be based in

00:01:13,160 --> 00:01:18,050
Berlin so if you are passing by one

00:01:14,810 --> 00:01:19,880
meter before doing this I was into

00:01:18,050 --> 00:01:21,830
research mostly like security research

00:01:19,880 --> 00:01:25,490
and after that I was working as a

00:01:21,830 --> 00:01:27,230
security engineer that's it so the

00:01:25,490 --> 00:01:29,630
overview of these talks is pretty much

00:01:27,230 --> 00:01:32,690
we're going to split in three parts the

00:01:29,630 --> 00:01:34,520
first one is an overview of the

00:01:32,690 --> 00:01:37,720
container Linux model which I guess for

00:01:34,520 --> 00:01:40,180
most of you is movies you know let's say

00:01:37,720 --> 00:01:42,410
then I'm gonna pretty much like recaps

00:01:40,180 --> 00:01:44,690
all the friction point that we found

00:01:42,410 --> 00:01:47,630
when using this model when applying this

00:01:44,690 --> 00:01:49,820
modern to the real world and so we learn

00:01:47,630 --> 00:01:51,530
a few lessons and we are trying to kind

00:01:49,820 --> 00:01:53,540
of like avoid the same problems when

00:01:51,530 --> 00:01:55,490
doing the same for fedora crow ass and

00:01:53,540 --> 00:01:57,050
then I'm gonna introduce like what we

00:01:55,490 --> 00:01:58,580
currently have as a model for fedora

00:01:57,050 --> 00:02:00,620
crows which is still a work in progress

00:01:58,580 --> 00:02:02,660
we are still like deploying and make

00:02:00,620 --> 00:02:04,640
implementing part of this but at least

00:02:02,660 --> 00:02:07,670
we have a rough idea of what we want to

00:02:04,640 --> 00:02:12,709
do so let's start from the container

00:02:07,670 --> 00:02:13,420
nukes model this picture is a diagram of

00:02:12,709 --> 00:02:17,290
our

00:02:13,420 --> 00:02:18,730
word in container lyrics and it's the

00:02:17,290 --> 00:02:22,830
world is mostly split in three parts

00:02:18,730 --> 00:02:22,830
there is something at the top which is

00:02:25,680 --> 00:02:31,090
there is something at the top which is

00:02:27,520 --> 00:02:32,709
our own infrastructure so what we as a

00:02:31,090 --> 00:02:34,750
company as karai's company we were

00:02:32,709 --> 00:02:37,360
running which was mostly like the back

00:02:34,750 --> 00:02:39,100
end for the out updates and then there

00:02:37,360 --> 00:02:42,970
is a bottom part which is instead like

00:02:39,100 --> 00:02:45,220
what our users were running and those

00:02:42,970 --> 00:02:47,440
users the context of container linux is

00:02:45,220 --> 00:02:49,870
that we provide a distribution for

00:02:47,440 --> 00:02:52,750
clusters so those users were running

00:02:49,870 --> 00:02:56,410
like nodes with container linux as part

00:02:52,750 --> 00:02:58,680
of a larger cluster so we on the bottom

00:02:56,410 --> 00:03:01,720
part this cluster which we kind of like

00:02:58,680 --> 00:03:03,970
generalize into a single machine which

00:03:01,720 --> 00:03:06,130
is the one on the right and then the

00:03:03,970 --> 00:03:08,680
rest of the cluster on the on the left

00:03:06,130 --> 00:03:10,239
side so if we focus on every single

00:03:08,680 --> 00:03:12,580
machine every single machine is

00:03:10,239 --> 00:03:14,950
basically running at least two component

00:03:12,580 --> 00:03:18,100
after these three they are always

00:03:14,950 --> 00:03:20,109
running the update energy which is the

00:03:18,100 --> 00:03:22,299
top level component which is connected

00:03:20,109 --> 00:03:25,000
to our infrastructure and then they are

00:03:22,299 --> 00:03:27,400
running at least one of those two one

00:03:25,000 --> 00:03:30,220
between locksmith and clue which is a

00:03:27,400 --> 00:03:32,530
component which talks to update engine

00:03:30,220 --> 00:03:34,750
in order to orchestrates applying

00:03:32,530 --> 00:03:37,060
reboots and those confidence they

00:03:34,750 --> 00:03:39,400
normally have some kind of like cluster

00:03:37,060 --> 00:03:41,440
our logic so they defer to the rest of

00:03:39,400 --> 00:03:42,400
the cluster for actually orchestrating

00:03:41,440 --> 00:03:45,010
these kind of reboots

00:03:42,400 --> 00:03:47,410
and I'm gonna get into more details

00:03:45,010 --> 00:03:48,700
about each one in a second just remember

00:03:47,410 --> 00:03:50,799
that there is this split between like

00:03:48,700 --> 00:03:54,310
our architecture every single machine

00:03:50,799 --> 00:03:56,170
and the and the infrastructure for the

00:03:54,310 --> 00:03:59,140
cluster that the users are actually

00:03:56,170 --> 00:04:00,489
running so we start from the top from

00:03:59,140 --> 00:04:04,150
the top part which is like the server

00:04:00,489 --> 00:04:06,760
side our back ends on that side we have

00:04:04,150 --> 00:04:09,130
one component or two components when you

00:04:06,760 --> 00:04:12,280
on how you look at that which is core

00:04:09,130 --> 00:04:14,769
update corrupted is a server it is for

00:04:12,280 --> 00:04:16,660
not run history tango it is an

00:04:14,769 --> 00:04:18,220
implementation of the Homa protocol

00:04:16,660 --> 00:04:22,300
which is a protocol for providing

00:04:18,220 --> 00:04:24,340
updates and it's based on XML from our

00:04:22,300 --> 00:04:25,570
side we use this as a normal web

00:04:24,340 --> 00:04:27,160
application with

00:04:25,570 --> 00:04:29,020
javascript interface and we also have

00:04:27,160 --> 00:04:31,180
some common line interface for the same

00:04:29,020 --> 00:04:32,770
api the key point is that this is

00:04:31,180 --> 00:04:34,410
proprietary code so this is something

00:04:32,770 --> 00:04:38,460
that is a company we were actively like

00:04:34,410 --> 00:04:42,220
soul selling to customers as a service

00:04:38,460 --> 00:04:44,440
except for like the large the large

00:04:42,220 --> 00:04:46,180
majority of user that were using our

00:04:44,440 --> 00:04:48,460
free offering in which case they were

00:04:46,180 --> 00:04:51,400
just like consuming did this but without

00:04:48,460 --> 00:04:54,490
access to the web interface or to the

00:04:51,400 --> 00:04:55,870
product to the product itself there are

00:04:54,490 --> 00:04:57,670
third-party implementation with this

00:04:55,870 --> 00:05:01,240
which is why I have like two names at

00:04:57,670 --> 00:05:03,040
the top and an interesting part of this

00:05:01,240 --> 00:05:05,830
is that it's a traditional application

00:05:03,040 --> 00:05:08,260
so it has like a back-end a front-end

00:05:05,830 --> 00:05:11,140
and some database which is Postgres if

00:05:08,260 --> 00:05:13,690
I'm not wrong and it is like stateful at

00:05:11,140 --> 00:05:16,210
every single level and it's not even

00:05:13,690 --> 00:05:17,860
like a distributed staff it's actually

00:05:16,210 --> 00:05:21,700
like linked to a specific database

00:05:17,860 --> 00:05:23,500
instance and as I said like this is the

00:05:21,700 --> 00:05:25,690
server side for the out updates but at

00:05:23,500 --> 00:05:29,100
the same time by the way that Yama

00:05:25,690 --> 00:05:31,960
protocol works we can also like track

00:05:29,100 --> 00:05:33,490
client statistic and usage metric and so

00:05:31,960 --> 00:05:36,280
that we can actually serve different

00:05:33,490 --> 00:05:39,280
updates to different clients this is

00:05:36,280 --> 00:05:41,650
both good and bad the problem is that

00:05:39,280 --> 00:05:43,630
this kind of architecture has several

00:05:41,650 --> 00:05:45,600
issues when you start distributing the

00:05:43,630 --> 00:05:48,940
setups especially like in the database

00:05:45,600 --> 00:05:50,740
and in general like this client tracking

00:05:48,940 --> 00:05:53,680
client statistic is very stressful on

00:05:50,740 --> 00:05:55,300
the database and this is fine in most

00:05:53,680 --> 00:05:57,820
cases let's say you can just like scale

00:05:55,300 --> 00:05:59,080
your resources as you as you prefer but

00:05:57,820 --> 00:06:00,550
at the same time like this is a key

00:05:59,080 --> 00:06:03,970
component of the auto-update

00:06:00,550 --> 00:06:06,250
infrastructure so the more pains and the

00:06:03,970 --> 00:06:09,220
more stress we can get out of this path

00:06:06,250 --> 00:06:12,160
the better it is and then we proceed

00:06:09,220 --> 00:06:13,930
like the counterpart of DS on the client

00:06:12,160 --> 00:06:16,030
side so on every single machine on every

00:06:13,930 --> 00:06:18,730
single node is the data engine which is

00:06:16,030 --> 00:06:21,100
a client expectedly for the same

00:06:18,730 --> 00:06:24,490
protocol that the update engine provides

00:06:21,100 --> 00:06:27,370
so it's a steal like an XML based on our

00:06:24,490 --> 00:06:29,080
protocol this company is pretty much a

00:06:27,370 --> 00:06:31,990
kitchen sink like it takes curve

00:06:29,080 --> 00:06:34,540
everything from periodically polling for

00:06:31,990 --> 00:06:36,340
new updates information downloading

00:06:34,540 --> 00:06:38,870
these new updates information

00:06:36,340 --> 00:06:41,930
applying them to the system which

00:06:38,870 --> 00:06:43,460
our case means we have an a/b scheme for

00:06:41,930 --> 00:06:45,800
the user partition so we always have

00:06:43,460 --> 00:06:48,050
like an active partition with the active

00:06:45,800 --> 00:06:50,870
slash user and a passive partition with

00:06:48,050 --> 00:06:53,030
the passive slash user and this company

00:06:50,870 --> 00:06:55,730
is in charge of like right into one

00:06:53,030 --> 00:06:57,650
activating it rebooting into the proper

00:06:55,730 --> 00:07:00,170
one or going back to the other one if

00:06:57,650 --> 00:07:03,590
something went wrong this is kind of

00:07:00,170 --> 00:07:05,180
like a huge I mean not not the largest

00:07:03,590 --> 00:07:07,400
software that I've seen was like it's

00:07:05,180 --> 00:07:09,680
kind of like a big project it's written

00:07:07,400 --> 00:07:12,500
in C++ originally it was written by

00:07:09,680 --> 00:07:14,690
Google there are multiple Forks of these

00:07:12,500 --> 00:07:17,540
like in the public but also like

00:07:14,690 --> 00:07:19,280
internally at Google our own is just

00:07:17,540 --> 00:07:21,380
like it's under our github organization

00:07:19,280 --> 00:07:24,290
and it's just like our own maintained

00:07:21,380 --> 00:07:26,630
fork at some point in the past but it's

00:07:24,290 --> 00:07:31,490
not ibly used by all the chrome OS for

00:07:26,630 --> 00:07:32,840
example for all the Chromebooks sorry so

00:07:31,490 --> 00:07:34,460
the key point is that this is like an

00:07:32,840 --> 00:07:37,970
high complexity piece of software is

00:07:34,460 --> 00:07:39,920
written in C++ it owns every single

00:07:37,970 --> 00:07:41,330
aspect of the art update story and from

00:07:39,920 --> 00:07:43,310
our point of view is kind of like we

00:07:41,330 --> 00:07:46,250
don't have unfortunately as a start-up

00:07:43,310 --> 00:07:48,320
we don't have like enough workforce to

00:07:46,250 --> 00:07:51,170
properly maintain this one so we almost

00:07:48,320 --> 00:07:54,050
like using it as we initially forked it

00:07:51,170 --> 00:07:55,970
but we touch it very very rarely so it's

00:07:54,050 --> 00:07:57,500
effectively like pretty much on maintain

00:07:55,970 --> 00:08:00,890
software from a point of view that it

00:07:57,500 --> 00:08:03,760
works and we don't touch it then there

00:08:00,890 --> 00:08:07,820
is the last piece of this story which is

00:08:03,760 --> 00:08:09,650
whenever we provide an update to some

00:08:07,820 --> 00:08:12,260
machine this machine is going to try to

00:08:09,650 --> 00:08:14,630
apply this update and then reboot in

00:08:12,260 --> 00:08:17,090
order to actually activate this result

00:08:14,630 --> 00:08:20,480
update this is because we follow like an

00:08:17,090 --> 00:08:22,790
atomic system were an update updates the

00:08:20,480 --> 00:08:24,470
whole OS as a whole without like single

00:08:22,790 --> 00:08:26,780
packages updates so in order to actually

00:08:24,470 --> 00:08:29,600
use this new update you need to reboot

00:08:26,780 --> 00:08:31,250
into that there is a problem with this

00:08:29,600 --> 00:08:33,200
model which is if we start pushing

00:08:31,250 --> 00:08:35,330
updates to all the nodes and you have

00:08:33,200 --> 00:08:37,040
like 20 nodes for a availability of your

00:08:35,330 --> 00:08:38,270
cluster and all of them are applying the

00:08:37,040 --> 00:08:39,920
same update at the same time and

00:08:38,270 --> 00:08:42,050
rebooting your availability just

00:08:39,920 --> 00:08:44,169
immediately disappear because then you

00:08:42,050 --> 00:08:45,790
have a downtime or the whole class

00:08:44,169 --> 00:08:48,430
so something that sits on the side of

00:08:45,790 --> 00:08:50,230
this update engine is another company

00:08:48,430 --> 00:08:52,180
which takes care of reboot coordination

00:08:50,230 --> 00:08:54,820
and it's basically like something which

00:08:52,180 --> 00:08:57,970
is in charge of deciding which node can

00:08:54,820 --> 00:08:59,260
be down rebooting at what time so

00:08:57,970 --> 00:09:01,329
initially we wrote something called

00:08:59,260 --> 00:09:03,850
locksmith which is a globally that is

00:09:01,329 --> 00:09:05,829
part of the operating system itself it

00:09:03,850 --> 00:09:08,199
provides a few strategy for rebooting

00:09:05,829 --> 00:09:10,990
like reboot immediately or reboot within

00:09:08,199 --> 00:09:12,130
a window of maintenance windows or

00:09:10,990 --> 00:09:14,769
reboot

00:09:12,130 --> 00:09:16,959
whenever you get some semaphore locked

00:09:14,769 --> 00:09:19,709
in a distributed database for example on

00:09:16,959 --> 00:09:22,420
Etsy - that was our initial presentation

00:09:19,709 --> 00:09:24,130
then as the move as the world moved

00:09:22,420 --> 00:09:25,480
forward into the kubernetes world

00:09:24,130 --> 00:09:28,600
somebody came up with like a different

00:09:25,480 --> 00:09:30,550
requirement which is kubernetes already

00:09:28,600 --> 00:09:32,980
have a distributed database and an API

00:09:30,550 --> 00:09:35,410
for like putting a retrieving object in

00:09:32,980 --> 00:09:37,029
there which is the kubernetes api but i

00:09:35,410 --> 00:09:39,190
don't have access to the HDD cluster

00:09:37,029 --> 00:09:41,589
itself because it's normally part the

00:09:39,190 --> 00:09:43,089
internal implementation of kubernetes so

00:09:41,589 --> 00:09:45,040
we brought another component which is

00:09:43,089 --> 00:09:47,380
similar but different which is a

00:09:45,040 --> 00:09:48,760
containerized version of the same logic

00:09:47,380 --> 00:09:52,779
which is called a container Linux

00:09:48,760 --> 00:09:54,699
operator or clue so we basically push

00:09:52,779 --> 00:09:56,920
this logic out of the operating system

00:09:54,699 --> 00:10:00,310
into a container this container is

00:09:56,920 --> 00:10:03,100
scheduled itself by kubernetes and the

00:10:00,310 --> 00:10:04,360
logic is that now this kind of like

00:10:03,100 --> 00:10:06,490
fleet-wide

00:10:04,360 --> 00:10:08,230
reboot coordination is done via

00:10:06,490 --> 00:10:09,970
kubernetes objects so we are pushing

00:10:08,230 --> 00:10:12,100
object - kubernetes in order to know

00:10:09,970 --> 00:10:14,740
which node is allowed to reboot and want

00:10:12,100 --> 00:10:16,480
to reboot and we are retrieving these

00:10:14,740 --> 00:10:19,180
nodes in order to get information about

00:10:16,480 --> 00:10:21,600
the state of the reboot across the

00:10:19,180 --> 00:10:24,010
cluster and this is this is actually

00:10:21,600 --> 00:10:25,690
this architecture actually requires two

00:10:24,010 --> 00:10:27,579
components at this point one which is

00:10:25,690 --> 00:10:29,649
the update operator itself so the

00:10:27,579 --> 00:10:31,540
manager and something that is in charge

00:10:29,649 --> 00:10:33,610
of like rebooting every single node so

00:10:31,540 --> 00:10:35,740
something like an agent on each node and

00:10:33,610 --> 00:10:37,420
that one is another container which is

00:10:35,740 --> 00:10:40,209
deployed as a daemon set on every

00:10:37,420 --> 00:10:42,130
machine so the key point in this slide

00:10:40,209 --> 00:10:44,500
is that those two components they

00:10:42,130 --> 00:10:47,110
basically implement a very very similar

00:10:44,500 --> 00:10:48,220
logic but they share pretty much no code

00:10:47,110 --> 00:10:49,959
between them because they run in

00:10:48,220 --> 00:10:51,579
completely different context and they

00:10:49,959 --> 00:10:53,949
also take completely different kind of

00:10:51,579 --> 00:10:55,820
like API and logic and stuff so there is

00:10:53,949 --> 00:10:59,660
a lot of overlaps but not all

00:10:55,820 --> 00:11:01,700
like compatibility and logic sharing and

00:10:59,660 --> 00:11:04,760
let's show him back take the picture

00:11:01,700 --> 00:11:06,860
that's just what we saw so far and now

00:11:04,760 --> 00:11:08,420
we're gonna progress it's like that was

00:11:06,860 --> 00:11:10,130
the point when reddit acquired core OS

00:11:08,420 --> 00:11:12,380
and it was working pretty well except

00:11:10,130 --> 00:11:14,120
for a few friction point this friction

00:11:12,380 --> 00:11:15,920
point were a bit like everywhere in this

00:11:14,120 --> 00:11:17,870
picture like the first one was the

00:11:15,920 --> 00:11:20,600
server side server side was proprietary

00:11:17,870 --> 00:11:22,490
software it didn't honestly get a lot of

00:11:20,600 --> 00:11:24,170
like revenues from that but it was still

00:11:22,490 --> 00:11:25,640
like something that we were selling the

00:11:24,170 --> 00:11:28,430
problem is that people also want to use

00:11:25,640 --> 00:11:30,230
it without paying royalties to us which

00:11:28,430 --> 00:11:32,390
is fine so they're yet to implement

00:11:30,230 --> 00:11:34,490
basically exactly the same stuff as open

00:11:32,390 --> 00:11:37,900
source which is a failure if you are if

00:11:34,490 --> 00:11:37,900
you care about like free software

00:11:37,960 --> 00:11:43,100
another like goal so these are kind of

00:11:40,940 --> 00:11:44,830
like goals that are derived from those

00:11:43,100 --> 00:11:47,390
friction points another goal was like

00:11:44,830 --> 00:11:49,790
this service was taking care of tracking

00:11:47,390 --> 00:11:51,830
stuff as well on top of like serving

00:11:49,790 --> 00:11:53,360
updates and that was and that part was

00:11:51,830 --> 00:11:55,400
actually causing causing us more

00:11:53,360 --> 00:11:57,830
friction so we can actually try to

00:11:55,400 --> 00:11:59,720
decouple those two problems and say okay

00:11:57,830 --> 00:12:01,310
if the out updates were working well and

00:11:59,720 --> 00:12:02,950
the problem were on the tracking part

00:12:01,310 --> 00:12:07,310
let's move the tracking somewhere else

00:12:02,950 --> 00:12:09,650
that way another idea that we had is

00:12:07,310 --> 00:12:11,930
kind of like we try to avoid exploiting

00:12:09,650 --> 00:12:13,100
an explosion in cardinality in the

00:12:11,930 --> 00:12:15,140
database because we are not tracking

00:12:13,100 --> 00:12:17,930
stuff anymore actively we can try to

00:12:15,140 --> 00:12:19,520
make it like as much stateless as

00:12:17,930 --> 00:12:23,150
possible so getting rid of all the

00:12:19,520 --> 00:12:25,190
statefulness were possible and then most

00:12:23,150 --> 00:12:27,290
of these was most of this model was

00:12:25,190 --> 00:12:30,650
fitting well for a company that was

00:12:27,290 --> 00:12:31,820
doing stuff like internally so people

00:12:30,650 --> 00:12:33,410
outside of the company were just

00:12:31,820 --> 00:12:35,690
consuming out updates they were not in

00:12:33,410 --> 00:12:38,180
charge of like looking how we were

00:12:35,690 --> 00:12:39,770
making release except for like they know

00:12:38,180 --> 00:12:41,780
that these contents goes inside this

00:12:39,770 --> 00:12:43,580
release but when we are actually taking

00:12:41,780 --> 00:12:46,100
a release or how we are rolling out to

00:12:43,580 --> 00:12:49,130
the to the cast to the to the cluster

00:12:46,100 --> 00:12:50,600
that's not our their concern so there

00:12:49,130 --> 00:12:54,050
was a lot of like private and hidden

00:12:50,600 --> 00:12:56,090
State which was as let's say slightly a

00:12:54,050 --> 00:12:58,490
problem for the for the general audience

00:12:56,090 --> 00:13:00,680
but also problem for us because then we

00:12:58,490 --> 00:13:02,090
started like selling instances of this

00:13:00,680 --> 00:13:04,370
that were like in other environments

00:13:02,090 --> 00:13:05,840
like in our gapped environment and so

00:13:04,370 --> 00:13:08,510
there was always like some kind of

00:13:05,840 --> 00:13:09,520
manual coordination point for syncing

00:13:08,510 --> 00:13:10,900
all the database and

00:13:09,520 --> 00:13:12,550
thinking all the customers and kind of

00:13:10,900 --> 00:13:16,090
like providing these kind of like

00:13:12,550 --> 00:13:17,830
bundles which was it was working if our

00:13:16,090 --> 00:13:19,450
company but if you are a community is

00:13:17,830 --> 00:13:22,840
not something that you are really

00:13:19,450 --> 00:13:25,390
looking forward for next step is like in

00:13:22,840 --> 00:13:26,650
the client side in the client side it

00:13:25,390 --> 00:13:29,560
was working pretty well we didn't have

00:13:26,650 --> 00:13:31,360
major problem the major problem was so

00:13:29,560 --> 00:13:33,250
the only problem was kind of like

00:13:31,360 --> 00:13:35,140
maintaining this beast like it's

00:13:33,250 --> 00:13:36,580
something that we got from from Google

00:13:35,140 --> 00:13:39,280
was working pretty well but we

00:13:36,580 --> 00:13:41,980
definitely don't have like the visible

00:13:39,280 --> 00:13:43,990
per team that Google have dedicated to

00:13:41,980 --> 00:13:46,180
this so we ended up with kind of like

00:13:43,990 --> 00:13:48,490
something very complex written in C++

00:13:46,180 --> 00:13:51,490
that is not one of our main languages

00:13:48,490 --> 00:13:52,960
and effectively like not getting any

00:13:51,490 --> 00:13:55,390
kind of development or stuff which is

00:13:52,960 --> 00:13:57,010
good and bad depending on which stage

00:13:55,390 --> 00:14:00,460
you are of this journey throughout

00:13:57,010 --> 00:14:02,710
updates one actual architectural problem

00:14:00,460 --> 00:14:04,600
is that everything was coupled into

00:14:02,710 --> 00:14:06,790
these update engine like it was doing

00:14:04,600 --> 00:14:08,230
discovery it was doing the deployments

00:14:06,790 --> 00:14:12,280
he was doing roll backs and everything

00:14:08,230 --> 00:14:15,010
else and it was also like mostly

00:14:12,280 --> 00:14:17,380
interactive like it offered a d-bus API

00:14:15,010 --> 00:14:19,360
that's how you do everything and it's

00:14:17,380 --> 00:14:22,360
kind of like you cannot configure this

00:14:19,360 --> 00:14:25,480
stuff initially on first boot in a

00:14:22,360 --> 00:14:26,980
declarative way this was this was built

00:14:25,480 --> 00:14:28,630
before system D so even the

00:14:26,980 --> 00:14:31,390
configuration itself was not like taking

00:14:28,630 --> 00:14:33,220
care of like overlays and snip and sorry

00:14:31,390 --> 00:14:35,140
and drop pins and things like that so

00:14:33,220 --> 00:14:39,280
it's kind of like it's a traditional

00:14:35,140 --> 00:14:42,070
old-style Linux Linux daemon and it's

00:14:39,280 --> 00:14:43,660
not particularly easy for administration

00:14:42,070 --> 00:14:45,070
to monitors which means that if

00:14:43,660 --> 00:14:46,840
something goes back with the out updates

00:14:45,070 --> 00:14:49,450
and there is a rollback and stuff like

00:14:46,840 --> 00:14:51,900
that you are kind of like supposed to

00:14:49,450 --> 00:14:54,400
manually look into every single node

00:14:51,900 --> 00:14:56,530
interactively or build your own tool to

00:14:54,400 --> 00:14:58,780
mate to monitor this stuff so these are

00:14:56,530 --> 00:15:00,460
all like goals friction that we saw

00:14:58,780 --> 00:15:04,570
goals that were trying to kind of like

00:15:00,460 --> 00:15:07,090
achieve when redoing this stuff and the

00:15:04,570 --> 00:15:10,210
last step is the reboot coordination the

00:15:07,090 --> 00:15:11,800
problem here is that just in our small

00:15:10,210 --> 00:15:13,090
world we already ended up with like two

00:15:11,800 --> 00:15:15,400
implementation of these that were

00:15:13,090 --> 00:15:17,950
running in different scenarios and the

00:15:15,400 --> 00:15:21,340
problem then the reason why we ended up

00:15:17,950 --> 00:15:22,899
there is that we initially built one

00:15:21,340 --> 00:15:25,240
implementation with the assumption it

00:15:22,899 --> 00:15:27,459
fitting in every single use case and

00:15:25,240 --> 00:15:28,839
then we had to actually move most of

00:15:27,459 --> 00:15:31,629
these logics somewhere else because it

00:15:28,839 --> 00:15:33,610
was not fitting another use case that we

00:15:31,629 --> 00:15:35,230
had so this time we're kind of like

00:15:33,610 --> 00:15:37,540
trying to future-proof the design a bit

00:15:35,230 --> 00:15:40,269
more so moving most of this logic of

00:15:37,540 --> 00:15:42,189
toast itself so that the customer if is

00:15:40,269 --> 00:15:44,949
running sorry the consumer if it's

00:15:42,189 --> 00:15:46,360
running kubernetes or any other cluster

00:15:44,949 --> 00:15:48,699
orchestration they can actually decide

00:15:46,360 --> 00:15:50,889
which back-end to use we are trying to

00:15:48,699 --> 00:15:52,600
future-proof a bit the design the

00:15:50,889 --> 00:15:55,569
copying it from a specific database

00:15:52,600 --> 00:15:57,429
because before it was only a CD or only

00:15:55,569 --> 00:15:57,970
kubernetes and some specific version

00:15:57,429 --> 00:16:00,879
kubernetes

00:15:57,970 --> 00:16:02,980
and we are at by doing this we hope to

00:16:00,879 --> 00:16:05,350
allow other people to implement like

00:16:02,980 --> 00:16:07,089
their own backends logic so if you want

00:16:05,350 --> 00:16:08,800
to implement like a normal back-end or a

00:16:07,089 --> 00:16:13,809
posterior back-end you can do whatever

00:16:08,800 --> 00:16:15,220
you want okay and the last step is not

00:16:13,809 --> 00:16:16,749
really technical it's more like the

00:16:15,220 --> 00:16:19,629
human process the human process behind

00:16:16,749 --> 00:16:21,550
this was again like design for a company

00:16:19,629 --> 00:16:23,860
which means that you basically trust

00:16:21,550 --> 00:16:26,920
every single component you don't need to

00:16:23,860 --> 00:16:28,779
have like 100% audit or review of what's

00:16:26,920 --> 00:16:30,399
going on because you know the few people

00:16:28,779 --> 00:16:32,860
that have access to stuff and you always

00:16:30,399 --> 00:16:35,850
can reconstruct what was going on

00:16:32,860 --> 00:16:38,559
via logs chats email and everything else

00:16:35,850 --> 00:16:41,649
which is not normally true for a

00:16:38,559 --> 00:16:43,389
distributed open-source team there were

00:16:41,649 --> 00:16:45,220
a lot of manual operation and manual

00:16:43,389 --> 00:16:46,509
coordination via chat like now when

00:16:45,220 --> 00:16:48,129
doing these now when doing that now

00:16:46,509 --> 00:16:50,290
please review these now please give me

00:16:48,129 --> 00:16:52,179
access to that and so on it so far

00:16:50,290 --> 00:16:54,519
and there was not a central public

00:16:52,179 --> 00:16:56,679
source of truth so the central source of

00:16:54,519 --> 00:16:58,870
truth was this internal communication

00:16:56,679 --> 00:17:00,579
channel so we are also trying to kind of

00:16:58,870 --> 00:17:03,100
like improve the process in that regard

00:17:00,579 --> 00:17:05,380
by kind of like having something which

00:17:03,100 --> 00:17:08,110
can be like reviewed audited and

00:17:05,380 --> 00:17:10,299
observed from the outside and being able

00:17:08,110 --> 00:17:12,370
to point people to what is the central

00:17:10,299 --> 00:17:16,059
public source of truth so that they can

00:17:12,370 --> 00:17:17,980
actually reproduce the whole flow so the

00:17:16,059 --> 00:17:20,439
new model more or less looks like this

00:17:17,980 --> 00:17:22,539
we still split in these three areas like

00:17:20,439 --> 00:17:23,860
some infrastructure from our side at

00:17:22,539 --> 00:17:26,529
this point is the federal infrastructure

00:17:23,860 --> 00:17:29,049
a local cluster that the user is running

00:17:26,529 --> 00:17:30,820
and then the local cluster has one

00:17:29,049 --> 00:17:33,639
specific machine that we are looking at

00:17:30,820 --> 00:17:36,650
and then the rest of the cluster so the

00:17:33,639 --> 00:17:39,110
key ideas the main points in this line

00:17:36,650 --> 00:17:41,780
is that we are actually trying to split

00:17:39,110 --> 00:17:43,850
the logic a bit more and moving some

00:17:41,780 --> 00:17:45,680
this company so as you can see like now

00:17:43,850 --> 00:17:48,470
there are two components on the top one

00:17:45,680 --> 00:17:48,980
which is like providing the out-of-date

00:17:48,470 --> 00:17:50,450
ins

00:17:48,980 --> 00:17:53,210
and the other one which is providing

00:17:50,450 --> 00:17:54,950
like the update payload then there are

00:17:53,210 --> 00:17:56,480
two component at the bottom as well one

00:17:54,950 --> 00:17:57,890
which is consuming the out-of-date

00:17:56,480 --> 00:18:00,440
scenes and the other one which is

00:17:57,890 --> 00:18:01,940
actually downloading and applying those

00:18:00,440 --> 00:18:04,400
updates and then there is another

00:18:01,940 --> 00:18:06,950
component which we kind of like pushed

00:18:04,400 --> 00:18:09,500
the left side was not there before it

00:18:06,950 --> 00:18:11,270
didn't exist which is something that you

00:18:09,500 --> 00:18:13,460
run on the cluster for doing these

00:18:11,270 --> 00:18:15,650
reboot coordination so what before was

00:18:13,460 --> 00:18:17,060
on the earth or at least there was some

00:18:15,650 --> 00:18:19,400
kind of like split between Austin

00:18:17,060 --> 00:18:22,940
cluster components now it's pushed to

00:18:19,400 --> 00:18:25,070
the cluster and so now it says airlock

00:18:22,940 --> 00:18:26,360
and at sea d3 or other because it's like

00:18:25,070 --> 00:18:27,680
it doesn't really matter what you're

00:18:26,360 --> 00:18:30,980
running on the rest of the cluster from

00:18:27,680 --> 00:18:32,270
the point of view of the node so let's

00:18:30,980 --> 00:18:34,370
start from the last point that we were

00:18:32,270 --> 00:18:38,120
touching how to do the release process

00:18:34,370 --> 00:18:40,490
itself we over all these a bit quite a

00:18:38,120 --> 00:18:43,940
bit are try and we basically ended up

00:18:40,490 --> 00:18:45,800
with what is our normal development

00:18:43,940 --> 00:18:47,300
process process which is you have a

00:18:45,800 --> 00:18:49,070
repository somewhere you open a pull

00:18:47,300 --> 00:18:50,510
request somebody reviews it get merge

00:18:49,070 --> 00:18:52,820
and then some other action is triggered

00:18:50,510 --> 00:18:54,680
after that and in the middle you run CI

00:18:52,820 --> 00:18:56,060
or whatever you want to run so that is

00:18:54,680 --> 00:18:57,530
exactly what we do by having a

00:18:56,060 --> 00:18:59,660
repository in github that contains

00:18:57,530 --> 00:19:01,310
definitions about like what are the

00:18:59,660 --> 00:19:04,610
current updates being rolled out to the

00:19:01,310 --> 00:19:07,310
cluster we basically implemented all the

00:19:04,610 --> 00:19:09,200
process that we had internally revealed

00:19:07,310 --> 00:19:10,730
for this scenario so we can do stuff

00:19:09,200 --> 00:19:13,070
that we were doing before which

00:19:10,730 --> 00:19:13,700
basically means we can push multiple

00:19:13,070 --> 00:19:15,860
rollouts

00:19:13,700 --> 00:19:19,160
in parallel rollouts means that we are

00:19:15,860 --> 00:19:21,290
pushing out updates gently let's say

00:19:19,160 --> 00:19:23,600
over a period of time to all the nodes

00:19:21,290 --> 00:19:25,700
that exist in the world we can pose some

00:19:23,600 --> 00:19:27,620
updated some roll out that is going on

00:19:25,700 --> 00:19:29,960
we can resume these updates if we think

00:19:27,620 --> 00:19:32,570
that it's it could be resumed we can

00:19:29,960 --> 00:19:34,280
have a date barriers which means we want

00:19:32,570 --> 00:19:36,560
all the nodes to pass through some

00:19:34,280 --> 00:19:38,180
specific updates we can signal when

00:19:36,560 --> 00:19:40,100
there are dead ends because we are human

00:19:38,180 --> 00:19:42,320
we make bugs from time to time we make

00:19:40,100 --> 00:19:44,030
releases that cannot proceed further we

00:19:42,320 --> 00:19:46,820
are out updates and so we need to signal

00:19:44,030 --> 00:19:49,190
this somehow to the client this process

00:19:46,820 --> 00:19:50,480
is automated which means that we have an

00:19:49,190 --> 00:19:52,370
initial step which is

00:19:50,480 --> 00:19:55,250
like opening this pull request and

00:19:52,370 --> 00:19:56,920
pushing the definition and then

00:19:55,250 --> 00:19:59,270
everything else can be kind of like

00:19:56,920 --> 00:20:01,010
automated in the public could be audited

00:19:59,270 --> 00:20:02,780
by anybody looking at that and there is

00:20:01,010 --> 00:20:03,950
no scrolling or like multiple private

00:20:02,780 --> 00:20:06,650
database that is actually no more

00:20:03,950 --> 00:20:09,380
database involved into this it's less

00:20:06,650 --> 00:20:11,570
eye catching than code updates no do I

00:20:09,380 --> 00:20:13,490
know whatever it could be added but it's

00:20:11,570 --> 00:20:16,460
definitely more developer divorce

00:20:13,490 --> 00:20:18,500
friendly then the other component the

00:20:16,460 --> 00:20:20,660
top is Cincinnati Cincinnati is the back

00:20:18,500 --> 00:20:22,940
end it's more or less what we had before

00:20:20,660 --> 00:20:24,890
we call oblate except that now it only

00:20:22,940 --> 00:20:27,530
does one thing which is update hinting

00:20:24,890 --> 00:20:29,690
which means the clients are periodically

00:20:27,530 --> 00:20:32,720
polling this server and the server is

00:20:29,690 --> 00:20:35,480
returning back a JSON object which is a

00:20:32,720 --> 00:20:37,520
dag a directed acyclic graph of the

00:20:35,480 --> 00:20:39,680
available updates and this is just

00:20:37,520 --> 00:20:41,330
basically in thing clients telling them

00:20:39,680 --> 00:20:44,120
a if you want there are these updates

00:20:41,330 --> 00:20:46,130
available this company's completely

00:20:44,120 --> 00:20:48,890
stateless as I say like in the process

00:20:46,130 --> 00:20:50,240
there is no States it's written as it's

00:20:48,890 --> 00:20:52,250
deployed in the federal infrastructure

00:20:50,240 --> 00:20:53,840
and we don't actually record anymore and

00:20:52,250 --> 00:20:55,220
use H matrix we just had like some

00:20:53,840 --> 00:20:57,080
matrix from the application to see if

00:20:55,220 --> 00:20:59,360
it's going well and that's it so by

00:20:57,080 --> 00:21:00,980
doing this we reduce the scope we reduce

00:20:59,360 --> 00:21:02,390
the protocol we four were eyes what is

00:21:00,980 --> 00:21:05,030
the graph model for the other updates

00:21:02,390 --> 00:21:08,450
and that's the war word that we are kind

00:21:05,030 --> 00:21:10,460
of like following then the component on

00:21:08,450 --> 00:21:12,590
the on the side of this one is still

00:21:10,460 --> 00:21:14,210
like on the node itself it's our PMI

00:21:12,590 --> 00:21:16,820
history which is used by other fedora

00:21:14,210 --> 00:21:18,380
flavors as well it does atomic OS

00:21:16,820 --> 00:21:21,320
management so it's kind of like git but

00:21:18,380 --> 00:21:23,450
for the root filesystem it's based on OS

00:21:21,320 --> 00:21:26,450
3 which means that you have kind of like

00:21:23,450 --> 00:21:28,610
saving this consumption by not having

00:21:26,450 --> 00:21:31,610
like duplicate copies of the same binary

00:21:28,610 --> 00:21:34,460
content it allows out updates atomic

00:21:31,610 --> 00:21:37,370
updates and atomic rollbacks an

00:21:34,460 --> 00:21:38,840
arbitrary number of deployments and you

00:21:37,370 --> 00:21:41,570
can also install like rpms

00:21:38,840 --> 00:21:43,490
it's written in C part of these at

00:21:41,570 --> 00:21:44,930
ported to last is still in progress and

00:21:43,490 --> 00:21:47,000
it basically bridges between two words

00:21:44,930 --> 00:21:51,590
the RPM the traditional PM world and the

00:21:47,000 --> 00:21:54,950
mutable OS word on the client side there

00:21:51,590 --> 00:21:57,290
is also something that pulls Cincinnati

00:21:54,950 --> 00:21:59,480
which is the syncretic client

00:21:57,290 --> 00:22:01,220
it's another component like this time

00:21:59,480 --> 00:22:03,890
it's fully declarative it's written in

00:22:01,220 --> 00:22:07,190
rust it checks for out of days and

00:22:03,890 --> 00:22:10,010
in charge of like triggering reboots it

00:22:07,190 --> 00:22:11,780
is a bit less complex than the update

00:22:10,010 --> 00:22:15,110
engine is it's a single state machine

00:22:11,780 --> 00:22:16,940
with less than ten states it mediates

00:22:15,110 --> 00:22:19,190
between these other components like

00:22:16,940 --> 00:22:20,090
Cincinnati rpms three and I'll lock that

00:22:19,190 --> 00:22:21,860
we're going to see you soon

00:22:20,090 --> 00:22:23,480
and it actually exposed matrix in the

00:22:21,860 --> 00:22:25,640
parameters form so that you can monitor

00:22:23,480 --> 00:22:28,910
the whole cluster the whole fleet of

00:22:25,640 --> 00:22:30,170
node from a single point of view this is

00:22:28,910 --> 00:22:31,550
the component that we started writing

00:22:30,170 --> 00:22:32,870
from scratch but in practice it's pretty

00:22:31,550 --> 00:22:34,520
much like we took the logic from

00:22:32,870 --> 00:22:36,470
locksmith and we kind of like reorganize

00:22:34,520 --> 00:22:39,250
it rash off a little bit and brought it

00:22:36,470 --> 00:22:42,880
in a component that worked with rpm os3

00:22:39,250 --> 00:22:45,650
in the last piece is air lock which is

00:22:42,880 --> 00:22:47,600
these logic that we pushed out of the

00:22:45,650 --> 00:22:49,820
node so it does the boot coordination

00:22:47,600 --> 00:22:51,980
it's actually like kind of like a server

00:22:49,820 --> 00:22:54,530
for this forcing Carty to ask for

00:22:51,980 --> 00:22:56,480
permission for rebooting but it's not

00:22:54,530 --> 00:23:00,950
anymore lost itself it's in a container

00:22:56,480 --> 00:23:02,480
somewhere so you basically do count

00:23:00,950 --> 00:23:07,220
accounting semaphore with recursive

00:23:02,480 --> 00:23:08,780
locking over HTTP and nothing more we

00:23:07,220 --> 00:23:11,480
have an implementation of these which is

00:23:08,780 --> 00:23:13,700
this iron lock which is a CD three as a

00:23:11,480 --> 00:23:15,440
back-end and we standardized the

00:23:13,700 --> 00:23:18,020
protocol between zinc RT and our log

00:23:15,440 --> 00:23:19,910
which means that we provide this but if

00:23:18,020 --> 00:23:21,680
you want to provide your own kubernetes

00:23:19,910 --> 00:23:23,150
based implementation or post less based

00:23:21,680 --> 00:23:24,950
implementation and you are the expert in

00:23:23,150 --> 00:23:27,200
this in your domain you are free to

00:23:24,950 --> 00:23:30,500
maintain these as your own container in

00:23:27,200 --> 00:23:32,810
deploy it as a container and that's the

00:23:30,500 --> 00:23:35,360
recap of what we just went through so

00:23:32,810 --> 00:23:38,120
it's like that's the split the server

00:23:35,360 --> 00:23:39,800
side the client side a client side which

00:23:38,120 --> 00:23:42,110
is checking for updates and a server

00:23:39,800 --> 00:23:44,450
side which is providing update scenes a

00:23:42,110 --> 00:23:47,120
server side which is provide proof which

00:23:44,450 --> 00:23:48,620
is providing os3 updates a client side

00:23:47,120 --> 00:23:50,720
which is downloading in a play

00:23:48,620 --> 00:23:52,400
applying these os3 updates and then

00:23:50,720 --> 00:23:54,530
something the cluster which takes care

00:23:52,400 --> 00:23:58,130
of like reboot management across the

00:23:54,530 --> 00:23:59,900
whole fleet and that was also we started

00:23:58,130 --> 00:24:01,370
a bit later we finish a bit late I can

00:23:59,900 --> 00:24:04,610
take a guess couple of question max

00:24:01,370 --> 00:24:06,170
among and these are the references for

00:24:04,610 --> 00:24:08,980
what I just talked about you have a

00:24:06,170 --> 00:24:08,980
question from yesterday so

00:24:34,080 --> 00:24:38,830
yeah so this that's a big picture and in

00:24:37,210 --> 00:24:41,020
this picture there is no kubernetes or

00:24:38,830 --> 00:24:42,970
appreciative vault at all and the idea

00:24:41,020 --> 00:24:44,530
is the we provide the initial

00:24:42,970 --> 00:24:47,410
implementations that'll because we want

00:24:44,530 --> 00:24:49,330
one to one party future parity between

00:24:47,410 --> 00:24:50,830
what we had in container Linux a what we

00:24:49,330 --> 00:24:53,290
have now in fedora caress which means

00:24:50,830 --> 00:24:54,670
the model that worked without kubernetes

00:24:53,290 --> 00:24:57,220
or open shipped in the middle so you

00:24:54,670 --> 00:24:58,690
just need basically somewhere to deploy

00:24:57,220 --> 00:25:00,700
this airlock which could be in the

00:24:58,690 --> 00:25:04,030
cluster itself but it gets tricky at

00:25:00,700 --> 00:25:06,130
some point and a database which is a

00:25:04,030 --> 00:25:07,960
database but again if you want to

00:25:06,130 --> 00:25:10,120
implement this stuff like in perl and

00:25:07,960 --> 00:25:12,070
using a my sequel database and you can

00:25:10,120 --> 00:25:14,080
talk to this somehow then it's like

00:25:12,070 --> 00:25:30,280
there is no kubernetes involved in any

00:25:14,080 --> 00:25:32,320
of these so I've ended like my personal

00:25:30,280 --> 00:25:34,030
answer is no because these are just like

00:25:32,320 --> 00:25:36,100
providing updates and your question is

00:25:34,030 --> 00:25:37,570
more about like when are you building a

00:25:36,100 --> 00:25:40,240
release do you actually send out

00:25:37,570 --> 00:25:41,860
messages and the answer is like no but

00:25:40,240 --> 00:25:43,540
we plan to if I'm not wrong but again

00:25:41,860 --> 00:25:51,670
this is just like about updates or not

00:25:43,540 --> 00:25:53,200
releases okay so you would like to get

00:25:51,670 --> 00:25:55,980
basically fed message whenever we do

00:25:53,200 --> 00:25:55,980
this kind of an Impala Sh

00:26:04,769 --> 00:26:09,519
I'm from Italy I live in Germany

00:26:07,659 --> 00:26:12,159
somebody in the u.s. decides to name

00:26:09,519 --> 00:26:13,209
stuff according like to American citizen

00:26:12,159 --> 00:26:14,979
I'm fine with that

00:26:13,209 --> 00:26:16,929
as long as I can pronounce them so it's

00:26:14,979 --> 00:26:19,269
like since you know this kind of like

00:26:16,929 --> 00:26:21,009
okay to pronounce so it's fine there are

00:26:19,269 --> 00:26:24,869
we other one I don't have an answer for

00:26:21,009 --> 00:26:28,419
that like again like so the protocol

00:26:24,869 --> 00:26:29,619
what is the protocol the particulars

00:26:28,419 --> 00:26:32,200
yeah so we didn't come up with this

00:26:29,619 --> 00:26:33,549
protocol I mean not me personally this

00:26:32,200 --> 00:26:35,829
is something that we share with the

00:26:33,549 --> 00:26:38,829
OpenShift organization so with the open

00:26:35,829 --> 00:26:40,719
ship product let's say as you can see

00:26:38,829 --> 00:26:43,839
like implementation it's under the open

00:26:40,719 --> 00:26:45,399
shipped or on github which means that we

00:26:43,839 --> 00:26:47,469
are basically just like piggybacking on

00:26:45,399 --> 00:26:49,209
some design that they did for the apt

00:26:47,469 --> 00:26:50,339
updates for open shift and we are

00:26:49,209 --> 00:26:52,329
basically providing another

00:26:50,339 --> 00:26:54,519
implementation and logic for the same

00:26:52,329 --> 00:27:09,820
protocol both server-side and

00:26:54,519 --> 00:27:11,379
client-side I would say that I submitted

00:27:09,820 --> 00:27:13,809
this presentation before actually

00:27:11,379 --> 00:27:15,549
implementing like half of this we

00:27:13,809 --> 00:27:17,349
actually tested it exactly once which

00:27:15,549 --> 00:27:20,109
was this Monday and it worked pretty

00:27:17,349 --> 00:27:21,579
well it didn't it didn't show any kind

00:27:20,109 --> 00:27:23,499
of like measure bugs and it works

00:27:21,579 --> 00:27:25,929
exactly as we were trying to do on the

00:27:23,499 --> 00:27:28,899
first try which is kind of like my point

00:27:25,929 --> 00:27:30,609
of view it's amazing well I could not

00:27:28,899 --> 00:27:32,349
say that like this model that we're

00:27:30,609 --> 00:27:33,940
still implementing Wars better than the

00:27:32,349 --> 00:27:36,789
other one that we proved for kind of

00:27:33,940 --> 00:27:38,109
like four years or a bit more so it's

00:27:36,789 --> 00:27:40,779
like that's a bit of a bold statement

00:27:38,109 --> 00:27:42,429
I'm not making it should according to

00:27:40,779 --> 00:27:44,649
what we are kind of like thinking and

00:27:42,429 --> 00:27:46,179
designing but it could have bugs

00:27:44,649 --> 00:27:49,570
anywhere and we could have kind of like

00:27:46,179 --> 00:27:50,950
missed stuff so it's I don't know it's

00:27:49,570 --> 00:27:55,929
an experiment at the end of the day like

00:27:50,950 --> 00:27:58,299
we are we are trying it okay I think I'm

00:27:55,929 --> 00:27:59,720
gonna close here and and leave the stage

00:27:58,299 --> 00:28:04,640
thank you very much

00:27:59,720 --> 00:28:04,640

YouTube URL: https://www.youtube.com/watch?v=KrfpCpppT1A


