Title: Flock 2018 - Building and running a container service on Fedora Atomic host
Publication date: 2018-09-08
Playlist: Flock 2018
Description: 
	Friday, August 10, 2018 
10:20am - 11:10am
@Meissen + Radebeul

Talk: Building and running a container service on Fedora Atomic host
Presenters: Spyros Trigazis

Description: This talk is about how the Fedora Atomic host is used as a core building block of a container service in a Cloud Infrastructure, private or public. OpenStack/Magnum is a light API service which allows cloud operators to provide container clusters on demand (mostly Kubernetes and Docker Swarm). Firstly, I will describe the architecture of the service, how the Magnum team is doing upstream development, how we add new features and the struggles to be up to date with the newest OS and software releases and how we benefit from the Fedora community. Then, on the downstream side, I'll share the experience of running our container service at CERN with hundreds of Fedora Atomic VMs for more than two years already and experiences from cloud operators in other sites. Finally, I will mention use cases at CERN from CI tools to physics analysis platforms.
Captions: 
	00:00:01,100 --> 00:00:06,660
so thank you for coming

00:00:03,500 --> 00:00:09,269
I'm Spiros two gases from certain

00:00:06,660 --> 00:00:11,639
emissions near there and I'm going to

00:00:09,269 --> 00:00:14,099
talk to you about our container service

00:00:11,639 --> 00:00:17,369
that we are basing on federate on coast

00:00:14,099 --> 00:00:21,840
and the absent project that we are using

00:00:17,369 --> 00:00:23,369
which is an openstack service so the

00:00:21,840 --> 00:00:26,099
OpenStack service that we use is

00:00:23,369 --> 00:00:28,680
OpenStack Magnum and I'm also the

00:00:26,099 --> 00:00:30,840
predicting lead of the project so I must

00:00:28,680 --> 00:00:32,300
talk about it because if I don't who

00:00:30,840 --> 00:00:34,890
else is gonna do it

00:00:32,300 --> 00:00:36,960
so Magnum is a community project from

00:00:34,890 --> 00:00:39,960
OpenStack and if you're not familiar

00:00:36,960 --> 00:00:41,430
with OpenStack I will simplify random

00:00:39,960 --> 00:00:44,640
words that you see that are OpenStack

00:00:41,430 --> 00:00:47,280
project names so Magnum is using

00:00:44,640 --> 00:00:49,950
keystone credentials it means that it's

00:00:47,280 --> 00:00:52,559
using the centralized authentication

00:00:49,950 --> 00:00:57,870
method of OpenStack which on the back

00:00:52,559 --> 00:01:01,020
end can be Kerberos or you can use a

00:00:57,870 --> 00:01:04,320
free API your Active Directory so this

00:01:01,020 --> 00:01:07,170
is like the entry point of applications

00:01:04,320 --> 00:01:11,250
that use that they develop better

00:01:07,170 --> 00:01:13,530
developed based on OpenStack magnum

00:01:11,250 --> 00:01:16,439
offers different cluster types so we

00:01:13,530 --> 00:01:18,990
have kubernetes which is by far the most

00:01:16,439 --> 00:01:21,240
popular one docker form and measures in

00:01:18,990 --> 00:01:24,439
this us which are the least popular at

00:01:21,240 --> 00:01:28,140
least from our from our perspective and

00:01:24,439 --> 00:01:30,630
the way that Magnum offers multi

00:01:28,140 --> 00:01:32,939
tendencies is actually the opposite with

00:01:30,630 --> 00:01:35,250
single tendency every user has its own

00:01:32,939 --> 00:01:37,320
cluster and he's responsible for the

00:01:35,250 --> 00:01:41,850
applications that are running there and

00:01:37,320 --> 00:01:42,840
it's guaranteed that he has control the

00:01:41,850 --> 00:01:45,420
machines that are around their

00:01:42,840 --> 00:01:47,399
applications running only their code so

00:01:45,420 --> 00:01:50,689
it could be virtual machines or physical

00:01:47,399 --> 00:01:50,689
servers that they own

00:01:51,770 --> 00:01:56,510
and these are some cool logos from the

00:01:54,770 --> 00:02:00,050
container registrations that we support

00:01:56,510 --> 00:02:02,510
with like 70% of the users are using

00:02:00,050 --> 00:02:05,240
kubernetes another 20% are using docker

00:02:02,510 --> 00:02:09,739
form and there are some outliers I would

00:02:05,240 --> 00:02:11,510
say using messes or dishes in an organ

00:02:09,739 --> 00:02:13,459
is in our organization at certain we

00:02:11,510 --> 00:02:16,670
have some measures users but they are

00:02:13,459 --> 00:02:19,280
not using OpenStack they are deploying

00:02:16,670 --> 00:02:23,450
on physical servers their own

00:02:19,280 --> 00:02:26,780
infrastructure some terminology about

00:02:23,450 --> 00:02:29,170
the project a cluster is made up of

00:02:26,780 --> 00:02:32,120
computing senses virtual or physical

00:02:29,170 --> 00:02:34,550
networks that are managed by the neutral

00:02:32,120 --> 00:02:37,580
service of OpenStack security groups

00:02:34,550 --> 00:02:40,850
which are part of Neutron and block

00:02:37,580 --> 00:02:43,040
storage based on the cinder service on

00:02:40,850 --> 00:02:46,160
the cinder project other resources like

00:02:43,040 --> 00:02:50,180
load balancers which are based on

00:02:46,160 --> 00:02:51,980
Octavia and on the backend could be what

00:02:50,180 --> 00:02:54,640
every solution you use may be open

00:02:51,980 --> 00:02:58,209
daylight tungsten tungsten fabric or

00:02:54,640 --> 00:03:00,320
some solution based on open usage and

00:02:58,209 --> 00:03:02,630
the one that I haven't mentioned which

00:03:00,320 --> 00:03:05,290
is a new addition to us is a shared file

00:03:02,630 --> 00:03:09,500
systems that will have integration with

00:03:05,290 --> 00:03:13,040
CFS and the manila project of OpenStack

00:03:09,500 --> 00:03:17,150
which basically it's a client for safe

00:03:13,040 --> 00:03:19,310
and safe FS also the cluster is where

00:03:17,150 --> 00:03:22,489
your containers are running so you see

00:03:19,310 --> 00:03:26,000
all these entities that are based

00:03:22,489 --> 00:03:29,030
infrastructure as a single thing and you

00:03:26,000 --> 00:03:31,340
talk to the cluster and with the API

00:03:29,030 --> 00:03:34,730
either it is the kubernetes api or the

00:03:31,340 --> 00:03:37,340
dr. Sharman API or the parade the

00:03:34,730 --> 00:03:40,519
marathon API that it's based that is

00:03:37,340 --> 00:03:43,000
used by measures and this us the project

00:03:40,519 --> 00:03:46,730
is focusing on life cycle operation

00:03:43,000 --> 00:03:48,350
operations like scaling up and down not

00:03:46,730 --> 00:03:52,340
auto scaling yet

00:03:48,350 --> 00:03:55,130
like how upgrading clusters and healing

00:03:52,340 --> 00:03:58,160
or replacing nodes so usually if a node

00:03:55,130 --> 00:04:01,190
doesn't work and you have 500 nodes

00:03:58,160 --> 00:04:03,980
which did use the fictional at least in

00:04:01,190 --> 00:04:04,580
our case and it's much easier to replace

00:04:03,980 --> 00:04:07,370
it

00:04:04,580 --> 00:04:09,050
from kubernetes or docker and just

00:04:07,370 --> 00:04:14,720
create another physical instance of fee

00:04:09,050 --> 00:04:16,820
per VM yeah also in each cluster we try

00:04:14,720 --> 00:04:19,220
to provide the self-contained monitoring

00:04:16,820 --> 00:04:21,230
solution so every class that has its own

00:04:19,220 --> 00:04:23,330
dashboards that monitors applications

00:04:21,230 --> 00:04:28,430
and the other line for structure like

00:04:23,330 --> 00:04:32,440
operating system matrix and matrix from

00:04:28,430 --> 00:04:32,440
the applications that the users defined

00:04:33,010 --> 00:04:40,540
the strongest selling point from the

00:04:36,530 --> 00:04:44,690
project is that it doesn't wrap the

00:04:40,540 --> 00:04:48,200
upstream api of doctor and kubernetes so

00:04:44,690 --> 00:04:51,080
we you can use Magnum to just bootstrap

00:04:48,200 --> 00:04:53,420
the cluster and then do some low-level

00:04:51,080 --> 00:04:57,050
operations on it but after that you're

00:04:53,420 --> 00:05:02,480
using the doctor and the coop CTL Azure

00:04:57,050 --> 00:05:05,210
issue and native api's magnum does all

00:05:02,480 --> 00:05:07,670
the picki infrastructure for all of

00:05:05,210 --> 00:05:10,460
these projects kubernetes has the most

00:05:07,670 --> 00:05:13,520
complicated one but also you can run

00:05:10,460 --> 00:05:17,510
measures and doctor Dedecker demon

00:05:13,520 --> 00:05:18,950
behind the LS and magnum creates those

00:05:17,510 --> 00:05:20,630
credentials and then with a single

00:05:18,950 --> 00:05:23,600
command that i will demonstrate later

00:05:20,630 --> 00:05:26,500
you can retrieve all the certificates

00:05:23,600 --> 00:05:28,940
required to access securely the cluster

00:05:26,500 --> 00:05:30,440
this is the architecture but we will not

00:05:28,940 --> 00:05:34,400
talk about because this is not for this

00:05:30,440 --> 00:05:36,919
audience but very briefly on the right

00:05:34,400 --> 00:05:39,350
it's a the magnum client and the

00:05:36,919 --> 00:05:41,360
interaction with OpenStack and on the

00:05:39,350 --> 00:05:43,640
right box is all the components that are

00:05:41,360 --> 00:05:45,890
used in OpenStack which are which are

00:05:43,640 --> 00:05:48,980
I'm not going to mention here and on the

00:05:45,890 --> 00:05:51,320
left it's the cluster that has dr the

00:05:48,980 --> 00:05:53,780
operating system and the applications

00:05:51,320 --> 00:05:56,510
and then when the bootstrap of the

00:05:53,780 --> 00:05:59,720
cluster is done the user is going on the

00:05:56,510 --> 00:06:02,390
left and using the recipe is that are

00:05:59,720 --> 00:06:05,120
common in public clouds private clouds

00:06:02,390 --> 00:06:07,639
and in solutions that you have deployed

00:06:05,120 --> 00:06:09,770
on your own

00:06:07,639 --> 00:06:15,439
and I will mention a few of these

00:06:09,770 --> 00:06:18,680
features briefly so what we did a year

00:06:15,439 --> 00:06:21,050
ago out I think it's running everything

00:06:18,680 --> 00:06:23,180
in containers and this is why I'm doing

00:06:21,050 --> 00:06:24,830
this presentation because we're starting

00:06:23,180 --> 00:06:27,229
using the stock fedora

00:06:24,830 --> 00:06:29,990
Tomic project we don't modified it to

00:06:27,229 --> 00:06:33,469
whatsoever we take the two cows or RAW

00:06:29,990 --> 00:06:37,610
images from get Fedora the talk and then

00:06:33,469 --> 00:06:38,930
whatever we need to any customization or

00:06:37,610 --> 00:06:41,960
additional features that we want to

00:06:38,930 --> 00:06:45,439
offer to the users and they're running

00:06:41,960 --> 00:06:48,110
in containers so when I say we we are

00:06:45,439 --> 00:06:50,000
operators of cloud that we offer a

00:06:48,110 --> 00:06:52,370
service and then the users deploy

00:06:50,000 --> 00:06:54,409
containers which are the application

00:06:52,370 --> 00:06:56,629
containers that are on their web

00:06:54,409 --> 00:07:00,110
applications or analytics or whatever

00:06:56,629 --> 00:07:03,770
they want um also we added full

00:07:00,110 --> 00:07:05,629
prometheus stack before we did that

00:07:03,770 --> 00:07:07,699
before other parameters operator was up

00:07:05,629 --> 00:07:09,199
so it's something that we did on our own

00:07:07,699 --> 00:07:11,690
and it's basically what the promises

00:07:09,199 --> 00:07:14,360
operators is doing and we're planning to

00:07:11,690 --> 00:07:17,419
move on that we added the upstream

00:07:14,360 --> 00:07:19,969
kubernetes dashboard and Cortinas even

00:07:17,419 --> 00:07:22,969
before becoming general available

00:07:19,969 --> 00:07:26,050
coordinates is the replacement of coop

00:07:22,969 --> 00:07:29,680
dns in Cornelis that you can use to have

00:07:26,050 --> 00:07:32,060
dns resolution in kubernetes cluster and

00:07:29,680 --> 00:07:35,210
another important feature that we are

00:07:32,060 --> 00:07:38,180
adding it's not there yet it's this one

00:07:35,210 --> 00:07:40,909
it's cluster Federation I'm sorry it's a

00:07:38,180 --> 00:07:43,370
cluster Federation but it means that you

00:07:40,909 --> 00:07:46,219
can join different kubernetes clusters

00:07:43,370 --> 00:07:47,979
in the single data center or kubernetes

00:07:46,219 --> 00:07:50,750
classes in different data centers and

00:07:47,979 --> 00:07:54,370
I'll talk about this one in the use

00:07:50,750 --> 00:07:56,930
cases and what you're working also on is

00:07:54,370 --> 00:07:59,029
adding different container runtimes

00:07:56,930 --> 00:08:01,580
at the moment everything is based on

00:07:59,029 --> 00:08:03,710
docker and this based on docker that is

00:08:01,580 --> 00:08:06,279
running that it's included in the Fedora

00:08:03,710 --> 00:08:06,279
tamiko's

00:08:06,669 --> 00:08:13,039
I'm finishing with Magnum so why we use

00:08:09,889 --> 00:08:15,649
Magnum and why someone else may want to

00:08:13,039 --> 00:08:17,599
use it if you haven't understand at the

00:08:15,649 --> 00:08:19,180
moment what it does but have experience

00:08:17,599 --> 00:08:23,169
with GAE or

00:08:19,180 --> 00:08:25,900
the AWS kubernetes service it's a very

00:08:23,169 --> 00:08:28,210
similar product or project in our case

00:08:25,900 --> 00:08:31,120
that we can someone create can created

00:08:28,210 --> 00:08:35,740
product product or offer it as a service

00:08:31,120 --> 00:08:37,390
to users we are a public organization so

00:08:35,740 --> 00:08:39,370
we just use the open source solutions

00:08:37,390 --> 00:08:41,469
for all these kind of things and it

00:08:39,370 --> 00:08:43,630
makes sense for users that for

00:08:41,469 --> 00:08:46,690
organization that have organizations

00:08:43,630 --> 00:08:48,820
that have at least five users and when I

00:08:46,690 --> 00:08:51,010
say users like different applications

00:08:48,820 --> 00:08:53,770
that we may be more users for managing

00:08:51,010 --> 00:08:56,110
and you may need more than ten clusters

00:08:53,770 --> 00:08:57,940
because if you can name your clusters is

00:08:56,110 --> 00:09:01,180
better and you have some experience it's

00:08:57,940 --> 00:09:06,400
better to manage them by hand or with

00:09:01,180 --> 00:09:08,650
ansible or with something else if you

00:09:06,400 --> 00:09:11,170
have an openstack cloud and you want to

00:09:08,650 --> 00:09:13,570
add Magnum accounting comes for free so

00:09:11,170 --> 00:09:16,330
it means whatever accounting you have

00:09:13,570 --> 00:09:18,670
someone has done with VMs this can

00:09:16,330 --> 00:09:22,029
extend to Container clusters because

00:09:18,670 --> 00:09:27,670
each cluster belongs to project or a

00:09:22,029 --> 00:09:29,050
user so accounting doesn't break also

00:09:27,670 --> 00:09:30,580
you see very I would have noticed that

00:09:29,050 --> 00:09:35,500
it's a very easy entry point for new

00:09:30,580 --> 00:09:38,620
users so when users start to experiment

00:09:35,500 --> 00:09:41,620
with containers they installed probably

00:09:38,620 --> 00:09:43,810
docker on the laptop and try some

00:09:41,620 --> 00:09:47,140
applications there and then it's very

00:09:43,810 --> 00:09:49,540
easy for them to switch at least docker

00:09:47,140 --> 00:09:51,760
swarm and then when they try to have

00:09:49,540 --> 00:09:53,470
more than three containers in their

00:09:51,760 --> 00:09:57,540
application they immediately go to

00:09:53,470 --> 00:09:57,540
kubernetes and they stay there forever

00:09:58,020 --> 00:10:03,630
also what's very important to us and

00:10:00,430 --> 00:10:06,310
it's also I'm here as I said it's the

00:10:03,630 --> 00:10:09,790
offering all the deployment that we have

00:10:06,310 --> 00:10:12,700
for running kubernetes and other engines

00:10:09,790 --> 00:10:16,000
is based around duratomic so we control

00:10:12,700 --> 00:10:18,760
very hard and what is the operating

00:10:16,000 --> 00:10:21,930
system that we allow you allowed to get

00:10:18,760 --> 00:10:21,930
into our data center

00:10:22,150 --> 00:10:27,460
and some notes from the operators

00:10:24,550 --> 00:10:29,380
perspective on what someone should pay

00:10:27,460 --> 00:10:30,880
attention to when they run a magnum or

00:10:29,380 --> 00:10:32,350
any other container service that's

00:10:30,880 --> 00:10:35,020
offered to other users and they're not

00:10:32,350 --> 00:10:39,460
consuming themselves the network design

00:10:35,020 --> 00:10:42,430
needs a lot of attention so to simplify

00:10:39,460 --> 00:10:44,020
things and assuming that users have a

00:10:42,430 --> 00:10:46,720
full open sky cloud with all the

00:10:44,020 --> 00:10:48,550
features and private tenant networks are

00:10:46,720 --> 00:10:51,730
cheap and they create them as they need

00:10:48,550 --> 00:10:53,830
you have a prayer a private network per

00:10:51,730 --> 00:10:56,290
cluster and optionally we have a

00:10:53,830 --> 00:10:58,420
floating a personal notes we also have

00:10:56,290 --> 00:11:00,040
clusters that don't have they're not

00:10:58,420 --> 00:11:02,890
reachable from the outside which is

00:11:00,040 --> 00:11:06,010
perfect for computer tasks and not burn

00:11:02,890 --> 00:11:07,510
public APIs and also what is very

00:11:06,010 --> 00:11:10,540
important to have a complete offering

00:11:07,510 --> 00:11:13,510
with kubernetes at least these two have

00:11:10,540 --> 00:11:15,490
a lot balancing as a service and load

00:11:13,510 --> 00:11:17,920
balancing from master one case is to

00:11:15,490 --> 00:11:20,230
have rigger clusters and scale the

00:11:17,920 --> 00:11:22,680
master nodes and have a chain and the

00:11:20,230 --> 00:11:26,080
other case is that you want expose

00:11:22,680 --> 00:11:29,380
services on the internet without using

00:11:26,080 --> 00:11:31,480
the report of the nodes another close

00:11:29,380 --> 00:11:34,900
crucial part that comes in day two is

00:11:31,480 --> 00:11:39,310
the container registry at CERN were

00:11:34,900 --> 00:11:42,160
heavy CentOS shop not fedora but we

00:11:39,310 --> 00:11:43,930
start to love it we started pushing the

00:11:42,160 --> 00:11:46,150
team that manages the operating systems

00:11:43,930 --> 00:11:48,790
in our own organization to accept a door

00:11:46,150 --> 00:11:51,310
as well so we have our own Koji and our

00:11:48,790 --> 00:11:54,850
own packages and we recommend the users

00:11:51,310 --> 00:11:57,220
to build containers based on CentOS or

00:11:54,850 --> 00:11:59,470
fedora and host them in our own registry

00:11:57,220 --> 00:12:03,279
not the cryo or other registers outside

00:11:59,470 --> 00:12:05,709
so we can track down everything that

00:12:03,279 --> 00:12:07,900
happens in order to center and when

00:12:05,709 --> 00:12:10,420
people leave the organization we can

00:12:07,900 --> 00:12:14,260
have still have access to their images

00:12:10,420 --> 00:12:17,110
and not trying to poke around the cryo

00:12:14,260 --> 00:12:22,000
or even way i/o and ask for credentials

00:12:17,110 --> 00:12:25,320
and also this improves latency but in

00:12:22,000 --> 00:12:25,320
some organization this is not an issue

00:12:25,770 --> 00:12:31,780
also someone who must notice that when

00:12:29,650 --> 00:12:32,940
you provide the self-contained service

00:12:31,780 --> 00:12:34,980
like

00:12:32,940 --> 00:12:37,850
like this one with Magnum and container

00:12:34,980 --> 00:12:42,269
clusters in a way you provide software

00:12:37,850 --> 00:12:45,120
so we must test and verify that works

00:12:42,269 --> 00:12:47,220
for us the operating system and since

00:12:45,120 --> 00:12:50,250
we're using atomic we have to test only

00:12:47,220 --> 00:12:55,019
once because we use it for all use cases

00:12:50,250 --> 00:12:57,420
and we also need to test the kubernetes

00:12:55,019 --> 00:13:01,560
containers or docker containers that are

00:12:57,420 --> 00:13:04,440
running as part of the cluster and what

00:13:01,560 --> 00:13:07,680
we need to also do is upgrade regularly

00:13:04,440 --> 00:13:10,350
because all the configuration parameters

00:13:07,680 --> 00:13:12,449
and the way that the classes are

00:13:10,350 --> 00:13:15,660
deployed are change rapidly every a

00:13:12,449 --> 00:13:17,579
couple of months so essentially it has a

00:13:15,660 --> 00:13:22,889
very fast page falling docker and

00:13:17,579 --> 00:13:24,839
kubernetes pace and have a couple of

00:13:22,889 --> 00:13:27,630
slides on what the work that we're doing

00:13:24,839 --> 00:13:31,639
with the atomic working group and how we

00:13:27,630 --> 00:13:35,250
ended up there so the reason that we

00:13:31,639 --> 00:13:38,060
chose it is before going to project

00:13:35,250 --> 00:13:40,110
atomic we were building Fedora on around

00:13:38,060 --> 00:13:43,410
OpenStack has a project called disk

00:13:40,110 --> 00:13:45,800
image builder that builds to cows that

00:13:43,410 --> 00:13:48,360
are designed to run on OpenStack and

00:13:45,800 --> 00:13:51,509
that meant that every time that they

00:13:48,360 --> 00:13:53,339
cannot changed every time that there was

00:13:51,509 --> 00:13:55,680
a security fix you had to rebuild and

00:13:53,339 --> 00:13:59,160
test again if the bin prostitute was

00:13:55,680 --> 00:14:02,790
proper and obviously all these works

00:13:59,160 --> 00:14:05,250
done by the Fedora project and the

00:14:02,790 --> 00:14:07,470
Sanders project if you send us so we

00:14:05,250 --> 00:14:10,019
tried to remove all the specificities

00:14:07,470 --> 00:14:12,569
that we had in the base image and we

00:14:10,019 --> 00:14:15,300
went for Fedora Tomic which is like the

00:14:12,569 --> 00:14:17,370
minimal layer and when we started

00:14:15,300 --> 00:14:19,889
kubernetes walls also included in the

00:14:17,370 --> 00:14:22,290
image and that was convenient but then

00:14:19,889 --> 00:14:25,800
when you wanted to do pump versions it

00:14:22,290 --> 00:14:27,209
wasn't very convenient because the

00:14:25,800 --> 00:14:31,139
packages should be built and then

00:14:27,209 --> 00:14:34,410
included in in a fedora Tomic release

00:14:31,139 --> 00:14:39,370
and then published on gate for the audit

00:14:34,410 --> 00:14:43,480
org and then we had to test again etc

00:14:39,370 --> 00:14:46,600
and when we see when the project atomic

00:14:43,480 --> 00:14:48,390
to decide to even minimize more the base

00:14:46,600 --> 00:14:51,100
operating system we forced ourselves

00:14:48,390 --> 00:14:54,460
forced herself to use only read-only

00:14:51,100 --> 00:14:57,010
containers to extend the host and this

00:14:54,460 --> 00:14:59,110
was a very good exercise because it made

00:14:57,010 --> 00:15:01,240
it didn't use is just a lot of

00:14:59,110 --> 00:15:02,980
discipline and because we will use

00:15:01,240 --> 00:15:05,080
system containers that are mentioned in

00:15:02,980 --> 00:15:07,210
a couple of slides this containers are

00:15:05,080 --> 00:15:09,880
read-only so you must design very

00:15:07,210 --> 00:15:12,010
carefully what you need to do another

00:15:09,880 --> 00:15:14,050
great advantage is that if something

00:15:12,010 --> 00:15:16,480
breaks in the operating system we ask

00:15:14,050 --> 00:15:20,050
Fedora project with the users don't ask

00:15:16,480 --> 00:15:21,670
us that we build the operating system

00:15:20,050 --> 00:15:24,010
events and of course we don't have to

00:15:21,670 --> 00:15:26,020
maintain our CI anymore that it was a

00:15:24,010 --> 00:15:28,839
very painful because we had the store

00:15:26,020 --> 00:15:32,650
artifacts rotate them make sure that we

00:15:28,839 --> 00:15:37,180
don't point the users to do old

00:15:32,650 --> 00:15:38,620
operating systems but when you start

00:15:37,180 --> 00:15:40,740
using a lot of the stock project you

00:15:38,620 --> 00:15:42,460
eventually become a contributor

00:15:40,740 --> 00:15:46,089
personally I like it very much

00:15:42,460 --> 00:15:47,860
and what I ended up is calming gaining

00:15:46,089 --> 00:15:52,510
the kubernetes packets for fedora and

00:15:47,860 --> 00:15:56,110
santa's and we I'm using the same digit

00:15:52,510 --> 00:15:59,200
as mentioned two days ago for a couple

00:15:56,110 --> 00:16:03,580
of months so that this did for CentOS

00:15:59,200 --> 00:16:06,220
and Fedora is exactly the same and it

00:16:03,580 --> 00:16:11,650
works pretty well so far so whatever

00:16:06,220 --> 00:16:13,600
changes I need to make in in fedora in

00:16:11,650 --> 00:16:17,230
the Fedora package they just go straight

00:16:13,600 --> 00:16:21,070
into centers and it hasn't break at any

00:16:17,230 --> 00:16:23,500
time at the moment and what we are doing

00:16:21,070 --> 00:16:26,440
now that we moved system containers is

00:16:23,500 --> 00:16:29,620
instead of we running for example Fedora

00:16:26,440 --> 00:16:33,310
28 at the moment but kubernetes that

00:16:29,620 --> 00:16:35,450
comes from Cody and the fedora repos is

00:16:33,310 --> 00:16:39,100
zero height so

00:16:35,450 --> 00:16:41,540
for some use cases like this we're using

00:16:39,100 --> 00:16:44,720
the stable branch of fedora

00:16:41,540 --> 00:16:46,790
but rawhide for the latest package that

00:16:44,720 --> 00:16:49,310
we want but one of these packages are

00:16:46,790 --> 00:16:51,740
graduating to the stable release which

00:16:49,310 --> 00:16:56,090
which the fedora dentate or whatever the

00:16:51,740 --> 00:16:58,220
stable one is but with a component is at

00:16:56,090 --> 00:17:00,230
least pace sometimes this this doesn't

00:16:58,220 --> 00:17:04,550
happen and it holds only for it for a

00:17:00,230 --> 00:17:07,520
month or so we are also early testers of

00:17:04,550 --> 00:17:09,860
Scorpio in the atomic utilities because

00:17:07,520 --> 00:17:13,040
we with stone containers will rely a lot

00:17:09,860 --> 00:17:17,540
of them and we contribute a lot to the

00:17:13,040 --> 00:17:21,680
system containers repo from protected

00:17:17,540 --> 00:17:23,300
atomic in github I haven't seen any

00:17:21,680 --> 00:17:25,520
talks about system containers and

00:17:23,300 --> 00:17:27,800
probably this is gonna change slightly

00:17:25,520 --> 00:17:30,140
or heavily with the derocker and we're

00:17:27,800 --> 00:17:31,970
looking forward to it but at least this

00:17:30,140 --> 00:17:34,220
is what we have now and we are

00:17:31,970 --> 00:17:37,280
investigating how we will replace it

00:17:34,220 --> 00:17:39,170
when we move to Fedora core on when and

00:17:37,280 --> 00:17:41,960
if the atomic utilities are going to be

00:17:39,170 --> 00:17:45,470
removed so the first example is how to

00:17:41,960 --> 00:17:48,290
install cubed couplet is like the most

00:17:45,470 --> 00:17:50,090
fun the most the core component of

00:17:48,290 --> 00:17:52,550
kubernetes that runs on every worker

00:17:50,090 --> 00:17:54,740
node and usually in the masters because

00:17:52,550 --> 00:17:56,600
also the control plane of kubernetes

00:17:54,740 --> 00:18:00,050
runs on containers that are managed by

00:17:56,600 --> 00:18:02,690
kubernetes and so this is pulling

00:18:00,050 --> 00:18:05,870
couplet from the federal registry and

00:18:02,690 --> 00:18:09,320
it's using OS 3 as a back-end instead of

00:18:05,870 --> 00:18:10,910
storing vehement in docker and - managed

00:18:09,320 --> 00:18:12,350
system means that it's a system

00:18:10,910 --> 00:18:15,410
container and it's like a super

00:18:12,350 --> 00:18:19,130
privileged container and also in the

00:18:15,410 --> 00:18:22,910
spec of the configuration of the OCI

00:18:19,130 --> 00:18:25,640
image and we have kept only the mountain

00:18:22,910 --> 00:18:27,500
aim space and we have given almost all

00:18:25,640 --> 00:18:30,950
capabilities that we could give the

00:18:27,500 --> 00:18:34,250
couplet and then you just install and

00:18:30,950 --> 00:18:36,170
it's like from the user perspective from

00:18:34,250 --> 00:18:38,780
from the configuration perspective

00:18:36,170 --> 00:18:40,820
better it's like having the package

00:18:38,780 --> 00:18:42,260
installed so you just go to the

00:18:40,820 --> 00:18:44,150
kubernetes and modify the parameters

00:18:42,260 --> 00:18:47,210
that you want and then you start using

00:18:44,150 --> 00:18:49,010
with systemctl service

00:18:47,210 --> 00:18:51,350
for the confirmed blasters that we

00:18:49,010 --> 00:18:53,900
wanted if we wanted the faster pace for

00:18:51,350 --> 00:18:57,980
daughter for kubernetes the stock docker

00:18:53,900 --> 00:19:00,740
that 14 atomic is fine but for dr. swarm

00:18:57,980 --> 00:19:02,720
and for people that wanted to use things

00:19:00,740 --> 00:19:07,280
like multistage built with daughter and

00:19:02,720 --> 00:19:09,950
not pod one and we wanted a new docker

00:19:07,280 --> 00:19:13,460
version so we used the fedora doctor

00:19:09,950 --> 00:19:18,380
system container and we used the repos

00:19:13,460 --> 00:19:20,420
from dr. from Dekker Inc and we

00:19:18,380 --> 00:19:22,940
installed the version that we want at

00:19:20,420 --> 00:19:24,790
the moment I think we run 1709 we

00:19:22,940 --> 00:19:28,780
haven't moved to waiting yet

00:19:24,790 --> 00:19:34,900
and then we just started as a system D

00:19:28,780 --> 00:19:39,440
service on the bottom on the bottom box

00:19:34,900 --> 00:19:42,170
it's a snapshot from a kubernetes master

00:19:39,440 --> 00:19:44,810
node so all the components though so

00:19:42,170 --> 00:19:47,030
kubernetes and friends are running in

00:19:44,810 --> 00:19:49,850
system containers which is the AP

00:19:47,030 --> 00:19:53,330
central mandolin scheduler + ET CD and

00:19:49,850 --> 00:19:54,950
flannel for the overlay network and at

00:19:53,330 --> 00:19:58,790
the moment as you can see we have even

00:19:54,950 --> 00:20:02,330
111 one for october noticed a couple of

00:19:58,790 --> 00:20:05,150
days 111 2 was released but in this way

00:20:02,330 --> 00:20:08,450
it's very easy to upgrade we just rebase

00:20:05,150 --> 00:20:11,090
the container that we want on top of

00:20:08,450 --> 00:20:14,690
that and we'll start and only the

00:20:11,090 --> 00:20:16,370
containers and it works fine and in the

00:20:14,690 --> 00:20:18,850
last container is the one that we wrote

00:20:16,370 --> 00:20:21,920
on on our own which is a specific

00:20:18,850 --> 00:20:25,400
service process that runs on all nodes

00:20:21,920 --> 00:20:29,570
in in OpenStack machines that you want

00:20:25,400 --> 00:20:34,220
to configure containers em just to

00:20:29,570 --> 00:20:38,180
configure nodes so now about the certain

00:20:34,220 --> 00:20:42,440
container service everything is based on

00:20:38,180 --> 00:20:45,020
OpenStack in our cloud and I have a typo

00:20:42,440 --> 00:20:46,970
we have 100,000 course more as you can

00:20:45,020 --> 00:20:48,650
see in the Box not in the letters but

00:20:46,970 --> 00:20:52,220
the important thing for this talk is

00:20:48,650 --> 00:20:56,210
that we have 1500 Fedora Tomatina 7 vm s

00:20:52,220 --> 00:20:59,379
we have occasional bursts to 3000

00:20:56,210 --> 00:21:05,389
but this is like a more conservative

00:20:59,379 --> 00:21:08,619
layout of our service and only for

00:21:05,389 --> 00:21:11,269
experiments or before a conference that

00:21:08,619 --> 00:21:14,649
physicist going to do extra analysis we

00:21:11,269 --> 00:21:17,539
deploy more nodes and in the middle of

00:21:14,649 --> 00:21:24,230
the screen that I have here you can see

00:21:17,539 --> 00:21:28,340
that we'll have 450 Magnum clusters so

00:21:24,230 --> 00:21:29,659
for the Magnum deployment and the first

00:21:28,340 --> 00:21:31,159
work that we needed to do is to

00:21:29,659 --> 00:21:35,139
integrate containers with the certain

00:21:31,159 --> 00:21:37,700
cloud meaning the changes that require

00:21:35,139 --> 00:21:41,600
software for of physics analysis

00:21:37,700 --> 00:21:44,509
specifically we have an in-house file

00:21:41,600 --> 00:21:47,570
system called CD NFS that is used to

00:21:44,509 --> 00:21:49,970
distribute software and run route

00:21:47,570 --> 00:21:53,600
analysis which is a programming language

00:21:49,970 --> 00:21:56,720
for to physics analysis and we also

00:21:53,600 --> 00:21:58,519
added all the services that we required

00:21:56,720 --> 00:22:01,490
for certain to install specific

00:21:58,519 --> 00:22:04,220
certificates we know the hosts as the

00:22:01,490 --> 00:22:06,559
system containers this layout is a bit

00:22:04,220 --> 00:22:08,960
old I think I need to upgrade it but

00:22:06,559 --> 00:22:11,779
after 2016 we run the service in

00:22:08,960 --> 00:22:14,899
production and we had only a couple of

00:22:11,779 --> 00:22:16,820
upgrades for adding new features but the

00:22:14,899 --> 00:22:21,409
layout of the service hasn't changed yet

00:22:16,820 --> 00:22:24,080
and since then so how users interact

00:22:21,409 --> 00:22:26,899
with our container service we have

00:22:24,080 --> 00:22:28,700
plastic templates which are describing

00:22:26,899 --> 00:22:32,019
what clatters look like apart from

00:22:28,700 --> 00:22:34,399
number of nodes and size of the VMS and

00:22:32,019 --> 00:22:36,559
these are the public cluster templates

00:22:34,399 --> 00:22:38,720
that we offer the users we have na na

00:22:36,559 --> 00:22:40,759
chained and non ETH solution so if

00:22:38,720 --> 00:22:44,629
someone has five nodes doesn't have to

00:22:40,759 --> 00:22:49,629
burn more quota and more cores to have

00:22:44,629 --> 00:22:51,710
hea for master nodes and with recommends

00:22:49,629 --> 00:22:53,990
the important part is that with

00:22:51,710 --> 00:22:56,450
recommends we offer to users consider

00:22:53,990 --> 00:22:58,610
elder daughter so one command is clutter

00:22:56,450 --> 00:23:01,009
create and then taking monitor with

00:22:58,610 --> 00:23:03,649
cluster list what's the status and when

00:23:01,009 --> 00:23:05,840
it reaches create complete they can just

00:23:03,649 --> 00:23:07,490
do cluster configuring to retrieve all

00:23:05,840 --> 00:23:09,900
the credentials and then they can talk

00:23:07,490 --> 00:23:13,830
with a native API

00:23:09,900 --> 00:23:15,900
and deploy their applications some use

00:23:13,830 --> 00:23:19,290
cases and some very nice pictures that

00:23:15,900 --> 00:23:21,720
Michael created and how are we using

00:23:19,290 --> 00:23:24,030
containers at CERN before I was talking

00:23:21,720 --> 00:23:26,460
about OpenStack and what many people are

00:23:24,030 --> 00:23:30,800
doing including us but why we need it

00:23:26,460 --> 00:23:35,600
and we need containers deploy easily

00:23:30,800 --> 00:23:38,210
batch batch farms so we have a lot of

00:23:35,600 --> 00:23:41,580
physics data to process and a lot of

00:23:38,210 --> 00:23:44,000
simulations that we need to do and for

00:23:41,580 --> 00:23:49,050
this kind of system we use Condor or

00:23:44,000 --> 00:23:51,690
HD Condor before LSF and with kubernetes

00:23:49,050 --> 00:23:53,730
it's much easier to scale out and

00:23:51,690 --> 00:23:57,540
develop skill to public clouds as we

00:23:53,730 --> 00:24:01,140
mentioned later and also with Jupiter

00:23:57,540 --> 00:24:03,750
and Python notebooks and our notebooks

00:24:01,140 --> 00:24:05,880
and user analysis is done interactively

00:24:03,750 --> 00:24:10,620
on the web browser in most of our cases

00:24:05,880 --> 00:24:14,580
now and the Jupiter project is a it has

00:24:10,620 --> 00:24:17,040
done a lot of work in deploying on top

00:24:14,580 --> 00:24:18,660
of kubernetes so we'll have a group that

00:24:17,040 --> 00:24:21,480
is managing a centralized service

00:24:18,660 --> 00:24:23,280
offering jupiter notebooks other use

00:24:21,480 --> 00:24:26,250
cases that are wrapping up is machine

00:24:23,280 --> 00:24:30,000
learning and when we try to add GPUs and

00:24:26,250 --> 00:24:33,480
of course we the infrastructure folks

00:24:30,000 --> 00:24:35,040
are using it a lot for running simple

00:24:33,480 --> 00:24:38,550
web applications continuous integration

00:24:35,040 --> 00:24:40,350
employment run OpenStack itself so we

00:24:38,550 --> 00:24:43,260
start for a few physical machines deploy

00:24:40,350 --> 00:24:45,630
OpenStack and then we add more compute

00:24:43,260 --> 00:24:48,150
nodes which run VMs in those games to

00:24:45,630 --> 00:24:50,610
run OpenStack again and those games

00:24:48,150 --> 00:24:52,350
create clusters and those clusters that

00:24:50,610 --> 00:24:54,540
are running on OpenStack maybe run

00:24:52,350 --> 00:24:59,250
OpenStack again so I think we are in

00:24:54,540 --> 00:25:02,700
three layers and each there and finally

00:24:59,250 --> 00:25:06,179
I would describe three use cases that

00:25:02,700 --> 00:25:09,200
were using kubernetes and everything

00:25:06,179 --> 00:25:09,200
runs on Fedora Tomic

00:25:09,590 --> 00:25:17,060
the first one that we did recently is

00:25:11,540 --> 00:25:19,400
running spark on on kubernetes and spark

00:25:17,060 --> 00:25:21,920
is using some research providers as

00:25:19,400 --> 00:25:24,290
backends so the first one that was

00:25:21,920 --> 00:25:29,180
introduced was yarn which is from the

00:25:24,290 --> 00:25:32,240
Apache ecosystem but when kubernetes

00:25:29,180 --> 00:25:34,250
became popular they added driver for

00:25:32,240 --> 00:25:39,140
spark to talk directly to kubernetes and

00:25:34,250 --> 00:25:41,300
submit jobs to kubernetes and so the

00:25:39,140 --> 00:25:45,800
data analytics working group that - I

00:25:41,300 --> 00:25:48,920
borrowed this slide is busy is creating

00:25:45,800 --> 00:25:50,870
Magnum clusters and then the spark

00:25:48,920 --> 00:25:53,690
community create the spark operator

00:25:50,870 --> 00:25:57,410
which is a way to a more easy way to

00:25:53,690 --> 00:26:01,940
manage spark and they submit jobs and

00:25:57,410 --> 00:26:04,460
the governmentís and they have done

00:26:01,940 --> 00:26:06,650
integration with shared file systems to

00:26:04,460 --> 00:26:09,860
share the artifacts of the analysis

00:26:06,650 --> 00:26:13,540
later on recently that I mentioned that

00:26:09,860 --> 00:26:18,110
we have usually Birds bursts and the

00:26:13,540 --> 00:26:20,470
this group created and $1000 so we just

00:26:18,110 --> 00:26:23,620
booted in 20 minutes

00:26:20,470 --> 00:26:28,700
$1,000 class that was around 4000 cores

00:26:23,620 --> 00:26:30,740
or not 4,000 2,000 course in total to

00:26:28,700 --> 00:26:33,110
run a big kubernetes cluster that was

00:26:30,740 --> 00:26:36,320
used in full capacity for analysis which

00:26:33,110 --> 00:26:40,090
Burke as I said again everything on

00:26:36,320 --> 00:26:44,360
Fedora atomic and another huge case is

00:26:40,090 --> 00:26:51,200
reusable analysis so in the the in the

00:26:44,360 --> 00:26:53,450
beginning of this century the storage

00:26:51,200 --> 00:26:55,940
was not a problem so we could store data

00:26:53,450 --> 00:26:58,700
and then when users wanted to do some

00:26:55,940 --> 00:27:01,250
computations and they could do it how

00:26:58,700 --> 00:27:04,370
many times they wanted and they could

00:27:01,250 --> 00:27:07,670
just do the analysis on demand but now

00:27:04,370 --> 00:27:10,510
with the highest data rate what we want

00:27:07,670 --> 00:27:14,200
to do and what sir wants to do is to

00:27:10,510 --> 00:27:17,150
reuse analysis that has done before and

00:27:14,200 --> 00:27:19,820
this group that manages say this an

00:27:17,150 --> 00:27:22,750
entire request project which is like an

00:27:19,820 --> 00:27:25,510
acronym from reusable analysis platform

00:27:22,750 --> 00:27:28,390
and they want the the huge container

00:27:25,510 --> 00:27:30,730
images as artifacts so when someone has

00:27:28,390 --> 00:27:32,530
done an analysis and it has also the

00:27:30,730 --> 00:27:35,080
results in the data that he used for the

00:27:32,530 --> 00:27:40,000
analysis it creates a docker image and

00:27:35,080 --> 00:27:42,850
pushes it in registry and all T all the

00:27:40,000 --> 00:27:48,370
workflow is managed by kubernetes jobs

00:27:42,850 --> 00:27:49,930
and it's each layer is a is a

00:27:48,370 --> 00:27:53,230
self-contained job that they are

00:27:49,930 --> 00:27:56,650
submitting to kubernetes that runs on

00:27:53,230 --> 00:27:59,740
our service I think this is as far as I

00:27:56,650 --> 00:28:02,740
can go to to explain what it is but if

00:27:59,740 --> 00:28:07,090
you're in the city we can sink later but

00:28:02,740 --> 00:28:11,010
this demonstrates that the solution that

00:28:07,090 --> 00:28:14,590
you have developed and when the modular

00:28:11,010 --> 00:28:16,030
way that kubernetes is created and the

00:28:14,590 --> 00:28:18,280
compatibility that we'll have with the

00:28:16,030 --> 00:28:20,500
latest kernel fedora has has allowed us

00:28:18,280 --> 00:28:22,810
to use all certain file systems that we

00:28:20,500 --> 00:28:25,620
have which is one it's called years

00:28:22,810 --> 00:28:27,640
which is seven specific and it holds the

00:28:25,620 --> 00:28:31,470
largest amount of data that we'll have

00:28:27,640 --> 00:28:34,300
around 200 petabytes also we had CFS and

00:28:31,470 --> 00:28:36,850
severe FS to distribute software and all

00:28:34,300 --> 00:28:40,080
these are integrated and running easily

00:28:36,850 --> 00:28:45,180
with kubernetes and on Fedora tamiko's

00:28:40,080 --> 00:28:49,060
in the xxx case that we recently did his

00:28:45,180 --> 00:28:51,790
federated kubernetes clusters so in the

00:28:49,060 --> 00:28:54,610
right picture you can see that we have a

00:28:51,790 --> 00:28:56,080
host cluster that is run on CERN and it

00:28:54,610 --> 00:29:01,780
runs the control plane of the batch

00:28:56,080 --> 00:29:04,060
system and the two who the Koopalings

00:29:01,780 --> 00:29:06,130
clusters on the top are running again on

00:29:04,060 --> 00:29:07,660
the Sun cloud but on a different

00:29:06,130 --> 00:29:10,480
coordinate in the data center and the

00:29:07,660 --> 00:29:12,940
third one is run on the T system cloud

00:29:10,480 --> 00:29:16,660
which is again which is OpenStack but

00:29:12,940 --> 00:29:19,510
this is just a coincidence and the team

00:29:16,660 --> 00:29:22,540
that running the batch firm and deployed

00:29:19,510 --> 00:29:25,270
there by hand kubernetes cluster and

00:29:22,540 --> 00:29:29,530
then all these clusters joined the host

00:29:25,270 --> 00:29:31,540
cluster at CERN so when we have computer

00:29:29,530 --> 00:29:34,450
deficit let's say and we want to add

00:29:31,540 --> 00:29:36,340
more compute capacity we just go to end

00:29:34,450 --> 00:29:39,220
the cloud that can offer kubernetes to

00:29:36,340 --> 00:29:43,210
us and with the same way that we have

00:29:39,220 --> 00:29:45,489
deployed the bad system at CERN we can

00:29:43,210 --> 00:29:47,440
extend it and deploy it to the public

00:29:45,489 --> 00:29:49,720
cloud and join and submit jobs and

00:29:47,440 --> 00:29:52,029
leverage the complete capacity this is

00:29:49,720 --> 00:29:54,850
ideal for computer jobs for computing

00:29:52,029 --> 00:29:57,220
intensive jobs when you want to transfer

00:29:54,850 --> 00:29:59,169
a lot of data this is another story

00:29:57,220 --> 00:30:01,929
because then you have to pay for the all

00:29:59,169 --> 00:30:03,759
the data and transfer but for

00:30:01,929 --> 00:30:08,230
simulations which were doing a lot of

00:30:03,759 --> 00:30:11,350
Monte Carlo simulations is perfect so a

00:30:08,230 --> 00:30:14,619
conclusion is that this talk was from

00:30:11,350 --> 00:30:17,139
the users perspective and what I want to

00:30:14,619 --> 00:30:19,869
highlight is Fedora Tomac and fedora it

00:30:17,139 --> 00:30:23,679
just works for us it's not the bleeding

00:30:19,869 --> 00:30:25,809
edge it's just a minimal Vista that

00:30:23,679 --> 00:30:29,440
worked for us very well and I wanna say

00:30:25,809 --> 00:30:31,480
yes now I don't include only certain but

00:30:29,440 --> 00:30:33,879
also the OpenStack community that is

00:30:31,480 --> 00:30:36,519
using our project everyone is using the

00:30:33,879 --> 00:30:39,669
same images and we're happy with with it

00:30:36,519 --> 00:30:43,149
and we just benefit from all the work

00:30:39,669 --> 00:30:46,409
done in the project upstream and it's

00:30:43,149 --> 00:30:48,999
mutable state allow us to sink even

00:30:46,409 --> 00:30:51,519
operators in different site we can sync

00:30:48,999 --> 00:30:54,100
and share exactly the same operating

00:30:51,519 --> 00:30:56,350
systems and tested and done in one

00:30:54,100 --> 00:30:59,919
datacenter just work in another easily

00:30:56,350 --> 00:31:01,720
which might be a lot different and the

00:30:59,919 --> 00:31:03,879
closing node and the reason that we

00:31:01,720 --> 00:31:06,789
moved this dock here is that we are

00:31:03,879 --> 00:31:08,769
looking forward to Fedora core because

00:31:06,789 --> 00:31:11,739
so far we had a lot of users saying and

00:31:08,769 --> 00:31:14,409
you're mostly deploying using federer

00:31:11,739 --> 00:31:18,549
Tomic what we need to use chorus because

00:31:14,409 --> 00:31:23,379
our organization uses it and because it

00:31:18,549 --> 00:31:25,570
has a bigger community for on the

00:31:23,379 --> 00:31:27,480
container ecosystem so now with this

00:31:25,570 --> 00:31:30,940
convergence we are very happy and we

00:31:27,480 --> 00:31:32,769
don't have to hear about these

00:31:30,940 --> 00:31:35,529
complaints again and they will just live

00:31:32,769 --> 00:31:38,139
here questions if you have about Magnum

00:31:35,529 --> 00:31:41,100
and to promote my project so that's it

00:31:38,139 --> 00:31:41,100
thank you

00:31:46,150 --> 00:31:49,900
do you have any questions

00:32:08,090 --> 00:32:15,080
we do upgrades but we when we do an

00:32:12,649 --> 00:32:19,760
upgrade in the cluster we don't upgrade

00:32:15,080 --> 00:32:22,340
the node in place we delete the nodes in

00:32:19,760 --> 00:32:25,639
some cases that we didn't have a clear

00:32:22,340 --> 00:32:26,240
path how to how to do it because it was

00:32:25,639 --> 00:32:29,049
an openstack

00:32:26,240 --> 00:32:31,639
specific issue that we had we just

00:32:29,049 --> 00:32:34,490
migrated the application in another

00:32:31,639 --> 00:32:36,830
cluster but when would the upgrades we

00:32:34,490 --> 00:32:54,200
try to delete a node and not do any

00:32:36,830 --> 00:32:58,070
English upgrades each our own hardware

00:32:54,200 --> 00:33:00,289
and we have deployed OpenStack so the

00:32:58,070 --> 00:33:02,440
it's our own cloud it's our own private

00:33:00,289 --> 00:33:02,440
cloud

00:33:10,000 --> 00:33:31,389
I'm sorry I didn't get the question and

00:33:17,889 --> 00:33:32,590
the only you know the only issue that we

00:33:31,389 --> 00:33:37,090
had it was mine

00:33:32,590 --> 00:33:40,389
it wasn't sorry the question yes and the

00:33:37,090 --> 00:33:43,450
question was if you have seen any issues

00:33:40,389 --> 00:33:46,179
with kubernetes on fedora because to

00:33:43,450 --> 00:33:49,509
build kubernetes we use a more recent

00:33:46,179 --> 00:33:54,789
goal and version then the absent project

00:33:49,509 --> 00:33:59,070
is using and no we haven't seen any the

00:33:54,789 --> 00:34:02,529
only one was a minor issue too

00:33:59,070 --> 00:34:06,279
that was specific to the spec file that

00:34:02,529 --> 00:34:08,290
was doing a check and when when the

00:34:06,279 --> 00:34:10,899
version was banned from one nine to one

00:34:08,290 --> 00:34:13,000
ten the comparison wasn't working

00:34:10,899 --> 00:34:14,889
because it was indeed using sambar but

00:34:13,000 --> 00:34:18,659
other than that we didn't have an issue

00:34:14,889 --> 00:34:21,609
the only issue it's not Fedora specific

00:34:18,659 --> 00:34:24,369
it's kind of the specific is that by

00:34:21,609 --> 00:34:29,379
default from project atomic docker you

00:34:24,369 --> 00:34:32,290
six systemd as a group driver and I have

00:34:29,379 --> 00:34:35,889
noticed that deletion might be a little

00:34:32,290 --> 00:34:39,250
slower of containers when using system

00:34:35,889 --> 00:34:41,560
thing versus a group of s and some

00:34:39,250 --> 00:34:43,510
features like monitoring the nodes and

00:34:41,560 --> 00:34:46,419
having the nice graphs in the kubernetes

00:34:43,510 --> 00:34:50,290
dashboard don't work with system D as a

00:34:46,419 --> 00:34:54,359
secret driver so we move we changed to

00:34:50,290 --> 00:34:54,359
use a group of s

00:35:04,870 --> 00:35:07,960
thank you

00:35:08,170 --> 00:35:11,320

YouTube URL: https://www.youtube.com/watch?v=xAK7Co9Prr0


