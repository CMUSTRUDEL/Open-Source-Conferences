Title: Frontiers of neural arts - GitHub Satellite 2020
Publication date: 2020-05-07
Playlist: GitHub Satellite 2020 - Play
Description: 
	Presented by Gene Kogan

Over the past several years, two trends in machine learning have 
converged to pique the curiosity of artists working with code: the 
growing popularity of powerful open source deep learning frameworks like
 Torch and TensorFlow, and the emergence of data-intensive generative 
models for hallucinating images, sounds, and text as though they came 
from the oeuvre of Shakespeare, Picasso, or just a gigantic database of 
digitized cats. This talk will review these developments, present 
various artworks, and offer a set of interdisciplinary tools and 
learning resources for artists and data scientists alike, if ever there 
was a difference to begin with.

Gene Kogan is an artist and a programmer who is interested in autonomous systems, collective intelligence, generative art, and computer science. He is a collaborator within numerous open-source software projects, and gives workshops and lectures on topics at the intersection of code and art. Gene initiated ml4a, a free book about machine learning for creative practice, and regularly publishes video lectures, writings, and tutorials to facilitate a greater public understanding of the subject. 

GitHub Satellite: A community connected by code

On May 6th, we threw a free virtual event featuring developers working together on the worldâ€™s software, announcements from the GitHub team, and inspiring performances by artists who code.

More information: https://githubsatellite.com
Schedule: https://githubsatellite.com/schedule/
Captions: 
	00:00:04,690 --> 00:00:10,460
[Music]

00:00:15,740 --> 00:00:21,150
okay hi everybody thanks for thanks to

00:00:19,619 --> 00:00:22,980
get help for inviting me I'm super

00:00:21,150 --> 00:00:25,529
thrilled and another there's pain in

00:00:22,980 --> 00:00:27,180
this my name is Jen Cogan and this is

00:00:25,529 --> 00:00:30,060
called frontiers of neural Arts and I'm

00:00:27,180 --> 00:00:31,980
just going to get straight into it so

00:00:30,060 --> 00:00:33,930
just a little bit of background about

00:00:31,980 --> 00:00:37,440
myself I first became interested in

00:00:33,930 --> 00:00:40,379
machine learning around 2007-2008 things

00:00:37,440 --> 00:00:42,180
like last night FM we're first beginning

00:00:40,379 --> 00:00:44,399
to experiment with things like music

00:00:42,180 --> 00:00:46,170
recommendation systems has a big music

00:00:44,399 --> 00:00:49,020
collector and really interested in music

00:00:46,170 --> 00:00:50,610
technology and this is kind of the first

00:00:49,020 --> 00:00:53,190
question that I first became interested

00:00:50,610 --> 00:00:55,789
in learned it was a machine learning

00:00:53,190 --> 00:00:59,640
problem how to recommend music to people

00:00:55,789 --> 00:01:01,530
and in that ecosystem I kind of learned

00:00:59,640 --> 00:01:03,480
that a lot of people interested in the

00:01:01,530 --> 00:01:05,610
intersection between music and machine

00:01:03,480 --> 00:01:07,140
learning are also interested in problems

00:01:05,610 --> 00:01:09,420
how they could do with creativity and

00:01:07,140 --> 00:01:11,640
composition and production of music I

00:01:09,420 --> 00:01:13,549
started working with a good friend of

00:01:11,640 --> 00:01:16,170
mine named Jeff Snyder who built these

00:01:13,549 --> 00:01:17,880
really awesome musical instruments like

00:01:16,170 --> 00:01:20,580
what you see here this is called a burl

00:01:17,880 --> 00:01:22,200
it's an electronic flute instrument and

00:01:20,580 --> 00:01:24,390
it's just the controller has a whole

00:01:22,200 --> 00:01:26,280
bunch of breath brush it has a breath

00:01:24,390 --> 00:01:28,229
pressure sensor and an embouchure sensor

00:01:26,280 --> 00:01:30,210
on the mouthpiece and then the whole

00:01:28,229 --> 00:01:33,540
bunch of capacitive touch sensors and

00:01:30,210 --> 00:01:35,159
the trick that we employed was to well

00:01:33,540 --> 00:01:36,630
it doesn't make any sound by itself and

00:01:35,159 --> 00:01:39,840
so we would try to hook it up with a

00:01:36,630 --> 00:01:41,700
digital synthesizer which we would try

00:01:39,840 --> 00:01:44,490
to then parameterize using the

00:01:41,700 --> 00:01:46,049
controllers on the on the Conda

00:01:44,490 --> 00:01:48,270
controller that the musician was playing

00:01:46,049 --> 00:01:51,479
so it's kind of inverting the normal

00:01:48,270 --> 00:01:53,430
process for for music you know instead

00:01:51,479 --> 00:01:55,350
of learning how to play an instrument

00:01:53,430 --> 00:02:00,840
the instrument sort of learns how to how

00:01:55,350 --> 00:02:03,439
to be played by how to be play taste of

00:02:00,840 --> 00:02:03,439
what it sounded like

00:02:03,650 --> 00:02:09,030
so this kind of got me interested in

00:02:06,660 --> 00:02:11,820
just in general into interactivity and

00:02:09,030 --> 00:02:14,190
new media arts I began to take interest

00:02:11,820 --> 00:02:16,590
in things like computer vision data

00:02:14,190 --> 00:02:20,430
visualization and all of these really

00:02:16,590 --> 00:02:22,890
cool things one thing that I made that

00:02:20,430 --> 00:02:24,510
that gained some traction a few years

00:02:22,890 --> 00:02:27,090
ago was Kinect projector toolkit what

00:02:24,510 --> 00:02:29,640
you see in the right side this enables

00:02:27,090 --> 00:02:31,890
somebody to calibrate a projector video

00:02:29,640 --> 00:02:34,709
projector with a Kinect depth camera and

00:02:31,890 --> 00:02:37,410
this would allow you to create sort of

00:02:34,709 --> 00:02:40,530
spatially aware body reactive

00:02:37,410 --> 00:02:42,390
projections this was kind of tweaked to

00:02:40,530 --> 00:02:44,820
work on this piece called machine

00:02:42,390 --> 00:02:46,410
learning with a with the robot and kind

00:02:44,820 --> 00:02:48,450
of keeping my machine learning projects

00:02:46,410 --> 00:02:50,760
to the side but kind of coming back to

00:02:48,450 --> 00:02:52,800
it in a big way starting around for

00:02:50,760 --> 00:02:56,400
maybe four or five years ago when when

00:02:52,800 --> 00:02:57,570
deep learning hit so most of the rest of

00:02:56,400 --> 00:03:00,030
the stuff I'm gonna show you in this

00:02:57,570 --> 00:03:02,310
presentation is is gonna have a lot to

00:03:00,030 --> 00:03:04,290
do with neural networks and particularly

00:03:02,310 --> 00:03:07,890
in the context of computer vision and

00:03:04,290 --> 00:03:09,630
the main thing that you want to keep in

00:03:07,890 --> 00:03:11,430
mind about these that will be relevant

00:03:09,630 --> 00:03:12,959
for the rest of the projects that I show

00:03:11,430 --> 00:03:15,690
you is that neural networks are really

00:03:12,959 --> 00:03:18,209
great at learning patterns and sort of

00:03:15,690 --> 00:03:20,010
salient patterns that define the things

00:03:18,209 --> 00:03:21,510
that we're interested in so for example

00:03:20,010 --> 00:03:23,610
this is a neural network looking at an

00:03:21,510 --> 00:03:26,790
image of a cat and all of these

00:03:23,610 --> 00:03:29,040
different cells represent the amount the

00:03:26,790 --> 00:03:31,380
the activation of a particular feature

00:03:29,040 --> 00:03:33,720
or pattern in the image at every

00:03:31,380 --> 00:03:36,150
possible spatial location and you can

00:03:33,720 --> 00:03:37,440
see that some of the cells appear to be

00:03:36,150 --> 00:03:39,720
looking for things on the foreground

00:03:37,440 --> 00:03:41,549
some on the background some are looking

00:03:39,720 --> 00:03:44,040
for simple things like edges and

00:03:41,549 --> 00:03:46,440
ingredients but all of them are sort of

00:03:44,040 --> 00:03:49,410
finding features and in the features

00:03:46,440 --> 00:03:53,700
that are sort of relevant to a cap and

00:03:49,410 --> 00:03:56,400
and now this kind of what when deep

00:03:53,700 --> 00:03:58,890
learning began to make inroads in

00:03:56,400 --> 00:04:00,900
computer vision many people wanted to

00:03:58,890 --> 00:04:02,970
know what do neural networks actually

00:04:00,900 --> 00:04:05,519
see because it's a little bit of a black

00:04:02,970 --> 00:04:07,590
box we don't really know exactly what

00:04:05,519 --> 00:04:09,870
the neurons are trained to recognize and

00:04:07,590 --> 00:04:12,420
so some of the experiments that began

00:04:09,870 --> 00:04:14,400
maybe around ten years ago looking at

00:04:12,420 --> 00:04:15,630
research like like what you see in the

00:04:14,400 --> 00:04:17,130
screen above

00:04:15,630 --> 00:04:20,550
tempted to answer this question by

00:04:17,130 --> 00:04:22,590
trying to you know it make interesting

00:04:20,550 --> 00:04:24,660
procedures for generating visuals that

00:04:22,590 --> 00:04:26,340
would help us understand what neural

00:04:24,660 --> 00:04:28,980
networks were seeing and what you're

00:04:26,340 --> 00:04:31,950
looking at here are what the process

00:04:28,980 --> 00:04:33,960
would be for example for this image you

00:04:31,950 --> 00:04:36,660
want to activate the bell pepper neuron

00:04:33,960 --> 00:04:39,120
that this neural network and so the idea

00:04:36,660 --> 00:04:41,850
here would be to to optimize the pixels

00:04:39,120 --> 00:04:43,710
of an image so as to maximally activate

00:04:41,850 --> 00:04:45,900
the bell pepper neuron or the computer

00:04:43,710 --> 00:04:48,870
keyboard neuron of the ostrich neuron

00:04:45,900 --> 00:04:50,760
and this would allow us to to see what

00:04:48,870 --> 00:04:53,130
those features look like to maybe

00:04:50,760 --> 00:04:54,660
inspect features the hidden features in

00:04:53,130 --> 00:04:57,060
the in the neural network that weren't

00:04:54,660 --> 00:04:59,850
labeled but that nevertheless we could

00:04:57,060 --> 00:05:02,370
then use to interpret exactly what the

00:04:59,850 --> 00:05:05,130
network was seeing and a couple of years

00:05:02,370 --> 00:05:08,040
after some of this work began to emerge

00:05:05,130 --> 00:05:11,610
it was taken to the next level by a few

00:05:08,040 --> 00:05:15,090
researchers at Google who developed the

00:05:11,610 --> 00:05:16,890
deep tree technique and before deep

00:05:15,090 --> 00:05:18,660
dream they've they've made a whole bunch

00:05:16,890 --> 00:05:21,090
of innovations to this visualization

00:05:18,660 --> 00:05:22,830
technique which made much more vivid

00:05:21,090 --> 00:05:24,990
pictures that were much more realistic

00:05:22,830 --> 00:05:27,720
you know you could see starfish and

00:05:24,990 --> 00:05:29,220
bananas and parachutes and so on and

00:05:27,720 --> 00:05:30,930
they're very very realistic and this was

00:05:29,220 --> 00:05:34,650
kind of when neural networks first

00:05:30,930 --> 00:05:37,140
caught the eye of you know new media and

00:05:34,650 --> 00:05:39,120
interactivity and artists new media

00:05:37,140 --> 00:05:42,870
artists and interactivity designers and

00:05:39,120 --> 00:05:45,330
so on so I was super interested in this

00:05:42,870 --> 00:05:49,020
technique I began to work with it myself

00:05:45,330 --> 00:05:52,050
and the first thing that I really wanted

00:05:49,020 --> 00:05:54,600
to do was to try to gain back some of

00:05:52,050 --> 00:05:57,210
the control that an artist is used to

00:05:54,600 --> 00:05:59,760
having with with techniques that they

00:05:57,210 --> 00:06:01,140
used to make make their art one of the

00:05:59,760 --> 00:06:02,730
things about deep dream is it felt a

00:06:01,140 --> 00:06:04,680
little bit like a magic trick like it

00:06:02,730 --> 00:06:08,220
would just make these you know crazy

00:06:04,680 --> 00:06:10,020
sort of psychedelic you know visuals but

00:06:08,220 --> 00:06:12,390
that they did all of the work themselves

00:06:10,020 --> 00:06:14,870
and I was really interested in kind of

00:06:12,390 --> 00:06:18,030
trying to gain back a lever of control

00:06:14,870 --> 00:06:19,560
that I could use to compose with and the

00:06:18,030 --> 00:06:21,870
first way that I tried to do that was

00:06:19,560 --> 00:06:24,870
using masks and the idea of masks would

00:06:21,870 --> 00:06:26,640
be to try to allocate different parts of

00:06:24,870 --> 00:06:28,979
the canvas to different neurons that I

00:06:26,640 --> 00:06:29,460
wanted to visualize and so here there's

00:06:28,979 --> 00:06:31,110
a mass

00:06:29,460 --> 00:06:34,380
which goes from left to right kind of

00:06:31,110 --> 00:06:36,870
you know zero to one and then from left

00:06:34,380 --> 00:06:39,000
to right one zero and each one was

00:06:36,870 --> 00:06:40,650
devoted to a particular neuron and then

00:06:39,000 --> 00:06:42,810
if you apply this mask during this

00:06:40,650 --> 00:06:45,930
iterative process that optimizes the

00:06:42,810 --> 00:06:48,110
pixels what you get is all on one side

00:06:45,930 --> 00:06:50,190
you know one particular feature is

00:06:48,110 --> 00:06:52,199
visualized on the other side another

00:06:50,190 --> 00:06:55,169
feature and in the middle it kind of

00:06:52,199 --> 00:06:57,690
sees the attempts this sort of kind of

00:06:55,169 --> 00:07:00,569
get features of both of these neurons

00:06:57,690 --> 00:07:01,860
made it sort of satisfies both of them a

00:07:00,569 --> 00:07:03,960
little bit and so this is kind of an

00:07:01,860 --> 00:07:05,729
example a full-size example of what that

00:07:03,960 --> 00:07:08,240
looks like you could see that there's

00:07:05,729 --> 00:07:11,220
two neurons here connected by this

00:07:08,240 --> 00:07:13,349
interpolated horizontal mask and the

00:07:11,220 --> 00:07:15,150
mask can be composed however you want so

00:07:13,349 --> 00:07:18,270
in this example that you're looking at

00:07:15,150 --> 00:07:19,979
the mask is circular so there's a sort

00:07:18,270 --> 00:07:23,729
of one neuron being visualized in the

00:07:19,979 --> 00:07:25,110
center of this of this of this rectangle

00:07:23,729 --> 00:07:27,509
and then on the outside you have a

00:07:25,110 --> 00:07:31,169
another neuron which appears more like

00:07:27,509 --> 00:07:33,000
flowers something like that the

00:07:31,169 --> 00:07:36,300
technique could also be used to generate

00:07:33,000 --> 00:07:38,729
video because it always begins with an

00:07:36,300 --> 00:07:39,960
input image the input image is what you

00:07:38,729 --> 00:07:42,240
start with and then you attempt to

00:07:39,960 --> 00:07:45,030
optimize the pixels so as to meet your

00:07:42,240 --> 00:07:48,030
objectives so you could just simply take

00:07:45,030 --> 00:07:51,030
the last picture that you made and plug

00:07:48,030 --> 00:07:53,219
it in as the input to the to the to the

00:07:51,030 --> 00:07:54,690
algorithm again to make the next frame

00:07:53,219 --> 00:07:55,169
in the sequence you do this over and

00:07:54,690 --> 00:07:57,180
over

00:07:55,169 --> 00:07:59,190
maybe in between you do things like you

00:07:57,180 --> 00:08:03,539
distort the canvas somehow or you

00:07:59,190 --> 00:08:05,430
rotated or you expanded somehow and this

00:08:03,539 --> 00:08:09,210
combined with masks gave me a whole lot

00:08:05,430 --> 00:08:11,490
more control ability to compose with

00:08:09,210 --> 00:08:13,050
these techniques to create the the kinds

00:08:11,490 --> 00:08:15,120
of visuals that I wanted to see and this

00:08:13,050 --> 00:08:18,150
was sort of what I did with it

00:08:15,120 --> 00:08:21,300
here's another example here and a

00:08:18,150 --> 00:08:23,370
picture of an eye was segmented into and

00:08:21,300 --> 00:08:26,820
then each of the segments became masks

00:08:23,370 --> 00:08:28,169
and then these masks were combined with

00:08:26,820 --> 00:08:31,110
a bunch of different neurons that I

00:08:28,169 --> 00:08:32,849
chose and I had to compose this final

00:08:31,110 --> 00:08:35,279
result and you can see that there it's

00:08:32,849 --> 00:08:39,329
really great I think kind of how the

00:08:35,279 --> 00:08:41,550
masks fit in and if there's no you know

00:08:39,329 --> 00:08:43,230
sort of there's no white space there's

00:08:41,550 --> 00:08:45,630
no blending

00:08:43,230 --> 00:08:46,980
just kind of it works for it just fits

00:08:45,630 --> 00:08:50,720
in very nicely and so I was always

00:08:46,980 --> 00:08:53,070
really really really enthused about that

00:08:50,720 --> 00:08:56,420
here this is kind of doing the same

00:08:53,070 --> 00:08:58,800
feedback loop but but also creating a

00:08:56,420 --> 00:09:00,420
completely never-ending loop so the

00:08:58,800 --> 00:09:02,490
trick about these two videos that you

00:09:00,420 --> 00:09:03,960
you probably won't notice for a little

00:09:02,490 --> 00:09:06,210
while is that there are perfect loops

00:09:03,960 --> 00:09:07,770
that are just three seconds long so if I

00:09:06,210 --> 00:09:10,230
hover over the video you'll see that

00:09:07,770 --> 00:09:12,060
they keep you know that it just keeps on

00:09:10,230 --> 00:09:14,040
rotating about if they're every three

00:09:12,060 --> 00:09:15,510
seconds and so this was something that I

00:09:14,040 --> 00:09:18,420
figured out how to do and it's a little

00:09:15,510 --> 00:09:20,760
bit of a cool illusion because it looks

00:09:18,420 --> 00:09:22,020
kind of like it's always expanding

00:09:20,760 --> 00:09:24,210
outward but it never really goes

00:09:22,020 --> 00:09:26,640
anywhere so it's kind of I think of it

00:09:24,210 --> 00:09:31,800
as the visual equivalent of a Shepard

00:09:26,640 --> 00:09:33,990
tone so this endless loop can be

00:09:31,800 --> 00:09:35,790
combined with other neural neural Arts

00:09:33,990 --> 00:09:37,680
technique so this is a technique called

00:09:35,790 --> 00:09:39,480
texture synthesis very closely related

00:09:37,680 --> 00:09:42,120
style transfer which I'll mention in an

00:09:39,480 --> 00:09:44,370
in in a few slides and here there's a

00:09:42,120 --> 00:09:47,670
texture synthesis based off of the

00:09:44,370 --> 00:09:49,800
famous Japanese painting by Hokusai the

00:09:47,670 --> 00:09:51,810
Great Wave off Kanagawa and this is a

00:09:49,800 --> 00:09:54,780
just kind of zooming in endlessly

00:09:51,810 --> 00:09:56,280
rotating and it never goes anywhere and

00:09:54,780 --> 00:10:00,840
it just kind of like keeps on rotating

00:09:56,280 --> 00:10:03,750
endlessly like an endless zoom using now

00:10:00,840 --> 00:10:07,200
Kandinsky as a source texture and same

00:10:03,750 --> 00:10:09,680
basic idea and less Luke and you know

00:10:07,200 --> 00:10:11,700
you can do this pretty pretty

00:10:09,680 --> 00:10:13,500
effectively with almost anything that

00:10:11,700 --> 00:10:16,080
you put into it even things like Google

00:10:13,500 --> 00:10:18,810
Maps end up working and this is probably

00:10:16,080 --> 00:10:20,400
my favorite why why I have it last it

00:10:18,810 --> 00:10:21,900
looks kind of like a nightmare you know

00:10:20,400 --> 00:10:24,000
you're just kind of on your phone inside

00:10:21,900 --> 00:10:25,710
of the nightmare zooming in and the map

00:10:24,000 --> 00:10:29,460
just never really really resolves into

00:10:25,710 --> 00:10:32,870
anything readable more examples this is

00:10:29,460 --> 00:10:35,850
the loops in the canvas and the masking

00:10:32,870 --> 00:10:39,180
combined with with the d-train technique

00:10:35,850 --> 00:10:40,560
and again really the idea is always to

00:10:39,180 --> 00:10:42,090
try to take these very powerful

00:10:40,560 --> 00:10:45,020
techniques but give back more

00:10:42,090 --> 00:10:48,990
compositional ability too

00:10:45,020 --> 00:10:52,770
asks or through canvas distortion or

00:10:48,990 --> 00:10:55,710
some other means another example of

00:10:52,770 --> 00:10:57,270
using source images as masks on the left

00:10:55,710 --> 00:11:00,240
you see the Mona Lisa pretty

00:10:57,270 --> 00:11:02,190
recognizable all of the segments of the

00:11:00,240 --> 00:11:04,500
Mona Lisa have been devoted into masks

00:11:02,190 --> 00:11:06,720
and this is also a perfect loop and then

00:11:04,500 --> 00:11:08,550
the fellow on the right which maybe some

00:11:06,720 --> 00:11:10,860
of you recognize if you keep track of

00:11:08,550 --> 00:11:13,770
the field of deep learning this is Yong

00:11:10,860 --> 00:11:16,380
Laocoon who's one of the pioneers of

00:11:13,770 --> 00:11:18,090
deep learning he first got convolutional

00:11:16,380 --> 00:11:21,750
neural networks to work on some kind of

00:11:18,090 --> 00:11:23,580
a meaningful applied problem so I

00:11:21,750 --> 00:11:25,110
mentioned earlier style transfer this is

00:11:23,580 --> 00:11:27,210
another technique that I've had quite a

00:11:25,110 --> 00:11:29,700
lot of fun with over the last over the

00:11:27,210 --> 00:11:31,800
last few years this is basically the

00:11:29,700 --> 00:11:33,330
recomposition regeneration of one image

00:11:31,800 --> 00:11:34,860
in the style of another image so what

00:11:33,330 --> 00:11:37,320
you're looking at is the Mona Lisa in a

00:11:34,860 --> 00:11:40,350
style of an NGO in the style of Hokusai

00:11:37,320 --> 00:11:42,030
and style of of Google Maps

00:11:40,350 --> 00:11:44,760
I really never get tired of using this

00:11:42,030 --> 00:11:46,320
Google Maps texture and style transfer

00:11:44,760 --> 00:11:49,020
has been one of my most popular

00:11:46,320 --> 00:11:51,660
techniques I've you I've installed it in

00:11:49,020 --> 00:11:52,980
the form of a mirror so what you're

00:11:51,660 --> 00:11:54,930
looking at here is an installation that

00:11:52,980 --> 00:11:56,430
I call cubist mirror it's called that

00:11:54,930 --> 00:11:58,260
because the first version of it was

00:11:56,430 --> 00:12:01,290
using a cubist painting but now it's

00:11:58,260 --> 00:12:03,360
it's using a variety of different

00:12:01,290 --> 00:12:05,610
paintings and it's basically a style

00:12:03,360 --> 00:12:07,860
transfer mirror so you get in front of

00:12:05,610 --> 00:12:10,920
it and you see yourself as though you're

00:12:07,860 --> 00:12:13,620
a Picasso painting or in the style of

00:12:10,920 --> 00:12:16,470
hocus I kids really love it as you can

00:12:13,620 --> 00:12:18,510
tell so I'm gonna switch gears and talk

00:12:16,470 --> 00:12:20,070
about another area of machine learning

00:12:18,510 --> 00:12:21,930
that really excites me and that's

00:12:20,070 --> 00:12:24,270
generative models and generative models

00:12:21,930 --> 00:12:28,050
are neural networks which are capable of

00:12:24,270 --> 00:12:32,340
actually synthesizing images or text or

00:12:28,050 --> 00:12:33,990
sound they are they're extreme of

00:12:32,340 --> 00:12:36,150
extreme interest to scientists and

00:12:33,990 --> 00:12:39,000
engineers and they have many many

00:12:36,150 --> 00:12:40,410
applications in content generation so

00:12:39,000 --> 00:12:43,280
content generation for things like

00:12:40,410 --> 00:12:47,450
chatbots question answering services

00:12:43,280 --> 00:12:51,930
video games you know perhaps even

00:12:47,450 --> 00:12:54,120
completely synthetic bands and music and

00:12:51,930 --> 00:12:56,720
all of this stuff is in the early

00:12:54,120 --> 00:12:58,040
pipeline but it's but it's you know

00:12:56,720 --> 00:13:00,770
rapidly and

00:12:58,040 --> 00:13:02,240
and also very interesting the

00:13:00,770 --> 00:13:04,100
application of generative models through

00:13:02,240 --> 00:13:06,020
reinforcement learning which is the area

00:13:04,100 --> 00:13:08,530
of machine learning which is concerned

00:13:06,020 --> 00:13:10,760
with creating agents or robot or robots

00:13:08,530 --> 00:13:12,800
robots need to be able to sort of

00:13:10,760 --> 00:13:15,170
imagine the future to have an

00:13:12,800 --> 00:13:17,660
imagination in order to play it out and

00:13:15,170 --> 00:13:20,210
decide kind of to decide on some kind of

00:13:17,660 --> 00:13:23,300
an action to take generative models or

00:13:20,210 --> 00:13:25,910
neural networks which is very very at

00:13:23,300 --> 00:13:30,050
the very very most abstract since they

00:13:25,910 --> 00:13:31,550
are trained on lots and lots of images

00:13:30,050 --> 00:13:33,020
or sounds or texts or whatever it is

00:13:31,550 --> 00:13:36,080
that they that you want to train on and

00:13:33,020 --> 00:13:37,550
it's capable of outputting images which

00:13:36,080 --> 00:13:39,800
look like they came from the original

00:13:37,550 --> 00:13:42,470
data whatever it is that you trained on

00:13:39,800 --> 00:13:45,290
whether it be faces or cats or dogs or

00:13:42,470 --> 00:13:48,110
trucks or you know or airplanes and so

00:13:45,290 --> 00:13:50,000
on and they output images which do not

00:13:48,110 --> 00:13:52,340
come from the original data but look as

00:13:50,000 --> 00:13:54,110
though they could have and there's kind

00:13:52,340 --> 00:13:55,940
of a few dominant paradigms for this

00:13:54,110 --> 00:13:57,620
autoencoders where the the sort of

00:13:55,940 --> 00:14:00,380
granddaddy of generative models they've

00:13:57,620 --> 00:14:01,550
been chugging along for the last ten to

00:14:00,380 --> 00:14:03,650
fifteen years

00:14:01,550 --> 00:14:06,380
generative adversarial networks or gans

00:14:03,650 --> 00:14:08,000
have sort of up all the press in the

00:14:06,380 --> 00:14:11,740
last few years they've been responsible

00:14:08,000 --> 00:14:16,790
for most of the most of the sort of like

00:14:11,740 --> 00:14:18,590
well the most visually the images that

00:14:16,790 --> 00:14:20,990
had the highest visual fidelity let's

00:14:18,590 --> 00:14:22,490
say and of the last few years and so

00:14:20,990 --> 00:14:26,110
lots of people are pretty excited about

00:14:22,490 --> 00:14:31,790
games and the first implementation of a

00:14:26,110 --> 00:14:33,650
of again which worked on images was a DC

00:14:31,790 --> 00:14:36,920
gun which was made by these people in

00:14:33,650 --> 00:14:38,770
2015 and they demonstrated really really

00:14:36,920 --> 00:14:40,610
wild you know

00:14:38,770 --> 00:14:42,730
characteristics of this of this

00:14:40,610 --> 00:14:44,870
technique they showed how you can do

00:14:42,730 --> 00:14:47,270
arithmetic on the feature space so you

00:14:44,870 --> 00:14:49,280
can take an image of a man with glasses

00:14:47,270 --> 00:14:49,820
and then find the latent vector for just

00:14:49,280 --> 00:14:51,650
man

00:14:49,820 --> 00:14:53,690
add the latent vector for a woman and

00:14:51,650 --> 00:14:55,850
you get man with glasses - man plus

00:14:53,690 --> 00:14:58,400
woman equals woman with glasses so you

00:14:55,850 --> 00:15:02,480
have this sort of ability to add and

00:14:58,400 --> 00:15:04,340
delete and modify features a generative

00:15:02,480 --> 00:15:08,480
images as though they're kind of like

00:15:04,340 --> 00:15:10,690
DNA so I use their implementation of

00:15:08,480 --> 00:15:13,090
dcen to make a

00:15:10,690 --> 00:15:15,580
a project called a book from the sky

00:15:13,090 --> 00:15:19,150
this was my first project with Ganz and

00:15:15,580 --> 00:15:22,720
it's basically trained on a dataset of

00:15:19,150 --> 00:15:24,850
handwritten Chinese characters and which

00:15:22,720 --> 00:15:26,980
I which was being collected at the

00:15:24,850 --> 00:15:28,630
University of Harbin basically for the

00:15:26,980 --> 00:15:30,700
purposes of optical character

00:15:28,630 --> 00:15:32,830
recognition I was really interested in

00:15:30,700 --> 00:15:35,110
using them for for Ganz and so I got

00:15:32,830 --> 00:15:36,430
permission to use this data set and it's

00:15:35,110 --> 00:15:38,710
really interesting to look at these

00:15:36,430 --> 00:15:41,200
interpolations and see the different

00:15:38,710 --> 00:15:44,860
ways that people write in Chinese I

00:15:41,200 --> 00:15:46,810
learned quite a lot a bit about Chinese

00:15:44,860 --> 00:15:49,240
handwriting you can read more in this

00:15:46,810 --> 00:15:51,040
blog post that I wrote about it this is

00:15:49,240 --> 00:15:53,260
another cut from there these are

00:15:51,040 --> 00:15:56,760
interpolations between different

00:15:53,260 --> 00:16:00,100
characters and you can see that the

00:15:56,760 --> 00:16:01,660
characters that lay in between are

00:16:00,100 --> 00:16:03,730
really quite striking because it's

00:16:01,660 --> 00:16:06,220
almost as though they stand for concepts

00:16:03,730 --> 00:16:08,470
that that exists in this sort of you

00:16:06,220 --> 00:16:10,150
know this conceptual fabric that we were

00:16:08,470 --> 00:16:12,670
given from language but but that the

00:16:10,150 --> 00:16:15,220
Bernal characters devoted to and so I

00:16:12,670 --> 00:16:17,620
was I was really quite yeah this was

00:16:15,220 --> 00:16:21,850
kind of a really satisfying project for

00:16:17,620 --> 00:16:26,250
me my most recent use of these I trained

00:16:21,850 --> 00:16:28,630
again on around 100,000 paintings

00:16:26,250 --> 00:16:30,910
collected from a website called wiki our

00:16:28,630 --> 00:16:34,420
wiki art is this open source collection

00:16:30,910 --> 00:16:37,990
of you know hundreds of years worth of

00:16:34,420 --> 00:16:40,930
art and you can see that it's capable of

00:16:37,990 --> 00:16:44,460
synthesizing what looks like medieval

00:16:40,930 --> 00:16:49,090
portraits to document archives to

00:16:44,460 --> 00:16:52,030
landscapes to photographs to you know

00:16:49,090 --> 00:16:57,880
all sorts of genres are kind of inside

00:16:52,030 --> 00:16:59,589
this inside this again again in the same

00:16:57,880 --> 00:17:02,620
spirit this is usually again called big

00:16:59,589 --> 00:17:04,839
again here I'm combining different

00:17:02,620 --> 00:17:07,089
classes that are inside again and

00:17:04,839 --> 00:17:09,250
finding so for example on the left side

00:17:07,089 --> 00:17:12,189
you have an owl and in the middle you

00:17:09,250 --> 00:17:14,829
have a dog and then on the right side

00:17:12,189 --> 00:17:17,589
you have basically an owl dog this is

00:17:14,829 --> 00:17:19,870
kind of in the same idea that I showed

00:17:17,589 --> 00:17:21,929
in the DCN paper in which they're able

00:17:19,870 --> 00:17:24,400
to add and subtract features that we'll

00:17:21,929 --> 00:17:26,170
another example of this alga

00:17:24,400 --> 00:17:29,080
it looks a little bit like a Harry

00:17:26,170 --> 00:17:31,900
Potter Harry Potter character I've

00:17:29,080 --> 00:17:35,880
always kind of really enjoyed this this

00:17:31,900 --> 00:17:39,150
one and this is a owl mixed with a cap

00:17:35,880 --> 00:17:42,310
like a Snow Leopard something like that

00:17:39,150 --> 00:17:46,000
this is a this is my favorite this is a

00:17:42,310 --> 00:17:48,610
whale or an orca and then a tram in the

00:17:46,000 --> 00:17:51,370
middle and then this is the whale plus

00:17:48,610 --> 00:17:53,860
the tram and the whale plus a tram ends

00:17:51,370 --> 00:17:57,040
up looking like a ferry so this is kind

00:17:53,860 --> 00:17:58,990
of like a really unexpected sender for

00:17:57,040 --> 00:18:01,510
this result the last thing I'll show you

00:17:58,990 --> 00:18:03,640
with big and this is a on the left side

00:18:01,510 --> 00:18:06,130
what you see is a video taken from

00:18:03,640 --> 00:18:08,230
YouTube this is planet earth

00:18:06,130 --> 00:18:10,810
no BBC documentary about how beautiful

00:18:08,230 --> 00:18:13,360
our earth is and then there's a pipeline

00:18:10,810 --> 00:18:15,760
in which each frame is analyzed and

00:18:13,360 --> 00:18:17,650
predicted for all the classes all of the

00:18:15,760 --> 00:18:19,720
different objects that appear in the

00:18:17,650 --> 00:18:21,490
image and that's here that becomes the

00:18:19,720 --> 00:18:25,060
label vector which you then feed into

00:18:21,490 --> 00:18:28,420
begin in order to generate an image with

00:18:25,060 --> 00:18:32,650
those labels and so in some sense it's

00:18:28,420 --> 00:18:35,080
it's began attempting to to regenerate

00:18:32,650 --> 00:18:37,630
the video on the left and so whenever it

00:18:35,080 --> 00:18:41,620
has you know mushrooms you'll see

00:18:37,630 --> 00:18:45,610
mushrooms whenever it has different

00:18:41,620 --> 00:18:48,100
kinds of birds and monkeys you'll see

00:18:45,610 --> 00:18:54,610
those see the monkeys kind of being

00:18:48,100 --> 00:18:56,410
generated they're foxes from undersea

00:18:54,610 --> 00:18:59,070
creatures sometimes it's a little unsure

00:18:56,410 --> 00:19:01,300
so it can be it can be pretty wild

00:18:59,070 --> 00:19:02,530
another category of generative models

00:19:01,300 --> 00:19:05,950
that I'm really interested in are these

00:19:02,530 --> 00:19:07,900
image to image translation networks the

00:19:05,950 --> 00:19:10,030
first one of these that gained a lot of

00:19:07,900 --> 00:19:12,340
the attention of the art world was pics

00:19:10,030 --> 00:19:14,080
the pics and so they demonstrated how

00:19:12,340 --> 00:19:16,630
you can train a neural network to

00:19:14,080 --> 00:19:18,700
basically create the most advanced image

00:19:16,630 --> 00:19:22,720
filters so one image filter which takes

00:19:18,700 --> 00:19:25,150
a label map and then converts it into a

00:19:22,720 --> 00:19:28,290
facade like a photograph of a building

00:19:25,150 --> 00:19:30,550
or a label map into a street view scene

00:19:28,290 --> 00:19:33,640
black-and-white photo to a color

00:19:30,550 --> 00:19:35,830
photograph taking satellite imagery and

00:19:33,640 --> 00:19:38,440
converting it into maps or possibly maps

00:19:35,830 --> 00:19:41,799
and satellite imagery day photos into

00:19:38,440 --> 00:19:43,360
photos edges no edge drawing since your

00:19:41,799 --> 00:19:44,799
photographs and so on lots of different

00:19:43,360 --> 00:19:46,509
examples of this

00:19:44,799 --> 00:19:48,970
my first project with this which was

00:19:46,509 --> 00:19:51,429
with some collaborators and in Milan an

00:19:48,970 --> 00:19:53,500
open dot lab for a project we called

00:19:51,429 --> 00:19:55,990
invisible cities this is named after the

00:19:53,500 --> 00:19:57,970
book by Italo Calvino and here what we

00:19:55,990 --> 00:20:01,299
did was we treat we downloaded lots of

00:19:57,970 --> 00:20:04,240
map tiles from OpenStreetMaps and and

00:20:01,299 --> 00:20:05,590
corresponding satellite imagery from a

00:20:04,240 --> 00:20:07,389
whole bunch of different cities and

00:20:05,590 --> 00:20:09,850
train picks the pics in order to convert

00:20:07,389 --> 00:20:12,399
the map tiles into the satellite imagery

00:20:09,850 --> 00:20:14,559
tiles and then what you could do is once

00:20:12,399 --> 00:20:17,830
you have these models you can take the

00:20:14,559 --> 00:20:19,750
map map tiles from from one city and run

00:20:17,830 --> 00:20:21,639
it through the generative model of

00:20:19,750 --> 00:20:23,470
another city and so you get this sort of

00:20:21,639 --> 00:20:25,659
city style transfer so for example on

00:20:23,470 --> 00:20:28,059
the left here you have overhead of Milan

00:20:25,659 --> 00:20:30,850
and then in the middle you have Milan

00:20:28,059 --> 00:20:33,039
that part of Milan in the style of Los

00:20:30,850 --> 00:20:33,610
Angeles and then Milan in the style of

00:20:33,039 --> 00:20:36,850
Venice

00:20:33,610 --> 00:20:40,419
so on and I also use pics the pics to

00:20:36,850 --> 00:20:42,789
create sort of I guess deep fakes before

00:20:40,419 --> 00:20:46,799
they were called that this is from and I

00:20:42,789 --> 00:20:50,379
think 2016 which I basically made like a

00:20:46,799 --> 00:20:54,159
president Trump or me puppet as I called

00:20:50,379 --> 00:20:56,350
it this is a little way of puppeteering

00:20:54,159 --> 00:20:58,570
the president's face and then one year

00:20:56,350 --> 00:21:01,149
later it was it was around this quality

00:20:58,570 --> 00:21:03,549
and this is sort of my hacked version of

00:21:01,149 --> 00:21:05,649
this idea of being able to synthesize

00:21:03,549 --> 00:21:08,320
somebody you know steal somebody's face

00:21:05,649 --> 00:21:10,269
essentially in their likeness but if you

00:21:08,320 --> 00:21:12,250
but of course you know now it's becoming

00:21:10,269 --> 00:21:14,649
hyper realistic and so that you're

00:21:12,250 --> 00:21:17,110
looking at work by by Nvidia in which

00:21:14,649 --> 00:21:18,820
they are demonstrating this technique to

00:21:17,110 --> 00:21:20,740
the point that it's almost impossible to

00:21:18,820 --> 00:21:22,750
distinguish the real faces from the from

00:21:20,740 --> 00:21:24,820
the fake ones and so I kind of think

00:21:22,750 --> 00:21:27,220
sometimes my job as an artist is to warn

00:21:24,820 --> 00:21:29,200
people about the future you know so

00:21:27,220 --> 00:21:31,240
something like being able to steal

00:21:29,200 --> 00:21:34,000
somebody's face there does you two like

00:21:31,240 --> 00:21:37,809
this and didn't seem so compelling in

00:21:34,000 --> 00:21:40,059
2016 when it looked like this but you

00:21:37,809 --> 00:21:42,070
know if you're if you're looking into it

00:21:40,059 --> 00:21:46,539
you may realize that it's actually quite

00:21:42,070 --> 00:21:47,830
a bit closer than it first seems so I'm

00:21:46,539 --> 00:21:49,840
also interested in interactive

00:21:47,830 --> 00:21:51,360
installations so here you can see an

00:21:49,840 --> 00:21:53,610
installation

00:21:51,360 --> 00:21:57,390
made at a museum in Berlin called future

00:21:53,610 --> 00:21:59,010
diem this is if you get a chance to be

00:21:57,390 --> 00:22:01,290
in Berlin once all this madness dies

00:21:59,010 --> 00:22:03,840
down you can go see it it's right in the

00:22:01,290 --> 00:22:08,210
center and the idea here is that you're

00:22:03,840 --> 00:22:11,520
able to draw label maps of you know just

00:22:08,210 --> 00:22:13,920
mountains and clouds and and water and

00:22:11,520 --> 00:22:16,230
grass and trees and so on and then

00:22:13,920 --> 00:22:18,420
what's what is that's converted to on

00:22:16,230 --> 00:22:20,580
the right side is what looks like a

00:22:18,420 --> 00:22:23,160
photograph it's not completely realistic

00:22:20,580 --> 00:22:25,770
but what looks like a photograph of what

00:22:23,160 --> 00:22:28,049
you just drew and along the same lines

00:22:25,770 --> 00:22:29,460
this is invisible cities except

00:22:28,049 --> 00:22:30,870
reimagined as an interactive

00:22:29,460 --> 00:22:33,360
installation so here you have these

00:22:30,870 --> 00:22:36,450
plastic pieces which correspond to two

00:22:33,360 --> 00:22:39,059
grass tips water into buildings and then

00:22:36,450 --> 00:22:41,309
as you move them around this little

00:22:39,059 --> 00:22:43,740
playpen and there's a camera which is

00:22:41,309 --> 00:22:46,290
picking them up and then converting the

00:22:43,740 --> 00:22:48,780
image into what looks like a satellite

00:22:46,290 --> 00:22:50,160
image of Berlin and so all of these are

00:22:48,780 --> 00:22:52,440
installations have been I've been

00:22:50,160 --> 00:22:54,090
pleased to use for the last little while

00:22:52,440 --> 00:22:55,260
another interactive installation shows

00:22:54,090 --> 00:22:57,780
with the collaborator of mine named

00:22:55,260 --> 00:23:00,360
andreas Jeff's card and the idea here is

00:22:57,780 --> 00:23:02,580
that you can draw musical instruments

00:23:00,360 --> 00:23:04,740
and then have the music music

00:23:02,580 --> 00:23:06,910
automatically generated using those

00:23:04,740 --> 00:23:09,959
instruments so let's see

00:23:06,910 --> 00:23:09,959
[Music]

00:23:16,980 --> 00:23:22,599
[Music]

00:23:27,830 --> 00:23:32,130
[Music]

00:23:41,160 --> 00:23:51,220
actually on another repository that had

00:23:48,730 --> 00:23:52,960
quite a lot of fun with boys glow this

00:23:51,220 --> 00:23:54,310
is an invertible generative model which

00:23:52,960 --> 00:23:56,860
means that you can take an existing

00:23:54,310 --> 00:23:59,350
photograph like a real image and then

00:23:56,860 --> 00:24:00,970
back projected into the latent space so

00:23:59,350 --> 00:24:03,520
that you could then generate the image

00:24:00,970 --> 00:24:05,110
with the model and there then once you

00:24:03,520 --> 00:24:06,610
have it inside the model you can do all

00:24:05,110 --> 00:24:09,430
of the things that the model allows you

00:24:06,610 --> 00:24:12,370
to do make feature edits subtractions

00:24:09,430 --> 00:24:14,140
and additions and so on and so you know

00:24:12,370 --> 00:24:16,090
I could take a picture of myself and

00:24:14,140 --> 00:24:19,840
make my hair blonde put glasses of

00:24:16,090 --> 00:24:21,790
myself makeup and so on and this is just

00:24:19,840 --> 00:24:23,350
you know playing with all of the

00:24:21,790 --> 00:24:25,690
different you know versions of myself

00:24:23,350 --> 00:24:28,840
that might exist on there in some

00:24:25,690 --> 00:24:30,730
parros's and you could also use it to

00:24:28,840 --> 00:24:32,530
make interpolation between yourself and

00:24:30,730 --> 00:24:34,330
other people this is me being converted

00:24:32,530 --> 00:24:37,660
into a certain Canadian pop star that

00:24:34,330 --> 00:24:40,360
you might all recognise this is me being

00:24:37,660 --> 00:24:43,420
converted into some heads of state

00:24:40,360 --> 00:24:46,990
that's well I'm sure you recognize all

00:24:43,420 --> 00:24:48,760
of them yeah well I'm just saying have

00:24:46,990 --> 00:24:51,760
you ever seen me in the room with them

00:24:48,760 --> 00:24:55,390
with any of them at the same time that's

00:24:51,760 --> 00:24:56,860
really all there's another app that I

00:24:55,390 --> 00:24:58,600
had little fun with maybe some of you

00:24:56,860 --> 00:25:00,040
remember face app it was pretty popular

00:24:58,600 --> 00:25:02,530
in the app store for a little while and

00:25:00,040 --> 00:25:04,450
iPhone and it allows you to take a

00:25:02,530 --> 00:25:06,550
picture of a face then put her a smile

00:25:04,450 --> 00:25:08,380
on it and so this is not my actual smile

00:25:06,550 --> 00:25:10,810
this is just a saps

00:25:08,380 --> 00:25:12,520
you know version of my smile and then

00:25:10,810 --> 00:25:13,720
what you could do with I thought it

00:25:12,520 --> 00:25:15,880
would be interesting to take the output

00:25:13,720 --> 00:25:17,470
of that image and then run it again

00:25:15,880 --> 00:25:18,880
through the smile filter and then take

00:25:17,470 --> 00:25:20,350
the output of that image and running

00:25:18,880 --> 00:25:21,230
again through the smile filter and so if

00:25:20,350 --> 00:25:22,910
you've ever been interested

00:25:21,230 --> 00:25:25,549
what that might look like it looks

00:25:22,910 --> 00:25:28,820
basically something like this and then

00:25:25,549 --> 00:25:31,370
right around now it stops picking up

00:25:28,820 --> 00:25:34,490
faces and so the algorithm basically

00:25:31,370 --> 00:25:35,780
breaks at this point so there's someone

00:25:34,490 --> 00:25:39,530
told me that this looks a little bit

00:25:35,780 --> 00:25:42,080
like a text win so I think that was kind

00:25:39,530 --> 00:25:43,760
of what I'm going for and then you know

00:25:42,080 --> 00:25:45,710
just having fun lots of ways I think

00:25:43,760 --> 00:25:47,860
there's a lot of fun to be had in these

00:25:45,710 --> 00:25:50,480
kinds of neural arts here I'm projecting

00:25:47,860 --> 00:25:52,130
Einsteins face into the Crab Nebula this

00:25:50,480 --> 00:25:55,160
is really closely related can style

00:25:52,130 --> 00:25:57,890
transfer and then so I started inserting

00:25:55,160 --> 00:25:59,650
myself into various paintings I know

00:25:57,890 --> 00:26:03,799
this is super creepy here at me behind

00:25:59,650 --> 00:26:08,570
me behind here yeah just kind of lots of

00:26:03,799 --> 00:26:12,410
having having fun with this yeah it's

00:26:08,570 --> 00:26:14,330
basically the the idea so I'm gonna tell

00:26:12,410 --> 00:26:18,169
you about one more one more project here

00:26:14,330 --> 00:26:19,730
called Abraham which is it's going a

00:26:18,169 --> 00:26:21,410
project to create what I call an

00:26:19,730 --> 00:26:23,360
autonomous artificial artist I've been

00:26:21,410 --> 00:26:26,260
working on this very slowly for the last

00:26:23,360 --> 00:26:28,429
two-and-a-half years and it's a

00:26:26,260 --> 00:26:30,890
futuristic project and so I'm making

00:26:28,429 --> 00:26:33,710
very slow incremental progress the

00:26:30,890 --> 00:26:35,750
objectives to create an agent which

00:26:33,710 --> 00:26:38,390
makes unique and original art and it

00:26:35,750 --> 00:26:41,330
demonstrates autonomy and that means

00:26:38,390 --> 00:26:43,190
that it's that it's an AI artist it's

00:26:41,330 --> 00:26:44,870
it's really sort of a missing piece in

00:26:43,190 --> 00:26:46,940
AI in my opinion which is which is

00:26:44,870 --> 00:26:49,280
autonomy and it kind of follows in the

00:26:46,940 --> 00:26:52,760
tradition of AI arts projects like Aaron

00:26:49,280 --> 00:26:54,200
by Harold Cohen and dancing fool and

00:26:52,760 --> 00:26:56,150
lots of these projects in which they

00:26:54,200 --> 00:26:58,790
attempted to create agents which which

00:26:56,150 --> 00:27:01,040
autonomously make works and so I've been

00:26:58,790 --> 00:27:03,110
interesting this idea of what really

00:27:01,040 --> 00:27:05,720
true autonomy looks like in in a

00:27:03,110 --> 00:27:08,600
computer and I thought that we can I

00:27:05,720 --> 00:27:11,860
looked in nature for some inspiration so

00:27:08,600 --> 00:27:14,870
I find inspiration from things like

00:27:11,860 --> 00:27:17,090
beehives and flocks of bees and termite

00:27:14,870 --> 00:27:19,700
mounds and all of these are examples of

00:27:17,090 --> 00:27:21,980
super organisms and the idea that I'm

00:27:19,700 --> 00:27:24,950
kind of after is this metaphor we have

00:27:21,980 --> 00:27:27,970
in which we associate some kind of

00:27:24,950 --> 00:27:30,590
intelligence no collective intelligence

00:27:27,970 --> 00:27:32,570
from a super organism you know so that

00:27:30,590 --> 00:27:34,370
that that the hive of bees has its own

00:27:32,570 --> 00:27:39,110
mind the hive mind

00:27:34,370 --> 00:27:41,870
and and this is this is kind of the the

00:27:39,110 --> 00:27:44,630
the the metaphor that I'm using to guide

00:27:41,870 --> 00:27:47,690
the development of this program and so

00:27:44,630 --> 00:27:50,240
my idea is to try to achieve this same

00:27:47,690 --> 00:27:53,000
sort of the same sort of collective

00:27:50,240 --> 00:27:56,480
intelligence by creating a generative

00:27:53,000 --> 00:27:58,610
art program whose behavior is yeah it is

00:27:56,480 --> 00:28:01,400
emerges from the collective intelligence

00:27:58,610 --> 00:28:02,960
of the people who who make it and so

00:28:01,400 --> 00:28:05,510
it's kind of this decentralized

00:28:02,960 --> 00:28:07,580
peer-to-peer network of of actors who

00:28:05,510 --> 00:28:10,580
are collaborating together to make

00:28:07,580 --> 00:28:12,380
something which behaves in earnest in a

00:28:10,580 --> 00:28:15,980
way that's unpredictable to each of them

00:28:12,380 --> 00:28:20,540
and so I I'm kind of really after this

00:28:15,980 --> 00:28:22,700
hive mind metaphor to try to to you know

00:28:20,540 --> 00:28:26,559
to demonstrate something that really

00:28:22,700 --> 00:28:28,670
truly has its own agency and the

00:28:26,559 --> 00:28:30,580
construction that I that I think might

00:28:28,670 --> 00:28:33,500
be able to achieve this is they

00:28:30,580 --> 00:28:35,330
decentralized the centralized machine

00:28:33,500 --> 00:28:39,380
learning process in which a generative

00:28:35,330 --> 00:28:42,320
model is trained and using a set of

00:28:39,380 --> 00:28:45,050
technologies which which allow a group

00:28:42,320 --> 00:28:47,510
of people to co-own that model together

00:28:45,050 --> 00:28:49,160
as a shared secret and so these are

00:28:47,510 --> 00:28:51,170
technologies including federated

00:28:49,160 --> 00:28:53,630
learning secure multi-party computation

00:28:51,170 --> 00:28:56,540
lots of different emerging techniques

00:28:53,630 --> 00:28:59,690
and insecure and private AI that allow a

00:28:56,540 --> 00:29:02,830
group of collaborators to to essentially

00:28:59,690 --> 00:29:06,200
co-owned a neural network together to

00:29:02,830 --> 00:29:08,809
and and and these are of course much

00:29:06,200 --> 00:29:10,820
more general that the idea is that you

00:29:08,809 --> 00:29:13,400
know a group of people can use machine

00:29:10,820 --> 00:29:17,320
learning to make something that creates

00:29:13,400 --> 00:29:20,300
value for them and and that can that can

00:29:17,320 --> 00:29:22,309
that can stand for for things that we

00:29:20,300 --> 00:29:23,960
find valuable in many many fields you

00:29:22,309 --> 00:29:25,730
know medical applications financial

00:29:23,960 --> 00:29:29,150
applications social applications and so

00:29:25,730 --> 00:29:30,920
on and maybe it can also make art and so

00:29:29,150 --> 00:29:32,360
this is kind of the the goal of this

00:29:30,920 --> 00:29:33,230
project that it's a really slow

00:29:32,360 --> 00:29:35,630
work-in-progress

00:29:33,230 --> 00:29:38,660
it's very multi dear multi-dimensional

00:29:35,630 --> 00:29:40,700
kind of draws from from these entropy

00:29:38,660 --> 00:29:43,250
centralization technologies from AI and

00:29:40,700 --> 00:29:45,580
computer art and and from philosophy of

00:29:43,250 --> 00:29:47,990
mind and and so it's it's really I know

00:29:45,580 --> 00:29:49,910
my new obsession

00:29:47,990 --> 00:29:51,350
okay so I just have a few seconds left

00:29:49,910 --> 00:29:53,090
I'm just gonna really quickly mention I

00:29:51,350 --> 00:29:54,860
do a lot of workshops I've been teaching

00:29:53,090 --> 00:29:56,930
workshops as a full-time job for the

00:29:54,860 --> 00:29:59,510
last couple of years and all of this has

00:29:56,930 --> 00:30:01,430
led into a book called machine learning

00:29:59,510 --> 00:30:03,770
for artists this is a website online at

00:30:01,430 --> 00:30:06,140
ml Freight I give her but IO where I

00:30:03,770 --> 00:30:07,700
create a home where I and along with

00:30:06,140 --> 00:30:09,970
collaborators including add Reyes who I

00:30:07,700 --> 00:30:13,010
mentioned earlier we work on lots of

00:30:09,970 --> 00:30:16,220
demonstrations tutorials applications

00:30:13,010 --> 00:30:17,900
that are able to show you how to use

00:30:16,220 --> 00:30:20,450
machine learning for your own work I

00:30:17,900 --> 00:30:22,880
also teach classes online and we can

00:30:20,450 --> 00:30:24,770
post those on ml for a so if you're

00:30:22,880 --> 00:30:27,440
interested in all of the 30 hours of

00:30:24,770 --> 00:30:28,730
this then you can go learn for yourself

00:30:27,440 --> 00:30:30,560
how this stuff works

00:30:28,730 --> 00:30:32,420
I'm also an advisor to run the way you

00:30:30,560 --> 00:30:35,780
probably just saw Chris and honest Asus

00:30:32,420 --> 00:30:37,970
I'm really big fan of Runway and very

00:30:35,780 --> 00:30:39,680
excited about the ability to kind of

00:30:37,970 --> 00:30:42,470
make this stuff super easy for people to

00:30:39,680 --> 00:30:44,570
use so so that's all the time I have

00:30:42,470 --> 00:30:46,750
thanks for listening and I'll see you

00:30:44,570 --> 00:30:46,750

YouTube URL: https://www.youtube.com/watch?v=Qyoz5p6WZUg


