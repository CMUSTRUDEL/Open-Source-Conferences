Title: Keynote: Sci-fi Destroys the World, Science Builds It - Daniel Wilson, Roboticist & Author
Publication date: 2018-03-12
Playlist: Embedded Linux Conference + OpenIoT Summit NA 2018
Description: 
	Keynote: Sci-fi Destroys the World, Science Builds It - Daniel Wilson, Roboticist & Author

Science fiction has a strong influence on real-world research and development, often shaping the look and behavior of new inventions. But every new technology alters the landscape of sci-fi. From dystopia to utopia to boring old reality, how does the exchange of ideas between Hollywood and academia and industry help prepare humankind to imagine new and amazing futures?

About Daniel Wilson
Daniel H. Wilson is a Cherokee citizen and author of the New York Times bestselling Robopocalypse and its sequel Robogenesis, as well as seven other books, including How to Survive a Robot Uprising, A Boy and His Bot, and Amped. He earned a PhD in Robotics from Carnegie Mellon University, as well as Masters degrees in Artificial Intelligence and Robotics. His latest novel is called The Clockwork Dynasty. Wilson lives in Portland, Oregon.
Captions: 
	00:00:00,000 --> 00:00:03,030
thanks a lot for that very kind

00:00:01,380 --> 00:00:03,840
introduction and thanks to all of you

00:00:03,030 --> 00:00:06,750
for being here

00:00:03,840 --> 00:00:09,330
I am excited and intimidated to be here

00:00:06,750 --> 00:00:12,420
because you know all that PhD stuff for

00:00:09,330 --> 00:00:15,030
me was about 12 years ago and since then

00:00:12,420 --> 00:00:17,760
I you know realized that Sciences is

00:00:15,030 --> 00:00:19,470
really hard and writing science fiction

00:00:17,760 --> 00:00:20,820
is just so much quicker you know there's

00:00:19,470 --> 00:00:23,430
you don't have to go into a laboratory

00:00:20,820 --> 00:00:24,750
you just hang out at the coffee shop so

00:00:23,430 --> 00:00:27,570
that's what I've been doing for the last

00:00:24,750 --> 00:00:31,529
decade and that's why today I want to

00:00:27,570 --> 00:00:34,739
talk to you about killer robots so yes

00:00:31,529 --> 00:00:37,739
they are a staple of science fiction but

00:00:34,739 --> 00:00:40,440
they're also real and so we need to talk

00:00:37,739 --> 00:00:43,559
about how we're going to avoid being

00:00:40,440 --> 00:00:45,600
sliced into tiny bloody pieces by robots

00:00:43,559 --> 00:00:49,289
that in our hubris we decided to build

00:00:45,600 --> 00:00:50,579
with buzz saws for hands and I can tell

00:00:49,289 --> 00:00:53,640
some of you were thinking about doing

00:00:50,579 --> 00:00:57,090
that just wrong just from looking out so

00:00:53,640 --> 00:00:59,309
that sounds silly but there are a lot of

00:00:57,090 --> 00:01:02,070
really smart really high-profile people

00:00:59,309 --> 00:01:04,920
that it turns out are really afraid of

00:01:02,070 --> 00:01:08,610
killer robots so people like Stephen

00:01:04,920 --> 00:01:10,409
Hawking Elon Musk Bill Gates Elon just

00:01:08,610 --> 00:01:12,960
gave 10 million dollars to the future of

00:01:10,409 --> 00:01:15,659
life Institute a group dedicated to

00:01:12,960 --> 00:01:18,299
mitigating existential risks facing

00:01:15,659 --> 00:01:21,000
humanity due to the development of human

00:01:18,299 --> 00:01:24,509
level AI and similar mission statements

00:01:21,000 --> 00:01:26,580
are shared by the lifeboat Institute the

00:01:24,509 --> 00:01:28,409
Cambridge Centre for existential risk

00:01:26,580 --> 00:01:30,600
the machine intelligence Research

00:01:28,409 --> 00:01:33,390
Institute and the future of humanity

00:01:30,600 --> 00:01:34,110
Institute so that is a lot of

00:01:33,390 --> 00:01:36,990
Institute's

00:01:34,110 --> 00:01:39,270
all right something must be up something

00:01:36,990 --> 00:01:40,920
has frightened our billionaires ladies

00:01:39,270 --> 00:01:43,710
and gentlemen we owe it to them to

00:01:40,920 --> 00:01:43,920
explore what that is so let's get into

00:01:43,710 --> 00:01:47,340
it

00:01:43,920 --> 00:01:50,220
let's talk about killer robots so the

00:01:47,340 --> 00:01:53,040
cultural icon of the killer robot goes

00:01:50,220 --> 00:01:56,040
all the way back to the inception of the

00:01:53,040 --> 00:01:58,290
word robot itself so the word is check

00:01:56,040 --> 00:02:01,229
and it means laborer and it was coined

00:01:58,290 --> 00:02:06,180
in are you are Rossum's Universal robots

00:02:01,229 --> 00:02:08,810
a play produced in 1920 in which robots

00:02:06,180 --> 00:02:12,540
revolted against their human masters and

00:02:08,810 --> 00:02:15,720
you guessed it killed all humans

00:02:12,540 --> 00:02:18,569
the earthbender would be proud so from

00:02:15,720 --> 00:02:20,519
there things really got going in the 30s

00:02:18,569 --> 00:02:23,370
and 40s in the era of pulp

00:02:20,519 --> 00:02:25,560
science-fiction movies so back then

00:02:23,370 --> 00:02:28,769
robots were monsters right and they were

00:02:25,560 --> 00:02:32,610
mostly concerned with kidnapping

00:02:28,769 --> 00:02:34,799
scantily clad women from square-jawed

00:02:32,610 --> 00:02:37,140
space heroes which has always confused

00:02:34,799 --> 00:02:38,790
me I don't know what they were planning

00:02:37,140 --> 00:02:40,680
to do I don't know what the end game was

00:02:38,790 --> 00:02:44,060
for those robots and as it turns out I

00:02:40,680 --> 00:02:48,329
don't really want to know actually so a

00:02:44,060 --> 00:02:50,430
hundred years later a lot has is the

00:02:48,329 --> 00:02:53,370
same and a lot hasn't changed with the

00:02:50,430 --> 00:02:56,790
idea of killer robots as monsters so

00:02:53,370 --> 00:02:59,069
this fundamental killer robot theme you

00:02:56,790 --> 00:03:02,310
know that we've got this creation made

00:02:59,069 --> 00:03:05,159
in the image of human kind and yet bent

00:03:02,310 --> 00:03:06,629
on destroying its own creators this has

00:03:05,159 --> 00:03:09,420
really been relevant all the way from

00:03:06,629 --> 00:03:12,060
Frankenstein's monster in 1818 up to

00:03:09,420 --> 00:03:15,780
David from last year's aliens movie in

00:03:12,060 --> 00:03:18,780
2017 it's an enduring theme and I think

00:03:15,780 --> 00:03:21,690
part of that reason why is that it

00:03:18,780 --> 00:03:24,780
really changes with the times so our

00:03:21,690 --> 00:03:28,349
robot stories have evolved alongside our

00:03:24,780 --> 00:03:30,720
society back in the 1920s ru r was

00:03:28,349 --> 00:03:32,760
really about the working-class revolting

00:03:30,720 --> 00:03:35,430
against their bosses so essentially it

00:03:32,760 --> 00:03:37,680
was about income inequality which makes

00:03:35,430 --> 00:03:40,739
sense whenever you're in a in a society

00:03:37,680 --> 00:03:42,900
what's ruled by robber barons early last

00:03:40,739 --> 00:03:45,030
century our society was more concerned

00:03:42,900 --> 00:03:47,609
was more religious and we see more

00:03:45,030 --> 00:03:50,129
stories about the fear of playing God

00:03:47,609 --> 00:03:52,590
and daring to create life and then later

00:03:50,129 --> 00:03:55,500
in the 60s the robot stories start to

00:03:52,590 --> 00:03:58,549
evolve into stuff like The Day the Earth

00:03:55,500 --> 00:04:00,900
Stood Still representing our fears about

00:03:58,549 --> 00:04:04,379
nuclear holocaust and creating tools

00:04:00,900 --> 00:04:07,560
that are too powerful and now today we

00:04:04,379 --> 00:04:11,459
we do have more nuanced robots than just

00:04:07,560 --> 00:04:13,829
terminators and Ultron's we have great

00:04:11,459 --> 00:04:17,190
stories like Blade Runner 2049 ex

00:04:13,829 --> 00:04:18,810
machina robot and Frank and her that are

00:04:17,190 --> 00:04:21,150
really exploring in depth our

00:04:18,810 --> 00:04:23,669
relationship with technology and I think

00:04:21,150 --> 00:04:26,230
that all of these stories resonate

00:04:23,669 --> 00:04:28,810
across the centuries because

00:04:26,230 --> 00:04:31,630
at its most basic level the killer robot

00:04:28,810 --> 00:04:34,330
represents a fundamental fear that we

00:04:31,630 --> 00:04:36,610
all have and that our ancestors have had

00:04:34,330 --> 00:04:39,610
and that's just the question of do we

00:04:36,610 --> 00:04:41,140
trust ourselves and in particular do we

00:04:39,610 --> 00:04:44,470
trust ourselves with these powerful

00:04:41,140 --> 00:04:46,570
tools that were able to build so by the

00:04:44,470 --> 00:04:50,380
way no data on my slides just pretty

00:04:46,570 --> 00:04:54,790
pictures ten years since the degree so

00:04:50,380 --> 00:04:57,130
we are a tool making species right so we

00:04:54,790 --> 00:04:58,930
have to be nature is actively trying to

00:04:57,130 --> 00:05:01,330
kill us at all times if we don't build

00:04:58,930 --> 00:05:03,790
tools to defend ourselves that's it for

00:05:01,330 --> 00:05:08,110
us our instinct to build these tools is

00:05:03,790 --> 00:05:09,910
innate and so imagine that for around a

00:05:08,110 --> 00:05:12,040
couple hundred thousand years at least

00:05:09,910 --> 00:05:14,440
Homo sapiens have been out there

00:05:12,040 --> 00:05:16,290
building more and more powerful tools

00:05:14,440 --> 00:05:19,210
and we've been using these tools to

00:05:16,290 --> 00:05:22,720
transform our planet and ourselves in

00:05:19,210 --> 00:05:25,150
good and bad ways and so what I wonder

00:05:22,720 --> 00:05:28,330
is would we have survived this long

00:05:25,150 --> 00:05:30,940
if fearing our tools was not an innate

00:05:28,330 --> 00:05:34,840
part of the process of building the

00:05:30,940 --> 00:05:37,270
tools so we have to be able to envision

00:05:34,840 --> 00:05:40,450
different futures the fear makes us

00:05:37,270 --> 00:05:42,880
think we're drawn toward the utopias and

00:05:40,450 --> 00:05:45,280
we veer away from the dystopian and I

00:05:42,880 --> 00:05:47,530
think that fear protects us as we grow

00:05:45,280 --> 00:05:50,920
and explore and become more powerful and

00:05:47,530 --> 00:05:53,080
capable as a species and you may also

00:05:50,920 --> 00:05:54,910
notice that what I'm trying to say here

00:05:53,080 --> 00:05:57,040
is envisioning all these different

00:05:54,910 --> 00:06:00,250
possible futures perhaps through some

00:05:57,040 --> 00:06:01,870
sort of science fiction you know not the

00:06:00,250 --> 00:06:04,750
science but science fiction

00:06:01,870 --> 00:06:06,790
I'm just equated that with being

00:06:04,750 --> 00:06:08,800
completely necessary for the survival of

00:06:06,790 --> 00:06:12,340
our species it's a very important thing

00:06:08,800 --> 00:06:13,930
for people to do so you can take that

00:06:12,340 --> 00:06:18,520
with a grain of salt since that's my

00:06:13,930 --> 00:06:21,250
profession all right so that said the

00:06:18,520 --> 00:06:24,010
real robots out there you know they're

00:06:21,250 --> 00:06:26,410
they're really out there and they must

00:06:24,010 --> 00:06:28,840
be a threat right why are our

00:06:26,410 --> 00:06:31,090
billionaires shaking in their loafers

00:06:28,840 --> 00:06:34,450
you know there must be some real reason

00:06:31,090 --> 00:06:36,790
so let's let's look at that I think that

00:06:34,450 --> 00:06:39,190
what we're afraid of is what we can't

00:06:36,790 --> 00:06:39,820
predict we're afraid that the rate of

00:06:39,190 --> 00:06:41,680
technological

00:06:39,820 --> 00:06:44,590
progress is getting out of control and

00:06:41,680 --> 00:06:46,960
ultimately where this is embodied by

00:06:44,590 --> 00:06:48,490
something called the singularity so it's

00:06:46,960 --> 00:06:50,710
worth mentioning that a science fiction

00:06:48,490 --> 00:06:53,970
author coined the term singularity

00:06:50,710 --> 00:06:56,560
Vernor Vinge II and the idea is that

00:06:53,970 --> 00:06:58,690
it's a hypothetical situation in which

00:06:56,560 --> 00:07:00,490
we build a machine that can build a

00:06:58,690 --> 00:07:01,990
smarter version of itself and then that

00:07:00,490 --> 00:07:04,660
version does the same thing and this

00:07:01,990 --> 00:07:08,170
iterates until we have a god-like

00:07:04,660 --> 00:07:11,920
intellect that's trapped in a box and so

00:07:08,170 --> 00:07:15,130
as it happens if that did occur that

00:07:11,920 --> 00:07:17,080
could cause some problems and as a

00:07:15,130 --> 00:07:18,760
science fiction author these are my

00:07:17,080 --> 00:07:20,650
bread and butter so I love these

00:07:18,760 --> 00:07:25,390
problems I love to think about them the

00:07:20,650 --> 00:07:28,300
first one is loss of humanity so human

00:07:25,390 --> 00:07:30,910
beings could copy their brains into

00:07:28,300 --> 00:07:33,100
computers or replace their brains with

00:07:30,910 --> 00:07:35,770
computers and then BOOM we don't have

00:07:33,100 --> 00:07:38,260
anymore people or at least not in the

00:07:35,770 --> 00:07:41,020
way that we understand people right now

00:07:38,260 --> 00:07:44,230
so a great example of this is in Cory

00:07:41,020 --> 00:07:46,420
Doctorow novella I Robot in which a

00:07:44,230 --> 00:07:48,280
father is searching for his daughter

00:07:46,420 --> 00:07:50,560
who's gone missing and when he finds her

00:07:48,280 --> 00:07:52,600
he realizes that she's made of copy of

00:07:50,560 --> 00:07:53,950
her brain and put it into a computer and

00:07:52,600 --> 00:07:56,080
then made a thousand more copies of

00:07:53,950 --> 00:07:59,410
herself and then one of those copies

00:07:56,080 --> 00:08:00,940
gets killed and he's left wondering

00:07:59,410 --> 00:08:03,250
whether he should be mourning for his

00:08:00,940 --> 00:08:05,920
daughter or how he should feel and she's

00:08:03,250 --> 00:08:08,350
entered into a post human existence and

00:08:05,920 --> 00:08:09,910
so she's really they've destroyed the

00:08:08,350 --> 00:08:12,310
natural ties between father and daughter

00:08:09,910 --> 00:08:14,710
and lost some humanity there another

00:08:12,310 --> 00:08:16,180
example of a negative outcome is

00:08:14,710 --> 00:08:18,940
economic collapse

00:08:16,180 --> 00:08:21,670
so when super intelligent robots are

00:08:18,940 --> 00:08:23,560
able to take every single job it'll

00:08:21,670 --> 00:08:25,810
leave nothing for humanity to do except

00:08:23,560 --> 00:08:27,340
you know starve to death right so this

00:08:25,810 --> 00:08:31,270
one is really interesting to think about

00:08:27,340 --> 00:08:33,640
what is the last job that human beings

00:08:31,270 --> 00:08:36,070
will keep you know as robots get smarter

00:08:33,640 --> 00:08:39,729
and more capable now when you ask people

00:08:36,070 --> 00:08:41,590
this question they usually well always

00:08:39,729 --> 00:08:44,350
come back with a version of why the

00:08:41,590 --> 00:08:45,610
robot will never take my job that's it's

00:08:44,350 --> 00:08:48,070
not gonna happen to me

00:08:45,610 --> 00:08:48,670
you know I Drive trucks right that's

00:08:48,070 --> 00:08:51,550
impossible

00:08:48,670 --> 00:08:53,380
so until it happens you know our

00:08:51,550 --> 00:08:54,760
instinct is to draw a line in the sand

00:08:53,380 --> 00:08:57,490
and if you look at the development of AI

00:08:54,760 --> 00:09:00,280
it's just lines in the sand just all the

00:08:57,490 --> 00:09:02,170
way to the horizon as we keep stepping

00:09:00,280 --> 00:09:09,220
back and realizing that machines are

00:09:02,170 --> 00:09:12,550
more and more capable so I think that

00:09:09,220 --> 00:09:15,730
there is one ability that we have that

00:09:12,550 --> 00:09:18,220
the robots can't take away from us um so

00:09:15,730 --> 00:09:20,530
think of this novel a million little

00:09:18,220 --> 00:09:23,650
things by James Frey so it's this huge

00:09:20,530 --> 00:09:25,570
bestseller it's a memoir its lauded it's

00:09:23,650 --> 00:09:27,130
incredibly valuable and then this one

00:09:25,570 --> 00:09:30,460
piece of information about the book

00:09:27,130 --> 00:09:33,700
changes we learned the fact that no

00:09:30,460 --> 00:09:35,500
person actually lived through what

00:09:33,700 --> 00:09:38,710
happened in the book so no human being

00:09:35,500 --> 00:09:40,270
experienced it and the author had lied

00:09:38,710 --> 00:09:42,670
about living through the sixties

00:09:40,270 --> 00:09:44,650
experiences of drug addiction so

00:09:42,670 --> 00:09:47,230
suddenly the book loses most of its

00:09:44,650 --> 00:09:50,440
value no the words change nothing in the

00:09:47,230 --> 00:09:53,860
book changed Oprah got very angry you

00:09:50,440 --> 00:09:56,140
know it got her angry DUP and and so I'm

00:09:53,860 --> 00:09:58,000
the question is what is it about the

00:09:56,140 --> 00:09:59,890
book that changed and the answer is the

00:09:58,000 --> 00:10:02,470
one thing that we have that the robots

00:09:59,890 --> 00:10:03,160
can't have and that's just the human

00:10:02,470 --> 00:10:05,950
experience

00:10:03,160 --> 00:10:07,990
the act of sucking air and being a human

00:10:05,950 --> 00:10:09,820
being on the planet Earth now if you

00:10:07,990 --> 00:10:12,070
don't think that that's valuable you

00:10:09,820 --> 00:10:14,560
should look around because as a society

00:10:12,070 --> 00:10:17,170
we're being taught exactly how to

00:10:14,560 --> 00:10:19,330
commodify and sell the human experience

00:10:17,170 --> 00:10:21,550
we've all become experts at this right

00:10:19,330 --> 00:10:24,310
we mark it and sell ourselves to each

00:10:21,550 --> 00:10:26,320
other as a form of entertainment so one

00:10:24,310 --> 00:10:28,210
example of course is we put pictures of

00:10:26,320 --> 00:10:32,260
our meals you know on the internet for

00:10:28,210 --> 00:10:34,120
everyone to share and the fact is a

00:10:32,260 --> 00:10:35,620
picture of your meal doesn't have any

00:10:34,120 --> 00:10:38,380
value associated with it

00:10:35,620 --> 00:10:40,900
someone has to eat that meal and you

00:10:38,380 --> 00:10:44,290
know there's no intrinsic value to the

00:10:40,900 --> 00:10:46,570
pictures of my mother's cats it's a it's

00:10:44,290 --> 00:10:50,110
the human experience that I share with

00:10:46,570 --> 00:10:52,870
my mother so between all of us we humans

00:10:50,110 --> 00:10:56,830
create this shared context that gives

00:10:52,870 --> 00:10:59,650
value to to just the fact that we are

00:10:56,830 --> 00:11:02,140
alive now moving on to another negative

00:10:59,650 --> 00:11:05,290
outcome we've got the Big Brother AI

00:11:02,140 --> 00:11:06,800
where we have a super friendly AI who

00:11:05,290 --> 00:11:10,370
begins to make all of our hardest

00:11:06,800 --> 00:11:13,310
for us and it's optimally for better or

00:11:10,370 --> 00:11:15,980
worse we end up as no longer the

00:11:13,310 --> 00:11:17,750
captains of the USS humanity and then

00:11:15,980 --> 00:11:20,600
that's closely related to outright

00:11:17,750 --> 00:11:23,089
slavery in which we're explicitly put to

00:11:20,600 --> 00:11:25,760
work as resources for our AI overlords

00:11:23,089 --> 00:11:27,950
or we're all thrown into the matrix and

00:11:25,760 --> 00:11:31,250
we never realize that we're just a race

00:11:27,950 --> 00:11:32,990
of very inefficient batteries oh and

00:11:31,250 --> 00:11:35,750
then of course there's my favorite which

00:11:32,990 --> 00:11:38,540
is the robopocalypse so when we begin to

00:11:35,750 --> 00:11:40,550
weaponize AIS and we use it to wage war

00:11:38,540 --> 00:11:43,339
on each other so these outcomes are all

00:11:40,550 --> 00:11:44,870
very very frightening they're scaring

00:11:43,339 --> 00:11:47,029
our billionaires and our physicists and

00:11:44,870 --> 00:11:48,290
a lot of people in movie theaters and

00:11:47,029 --> 00:11:49,760
everywhere I'm sure Michael Bay is going

00:11:48,290 --> 00:11:52,220
to terrify people with this stuff and

00:11:49,760 --> 00:11:54,260
all of these existential threats are

00:11:52,220 --> 00:11:56,360
predicated to some degree on the

00:11:54,260 --> 00:11:58,070
singularity happening and the problem

00:11:56,360 --> 00:11:59,329
though is that if you go ask researchers

00:11:58,070 --> 00:12:01,820
if you talk to people that are building

00:11:59,329 --> 00:12:04,250
AI I can't think of a single person who

00:12:01,820 --> 00:12:06,350
predicts a spontaneous singularity

00:12:04,250 --> 00:12:08,000
happening overnight so in fact most

00:12:06,350 --> 00:12:10,250
people consider the spontaneous

00:12:08,000 --> 00:12:13,370
explosion of intelligence to be really

00:12:10,250 --> 00:12:15,350
similar to the idea of elves sneaking

00:12:13,370 --> 00:12:17,480
into the cobbler shop and building all

00:12:15,350 --> 00:12:19,730
of the shoes from them at night and then

00:12:17,480 --> 00:12:21,290
Wow presto you walk into your laboratory

00:12:19,730 --> 00:12:23,779
in the morning and somebody wrote your

00:12:21,290 --> 00:12:25,910
thesis for you apparently you know it

00:12:23,779 --> 00:12:27,589
doesn't seem to happen so instead I

00:12:25,910 --> 00:12:29,360
think there will be a long iterative

00:12:27,589 --> 00:12:31,100
process of learning how these systems

00:12:29,360 --> 00:12:33,800
can be created and the best way to

00:12:31,100 --> 00:12:35,600
ensure that they're safe and what I

00:12:33,800 --> 00:12:37,810
really want to talk about now is what

00:12:35,600 --> 00:12:40,820
actually scares the hell out of me so

00:12:37,810 --> 00:12:43,399
moving one step at a time you know we

00:12:40,820 --> 00:12:46,430
have walked together into this new age

00:12:43,399 --> 00:12:48,829
and for the first time technologists

00:12:46,430 --> 00:12:50,839
have access to this amazing array of new

00:12:48,829 --> 00:12:52,670
technologies right we've got speech

00:12:50,839 --> 00:12:53,600
recognition and face recognition all the

00:12:52,670 --> 00:12:56,240
things that we've been hearing about

00:12:53,600 --> 00:12:58,399
today machine learning and we can just

00:12:56,240 --> 00:13:02,390
cherry-pick these things and plug them

00:12:58,399 --> 00:13:03,649
into our devices and of course that's

00:13:02,390 --> 00:13:05,480
exactly what we're doing

00:13:03,649 --> 00:13:07,880
we're taking all of these things that

00:13:05,480 --> 00:13:10,750
we're giving people exactly what they

00:13:07,880 --> 00:13:16,910
want and I think of this right now as

00:13:10,750 --> 00:13:18,860
the technological age of candy so it's

00:13:16,910 --> 00:13:20,060
just candy it's exactly what you want to

00:13:18,860 --> 00:13:22,340
eat but if all you eat is

00:13:20,060 --> 00:13:24,500
candy you know what happens you get sick

00:13:22,340 --> 00:13:26,840
so I think that's what's happening right

00:13:24,500 --> 00:13:29,390
now we use mapping algorithms algorithms

00:13:26,840 --> 00:13:31,280
to get from one place to another but we

00:13:29,390 --> 00:13:33,230
are left not actually even knowing how

00:13:31,280 --> 00:13:35,660
to navigate our own cities and

00:13:33,230 --> 00:13:38,030
neighborhoods we've got Facebook and

00:13:35,660 --> 00:13:40,400
Twitter feeds that are giving us curated

00:13:38,030 --> 00:13:42,590
versions of a world that create

00:13:40,400 --> 00:13:44,240
divisions and our thinking in our

00:13:42,590 --> 00:13:46,340
dividing our nation our families and

00:13:44,240 --> 00:13:49,850
we're checking our smartphones in these

00:13:46,340 --> 00:13:51,770
endless cycles of dopamine reward like

00:13:49,850 --> 00:13:54,650
you know rats hitting the cocaine button

00:13:51,770 --> 00:13:57,350
and some unregulated experiment and it's

00:13:54,650 --> 00:13:59,690
not just the adults who are guinea pigs

00:13:57,350 --> 00:14:03,890
in this situation it's its children too

00:13:59,690 --> 00:14:06,250
so and as a father this is something

00:14:03,890 --> 00:14:10,160
that I think about quite a bit I've got

00:14:06,250 --> 00:14:13,400
an Amazon echo in my kitchen and I've

00:14:10,160 --> 00:14:16,520
got a five-year-old son who finally can

00:14:13,400 --> 00:14:17,960
say Alexa after years of saying Alexa

00:14:16,520 --> 00:14:20,810
and trying to tell her what to do and

00:14:17,960 --> 00:14:22,790
now he can order her around he can tell

00:14:20,810 --> 00:14:25,760
her exactly what to do and she does it

00:14:22,790 --> 00:14:28,570
and so for the first time in the history

00:14:25,760 --> 00:14:32,300
of humanity we can talk to a little

00:14:28,570 --> 00:14:34,970
black cube and then have it respond to

00:14:32,300 --> 00:14:36,980
us in the voice of a woman and it's

00:14:34,970 --> 00:14:38,900
doing speech recognition we we've never

00:14:36,980 --> 00:14:40,940
had this ability in the history of

00:14:38,900 --> 00:14:43,880
humanity so you can imagine our brains

00:14:40,940 --> 00:14:46,880
are not quite inoculated to this

00:14:43,880 --> 00:14:48,500
experience and so I think that it

00:14:46,880 --> 00:14:50,750
shouldn't come as any surprise that our

00:14:48,500 --> 00:14:52,880
brains will map those interactions with

00:14:50,750 --> 00:14:56,450
machines onto our interactions with

00:14:52,880 --> 00:14:59,240
humans and so the question for me is do

00:14:56,450 --> 00:15:00,830
I want my son to be involved in an

00:14:59,240 --> 00:15:03,860
interaction in which he's giving orders

00:15:00,830 --> 00:15:06,370
to a subservient a woman who complies

00:15:03,860 --> 00:15:09,140
instantly regardless of how rude he is

00:15:06,370 --> 00:15:12,230
his mother definitely is not into that

00:15:09,140 --> 00:15:15,740
interaction she's not pumped about that

00:15:12,230 --> 00:15:18,470
so to be clear I don't really care about

00:15:15,740 --> 00:15:20,870
Alexa you know don't tell her I said

00:15:18,470 --> 00:15:23,810
that though I'm not worried about her

00:15:20,870 --> 00:15:25,100
she's she's a pile of code she doesn't

00:15:23,810 --> 00:15:27,200
have feelings or thoughts but I'm

00:15:25,100 --> 00:15:28,970
worried about my son I care about him I

00:15:27,200 --> 00:15:30,710
I want to know what kind of person he's

00:15:28,970 --> 00:15:32,220
going to become is he going to have

00:15:30,710 --> 00:15:33,870
interactions that help

00:15:32,220 --> 00:15:37,410
learn to be polite and thoughtful and

00:15:33,870 --> 00:15:39,930
kind and so in our house we say please

00:15:37,410 --> 00:15:42,900
and thank you to our Alexa and we do it

00:15:39,930 --> 00:15:45,630
not because she insists but because I

00:15:42,900 --> 00:15:48,890
insist and I think that's a really

00:15:45,630 --> 00:15:53,310
important distinction moving forward

00:15:48,890 --> 00:15:55,110
because technologists you know we have a

00:15:53,310 --> 00:15:56,430
lot of experience creating safe consumer

00:15:55,110 --> 00:15:59,160
products that are going to go into the

00:15:56,430 --> 00:16:00,930
home and be used by people so that's not

00:15:59,160 --> 00:16:03,180
to say that you know you can ever really

00:16:00,930 --> 00:16:05,460
win because if you put any object into

00:16:03,180 --> 00:16:07,170
enough people's homes they would get

00:16:05,460 --> 00:16:09,420
hurt they're gonna get in the bathtub

00:16:07,170 --> 00:16:11,070
with it or lick it or put their fingers

00:16:09,420 --> 00:16:14,040
in it or what people are very curious

00:16:11,070 --> 00:16:15,840
mammals and it doesn't always end well

00:16:14,040 --> 00:16:17,730
for them but and I'm sure that this is

00:16:15,840 --> 00:16:19,710
gonna be a much more difficult problem

00:16:17,730 --> 00:16:21,840
once we have autonomous lifelike

00:16:19,710 --> 00:16:23,700
machines that are that are in our homes

00:16:21,840 --> 00:16:25,410
as well but at the end of the day I

00:16:23,700 --> 00:16:28,080
don't think that physical safety

00:16:25,410 --> 00:16:30,440
concerns are the biggest challenge for

00:16:28,080 --> 00:16:32,580
developing new products I think that

00:16:30,440 --> 00:16:34,020
what's the bigger challenge is something

00:16:32,580 --> 00:16:36,450
that we've never really had to deal with

00:16:34,020 --> 00:16:39,180
before which is building ethical

00:16:36,450 --> 00:16:41,730
products and I really see two main

00:16:39,180 --> 00:16:46,260
dimensions to this problem so explicit

00:16:41,730 --> 00:16:47,760
and implicit behavior so explicitly how

00:16:46,260 --> 00:16:49,530
do you build a machine that's autonomous

00:16:47,760 --> 00:16:51,300
it's gonna make moral judgments you know

00:16:49,530 --> 00:16:53,250
how's the robot gonna choose in the

00:16:51,300 --> 00:16:55,200
thought experiment you know which human

00:16:53,250 --> 00:16:57,330
life does it save if it can only save

00:16:55,200 --> 00:16:59,130
one person from a burning building does

00:16:57,330 --> 00:17:01,710
it save the child who's got more years

00:16:59,130 --> 00:17:03,600
to live does it save its owner you know

00:17:01,710 --> 00:17:06,030
who's like got an excellent credit score

00:17:03,600 --> 00:17:09,180
and you know has still owes a lot of

00:17:06,030 --> 00:17:10,950
money so it needs to be earning or does

00:17:09,180 --> 00:17:13,320
it just sit back and watch you know in

00:17:10,950 --> 00:17:15,750
order to remove itself from from any

00:17:13,320 --> 00:17:18,060
liability for its parent corporation so

00:17:15,750 --> 00:17:20,550
I think this is actually a great problem

00:17:18,060 --> 00:17:22,710
because as sort of a technologically

00:17:20,550 --> 00:17:24,600
maturing society this is this is the

00:17:22,710 --> 00:17:27,240
kind of problem that that we need to

00:17:24,600 --> 00:17:30,450
solve we need to be able to quantify our

00:17:27,240 --> 00:17:32,280
values as a society and embody them in

00:17:30,450 --> 00:17:35,250
our machines and and I kind of love the

00:17:32,280 --> 00:17:37,550
idea as opposed to the you know the the

00:17:35,250 --> 00:17:40,710
robot war I love the idea of our

00:17:37,550 --> 00:17:43,290
artifacts that are in our lives being

00:17:40,710 --> 00:17:44,830
Paragons and really representing our

00:17:43,290 --> 00:17:45,970
values and and

00:17:44,830 --> 00:17:48,789
acting those out and being something

00:17:45,970 --> 00:17:51,220
that that you could aspire to behave as

00:17:48,789 --> 00:17:53,110
well as so it's an idea that's been

00:17:51,220 --> 00:17:54,909
explored in science fiction so the

00:17:53,110 --> 00:17:57,100
founding father of artificial

00:17:54,909 --> 00:17:58,480
intelligence John McCarthy the guy who

00:17:57,100 --> 00:18:00,760
invented the term artificial

00:17:58,480 --> 00:18:02,380
intelligence he wrote hundreds of books

00:18:00,760 --> 00:18:05,110
and articles it was brilliant

00:18:02,380 --> 00:18:06,700
but he only wrote one fictional story it

00:18:05,110 --> 00:18:09,640
was about this it was called the robot

00:18:06,700 --> 00:18:12,490
and the baby and it imagined this future

00:18:09,640 --> 00:18:14,860
in which a personal service robot is

00:18:12,490 --> 00:18:19,029
watching over a baby that's being

00:18:14,860 --> 00:18:20,860
neglected and abused so the robot takes

00:18:19,029 --> 00:18:22,870
the baby out into the street to remove

00:18:20,860 --> 00:18:25,000
it from this dangerous situation and

00:18:22,870 --> 00:18:26,409
then a police officer comes over and

00:18:25,000 --> 00:18:28,600
says why robot why are you carrying

00:18:26,409 --> 00:18:30,039
around a baby and the robot won't tell

00:18:28,600 --> 00:18:32,470
the police officer who it is or where it

00:18:30,039 --> 00:18:35,409
came from because it's balancing privacy

00:18:32,470 --> 00:18:38,740
with safety and the really prescient

00:18:35,409 --> 00:18:40,630
part of this story is that these ethical

00:18:38,740 --> 00:18:42,730
values that this robot is trying to work

00:18:40,630 --> 00:18:45,220
its way through they're decided in the

00:18:42,730 --> 00:18:48,429
story by congressional committees that

00:18:45,220 --> 00:18:51,700
argued different ethics that should be

00:18:48,429 --> 00:18:53,169
put into the consumer product so I think

00:18:51,700 --> 00:18:54,730
that that could be in our future for

00:18:53,169 --> 00:18:56,850
better or worse God knows what kind of

00:18:54,730 --> 00:19:01,000
machines are gonna get honestly but as

00:18:56,850 --> 00:19:03,100
an aside I published the robot on the

00:19:01,000 --> 00:19:06,490
baby in an anthology that I co edited

00:19:03,100 --> 00:19:08,799
and just to brag about it

00:19:06,490 --> 00:19:11,380
um one of the most surreal moments of my

00:19:08,799 --> 00:19:13,389
life was editing the language of John

00:19:11,380 --> 00:19:16,510
McCarthy's story when he suddenly

00:19:13,389 --> 00:19:18,850
switches into Lisp in order to actually

00:19:16,510 --> 00:19:20,649
go through the utility values of each of

00:19:18,850 --> 00:19:23,159
these decisions and work out the math of

00:19:20,649 --> 00:19:25,899
why the robot was vacillating between

00:19:23,159 --> 00:19:27,639
safety and privacy and so I had this

00:19:25,899 --> 00:19:31,330
like moment in my life that was this

00:19:27,639 --> 00:19:34,990
golden moment of of editing language and

00:19:31,330 --> 00:19:37,210
then debugging Lisp code I was just like

00:19:34,990 --> 00:19:40,419
this is it this is the synthesis of

00:19:37,210 --> 00:19:41,940
everything so that was that was just a

00:19:40,419 --> 00:19:45,419
nice moment that I want to share so

00:19:41,940 --> 00:19:48,399
aside from explicit behaviors like that

00:19:45,419 --> 00:19:51,429
implicitly how is the presence of a

00:19:48,399 --> 00:19:54,460
lifelike machine going to affect us and

00:19:51,429 --> 00:19:56,640
and our children so at my house I also

00:19:54,460 --> 00:19:59,850
have a seven-year-old daughter who

00:19:56,640 --> 00:20:02,970
runs right past me to go hug her mother

00:19:59,850 --> 00:20:07,289
and I aggressively self-promote I have a

00:20:02,970 --> 00:20:09,840
lot of songs about how great dad is dad

00:20:07,289 --> 00:20:12,450
is great what a handsome guy they don't

00:20:09,840 --> 00:20:15,210
work they don't work and I wonder you

00:20:12,450 --> 00:20:17,159
know how are either of us going to be

00:20:15,210 --> 00:20:19,769
able to compete whenever there's a robot

00:20:17,159 --> 00:20:21,539
in our household whenever we've got our

00:20:19,769 --> 00:20:24,179
children running right past us to go hug

00:20:21,539 --> 00:20:26,100
their machines because kids are and

00:20:24,179 --> 00:20:29,370
we'll be interacting with lifelike

00:20:26,100 --> 00:20:30,929
machines and so the question is can they

00:20:29,370 --> 00:20:33,630
tell the difference between what's fake

00:20:30,929 --> 00:20:35,399
and what's real and we're starting to

00:20:33,630 --> 00:20:38,279
find out so researchers at the

00:20:35,399 --> 00:20:40,470
University of Washington have videotaped

00:20:38,279 --> 00:20:44,570
80 preschoolers interacting with a

00:20:40,470 --> 00:20:47,460
stuffed dog and with an AIBO robot puppy

00:20:44,570 --> 00:20:48,720
so they found that most kids understood

00:20:47,460 --> 00:20:51,450
the difference between these two things

00:20:48,720 --> 00:20:54,029
they know that AIBO is not alive but

00:20:51,450 --> 00:20:56,039
they still treat it as a moral entity so

00:20:54,029 --> 00:20:58,740
they treat it like a real dog they

00:20:56,039 --> 00:21:00,779
handle AIBO more gently than the stuffed

00:20:58,740 --> 00:21:03,240
dog they talk to it more frequently they

00:21:00,779 --> 00:21:06,990
engage in reciprocal activities with it

00:21:03,240 --> 00:21:10,769
and I find this most interesting when a

00:21:06,990 --> 00:21:13,529
researcher smacks both dogs really just

00:21:10,769 --> 00:21:16,230
a sharp tap the children stare at the I

00:21:13,529 --> 00:21:18,809
bow for twice as long and they're trying

00:21:16,230 --> 00:21:21,720
to figure out whether it was okay to hit

00:21:18,809 --> 00:21:24,630
this lifelike machine so if a technology

00:21:21,720 --> 00:21:25,889
looks and acts alive then children will

00:21:24,630 --> 00:21:27,510
naturally treat it that way

00:21:25,889 --> 00:21:29,970
even if they know that it isn't really

00:21:27,510 --> 00:21:32,159
true and it's scary to me to think that

00:21:29,970 --> 00:21:34,519
negative interactions with lifelike

00:21:32,159 --> 00:21:37,529
technology like kicking a robot dog

00:21:34,519 --> 00:21:39,960
could be mapped back on to real humans

00:21:37,529 --> 00:21:42,419
and animals but it's also reassuring to

00:21:39,960 --> 00:21:44,429
me that kids naturally empathize with

00:21:42,419 --> 00:21:47,580
lifelike machines and they want to treat

00:21:44,429 --> 00:21:49,799
them well so how do we ensure that we

00:21:47,580 --> 00:21:52,110
have positive interactions with our AI

00:21:49,799 --> 00:21:54,870
so there's another really great study at

00:21:52,110 --> 00:21:57,120
u-dub 60 grade schoolers playing

00:21:54,870 --> 00:22:00,750
tic-tac-toe with a really realistic

00:21:57,120 --> 00:22:05,130
virtual face on a screen and when the

00:22:00,750 --> 00:22:06,870
virtual character makes a dumb move the

00:22:05,130 --> 00:22:10,020
research are standing nearby calls it

00:22:06,870 --> 00:22:12,030
really stupid you're really stupid robot

00:22:10,020 --> 00:22:13,950
and so half the time the virtual

00:22:12,030 --> 00:22:16,020
character doesn't say anything and the

00:22:13,950 --> 00:22:18,210
other half it's a man's morale treatment

00:22:16,020 --> 00:22:19,620
by saying hey that's not okay you're

00:22:18,210 --> 00:22:20,160
hurting my feelings whenever you call me

00:22:19,620 --> 00:22:22,800
stupid

00:22:20,160 --> 00:22:25,080
so when the character keeps quiet they

00:22:22,800 --> 00:22:28,470
found that only half the kids reported

00:22:25,080 --> 00:22:31,560
that the insult was not okay and that

00:22:28,470 --> 00:22:34,620
number jumps to over 90% when the robot

00:22:31,560 --> 00:22:37,140
stands up for itself and so what I see

00:22:34,620 --> 00:22:39,090
is that it's up to the robot to tell us

00:22:37,140 --> 00:22:41,220
how we should be interacting with them

00:22:39,090 --> 00:22:41,940
and if a robot demands to be treated

00:22:41,220 --> 00:22:43,380
with respect

00:22:41,940 --> 00:22:45,390
children are happy to comply with that

00:22:43,380 --> 00:22:48,120
and if it doesn't then they're more

00:22:45,390 --> 00:22:51,360
likely to see nothing wrong with abusing

00:22:48,120 --> 00:22:54,120
a very lifelike robot so in other words

00:22:51,360 --> 00:22:55,590
it's up to the technologists to define

00:22:54,120 --> 00:22:57,900
this ethical dimension of our

00:22:55,590 --> 00:23:00,690
interactions with machines and it's not

00:22:57,900 --> 00:23:02,640
really up to us so I'm hoping and

00:23:00,690 --> 00:23:05,040
predicting that this kind of research is

00:23:02,640 --> 00:23:07,170
going to bring in a new age after the

00:23:05,040 --> 00:23:10,590
age of candy the age of meat and

00:23:07,170 --> 00:23:12,480
potatoes so instead of getting exactly

00:23:10,590 --> 00:23:15,150
what we want from our technology which

00:23:12,480 --> 00:23:16,440
isn't always what we need I'm hoping

00:23:15,150 --> 00:23:18,360
that our future interactions are going

00:23:16,440 --> 00:23:21,360
to be designed to give us what we need

00:23:18,360 --> 00:23:23,520
so if I'm rude to Alexa I feel like she

00:23:21,360 --> 00:23:26,250
should ask me for an apology

00:23:23,520 --> 00:23:28,170
I want mapping algorithms in my car to

00:23:26,250 --> 00:23:29,040
help me actually learn how to do it

00:23:28,170 --> 00:23:31,890
myself

00:23:29,040 --> 00:23:33,960
that's a harder challenge but if you

00:23:31,890 --> 00:23:35,370
have two products on the market and they

00:23:33,960 --> 00:23:37,620
both do the same thing one of them is

00:23:35,370 --> 00:23:39,420
gonna slowly help you actually learn to

00:23:37,620 --> 00:23:41,040
navigate on your own I think consumers

00:23:39,420 --> 00:23:43,200
will pick the one that that makes them

00:23:41,040 --> 00:23:45,660
better instead of just giving them

00:23:43,200 --> 00:23:48,330
exactly what they want immediately

00:23:45,660 --> 00:23:51,120
you know this self-destructive nature of

00:23:48,330 --> 00:23:53,040
eating candy all day is starting to make

00:23:51,120 --> 00:23:54,780
an impact on us and I and I've seen it I

00:23:53,040 --> 00:23:57,240
think we've all seen it so a consortium

00:23:54,780 --> 00:23:58,800
of investors recently wrote an open

00:23:57,240 --> 00:24:00,600
letter to Apple demanding that they

00:23:58,800 --> 00:24:04,080
figure out how to help people monitor

00:24:00,600 --> 00:24:06,270
that coking rat cocaine cycle that's

00:24:04,080 --> 00:24:08,280
happening with our phones you know we've

00:24:06,270 --> 00:24:09,750
all seen scenes of families out to

00:24:08,280 --> 00:24:11,850
dinner where everybody's sitting at a

00:24:09,750 --> 00:24:14,220
table staring at their phone and it's

00:24:11,850 --> 00:24:16,170
kind of horrific and I think a backlash

00:24:14,220 --> 00:24:19,020
is starting to happen so Zuckerberg

00:24:16,170 --> 00:24:21,030
recently acknowledged research that

00:24:19,020 --> 00:24:23,070
shows Facebook makes people feel bad and

00:24:21,030 --> 00:24:24,490
he said his new year's resolution is to

00:24:23,070 --> 00:24:27,100
fix Facebook

00:24:24,490 --> 00:24:29,740
and their objective is no longer to just

00:24:27,100 --> 00:24:31,840
surface relevant content but to

00:24:29,740 --> 00:24:34,150
prioritize meaningful social

00:24:31,840 --> 00:24:35,500
interactions on their platform even if

00:24:34,150 --> 00:24:37,150
it decreases some of their engagement

00:24:35,500 --> 00:24:39,910
metrics so they're starting to look at a

00:24:37,150 --> 00:24:41,530
long-term outcome how are we actually

00:24:39,910 --> 00:24:43,900
affecting people's lives rather than

00:24:41,530 --> 00:24:47,680
just focusing on short-term

00:24:43,900 --> 00:24:50,800
dopamine's cycles so all of this gives

00:24:47,680 --> 00:24:53,200
me hope that our technology will make us

00:24:50,800 --> 00:24:56,130
better people and there is there's some

00:24:53,200 --> 00:24:58,840
precedence so in the 1960s a group of

00:24:56,130 --> 00:25:01,270
psychologists was asked to study the

00:24:58,840 --> 00:25:04,360
impact of television programs which is a

00:25:01,270 --> 00:25:07,180
pretty new phenomenon on young minds and

00:25:04,360 --> 00:25:10,120
the end result for them in a limited

00:25:07,180 --> 00:25:12,010
case was with Sesame Street so this is a

00:25:10,120 --> 00:25:14,080
pretty sensational achievement you know

00:25:12,010 --> 00:25:17,230
for the medium that positively

00:25:14,080 --> 00:25:21,400
influenced millions of young minds out

00:25:17,230 --> 00:25:23,920
there in the world and I'm hoping that

00:25:21,400 --> 00:25:26,110
that this will this will also impact our

00:25:23,920 --> 00:25:29,800
next generation of technology so killer

00:25:26,110 --> 00:25:32,770
robots are scary and our fear of them I

00:25:29,800 --> 00:25:34,960
think is natural and not necessarily a

00:25:32,770 --> 00:25:37,120
bad thing and I do think that we should

00:25:34,960 --> 00:25:38,310
all spend a lot of money buying science

00:25:37,120 --> 00:25:41,950
fiction so that we can envision

00:25:38,310 --> 00:25:44,290
different possible futures and examine

00:25:41,950 --> 00:25:46,390
that fear but the killer robots that we

00:25:44,290 --> 00:25:47,470
should be afraid of I don't think

00:25:46,390 --> 00:25:48,760
they're being built in secret

00:25:47,470 --> 00:25:50,620
laboratories I don't think they're

00:25:48,760 --> 00:25:52,600
coming back from the future or from

00:25:50,620 --> 00:25:54,010
outer space or they're even on

00:25:52,600 --> 00:25:56,140
battlefields I think that the real

00:25:54,010 --> 00:25:58,210
killer robots are under Christmas trees

00:25:56,140 --> 00:26:00,460
they're in our living rooms our kitchens

00:25:58,210 --> 00:26:02,500
you guys are building them they're

00:26:00,460 --> 00:26:05,680
coming down the pipeline and these new

00:26:02,500 --> 00:26:06,700
technologies can hurt us or help us but

00:26:05,680 --> 00:26:09,180
I'm hoping that if we play our cards

00:26:06,700 --> 00:26:11,590
right we're much more likely to earn

00:26:09,180 --> 00:26:14,020
powerful allies rather than being

00:26:11,590 --> 00:26:15,790
chopped into tiny bits by robots that in

00:26:14,020 --> 00:26:18,750
our hubris we've built with buzz saws

00:26:15,790 --> 00:26:25,619
for hands thank you very much

00:26:18,750 --> 00:26:25,619

YouTube URL: https://www.youtube.com/watch?v=h8MbXIogXYQ


