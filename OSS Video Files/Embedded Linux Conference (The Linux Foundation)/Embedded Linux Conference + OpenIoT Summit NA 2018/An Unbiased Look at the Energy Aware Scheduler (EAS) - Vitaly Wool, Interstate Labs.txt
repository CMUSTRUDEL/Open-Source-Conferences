Title: An Unbiased Look at the Energy Aware Scheduler (EAS) - Vitaly Wool, Interstate Labs
Publication date: 2018-03-13
Playlist: Embedded Linux Conference + OpenIoT Summit NA 2018
Description: 
	An Unbiased Look at the Energy Aware Scheduler (EAS) - Vitaly Wool, Interstate Labs

The big.LITTLE ARM SoCs have recently become a primary choice for many consumer electronic devices requiring high computing capacity and aiming for lower power consumption at the same time. As big.LITTLE is a heterogeneous architecture it puts special requirements on the OS scheduler for efficient operation, and Linux's completely fair scheduler (CFS) is not a good match.

To make matters worse, there happened to be several competing implementations of a scheduler for big.LITTLE, and none of them ideal. The Linaro version, called Energy Aware Scheduler, however, has taken the lead and is now used in most Linux/Android powered devices, with some funky modifications from hardware vendors.

This talk will provide a description of the EAS essentials and how it compares to the alternatives, briefly cover its vendor variations and then concentrate on what's good and what's not so good in it. 

About Vitaly Wool
Vitaly Wool, Senior Developer and Linux enthusiast, graduated M.Sc. in Computer Science from St. Petersburg State Univ. in 2002, worked for starters with real-time OSes as VxWorks and RTEMS mostly for PowerPC platforms. Vitaly moved to Moscow in 2003 where he started to work on embedded Linux projects for other platforms and architectures for a variety of companies including MontaVista and Mentor Graphics. Now he works as a Staff Consultant/Linux expert for different companies in the Nordic and Baltic regions, primarily helping out Sony Mobile resolve power consumption and performance issues.
Captions: 
	00:00:00,030 --> 00:00:07,770
hello everyone my name is vitaly wool

00:00:03,360 --> 00:00:11,690
and this talk is about energyaware

00:00:07,770 --> 00:00:15,990
scheduler and things that are related to

00:00:11,690 --> 00:00:19,590
its development in current status and

00:00:15,990 --> 00:00:22,500
I'll try to provide an unbiased look on

00:00:19,590 --> 00:00:29,760
that very scheduler we'll start with the

00:00:22,500 --> 00:00:35,399
introduction then we'll pass over to the

00:00:29,760 --> 00:00:40,290
EAS as such as it began then we'll talk

00:00:35,399 --> 00:00:43,800
about the main sort of rival of EAS

00:00:40,290 --> 00:00:47,579
called a Qualcomm HMP scheduler then

00:00:43,800 --> 00:00:50,730
we'll do some comparisons and figure out

00:00:47,579 --> 00:00:55,940
what the way forward was and wrap up so

00:00:50,730 --> 00:00:59,039
nothing out of ordinary but still I

00:00:55,940 --> 00:01:04,040
thought it would be nice to have some

00:00:59,039 --> 00:01:10,979
kind of a summary slide in the beginning

00:01:04,040 --> 00:01:13,439
I am representing console core group the

00:01:10,979 --> 00:01:16,130
services company specializing in

00:01:13,439 --> 00:01:19,020
embedded Linux in open source software

00:01:16,130 --> 00:01:22,100
doing hardware software build design

00:01:19,020 --> 00:01:26,970
development and also training services

00:01:22,100 --> 00:01:30,350
it's working off of San Jose with the

00:01:26,970 --> 00:01:34,070
engineering presence worldwide I think

00:01:30,350 --> 00:01:38,640
man do we have presence in art arctic

00:01:34,070 --> 00:01:41,549
not yet so so well worldwide is a little

00:01:38,640 --> 00:01:46,590
bit of an exaggeration we're not

00:01:41,549 --> 00:01:49,610
covering Antarctica yet also I happen to

00:01:46,590 --> 00:01:52,350
mentor a group of postgraduate students

00:01:49,610 --> 00:01:55,229
under the name of interstate labs and

00:01:52,350 --> 00:01:56,570
that happens mostly in st. Petersburg

00:01:55,229 --> 00:01:59,399
Russia

00:01:56,570 --> 00:02:01,170
just for the record these guys are not

00:01:59,399 --> 00:02:03,680
the guys are the tempering with the US

00:02:01,170 --> 00:02:03,680
elections

00:02:06,960 --> 00:02:16,500
okay so we're passing over to the more

00:02:13,800 --> 00:02:20,520
important part of the introduction and

00:02:16,500 --> 00:02:26,330
that is where the need for EAS the

00:02:20,520 --> 00:02:31,350
energy wear scheduler came from and to

00:02:26,330 --> 00:02:33,540
be specific about that we need to

00:02:31,350 --> 00:02:38,100
consider what's there in the kernel

00:02:33,540 --> 00:02:41,280
right now and what's prevailing what's

00:02:38,100 --> 00:02:45,150
used by pretty much any appliance that

00:02:41,280 --> 00:02:48,450
has Linux kernel inside is called the

00:02:45,150 --> 00:02:53,760
completely fair scheduler which main

00:02:48,450 --> 00:02:57,209
idea is to maintain balance or in terms

00:02:53,760 --> 00:03:01,770
of CFS fairness in providing processor

00:02:57,209 --> 00:03:05,160
time to tasks it maintains the amount of

00:03:01,770 --> 00:03:12,209
time provided to a given task to

00:03:05,160 --> 00:03:16,590
determine if balancing is needed and the

00:03:12,209 --> 00:03:20,030
beam structure used in CFS is a time

00:03:16,590 --> 00:03:22,790
ordered red black tree which guarantees

00:03:20,030 --> 00:03:27,720
high responsiveness and performance and

00:03:22,790 --> 00:03:32,300
this is basically why you know when the

00:03:27,720 --> 00:03:35,430
decision was about to be made if you

00:03:32,300 --> 00:03:39,840
implement a scheduler from scratch for

00:03:35,430 --> 00:03:43,170
the needs of bed little systems or to do

00:03:39,840 --> 00:03:45,360
something on top of CFS the decision was

00:03:43,170 --> 00:03:50,239
made in favor of CFS so this is an

00:03:45,360 --> 00:03:50,239
important detail CFS is a good scheduler

00:03:53,540 --> 00:04:02,610
CFS is sorting tasks in ascending order

00:03:57,769 --> 00:04:05,670
in that very red black tree and the

00:04:02,610 --> 00:04:11,430
leftmost task of the red black tree is

00:04:05,670 --> 00:04:14,000
picked up next now when the decision for

00:04:11,430 --> 00:04:17,590
the nest that's sorry for the next task

00:04:14,000 --> 00:04:21,050
to be put on the CPU is made

00:04:17,590 --> 00:04:25,250
that's because the left-mouse task has

00:04:21,050 --> 00:04:29,330
the least spent execution time and that

00:04:25,250 --> 00:04:34,310
is the very task that needs fairness the

00:04:29,330 --> 00:04:40,690
most what we also need to mention about

00:04:34,310 --> 00:04:44,120
CFS operation principles is that CFS

00:04:40,690 --> 00:04:47,180
works with fairness not only to tasks

00:04:44,120 --> 00:04:50,030
but to CPU cores so it can see there is

00:04:47,180 --> 00:04:54,290
all CPUs to be the same which works very

00:04:50,030 --> 00:04:56,480
well in SMP systems but in some more

00:04:54,290 --> 00:05:07,280
complicated cases that we'll be covering

00:04:56,480 --> 00:05:10,730
it doesn't okay what are the cases the

00:05:07,280 --> 00:05:14,150
main case that we'll be concentrating on

00:05:10,730 --> 00:05:17,000
is the big.little architecture which is

00:05:14,150 --> 00:05:22,250
a heterogeneous processor architecture

00:05:17,000 --> 00:05:25,310
which uses two types of course combined

00:05:22,250 --> 00:05:28,300
into clusters so the little coarse off

00:05:25,310 --> 00:05:31,760
of the cope so-called little cluster or

00:05:28,300 --> 00:05:36,470
silver cluster are designed for maximum

00:05:31,760 --> 00:05:40,550
power efficiency and the big cores off

00:05:36,470 --> 00:05:43,520
of the big cluster or golden cluster in

00:05:40,550 --> 00:05:51,310
Qualcomm terminology should provide

00:05:43,520 --> 00:05:54,110
maximal computing power and each task in

00:05:51,310 --> 00:05:57,530
the big.little architecture may be

00:05:54,110 --> 00:06:00,350
scheduled for execution either on big or

00:05:57,530 --> 00:06:06,620
little core there's no limitation to

00:06:00,350 --> 00:06:10,750
that and the aim is for high peak

00:06:06,620 --> 00:06:14,180
performance with low mean power so

00:06:10,750 --> 00:06:16,100
big.little architecture targets high

00:06:14,180 --> 00:06:20,210
peak performance with low mean power

00:06:16,100 --> 00:06:23,540
which is specific for battery operated

00:06:20,210 --> 00:06:25,840
mobile devices well primarily Android

00:06:23,540 --> 00:06:25,840
devices

00:06:28,139 --> 00:06:35,400
so if we take big.little in a nutshell

00:06:31,919 --> 00:06:37,870
the key once again is task placement

00:06:35,400 --> 00:06:42,750
this is the key for high peak

00:06:37,870 --> 00:06:46,630
performance with low mean power because

00:06:42,750 --> 00:06:50,139
wrong distribution of tasks between the

00:06:46,630 --> 00:06:55,120
cores will most likely kill the big

00:06:50,139 --> 00:06:57,850
little advantages so with that said

00:06:55,120 --> 00:07:00,729
big.little puts high requirements on

00:06:57,850 --> 00:07:02,650
scheduler because the scheduler should

00:07:00,729 --> 00:07:05,650
be aware that there are two types of

00:07:02,650 --> 00:07:07,990
course and well the cores are different

00:07:05,650 --> 00:07:11,380
the power consumption of the cores are

00:07:07,990 --> 00:07:15,729
different the performance of the cores

00:07:11,380 --> 00:07:19,210
are different so the scheduler should be

00:07:15,729 --> 00:07:21,520
energy aware and it should communicate

00:07:19,210 --> 00:07:28,180
with the dynamic voltage and scaling

00:07:21,520 --> 00:07:31,450
subsystem and also while scheduling in

00:07:28,180 --> 00:07:34,900
fact is always a bit of a crystal ball

00:07:31,450 --> 00:07:37,090
type of operation because you need to

00:07:34,900 --> 00:07:40,840
make a decision basing on the tasks

00:07:37,090 --> 00:07:44,610
future activity right but in this

00:07:40,840 --> 00:07:49,590
particular case it's even more like that

00:07:44,610 --> 00:07:53,050
because you need to account both for

00:07:49,590 --> 00:08:02,550
tasks demands and for possible power

00:07:53,050 --> 00:08:07,979
impact any questions so far okay

00:08:02,550 --> 00:08:13,169
so scheduler for big little can we use

00:08:07,979 --> 00:08:15,350
CFS well CFS is a good scheduler but

00:08:13,169 --> 00:08:18,720
it's not really a good fit for

00:08:15,350 --> 00:08:21,990
big.little architecture because it's not

00:08:18,720 --> 00:08:25,590
energy where it doesn't know it cannot

00:08:21,990 --> 00:08:32,430
distinguish between big cores and little

00:08:25,590 --> 00:08:34,890
cores so once again we want to use CFS

00:08:32,430 --> 00:08:37,650
because it's good but we can't use it

00:08:34,890 --> 00:08:40,200
directly because it lacks some important

00:08:37,650 --> 00:08:43,200
knowledge to make right decisions so the

00:08:40,200 --> 00:08:46,230
idea was to extend CFS to be applicable

00:08:43,200 --> 00:08:49,170
to non SMP and well first of all big

00:08:46,230 --> 00:08:57,120
little architectures and this work dates

00:08:49,170 --> 00:09:00,000
back to 2013 and there were two main

00:08:57,120 --> 00:09:02,490
competing implementations first came

00:09:00,000 --> 00:09:05,070
from Qualcomm code Aurora

00:09:02,490 --> 00:09:08,160
and the other one from arm lean arrow

00:09:05,070 --> 00:09:16,110
and the latter one is called

00:09:08,160 --> 00:09:21,959
EAS and we'll concentrate on that one so

00:09:16,110 --> 00:09:27,150
let's pass over to es in detail the

00:09:21,959 --> 00:09:29,880
basic principles of es are that we need

00:09:27,150 --> 00:09:34,230
to schedule tasks considering energy

00:09:29,880 --> 00:09:37,500
implications and the decision should be

00:09:34,230 --> 00:09:43,500
made basing on both topology and power

00:09:37,500 --> 00:09:48,360
management features and that of course

00:09:43,500 --> 00:09:51,690
employees work load calculation and

00:09:48,360 --> 00:09:54,750
workload calculation within AES is

00:09:51,690 --> 00:09:57,930
implemented independently or mostly

00:09:54,750 --> 00:10:00,420
independently and in fact it's using the

00:09:57,930 --> 00:10:05,220
calculations that have already been

00:10:00,420 --> 00:10:12,380
there for CFS and those are called built

00:10:05,220 --> 00:10:12,380
or PLT per entity load tracking

00:10:13,100 --> 00:10:18,329
that's the scheduling feature that is

00:10:15,720 --> 00:10:24,990
already in mainline has been there since

00:10:18,329 --> 00:10:28,110
3.8 and the main idea of belt is that

00:10:24,990 --> 00:10:30,540
process can actually contribute to load

00:10:28,110 --> 00:10:36,809
even if it's not actually running at the

00:10:30,540 --> 00:10:41,059
moment so is calculated basing on the

00:10:36,809 --> 00:10:45,180
geometric series with the decay factor

00:10:41,059 --> 00:10:50,759
when the total load is composed of the

00:10:45,180 --> 00:10:53,129
load taken from the last sample plus the

00:10:50,759 --> 00:10:57,170
decayed load from the previous sample

00:10:53,129 --> 00:11:00,149
plus the double decayed load from the

00:10:57,170 --> 00:11:02,699
sample that was before the previous and

00:11:00,149 --> 00:11:08,870
so on and so forth as you can see on the

00:11:02,699 --> 00:11:15,439
formula and q is the decay factor and

00:11:08,870 --> 00:11:18,870
delt itself uses very soft decay factor

00:11:15,439 --> 00:11:24,720
so that the load contribution house

00:11:18,870 --> 00:11:29,910
after 32 milliseconds and this this

00:11:24,720 --> 00:11:33,749
choice of q is well and has never been

00:11:29,910 --> 00:11:39,029
obvious but he has just followed what

00:11:33,749 --> 00:11:43,470
was there in the mainline and as you

00:11:39,029 --> 00:11:50,429
will see later in the slides that didn't

00:11:43,470 --> 00:11:54,290
work exactly that well so and then we

00:11:50,429 --> 00:11:58,910
have a very nice picture of how ears

00:11:54,290 --> 00:11:58,910
together with Pelt is operating

00:12:01,889 --> 00:12:11,730
so if we are to schedule a task to a big

00:12:07,170 --> 00:12:14,339
little system we picked CPU we pick the

00:12:11,730 --> 00:12:17,699
core with sufficient spare capacity and

00:12:14,339 --> 00:12:20,639
smallest energy impact so if we look

00:12:17,699 --> 00:12:24,359
into this particular picture then we can

00:12:20,639 --> 00:12:27,779
see that both little core number two and

00:12:24,359 --> 00:12:30,600
big core number two can handle the

00:12:27,779 --> 00:12:35,759
scheduled tasks without raising the

00:12:30,600 --> 00:12:40,319
operating frequency which is represented

00:12:35,759 --> 00:12:43,529
by dashed line at the same time we do

00:12:40,319 --> 00:12:46,290
know that big cores are more power

00:12:43,529 --> 00:12:48,179
hungry so if the task is actually

00:12:46,290 --> 00:12:51,499
fitting into one of the smaller course

00:12:48,179 --> 00:12:57,439
it will go into the smaller core as

00:12:51,499 --> 00:13:00,989
opposed to CFS in this case because CFS

00:12:57,439 --> 00:13:03,149
could equally well schedule this task on

00:13:00,989 --> 00:13:06,419
little core number two and big core

00:13:03,149 --> 00:13:17,459
number two because for CFS they would be

00:13:06,419 --> 00:13:19,970
equal okay and now let's take a quick

00:13:17,459 --> 00:13:26,339
look at what was happening in San Diego

00:13:19,970 --> 00:13:30,739
with Qualcomm and their scheduler so the

00:13:26,339 --> 00:13:36,029
Qualcomm HMP scheduler which is

00:13:30,739 --> 00:13:39,629
deciphered as Qualcomm heterogeneous

00:13:36,029 --> 00:13:42,959
multi-processing scheduler even though

00:13:39,629 --> 00:13:50,220
there are alternative variants like

00:13:42,959 --> 00:13:53,699
Qualcomm high-maintenance parachute that

00:13:50,220 --> 00:13:58,799
scheduler operates basing on similar

00:13:53,699 --> 00:14:01,769
principles but there are some

00:13:58,799 --> 00:14:04,230
significant differences to the narrows

00:14:01,769 --> 00:14:08,369
es and we're gonna concentrate on these

00:14:04,230 --> 00:14:12,329
differences in Qualcomm HMP scheduler

00:14:08,369 --> 00:14:14,249
the tasks are divided into groups by

00:14:12,329 --> 00:14:14,910
importance so there are less important

00:14:14,249 --> 00:14:19,320
and more

00:14:14,910 --> 00:14:21,270
tests and by size yeah well when we

00:14:19,320 --> 00:14:23,330
speak about size of a task we need to

00:14:21,270 --> 00:14:27,330
define that too right

00:14:23,330 --> 00:14:32,400
so size is a measure of load that this

00:14:27,330 --> 00:14:35,130
task produces and task may be big when

00:14:32,400 --> 00:14:37,350
the load is big little when the produced

00:14:35,130 --> 00:14:40,230
load is little or other when it doesn't

00:14:37,350 --> 00:14:44,880
exactly fall in either of the two

00:14:40,230 --> 00:14:48,090
previous categories and thresholds for

00:14:44,880 --> 00:14:50,190
defining which tasks are big which tasks

00:14:48,090 --> 00:14:53,780
a little and which are neither of those

00:14:50,190 --> 00:14:59,780
are parametrized so there is a huge

00:14:53,780 --> 00:15:01,680
possibility of customizing q hm P

00:14:59,780 --> 00:15:06,240
depending on the system that you're

00:15:01,680 --> 00:15:08,550
running it on and then scheduling a task

00:15:06,240 --> 00:15:13,230
well obviously should depend on its

00:15:08,550 --> 00:15:18,540
properties and then again we need to

00:15:13,230 --> 00:15:20,730
define task size in a precise way it's

00:15:18,540 --> 00:15:29,450
done basing on the task demand

00:15:20,730 --> 00:15:29,450
calculation test demand calculation is

00:15:29,510 --> 00:15:38,880
calculated based on the formula that you

00:15:31,740 --> 00:15:41,670
can see so it's Delta time the time our

00:15:38,880 --> 00:15:45,330
task running on a core in a period of

00:15:41,670 --> 00:15:48,870
time now multiplied by the current

00:15:45,330 --> 00:15:53,030
frequency of this core divided by the

00:15:48,870 --> 00:15:53,030
maximum frequency across all cores

00:15:53,120 --> 00:15:58,560
so we do account for differences in

00:15:55,740 --> 00:16:00,510
maximum frequency between the golden

00:15:58,560 --> 00:16:07,710
cluster in the silver cluster just in

00:16:00,510 --> 00:16:10,890
case and then to be more stable we need

00:16:07,710 --> 00:16:14,520
to calculate task demand over several

00:16:10,890 --> 00:16:18,030
sliding windows and this is also a

00:16:14,520 --> 00:16:21,810
parameter called

00:16:18,030 --> 00:16:26,870
n and usually in most implementations we

00:16:21,810 --> 00:16:31,380
said n equal to 5 and then we either

00:16:26,870 --> 00:16:33,960
calculate average demand or we take the

00:16:31,380 --> 00:16:39,510
maximum demand or we do some kind of

00:16:33,960 --> 00:16:43,680
combo and some testing results showed

00:16:39,510 --> 00:16:47,190
that the best possible situation is when

00:16:43,680 --> 00:16:51,870
we calculate demand as being a maximum

00:16:47,190 --> 00:16:56,120
of the first demand in the series the

00:16:51,870 --> 00:17:02,550
one that is most recent and the average

00:16:56,120 --> 00:17:04,380
calculated over all the samples as I

00:17:02,550 --> 00:17:06,360
said we already account for difference

00:17:04,380 --> 00:17:08,880
in maximum frequency between the big

00:17:06,360 --> 00:17:11,220
cluster and little cluster but we also

00:17:08,880 --> 00:17:14,010
need to account for high performance of

00:17:11,220 --> 00:17:17,700
beakers even when they are operating on

00:17:14,010 --> 00:17:21,600
the same frequency and then we add a

00:17:17,700 --> 00:17:26,100
coefficient called max possible

00:17:21,600 --> 00:17:29,070
efficiency and we scale demand according

00:17:26,100 --> 00:17:32,850
to that coefficient once again it is

00:17:29,070 --> 00:17:36,750
also a parameter and it's usually set to

00:17:32,850 --> 00:17:40,080
two so we consider usually we consider

00:17:36,750 --> 00:17:43,940
big course being twice as efficient a

00:17:40,080 --> 00:17:43,940
small core on the same frequency

00:17:48,210 --> 00:17:54,090
so then we have a pretty good

00:17:51,360 --> 00:17:59,159
understanding of what big and small

00:17:54,090 --> 00:18:02,159
tasks in queue H&P actually are so a

00:17:59,159 --> 00:18:06,749
small task would be a periodic task with

00:18:02,159 --> 00:18:11,879
short execution time and a big task is a

00:18:06,749 --> 00:18:14,700
task producing high CPU load well being

00:18:11,879 --> 00:18:20,429
high CPU load also a parameter and

00:18:14,700 --> 00:18:22,740
usually it means around 90% in Android

00:18:20,429 --> 00:18:26,249
there are some background threads that

00:18:22,740 --> 00:18:28,259
can become a big tasks if we only take

00:18:26,249 --> 00:18:30,570
CPU load but we don't want to do that

00:18:28,259 --> 00:18:34,919
because we don't want to schedule

00:18:30,570 --> 00:18:37,259
background tasks on big cores so the

00:18:34,919 --> 00:18:41,210
importance of the task should also be

00:18:37,259 --> 00:18:45,480
accounted when we consider it as big

00:18:41,210 --> 00:18:49,649
once again it is important to note that

00:18:45,480 --> 00:18:53,549
some tasks are neither big nor small so

00:18:49,649 --> 00:18:57,269
we say there other and the tasks can

00:18:53,549 --> 00:19:00,119
change their size over time while small

00:18:57,269 --> 00:19:03,869
tasks may become other big tasks may

00:19:00,119 --> 00:19:06,539
become other basically small tasks

00:19:03,869 --> 00:19:08,009
shouldn't become big and big tasks

00:19:06,539 --> 00:19:09,960
shouldn't become small and if that

00:19:08,009 --> 00:19:12,840
happens in your system then probably

00:19:09,960 --> 00:19:15,019
thresholds were selected in the wrong

00:19:12,840 --> 00:19:15,019
way

00:19:16,220 --> 00:19:25,049
also when we're speaking about qgp while

00:19:20,309 --> 00:19:27,749
the high maintenance parachute we need

00:19:25,049 --> 00:19:35,340
to understand that it's tightly coupled

00:19:27,749 --> 00:19:38,039
with the CPU frag governor called

00:19:35,340 --> 00:19:41,249
interactive which has never made its way

00:19:38,039 --> 00:19:43,950
in the mainline Linux kernel so it's

00:19:41,249 --> 00:19:47,009
basically an out of tree governor and

00:19:43,950 --> 00:19:50,999
then it was heavily patched by cock-on

00:19:47,009 --> 00:19:53,429
quorum to communicate with the scheduler

00:19:50,999 --> 00:19:57,990
in the best way or well in the way they

00:19:53,429 --> 00:20:00,779
considered best so this created a pile

00:19:57,990 --> 00:20:03,169
of code that is not maintainable and not

00:20:00,779 --> 00:20:03,169
mainline

00:20:03,289 --> 00:20:09,200
although all for the good reasons so QA

00:20:07,070 --> 00:20:10,970
GMP is tightly coupled with heavily

00:20:09,200 --> 00:20:13,610
patched governor

00:20:10,970 --> 00:20:16,929
it says performance in the slides that's

00:20:13,610 --> 00:20:20,169
not correct it should be interactive

00:20:16,929 --> 00:20:20,169
sorry about that

00:20:20,440 --> 00:20:29,539
okay now we're passing over to the fun

00:20:23,990 --> 00:20:35,720
part comparisons and the first one the

00:20:29,539 --> 00:20:43,909
first comparison was to measure power

00:20:35,720 --> 00:20:47,570
consumption for YouTube playback and as

00:20:43,909 --> 00:20:57,019
you can see on the slide there's a clear

00:20:47,570 --> 00:20:59,740
win in es es is almost 20% better in

00:20:57,019 --> 00:21:04,999
power consumption compared equation P

00:20:59,740 --> 00:21:07,460
when you do the YouTube playback the

00:21:04,999 --> 00:21:09,649
other tests showed roughly the same

00:21:07,460 --> 00:21:16,399
power consumption but then we did

00:21:09,649 --> 00:21:18,980
another test for framedrops and it

00:21:16,399 --> 00:21:23,690
turned out that when there is a bursty

00:21:18,980 --> 00:21:28,249
load then es doesn't really work that

00:21:23,690 --> 00:21:31,039
well for instance if we take the Chrome

00:21:28,249 --> 00:21:35,840
scrolling which is represented on the

00:21:31,039 --> 00:21:42,559
slide on the graph as the first two

00:21:35,840 --> 00:21:47,990
columns we can see that yes is more than

00:21:42,559 --> 00:21:50,419
twice worse than Q in P in the number of

00:21:47,990 --> 00:21:55,480
frame drops so the quality of service is

00:21:50,419 --> 00:21:55,480
actually degraded if we take a yes okay

00:21:56,860 --> 00:22:00,700
here comes the grumpy cat

00:22:01,800 --> 00:22:06,010
and we're really upset together with the

00:22:04,360 --> 00:22:11,050
grumpy cat because we don't know what to

00:22:06,010 --> 00:22:13,810
pick up because on one hand es works

00:22:11,050 --> 00:22:16,260
best with a steady load showing

00:22:13,810 --> 00:22:19,210
excellent power consumption results and

00:22:16,260 --> 00:22:22,240
acceptable quality of service but when

00:22:19,210 --> 00:22:26,920
it comes to bursts allowed es don't

00:22:22,240 --> 00:22:31,440
doesn't cut that well and well then the

00:22:26,920 --> 00:22:31,440
equation P seems to behave a lot better

00:22:36,120 --> 00:22:45,280
so what to pick up let's try to

00:22:39,130 --> 00:22:49,930
summarize QE Chimpy has a strong focus

00:22:45,280 --> 00:22:51,700
on performance while es together with

00:22:49,930 --> 00:22:56,770
belt is more focused on power

00:22:51,700 --> 00:23:02,940
conservation KMP is complex out of tree

00:22:56,770 --> 00:23:07,690
has alpha skated code very flexible but

00:23:02,940 --> 00:23:11,440
not really maintainable and couldn't

00:23:07,690 --> 00:23:14,980
ever be mainline also it's worth to note

00:23:11,440 --> 00:23:18,010
that while with the flexibility of

00:23:14,980 --> 00:23:22,690
having many parameters to tune a comes

00:23:18,010 --> 00:23:26,560
also an issue of combinations that have

00:23:22,690 --> 00:23:29,530
never been tested and I think we get

00:23:26,560 --> 00:23:31,420
some feedback that it's around 90% of

00:23:29,530 --> 00:23:36,550
the combinations that have never been

00:23:31,420 --> 00:23:40,840
tested so well that doesn't look too

00:23:36,550 --> 00:23:42,400
good and yes is looking good but it's

00:23:40,840 --> 00:23:47,940
not delivering the quality of service

00:23:42,400 --> 00:23:47,940
right so once again what to pick up

00:23:53,360 --> 00:23:56,360
yeah

00:24:07,450 --> 00:24:12,310
yes that's a very good question

00:24:10,060 --> 00:24:15,390
you mean you mean the decay coefficient

00:24:12,310 --> 00:24:15,390
the queue or

00:24:30,330 --> 00:24:37,710
right right if you if you don't mind I

00:24:35,040 --> 00:24:39,570
will postpone this question because I

00:24:37,710 --> 00:24:41,790
believe the answer is given later in the

00:24:39,570 --> 00:24:44,760
slides and if it doesn't then you're

00:24:41,790 --> 00:24:56,430
very welcome to ask again okay any other

00:24:44,760 --> 00:24:58,830
questions so far been well well we we

00:24:56,430 --> 00:25:00,990
measure Birkin well it's not exactly

00:24:58,830 --> 00:25:05,250
power consumption but we measure average

00:25:00,990 --> 00:25:08,490
current so the the result was showing

00:25:05,250 --> 00:25:19,080
average current per of the time of

00:25:08,490 --> 00:25:21,120
YouTube playback what well given that we

00:25:19,080 --> 00:25:24,480
use what well it should it should have

00:25:21,120 --> 00:25:25,920
been probably set explicitly but we

00:25:24,480 --> 00:25:30,230
measured using the external battery

00:25:25,920 --> 00:25:34,860
which maintains a stable voltage of 4.0

00:25:30,230 --> 00:25:38,820
and with that said now you can it can

00:25:34,860 --> 00:25:44,190
use simple multiplication skills to to

00:25:38,820 --> 00:25:47,130
produce the power result so this yeah

00:25:44,190 --> 00:25:51,510
this probably had to be said but yeah

00:25:47,130 --> 00:25:55,340
the result is in milliamps because we we

00:25:51,510 --> 00:25:59,430
measure average current but it's easily

00:25:55,340 --> 00:26:03,690
convertible into the real power

00:25:59,430 --> 00:26:10,170
consumption because the voltage is

00:26:03,690 --> 00:26:16,200
constant any other questions so far okay

00:26:10,170 --> 00:26:19,530
so the way forward for AES was to move

00:26:16,200 --> 00:26:24,540
forward with es because well it's

00:26:19,530 --> 00:26:28,200
maintainable and the code was a lot

00:26:24,540 --> 00:26:30,990
easier to learn and once again

00:26:28,200 --> 00:26:37,530
es stood the chance of mainlining while

00:26:30,990 --> 00:26:41,250
campi didn't but what what could we do

00:26:37,530 --> 00:26:43,890
to improve it to show better results

00:26:41,250 --> 00:26:47,160
with regard to quality of service

00:26:43,890 --> 00:26:51,510
and the answer was well we can use test

00:26:47,160 --> 00:26:55,170
demand calculation from qmp so we can

00:26:51,510 --> 00:27:00,150
take it off creation P and use it as a

00:26:55,170 --> 00:27:05,390
separate module replacing belt so here

00:27:00,150 --> 00:27:10,400
comes the Walt arrival to belt which is

00:27:05,390 --> 00:27:14,460
window assisted low tracking and in fact

00:27:10,400 --> 00:27:17,280
it does implement in window demand

00:27:14,460 --> 00:27:25,740
calculation which was previously

00:27:17,280 --> 00:27:29,340
implemented within creation B so here on

00:27:25,740 --> 00:27:34,740
the next slide you can see the block

00:27:29,340 --> 00:27:40,110
scheme of updating the task average

00:27:34,740 --> 00:27:46,500
demand explained or depicted in more

00:27:40,110 --> 00:27:48,419
detail so where the Delta is the time of

00:27:46,500 --> 00:27:55,980
task running on the core in a period of

00:27:48,419 --> 00:28:01,190
time in a window so if we are in the new

00:27:55,980 --> 00:28:01,190
window then we

00:28:05,830 --> 00:28:11,830
update the history I think they know and

00:28:09,289 --> 00:28:15,279
yes here are mixed up sorry about that

00:28:11,830 --> 00:28:18,200
but at least we can verify it now so if

00:28:15,279 --> 00:28:21,379
if this is a new window then we update

00:28:18,200 --> 00:28:25,610
history we drop the last sample we move

00:28:21,379 --> 00:28:30,259
the all the other samples by one and

00:28:25,610 --> 00:28:35,169
then we get the new sample now with

00:28:30,259 --> 00:28:38,539
update history function and if this is

00:28:35,169 --> 00:28:41,119
not a new window if we're executing from

00:28:38,539 --> 00:28:45,980
the window that we've done some samples

00:28:41,119 --> 00:28:52,669
before then we update the average and

00:28:45,980 --> 00:28:57,259
thus we give the runnable some variable

00:28:52,669 --> 00:29:02,860
to get it and it's the result that we

00:28:57,259 --> 00:29:06,499
use for estimation of utilization of CPU

00:29:02,860 --> 00:29:10,159
it's important to mention that we use

00:29:06,499 --> 00:29:12,080
samples obtained from the last window

00:29:10,159 --> 00:29:14,240
not the current window because for the

00:29:12,080 --> 00:29:23,450
current window we don't have all the

00:29:14,240 --> 00:29:25,580
results here well what else Walt is also

00:29:23,450 --> 00:29:29,990
tightly coupled with the CPU frac it

00:29:25,580 --> 00:29:34,549
provides data to CPU frack about the CPU

00:29:29,990 --> 00:29:36,529
utilization and it also notifies

00:29:34,549 --> 00:29:40,549
governor the CPU for governor about

00:29:36,529 --> 00:29:44,690
inter cluster migrations because CPU for

00:29:40,549 --> 00:29:50,240
governor operates only on a single

00:29:44,690 --> 00:29:52,639
cluster so it operates well there are

00:29:50,240 --> 00:29:54,710
two instances of each governor whatever

00:29:52,639 --> 00:29:56,960
governor issues and one operating on a

00:29:54,710 --> 00:29:59,480
big cluster and one operating on little

00:29:56,960 --> 00:30:00,320
one and they basically are agnostic

00:29:59,480 --> 00:30:05,090
about each other

00:30:00,320 --> 00:30:08,809
so we need to notify CPU for a governor

00:30:05,090 --> 00:30:11,740
if we're transferring a task from one

00:30:08,809 --> 00:30:11,740
cluster to the other

00:30:15,330 --> 00:30:24,010
and then we have a picture of CPU load

00:30:20,470 --> 00:30:30,010
tracking compared between Walton pelt

00:30:24,010 --> 00:30:32,679
and they're really looking quite much

00:30:30,010 --> 00:30:37,390
the same right especially if we take the

00:30:32,679 --> 00:30:44,230
upper ones but if we magnify strongly we

00:30:37,390 --> 00:30:50,710
can see that belt is actually ramping up

00:30:44,230 --> 00:30:55,419
slower and decaying slower also they

00:30:50,710 --> 00:31:01,809
were also some initiatives to change the

00:30:55,419 --> 00:31:07,510
decay factor which were not accepted

00:31:01,809 --> 00:31:10,299
that well by the community so well belt

00:31:07,510 --> 00:31:12,490
was considered to be obsolete basically

00:31:10,299 --> 00:31:16,630
and that's also the result

00:31:12,490 --> 00:31:21,820
interpretation so now we have a happy

00:31:16,630 --> 00:31:24,340
anymore because now we get all the way

00:31:21,820 --> 00:31:28,929
to something that works equally well

00:31:24,340 --> 00:31:31,630
power consumption wise and performance

00:31:28,929 --> 00:31:34,240
wise quality of service avoid because

00:31:31,630 --> 00:31:37,830
Walt is ramping up and down faster and

00:31:34,240 --> 00:31:40,059
the fact that it's ramping down faster

00:31:37,830 --> 00:31:42,520
it's also quite important for power

00:31:40,059 --> 00:31:45,220
consumption and the way the fact that

00:31:42,520 --> 00:31:50,470
it's ramping up faster is important for

00:31:45,220 --> 00:31:54,010
QoS but still given the possible spikes

00:31:50,470 --> 00:31:57,510
due to less stable operation of Walt we

00:31:54,010 --> 00:31:59,559
were concerned about power consumption a

00:31:57,510 --> 00:32:04,390
possible increase in power consumption

00:31:59,559 --> 00:32:07,090
but they were tests done showing that

00:32:04,390 --> 00:32:09,330
there is no actual huge power

00:32:07,090 --> 00:32:13,299
consumption impact because we don't have

00:32:09,330 --> 00:32:16,200
the need for frequency boosting which we

00:32:13,299 --> 00:32:16,200
had with AES

00:32:18,990 --> 00:32:31,180
okay the wrap-up there is a nice summary

00:32:28,210 --> 00:32:35,440
between Pelt and Walt you know put

00:32:31,180 --> 00:32:38,230
together in a table but that that's

00:32:35,440 --> 00:32:41,650
light I'm really I don't feel the need

00:32:38,230 --> 00:32:43,270
to read it it's it's there for your

00:32:41,650 --> 00:32:45,420
convenience and for my convenience if

00:32:43,270 --> 00:32:47,230
you want to ask some questions about it

00:32:45,420 --> 00:32:49,930
you're very welcome

00:32:47,230 --> 00:32:53,260
otherwise I'll just pass over to the

00:32:49,930 --> 00:32:56,350
current status of vias and the current

00:32:53,260 --> 00:32:59,680
status is that Walt has become the first

00:32:56,350 --> 00:33:01,510
and main choice for it due to the

00:32:59,680 --> 00:33:04,480
reasons that we have talked about just

00:33:01,510 --> 00:33:07,950
recently and first of all the bear qos

00:33:04,480 --> 00:33:14,590
but also that there is no power degrade

00:33:07,950 --> 00:33:18,660
and yes Walt is effectively as plus the

00:33:14,590 --> 00:33:24,250
jasc accounting from cue HMP so whatever

00:33:18,660 --> 00:33:29,710
bad words I had said about QE GMP in the

00:33:24,250 --> 00:33:33,550
past well actually a huge pile of code

00:33:29,710 --> 00:33:40,960
from KMP turned out to be useful and

00:33:33,550 --> 00:33:43,720
important and also it's always a good

00:33:40,960 --> 00:33:47,290
thing when two competing implementations

00:33:43,720 --> 00:33:49,960
converge and the resulting

00:33:47,290 --> 00:33:54,580
implementation takes the best out of two

00:33:49,960 --> 00:33:56,590
worlds so that's that's how open source

00:33:54,580 --> 00:34:02,560
should work that's how collaboration

00:33:56,590 --> 00:34:07,000
should work and yeah that's just a good

00:34:02,560 --> 00:34:09,640
thing there are still some small things

00:34:07,000 --> 00:34:14,380
some small deficiencies that we believe

00:34:09,640 --> 00:34:20,560
are therefore the current es / Walt

00:34:14,380 --> 00:34:24,610
implementation and as a funny thing

00:34:20,560 --> 00:34:29,470
those are the deficiencies that were not

00:34:24,610 --> 00:34:30,879
there in tuition P for instance there is

00:34:29,470 --> 00:34:35,220
no notion of big

00:34:30,879 --> 00:34:38,409
small tasks in a yes and sometimes

00:34:35,220 --> 00:34:42,069
sometimes that leads to suboptimal

00:34:38,409 --> 00:34:47,889
results for instance when we can see

00:34:42,069 --> 00:34:51,909
there a s and tasks backing as you know

00:34:47,889 --> 00:34:54,190
due to the algorithms yes wouldn't

00:34:51,909 --> 00:35:01,089
packet ask if that would mean raising

00:34:54,190 --> 00:35:05,920
CPU frequency but on the other hand as

00:35:01,089 --> 00:35:10,779
shown on the graph to the left the power

00:35:05,920 --> 00:35:12,849
consumption may be bigger if we have two

00:35:10,779 --> 00:35:15,430
cores operating on a smaller frequency

00:35:12,849 --> 00:35:18,940
as opposed to a single core on a

00:35:15,430 --> 00:35:22,299
slightly higher frequency that changes

00:35:18,940 --> 00:35:25,750
when the frequencies arising but on

00:35:22,299 --> 00:35:30,519
small frequencies that is usually the

00:35:25,750 --> 00:35:34,809
case so sometimes it is actually better

00:35:30,519 --> 00:35:38,650
to pack tasks even though packing would

00:35:34,809 --> 00:35:42,700
mean raising a frequency for the CPU

00:35:38,650 --> 00:35:44,680
that the task ends up upon as opposed to

00:35:42,700 --> 00:35:50,430
having two CPUs running at the same time

00:35:44,680 --> 00:35:50,430
for instance on the picture to the right

00:35:52,170 --> 00:35:59,559
we can see that the second small CPU is

00:35:57,279 --> 00:36:03,400
basically and used so it's better to

00:35:59,559 --> 00:36:06,160
switch it off and put in the task being

00:36:03,400 --> 00:36:10,750
scheduled on the first CPU provided that

00:36:06,160 --> 00:36:12,819
the frequencies are low enough but we

00:36:10,750 --> 00:36:15,789
cannot do it in yes because there is no

00:36:12,819 --> 00:36:18,789
such algorithmic code in yes and we

00:36:15,789 --> 00:36:20,380
could do it when Q Asian P because if

00:36:18,789 --> 00:36:23,460
the task is small it would have been

00:36:20,380 --> 00:36:23,460
placed like that

00:36:29,210 --> 00:36:40,099
oops conclusions well we were speaking

00:36:36,589 --> 00:36:41,540
about big.little and es n es is the

00:36:40,099 --> 00:36:44,059
primary choice for big little

00:36:41,540 --> 00:36:46,220
architectures and well big.little

00:36:44,059 --> 00:36:47,720
architecture puts high demand on the

00:36:46,220 --> 00:36:51,440
system software and especially on the

00:36:47,720 --> 00:36:53,260
scheduler and also on the dynamic

00:36:51,440 --> 00:36:58,220
voltage and frequency scaling

00:36:53,260 --> 00:37:01,819
implementation so given those high

00:36:58,220 --> 00:37:04,790
demands it's hard to be perfect right

00:37:01,819 --> 00:37:10,550
and es is not perfect but it is still

00:37:04,790 --> 00:37:16,970
the way to go and this I believe is the

00:37:10,550 --> 00:37:19,400
unbiased view on yes ok as the last

00:37:16,970 --> 00:37:21,050
thing I would like to thank people who

00:37:19,400 --> 00:37:24,200
are not there but who were helping me

00:37:21,050 --> 00:37:27,559
out making this presentation

00:37:24,200 --> 00:37:33,230
Vlad risky I'm working with him in

00:37:27,559 --> 00:37:35,619
Sweden on multiple projects related to

00:37:33,230 --> 00:37:40,280
Android power and performance

00:37:35,619 --> 00:37:41,869
optimizations Antonio Gaurav that's one

00:37:40,280 --> 00:37:45,430
of the students I've been talking about

00:37:41,869 --> 00:37:45,430
in the beginning of my talk

00:37:45,700 --> 00:37:51,980
Danny nuclear bomb who made pictures of

00:37:49,520 --> 00:37:56,480
happy not so happy animals that I used

00:37:51,980 --> 00:37:59,869
in my slides and then my wife was also

00:37:56,480 --> 00:38:02,780
an inspiration for me and she also shown

00:37:59,869 --> 00:38:06,380
a lot of patience while I was preparing

00:38:02,780 --> 00:38:08,720
the slides and also I would like to

00:38:06,380 --> 00:38:11,390
thank you all for the attention this

00:38:08,720 --> 00:38:14,740
presentation is over and you're very

00:38:11,390 --> 00:38:17,980
welcome to ask any questions you have

00:38:14,740 --> 00:38:17,980
yes please

00:38:53,180 --> 00:39:00,950
well preference to keep our CPU on a

00:38:58,220 --> 00:39:02,240
core to keep the caches hot yeah it's

00:39:00,950 --> 00:39:04,549
definitely a good thing and it's

00:39:02,240 --> 00:39:08,240
implemented I think in in in yeah it's

00:39:04,549 --> 00:39:10,220
implemented in the latest yes anyway I'm

00:39:08,240 --> 00:39:12,920
not entirely sure if it was there at the

00:39:10,220 --> 00:39:18,019
time we tested the EES belt versus

00:39:12,920 --> 00:39:19,999
creation P as far as I can tell the the

00:39:18,019 --> 00:39:22,670
main problem for framedrops was

00:39:19,999 --> 00:39:27,769
different and that was that the cpu

00:39:22,670 --> 00:39:29,990
wasn't ramping up very quickly because I

00:39:27,769 --> 00:39:31,789
mean we need to raise the frequency if

00:39:29,990 --> 00:39:36,619
the load goes beyond a certain threshold

00:39:31,789 --> 00:39:42,440
and now the data that belt was supplying

00:39:36,619 --> 00:39:44,599
to the devfs was lagging in time so we

00:39:42,440 --> 00:39:46,640
we had bursty loads but the cpu

00:39:44,599 --> 00:39:53,829
frequency wasn't raised in a timely

00:39:46,640 --> 00:39:53,829
manner does this answer your question

00:40:04,350 --> 00:40:11,110
that depends on the architecture and it

00:40:07,420 --> 00:40:13,810
depends on operating point that we're

00:40:11,110 --> 00:40:15,400
switching from and to usually if we need

00:40:13,810 --> 00:40:17,280
to raise the voltage then it takes a

00:40:15,400 --> 00:40:21,010
significant while well I mean

00:40:17,280 --> 00:40:23,740
significant in terms of microseconds if

00:40:21,010 --> 00:40:25,600
if we don't have to raise the voltage to

00:40:23,740 --> 00:40:31,470
jump from one OPP to another then it's

00:40:25,600 --> 00:40:31,470
usually quite fast yes please

00:40:59,320 --> 00:41:08,180
well the energy costs are part of part

00:41:04,550 --> 00:41:13,130
of EES and and it was in in one of the

00:41:08,180 --> 00:41:16,160
first slides when when we were not

00:41:13,130 --> 00:41:19,700
talking about booting a task either on

00:41:16,160 --> 00:41:24,050
one of the available Baker's or one of

00:41:19,700 --> 00:41:27,920
the available little course speaking

00:41:24,050 --> 00:41:32,150
about shutting down ramping up a certain

00:41:27,920 --> 00:41:35,930
CPU that's also the thing thats related

00:41:32,150 --> 00:41:40,960
to CPU idle so that's that's another

00:41:35,930 --> 00:41:44,150
mechanism in the kernel and I believe

00:41:40,960 --> 00:41:46,180
this goes a little bit beyond the the

00:41:44,150 --> 00:41:49,520
gold of his goal of his presentation

00:41:46,180 --> 00:41:51,260
this this is a complicated well not that

00:41:49,520 --> 00:41:55,190
complicated but this is a separate thing

00:41:51,260 --> 00:41:57,190
and we can discuss it later because

00:41:55,190 --> 00:42:06,370
otherwise I believe it will take some

00:41:57,190 --> 00:42:06,370
minutes to complete then

00:42:21,600 --> 00:42:28,270
that's that's a good question but I

00:42:24,550 --> 00:42:31,950
believe it needs some kind of extra

00:42:28,270 --> 00:42:35,320
definition are you talking about an

00:42:31,950 --> 00:42:38,350
ancient P system that is running a basic

00:42:35,320 --> 00:42:45,390
CFS scheduler or are you talking about

00:42:38,350 --> 00:42:45,390
es running on an SMP not Asian P system

00:43:20,339 --> 00:43:24,609
well it depends independent depends on

00:43:23,080 --> 00:43:30,450
the load depends on the type of

00:43:24,609 --> 00:43:34,230
operation if if we take a if we take the

00:43:30,450 --> 00:43:37,359
scrolling and chrome type of thing that

00:43:34,230 --> 00:43:40,330
es was lagging in quality of service

00:43:37,359 --> 00:43:42,099
wise compared to Q HMP then there's

00:43:40,330 --> 00:43:45,970
gonna be a huge difference in power

00:43:42,099 --> 00:43:49,030
consumption to go all the way up to the

00:43:45,970 --> 00:43:53,230
same quality of service if we just if we

00:43:49,030 --> 00:44:00,970
just use CFS for for other loads there

00:43:53,230 --> 00:44:06,940
can be a smaller difference so it's it's

00:44:00,970 --> 00:44:12,330
about energy awareness right any other

00:44:06,940 --> 00:44:12,330
questions yes please

00:44:33,550 --> 00:44:38,960
you mean you mean it's possible to

00:44:36,080 --> 00:44:41,870
configure Q HMP so that it's more power

00:44:38,960 --> 00:44:50,210
conserving and less performance oriented

00:44:41,870 --> 00:44:54,170
right yes

00:44:50,210 --> 00:44:57,290
yes that's that's true of course yes

00:44:54,170 --> 00:45:03,020
also has some parameters even not in

00:44:57,290 --> 00:45:04,670
such large number as q hm p but to go

00:45:03,020 --> 00:45:07,760
all the way up to the quality of service

00:45:04,670 --> 00:45:14,150
that equation p was able to provide we

00:45:07,760 --> 00:45:17,210
needed to turn boosting for for the

00:45:14,150 --> 00:45:20,420
bursty loads which is also I mean it's

00:45:17,210 --> 00:45:27,950
it's also a parameter but if we turn on

00:45:20,420 --> 00:45:30,290
boosting then the whole advantage of yes

00:45:27,950 --> 00:45:32,870
that was there in terms of power

00:45:30,290 --> 00:45:36,200
consumption goes away and in fact in

00:45:32,870 --> 00:45:38,780
most cases it becomes inferior to q hm p

00:45:36,200 --> 00:45:45,500
in terms of power consumption because we

00:45:38,780 --> 00:45:49,670
we just now we basically just have all

00:45:45,500 --> 00:45:53,690
the frequencies up just just because we

00:45:49,670 --> 00:45:57,440
do not rely on getting the information

00:45:53,690 --> 00:46:04,250
about cpu load in a timely manner so we

00:45:57,440 --> 00:46:06,290
mitigate that by just churning the

00:46:04,250 --> 00:46:12,220
frequencies up if we see that the chrome

00:46:06,290 --> 00:46:14,660
is launching right so that that that is

00:46:12,220 --> 00:46:21,970
basically a method but it's more of a

00:46:14,660 --> 00:46:21,970
workaround right yes please

00:46:35,740 --> 00:46:42,500
that's a very interesting thing to try

00:46:38,540 --> 00:46:50,870
out but we were really concentrating on

00:46:42,500 --> 00:46:53,110
embedded stuff any other questions yes

00:46:50,870 --> 00:46:53,110
please

00:47:09,619 --> 00:47:16,490
I need to double-check I don't think it

00:47:11,839 --> 00:47:18,700
made much difference III don't think so

00:47:16,490 --> 00:47:18,700
No

00:47:26,540 --> 00:47:29,680
any other questions

00:47:34,359 --> 00:47:39,179
well I guess not well thanks again for

00:47:36,489 --> 00:47:39,179
your attention

00:47:40,440 --> 00:47:45,070

YouTube URL: https://www.youtube.com/watch?v=UGhKeCyOIJM


