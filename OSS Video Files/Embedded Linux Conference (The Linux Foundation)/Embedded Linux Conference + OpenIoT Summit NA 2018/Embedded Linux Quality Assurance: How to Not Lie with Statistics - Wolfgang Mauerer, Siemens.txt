Title: Embedded Linux Quality Assurance: How to Not Lie with Statistics - Wolfgang Mauerer, Siemens
Publication date: 2018-03-13
Playlist: Embedded Linux Conference + OpenIoT Summit NA 2018
Description: 
	Embedded Linux Quality Assurance: How to Not Lie with Statistics - Wolfgang Mauerer, Siemens AG/OTH Regensburg

Embedded Linux drives an every-increasing number of appliances in many domains and applications, some even real-time and/or safety critical. Traditional quality assurance of such systems is based on testing and formal verification, but the huge amount of code and the rapid dynamics of the Linux ecosystem, as well as fundamental limitations of formal methods make these approaches unsatisfactory.

Statistical quality assurance for reliability, error rates, maximal latencies etc. is needed. We will discuss current best practises, how to design and run automated statistical tests that capture relevant information, and how to properly evaluate the resulting data. Practical real-world examples and recipes are played through using the open source R language. Most importantly, we identify common mistakes in (over-)interpreting statistical results and predictions that may eventually harm people. 

About Wolfgang Mauerer
Wolfgang Mauerer is a professor of theoretical computer science at the Technical University Regensburg, and a senior key expert at Siemens Corporate Research, Competence Centre Embedded Linux. He serves on the technical steering committee of the Linux Foundation's Civil Infrastructure platform. His academic research deals with socio-technical software engineering, and the industrial use of open source. His speaking experience includes ELC, ABS, ELCE, OSSJ, OSSNA, and many other industrial and scientific conferences.
Captions: 
	00:00:00,030 --> 00:00:06,089
seemingly not yes so welcome to the

00:00:03,149 --> 00:00:08,610
final session of the day my name is

00:00:06,089 --> 00:00:10,260
Wolfgang Mora a hour simultaneously

00:00:08,610 --> 00:00:12,870
broke for two companies one is a

00:00:10,260 --> 00:00:15,030
university the Technical University of

00:00:12,870 --> 00:00:16,460
Applied Sciences in Regensburg and the

00:00:15,030 --> 00:00:18,869
other one you may have heard of is

00:00:16,460 --> 00:00:21,930
Siemens corporate research where I'm

00:00:18,869 --> 00:00:23,880
active in the embedded Linux team and

00:00:21,930 --> 00:00:26,430
have been there for like ten years by

00:00:23,880 --> 00:00:28,619
now so what I'm going to talk to you in

00:00:26,430 --> 00:00:31,920
this light session I guess you're all

00:00:28,619 --> 00:00:34,350
very exhausted already from a day full

00:00:31,920 --> 00:00:36,690
of learning and knowledge so I'm the

00:00:34,350 --> 00:00:38,670
more happy that you came here to be

00:00:36,690 --> 00:00:41,489
tortured with a little statistics at the

00:00:38,670 --> 00:00:44,550
end of the day is about embedded Linux

00:00:41,489 --> 00:00:46,860
Quality Assurance and I decided to give

00:00:44,550 --> 00:00:50,820
it the catchy subtitle how to not live

00:00:46,860 --> 00:00:54,090
with statistics I may say a few words

00:00:50,820 --> 00:00:55,710
about that later on hey I already did

00:00:54,090 --> 00:00:58,230
say a little something about me Siemens

00:00:55,710 --> 00:00:59,910
corporate technology and the University

00:00:58,230 --> 00:01:02,399
of Oregon's books of what I'm assuming

00:00:59,910 --> 00:01:05,280
about you is that you are in some way or

00:01:02,399 --> 00:01:06,960
another in the Linux system building

00:01:05,280 --> 00:01:11,310
business that your software architecture

00:01:06,960 --> 00:01:13,590
that your system architects safety am

00:01:11,310 --> 00:01:15,930
sorry statistical methods are

00:01:13,590 --> 00:01:17,880
increasingly applied in safety critical

00:01:15,930 --> 00:01:20,340
domains in real-time domains I'm not

00:01:17,880 --> 00:01:22,530
assuming much real-time knowledge

00:01:20,340 --> 00:01:25,530
besides our familiarity with some

00:01:22,530 --> 00:01:27,270
elementary data sets maybe or knowledge

00:01:25,530 --> 00:01:30,720
about safety critical processes I've

00:01:27,270 --> 00:01:33,750
spoken a lot about that the last time I

00:01:30,720 --> 00:01:35,640
was here at ELC and most importantly I'm

00:01:33,750 --> 00:01:39,630
also not assuming much statistical

00:01:35,640 --> 00:01:42,450
knowledge that's deliberate if any

00:01:39,630 --> 00:01:44,130
statistic should be in the audience I'm

00:01:42,450 --> 00:01:48,119
pretty sure you will have lots of

00:01:44,130 --> 00:01:50,040
reasons to complain because this is of

00:01:48,119 --> 00:01:52,110
course not this is of course not not the

00:01:50,040 --> 00:01:55,759
statistics like J it's not in supposed

00:01:52,110 --> 00:01:57,540
to be a statistics lecture so this test

00:01:55,759 --> 00:01:59,549
statisticians Omega that's such a

00:01:57,540 --> 00:02:02,310
complicated word take what I say with a

00:01:59,549 --> 00:02:06,180
grain of salt and the rest take it as

00:02:02,310 --> 00:02:07,979
simplified recipes so I was already

00:02:06,180 --> 00:02:09,959
mentioning how to not lie with

00:02:07,979 --> 00:02:13,210
statistics this whole lying the

00:02:09,959 --> 00:02:16,150
statistics phrase or don't trust

00:02:13,210 --> 00:02:18,670
mystics that you haven't made up

00:02:16,150 --> 00:02:21,880
yourself belongs to the most stupid

00:02:18,670 --> 00:02:24,850
sayings in science because statistics is

00:02:21,880 --> 00:02:26,380
actually what drives our world in an

00:02:24,850 --> 00:02:28,390
increasing fashion when you think of

00:02:26,380 --> 00:02:30,730
machine learning when you think of all

00:02:28,390 --> 00:02:32,470
the mathematical optimization techniques

00:02:30,730 --> 00:02:34,810
that we apply when you think of all

00:02:32,470 --> 00:02:36,550
these stochastic algorithms that we

00:02:34,810 --> 00:02:39,130
apply but still I wanted to get people

00:02:36,550 --> 00:02:41,530
into my session so I chose this catchy

00:02:39,130 --> 00:02:43,600
one how I should have actually called it

00:02:41,530 --> 00:02:46,030
this not how to not live with statistics

00:02:43,600 --> 00:02:47,830
especially because I don't want I don't

00:02:46,030 --> 00:02:50,530
want to insult anyone who has used

00:02:47,830 --> 00:02:53,260
statistical methods in a way that I'm

00:02:50,530 --> 00:02:55,290
not recommending in this session is more

00:02:53,260 --> 00:02:56,890
to the point how to avoid incidentally

00:02:55,290 --> 00:02:59,470
over-interpreting measured data

00:02:56,890 --> 00:03:02,020
presenting data in not so useful ways

00:02:59,470 --> 00:03:05,140
and or drawing unsupported conclusions

00:03:02,020 --> 00:03:07,300
from statistical analysis which is

00:03:05,140 --> 00:03:09,160
scientifically precise but I guess would

00:03:07,300 --> 00:03:10,690
have made the program committee a bit

00:03:09,160 --> 00:03:13,030
angry because they would have needed an

00:03:10,690 --> 00:03:17,860
extra-wide website just to accommodate

00:03:13,030 --> 00:03:20,620
the title ok so but let me let me get

00:03:17,860 --> 00:03:23,250
right into the talk where do we need

00:03:20,620 --> 00:03:25,720
statistical methods in embedded quality

00:03:23,250 --> 00:03:29,470
embedded Linux quality assurance and

00:03:25,720 --> 00:03:31,570
that's these days actually quite a lot

00:03:29,470 --> 00:03:33,940
of topics where statistical methods come

00:03:31,570 --> 00:03:37,510
into play we want to determine we want a

00:03:33,940 --> 00:03:39,910
certain sum some several functional

00:03:37,510 --> 00:03:41,410
properties of our systems like speed and

00:03:39,910 --> 00:03:44,080
throughput of course we want to measure

00:03:41,410 --> 00:03:47,530
how fast our systems compute we want to

00:03:44,080 --> 00:03:50,380
measure how much data we can process per

00:03:47,530 --> 00:03:52,840
time you need in real-time systems we

00:03:50,380 --> 00:03:55,720
are concerned with response latencies

00:03:52,840 --> 00:03:58,150
and things like that but we also need to

00:03:55,720 --> 00:03:59,920
deal with things like build consistency

00:03:58,150 --> 00:04:01,210
if you think of long term supported

00:03:59,920 --> 00:04:04,120
systems then we should make sure that

00:04:01,210 --> 00:04:06,280
our system that's being built now can be

00:04:04,120 --> 00:04:07,990
reproduced in exactly the same way in 10

00:04:06,280 --> 00:04:10,300
years time which require some

00:04:07,990 --> 00:04:11,680
statistical methods we of course also

00:04:10,300 --> 00:04:14,590
interested in non-functional properties

00:04:11,680 --> 00:04:16,239
like stability availability scalability

00:04:14,590 --> 00:04:18,580
correctness you name it all these illa

00:04:16,239 --> 00:04:20,950
T's that are typically that typically

00:04:18,580 --> 00:04:23,230
cannot be measured directly but that can

00:04:20,950 --> 00:04:25,690
just be determined via some indirect

00:04:23,230 --> 00:04:26,810
measurement bias determining other

00:04:25,690 --> 00:04:29,780
properties and then stir

00:04:26,810 --> 00:04:35,120
testicle inferring how far we've come in

00:04:29,780 --> 00:04:36,800
these qualities or realities testing is

00:04:35,120 --> 00:04:38,900
of course one field where statistics

00:04:36,800 --> 00:04:42,040
plays an increasing role we collect lots

00:04:38,900 --> 00:04:44,389
of data with continuous integration

00:04:42,040 --> 00:04:47,630
tests with load testing and we use

00:04:44,389 --> 00:04:50,300
statistics to for instance check if our

00:04:47,630 --> 00:04:52,400
process efficiency is good so if we are

00:04:50,300 --> 00:04:55,100
not checking in too many bugs if we are

00:04:52,400 --> 00:04:57,410
actually trying to reduce the amount of

00:04:55,100 --> 00:04:59,060
bugs compared to the amount of new bugs

00:04:57,410 --> 00:05:01,700
that we bring into our system that's a

00:04:59,060 --> 00:05:04,040
statistical problem it can be also used

00:05:01,700 --> 00:05:06,530
to detect gradual changes in process or

00:05:04,040 --> 00:05:08,630
detect if changes to the process

00:05:06,530 --> 00:05:13,340
actually have had good or bad outcomes

00:05:08,630 --> 00:05:15,740
same thing goes for reviewing patches we

00:05:13,340 --> 00:05:19,340
use statistics to test how efficient is

00:05:15,740 --> 00:05:21,889
review we can use we don't necessarily

00:05:19,340 --> 00:05:24,110
use statistics we should use statistics

00:05:21,889 --> 00:05:25,960
more often we could use statistics to

00:05:24,110 --> 00:05:29,720
determine if the review is actually

00:05:25,960 --> 00:05:33,200
sufficient so if it prevents bugs from

00:05:29,720 --> 00:05:35,860
going into the system if the relevant

00:05:33,200 --> 00:05:38,690
areas are covered and so on

00:05:35,860 --> 00:05:40,310
certification of systems is another area

00:05:38,690 --> 00:05:42,229
where statistics is becoming

00:05:40,310 --> 00:05:45,140
increasingly more and more important so

00:05:42,229 --> 00:05:47,750
in the good old olden days that maybe

00:05:45,140 --> 00:05:49,820
proceeded more up that may become before

00:05:47,750 --> 00:05:52,010
my career my own career in computing

00:05:49,820 --> 00:05:54,020
people used to build systems that were

00:05:52,010 --> 00:05:57,350
completely deterministic you wrote code

00:05:54,020 --> 00:05:58,760
you proved that the code was complete

00:05:57,350 --> 00:06:01,510
that the code was working as expected

00:05:58,760 --> 00:06:04,100
and then that was more or less

00:06:01,510 --> 00:06:06,169
disregarding some formally details of

00:06:04,100 --> 00:06:08,720
certification these days if you think of

00:06:06,169 --> 00:06:10,370
Linux systems that contains millions and

00:06:08,720 --> 00:06:12,260
millions and millions of lines of code

00:06:10,370 --> 00:06:13,910
we of course cannot go with these

00:06:12,260 --> 00:06:16,550
approaches anymore but need to

00:06:13,910 --> 00:06:18,440
statistically ascertain that we satisfy

00:06:16,550 --> 00:06:22,400
the properties that are required for

00:06:18,440 --> 00:06:26,750
safety and other certification criteria

00:06:22,400 --> 00:06:29,300
and even if we could still apply the

00:06:26,750 --> 00:06:33,830
aforementioned formal methods formal

00:06:29,300 --> 00:06:35,330
methods to ensuring system quality there

00:06:33,830 --> 00:06:38,460
are there are people who still do that

00:06:35,330 --> 00:06:42,389
who use formal verification methods

00:06:38,460 --> 00:06:44,009
for instance do you schedule ability

00:06:42,389 --> 00:06:46,020
analysis on roofing systems and so on

00:06:44,009 --> 00:06:48,389
but these people these days also

00:06:46,020 --> 00:06:50,840
implicitly rely on statistical

00:06:48,389 --> 00:06:54,120
techniques because for instance most

00:06:50,840 --> 00:06:57,210
techniques that to schedule ability

00:06:54,120 --> 00:07:01,069
analysis require worst case execution

00:06:57,210 --> 00:07:03,360
time either calculations or measurements

00:07:01,069 --> 00:07:06,150
calculations are considering modern

00:07:03,360 --> 00:07:08,370
processes complexity not really feasible

00:07:06,150 --> 00:07:10,410
these days so effectively in the end of

00:07:08,370 --> 00:07:12,030
the day when you're doing any schedule

00:07:10,410 --> 00:07:14,960
ability analysis and it proves on

00:07:12,030 --> 00:07:19,490
real-time systems you're also relying on

00:07:14,960 --> 00:07:22,380
statistical results that gave us tomates

00:07:19,490 --> 00:07:27,199
four-armed parameters like worst case

00:07:22,380 --> 00:07:30,360
execution times good so enough reason to

00:07:27,199 --> 00:07:35,400
think about statistics in embedded Linux

00:07:30,360 --> 00:07:37,289
development why is deploying statistical

00:07:35,400 --> 00:07:40,530
techniques hard in our domain and that

00:07:37,289 --> 00:07:42,570
that's for the for the obvious reasons

00:07:40,530 --> 00:07:44,699
so we're dealing with very rapidly

00:07:42,570 --> 00:07:47,430
changing systems so statements that we

00:07:44,699 --> 00:07:51,330
make need to be computed over and over

00:07:47,430 --> 00:07:53,099
again to be to be accurate we're dealing

00:07:51,330 --> 00:07:55,860
with large volumes of code that doesn't

00:07:53,099 --> 00:07:57,570
necessarily make it easier to analyze

00:07:55,860 --> 00:08:02,909
systems we have non-overlapping

00:07:57,570 --> 00:08:05,849
communities that talk about things in

00:08:02,909 --> 00:08:10,949
very different ways and that makes an

00:08:05,849 --> 00:08:13,199
analysis of the processes and of related

00:08:10,949 --> 00:08:14,849
things quite different because data that

00:08:13,199 --> 00:08:16,680
you get from one community may mean

00:08:14,849 --> 00:08:20,360
something very different than data that

00:08:16,680 --> 00:08:24,840
you get from another community and so on

00:08:20,360 --> 00:08:27,000
good but moving on to the main point

00:08:24,840 --> 00:08:31,530
beyond the motivation is dealing with

00:08:27,000 --> 00:08:33,599
data how do we deal with data that can

00:08:31,530 --> 00:08:37,740
be statistically analyzed that comes up

00:08:33,599 --> 00:08:40,320
in our daily work and actually I have to

00:08:37,740 --> 00:08:42,149
admit that when I was planning this talk

00:08:40,320 --> 00:08:45,360
I was planning for a much wider scope

00:08:42,149 --> 00:08:47,610
that I would on that I would cover then

00:08:45,360 --> 00:08:50,420
I did prepare all the slides about

00:08:47,610 --> 00:08:52,200
everything that I wanted to say then I

00:08:50,420 --> 00:08:54,450
gave this talk to

00:08:52,200 --> 00:08:56,610
me one time and after like one and a

00:08:54,450 --> 00:08:59,310
half hours I stopped talking to myself

00:08:56,610 --> 00:09:00,960
because I wasn't even halfway through so

00:08:59,310 --> 00:09:03,900
I had to reduce quite a lot of what I

00:09:00,960 --> 00:09:06,200
was initially planning so maybe some

00:09:03,900 --> 00:09:08,670
things may seem quite obvious to the

00:09:06,200 --> 00:09:11,220
statisticians and to those who have

00:09:08,670 --> 00:09:14,970
already done statistics but on the other

00:09:11,220 --> 00:09:16,710
hand the examples of doing things

00:09:14,970 --> 00:09:20,610
incorrectly I've chosen are all examples

00:09:16,710 --> 00:09:22,710
that I've met in my industrial job so

00:09:20,610 --> 00:09:27,180
there seems to be some need to go for

00:09:22,710 --> 00:09:29,790
the for the simple stuff so before we

00:09:27,180 --> 00:09:32,160
can do statistics we of course need to

00:09:29,790 --> 00:09:35,430
measure data in some way or another and

00:09:32,160 --> 00:09:37,380
that is already in one of the very very

00:09:35,430 --> 00:09:39,240
elementary problems a problem that seems

00:09:37,380 --> 00:09:41,400
very simple you record some data that

00:09:39,240 --> 00:09:43,350
you get from the system then you store

00:09:41,400 --> 00:09:47,430
these data and later on you process the

00:09:43,350 --> 00:09:50,580
data but actually there's three things

00:09:47,430 --> 00:09:53,760
you need to consider that often go

00:09:50,580 --> 00:09:56,100
completely unnoticed the first thing is

00:09:53,760 --> 00:09:58,410
about reproducibility of course when

00:09:56,100 --> 00:10:01,320
someone is doing a measurement he or she

00:09:58,410 --> 00:10:03,540
knows exactly what he or she measured

00:10:01,320 --> 00:10:05,100
but that may change completely in a week

00:10:03,540 --> 00:10:08,070
that may change completely in a month

00:10:05,100 --> 00:10:09,600
and it very very often it's very often

00:10:08,070 --> 00:10:11,670
the case that of course that you get

00:10:09,600 --> 00:10:14,040
data from someone who doesn't really

00:10:11,670 --> 00:10:16,230
know how doesn't really remember anymore

00:10:14,040 --> 00:10:18,360
how he acquired the data in the first

00:10:16,230 --> 00:10:20,790
place and that's then obviously a bad

00:10:18,360 --> 00:10:23,880
start to learning insights from these

00:10:20,790 --> 00:10:26,670
data so reproducibility can others

00:10:23,880 --> 00:10:28,560
represent or interpret the results

00:10:26,670 --> 00:10:30,450
you've been measuring that's one of the

00:10:28,560 --> 00:10:32,160
very fundamental things we need to take

00:10:30,450 --> 00:10:34,380
care of when doing statistics and I'll

00:10:32,160 --> 00:10:36,450
show some very simple yet effective

00:10:34,380 --> 00:10:39,690
recipes on how to achieve that in the

00:10:36,450 --> 00:10:42,120
next slide a second question that's

00:10:39,690 --> 00:10:44,850
relevant for statistical data is

00:10:42,120 --> 00:10:46,920
duration when is certainty about what I

00:10:44,850 --> 00:10:48,810
want to infer from the data when can

00:10:46,920 --> 00:10:50,940
certainty about what I want to infer

00:10:48,810 --> 00:10:53,030
from the data be achieved that's

00:10:50,940 --> 00:10:55,530
particularly important when you test

00:10:53,030 --> 00:10:58,140
like for example for real time

00:10:55,530 --> 00:11:01,110
properties so how long for how long do

00:10:58,140 --> 00:11:03,120
you need to inspect a system before you

00:11:01,110 --> 00:11:05,339
can draw conclusions that it will really

00:11:03,120 --> 00:11:07,890
ever produce with high certainty never

00:11:05,339 --> 00:11:09,960
produce any bad results in production

00:11:07,890 --> 00:11:13,650
runs is that five minutes is that ten

00:11:09,960 --> 00:11:15,300
minutes is that more like 10 days and so

00:11:13,650 --> 00:11:18,630
on and we need to ensure traceability

00:11:15,300 --> 00:11:20,520
that we do not only know that we do not

00:11:18,630 --> 00:11:22,980
only tell to others how to interpret the

00:11:20,520 --> 00:11:25,050
results but that we also tell to others

00:11:22,980 --> 00:11:27,570
what exactly be measured in a system so

00:11:25,050 --> 00:11:29,460
where we puts a trace points into the

00:11:27,570 --> 00:11:31,110
kernel that often non-standard ones

00:11:29,460 --> 00:11:33,510
where we hooked into the system

00:11:31,110 --> 00:11:36,420
what's a resolutions timer resolutions

00:11:33,510 --> 00:11:42,360
for instance we used when recorded our

00:11:36,420 --> 00:11:44,820
data and so on and actually the answer

00:11:42,360 --> 00:11:46,980
to at least two of the questioners

00:11:44,820 --> 00:11:53,430
traceability and reproducibility is

00:11:46,980 --> 00:11:56,190
extremely simple when we when we record

00:11:53,430 --> 00:12:00,930
so the the a very good way to achieve

00:11:56,190 --> 00:12:06,360
these two properties is to record data

00:12:00,930 --> 00:12:11,520
in a way that's called tidy data so tidy

00:12:06,360 --> 00:12:14,610
data was actually in when I when I talk

00:12:11,520 --> 00:12:16,200
about these three things that you need

00:12:14,610 --> 00:12:17,490
to make these three properties that you

00:12:16,200 --> 00:12:21,990
need to make sure that your dates are

00:12:17,490 --> 00:12:23,430
tidy they look so obvious people usually

00:12:21,990 --> 00:12:25,860
say you have a how should I do it any

00:12:23,430 --> 00:12:27,690
differently however on the other side

00:12:25,860 --> 00:12:29,310
when you tell someone to record data

00:12:27,690 --> 00:12:31,110
when you tell students to record data

00:12:29,310 --> 00:12:34,200
when you tell engineer's to record data

00:12:31,110 --> 00:12:36,510
it never comes out in this tidy form so

00:12:34,200 --> 00:12:39,150
that's these three rules are really one

00:12:36,510 --> 00:12:41,760
of the very gems of statistics and it

00:12:39,150 --> 00:12:44,190
has been it has taken an astonishing

00:12:41,760 --> 00:12:45,930
long while until these free rules were

00:12:44,190 --> 00:12:49,440
really formally adapted by the physics

00:12:45,930 --> 00:12:51,180
community so for instance in the our new

00:12:49,440 --> 00:12:53,670
art world that's a statistic language

00:12:51,180 --> 00:12:55,650
that I'm going to introduce later on the

00:12:53,670 --> 00:12:58,140
first papers that introduced this tidy

00:12:55,650 --> 00:13:01,170
data form really only appeared like 15

00:12:58,140 --> 00:13:04,350
years ago although of course you may

00:13:01,170 --> 00:13:06,750
have seen this form before either in

00:13:04,350 --> 00:13:10,200
your work or in lectures that's of

00:13:06,750 --> 00:13:12,089
course one of codes normal forms for

00:13:10,200 --> 00:13:14,010
databases the third normal forms in this

00:13:12,089 --> 00:13:16,530
world it's been known for a while in

00:13:14,010 --> 00:13:17,440
statistics in statistical software

00:13:16,530 --> 00:13:20,529
engineer

00:13:17,440 --> 00:13:24,700
it has not really completely caught up

00:13:20,529 --> 00:13:27,310
yet so how do you produce reproducible

00:13:24,700 --> 00:13:30,639
data data in a tidy form three simple

00:13:27,310 --> 00:13:32,800
rules whatever you measure whatever

00:13:30,639 --> 00:13:34,329
variables you measure of course you put

00:13:32,800 --> 00:13:38,529
your measurement results into some

00:13:34,329 --> 00:13:41,769
matrix each variable forms our column

00:13:38,529 --> 00:13:44,350
one single column rule number two each

00:13:41,769 --> 00:13:46,660
observation four columns are recurrent

00:13:44,350 --> 00:13:50,949
that way one column each observation

00:13:46,660 --> 00:13:53,290
forms one row and each type of

00:13:50,949 --> 00:13:55,510
observational unit forms a table that's

00:13:53,290 --> 00:14:05,740
my data base lingo linguae so we should

00:13:55,510 --> 00:14:09,250
model say eight forms is in one file so

00:14:05,740 --> 00:14:11,260
I said it's not so obvious that data

00:14:09,250 --> 00:14:14,199
should be measured in that form

00:14:11,260 --> 00:14:15,940
what you usually find is data in the

00:14:14,199 --> 00:14:17,860
messy form by definition everything

00:14:15,940 --> 00:14:19,899
that's not in tidy form is in messy form

00:14:17,860 --> 00:14:21,310
in a massive form could be something

00:14:19,899 --> 00:14:23,589
like this you're doing latency

00:14:21,310 --> 00:14:26,410
measurements on different systems on

00:14:23,589 --> 00:14:28,810
armed systems on x86 systems and you're

00:14:26,410 --> 00:14:31,510
measuring the latencies and the

00:14:28,810 --> 00:14:34,120
difference in areas and the low loads in

00:14:31,510 --> 00:14:35,560
the area under high loads in Aereo given

00:14:34,120 --> 00:14:41,170
some network load and so on and then a

00:14:35,560 --> 00:14:44,199
typical way how engineers to organize

00:14:41,170 --> 00:14:48,430
the data is that way so we have all the

00:14:44,199 --> 00:14:50,920
measurement results here and we are

00:14:48,430 --> 00:14:54,640
consecutively write down numbers for the

00:14:50,920 --> 00:14:57,070
measurements so that's not in the tidy

00:14:54,640 --> 00:14:59,649
form outlined before it's quite simple

00:14:57,070 --> 00:15:02,529
to bring it into tidy form when you do

00:14:59,649 --> 00:15:08,170
that you have a table that gets less

00:15:02,529 --> 00:15:11,740
white but much much taller so there as I

00:15:08,170 --> 00:15:15,389
said one of one variable that's measured

00:15:11,740 --> 00:15:18,910
forms one column and each observation

00:15:15,389 --> 00:15:20,680
that is taken forms a row so you see

00:15:18,910 --> 00:15:23,470
here the main observation that we take

00:15:20,680 --> 00:15:25,960
is about the value of the latency that

00:15:23,470 --> 00:15:29,470
we record

00:15:25,960 --> 00:15:31,270
there's only one single value in every

00:15:29,470 --> 00:15:34,690
row as compared to the Messi formant

00:15:31,270 --> 00:15:36,880
where we had multiple values in a single

00:15:34,690 --> 00:15:40,420
row multiple observations in a row that

00:15:36,880 --> 00:15:42,130
may seem like a total detail change when

00:15:40,420 --> 00:15:45,520
you see that for the first time but are

00:15:42,130 --> 00:15:48,520
the secret intention of this core of

00:15:45,520 --> 00:15:51,720
this talk of course is to convince you

00:15:48,520 --> 00:15:54,280
that languages like nu are very good to

00:15:51,720 --> 00:15:56,320
visualize and to analyze statistical

00:15:54,280 --> 00:15:59,920
data and we'll see some examples later

00:15:56,320 --> 00:16:02,740
on how just bringing data into this form

00:15:59,920 --> 00:16:06,850
into this tidy form essentially reduces

00:16:02,740 --> 00:16:09,430
plotting operations to one-liners who

00:16:06,850 --> 00:16:11,230
those of you who have worked with data

00:16:09,430 --> 00:16:14,650
and other form who have used extensive

00:16:11,230 --> 00:16:17,320
set scripts or scripts gnuplot scripts

00:16:14,650 --> 00:16:19,480
whatnot and know how much effort it

00:16:17,320 --> 00:16:21,700
often takes to massage data into a form

00:16:19,480 --> 00:16:23,800
that's presentable we'll realize that

00:16:21,700 --> 00:16:25,720
the others will just see that if you

00:16:23,800 --> 00:16:28,050
have data in that form you only need one

00:16:25,720 --> 00:16:30,730
liners if you have a proper statistical

00:16:28,050 --> 00:16:33,960
environment and of course a proper

00:16:30,730 --> 00:16:36,670
statistical environment is the our

00:16:33,960 --> 00:16:39,910
language of the our ecosystem in my

00:16:36,670 --> 00:16:42,340
opinion it's an open source it's one of

00:16:39,910 --> 00:16:44,860
the oldest open source codes basically

00:16:42,340 --> 00:16:47,860
it's astonishingly little used in the

00:16:44,860 --> 00:16:50,590
Linux community but it's the default a

00:16:47,860 --> 00:16:53,620
de-facto standard so to speak in the

00:16:50,590 --> 00:16:56,230
statistics world and all the examples

00:16:53,620 --> 00:16:58,780
that I will be showing are based on the

00:16:56,230 --> 00:17:01,450
our language so I promised a hands-on

00:16:58,780 --> 00:17:05,020
approach I'm not sure yet if I will

00:17:01,450 --> 00:17:06,670
really be so mad as to do a live example

00:17:05,020 --> 00:17:09,610
if time will permit that because live

00:17:06,670 --> 00:17:12,910
examples go wrong as all statisticians

00:17:09,610 --> 00:17:15,460
know with 99.9% probability so go see if

00:17:12,910 --> 00:17:17,110
there's some time left but I have all

00:17:15,460 --> 00:17:19,630
the are commands that you need to

00:17:17,110 --> 00:17:21,550
reproduce the plots I'm showing and the

00:17:19,630 --> 00:17:22,960
plotting mechanism that I'm using is

00:17:21,550 --> 00:17:25,570
also based on something that's very

00:17:22,960 --> 00:17:29,050
common in the statistical world but

00:17:25,570 --> 00:17:30,940
that's again astonishingly little used

00:17:29,050 --> 00:17:32,710
in the embedded Linux community and

00:17:30,940 --> 00:17:36,460
that's the grammar of graphics that's a

00:17:32,710 --> 00:17:39,100
language to describe not how to plot

00:17:36,460 --> 00:17:40,900
stuff like you do in a new plot or in

00:17:39,100 --> 00:17:42,610
and other such programs where you say I

00:17:40,900 --> 00:17:45,100
want this color for that I want this

00:17:42,610 --> 00:17:47,140
chart time for that but I really say how

00:17:45,100 --> 00:17:49,120
the data should be plotted and then the

00:17:47,140 --> 00:17:52,180
language takes care of the rest

00:17:49,120 --> 00:17:54,580
which also accounts for why the plotting

00:17:52,180 --> 00:17:57,900
examples I'll be showing will be so

00:17:54,580 --> 00:17:57,900
short and to the point

00:18:00,390 --> 00:18:08,230
good so regardless of the technical

00:18:06,550 --> 00:18:10,320
details how we do this statistical

00:18:08,230 --> 00:18:13,590
analysis there's basically three ways

00:18:10,320 --> 00:18:15,820
how to understand data it's first a

00:18:13,590 --> 00:18:20,770
descriptive analysis that's working with

00:18:15,820 --> 00:18:22,810
numerical summaries showings things like

00:18:20,770 --> 00:18:25,600
mean values or standard deviations and

00:18:22,810 --> 00:18:29,020
so on that you're all familiar from

00:18:25,600 --> 00:18:32,710
school the second level is exploratory

00:18:29,020 --> 00:18:35,170
or explorative exploratory analysis they

00:18:32,710 --> 00:18:37,780
use arm by use visualization techniques

00:18:35,170 --> 00:18:39,400
to get a better understanding about the

00:18:37,780 --> 00:18:41,980
data than is possible with simple

00:18:39,400 --> 00:18:44,470
numerical summaries and the third stage

00:18:41,980 --> 00:18:46,570
is confirmatory analysis where you

00:18:44,470 --> 00:18:50,080
really use statistical testing for most

00:18:46,570 --> 00:18:52,630
aristocratic to ascertain specific

00:18:50,080 --> 00:18:55,270
properties of your data I'm going to

00:18:52,630 --> 00:18:57,370
focus on point number two of course if

00:18:55,270 --> 00:19:01,120
you come from the old world from the

00:18:57,370 --> 00:19:02,770
world where where formal proofs are

00:19:01,120 --> 00:19:04,750
required where you doing things like

00:19:02,770 --> 00:19:07,450
schedule ability analysis and so on then

00:19:04,750 --> 00:19:09,370
you want this confirmatory analysis is

00:19:07,450 --> 00:19:11,650
final step but as I will be arguing in

00:19:09,370 --> 00:19:14,290
most cases in most cases it really

00:19:11,650 --> 00:19:17,410
doesn't add much value to what you see

00:19:14,290 --> 00:19:20,140
from the Explorer exploratory analysis

00:19:17,410 --> 00:19:21,480
but the exploratory analysis gives you

00:19:20,140 --> 00:19:24,280
quite a head start

00:19:21,480 --> 00:19:33,700
compared to simple descriptive analysis

00:19:24,280 --> 00:19:36,280
that we often see good yes it's time is

00:19:33,700 --> 00:19:38,920
flying I'm going to go over this slide

00:19:36,280 --> 00:19:43,660
very quickly when when you talk about a

00:19:38,920 --> 00:19:45,450
term it's also it's I guess clear that

00:19:43,660 --> 00:19:48,070
there's there's different type of data

00:19:45,450 --> 00:19:49,990
just to make to make sure we're speaking

00:19:48,070 --> 00:19:51,940
about the same things basically we can

00:19:49,990 --> 00:19:52,760
distinguish between two types of data

00:19:51,940 --> 00:19:57,520
category

00:19:52,760 --> 00:20:00,440
data and quantitative data you all know

00:19:57,520 --> 00:20:04,100
intuitively what this means categorical

00:20:00,440 --> 00:20:06,380
data is something like binary values say

00:20:04,100 --> 00:20:08,450
dead or alive or system is AB system is

00:20:06,380 --> 00:20:12,230
down system is broken system is

00:20:08,450 --> 00:20:13,850
operational we can drive that a little

00:20:12,230 --> 00:20:15,860
further for instance by extending it to

00:20:13,850 --> 00:20:17,690
colors like you have a color red blue

00:20:15,860 --> 00:20:19,370
green and so on these are of course

00:20:17,690 --> 00:20:21,470
different colors but it doesn't make

00:20:19,370 --> 00:20:23,510
sense to assign either numerical values

00:20:21,470 --> 00:20:26,180
to this color you cannot say blue must

00:20:23,510 --> 00:20:29,420
be one in red must be free likewise you

00:20:26,180 --> 00:20:31,400
cannot say a blue is bigger than red or

00:20:29,420 --> 00:20:34,310
red is better than green that doesn't

00:20:31,400 --> 00:20:37,070
make sense but there's a third category

00:20:34,310 --> 00:20:40,130
of other third type of categorical

00:20:37,070 --> 00:20:42,080
values namely ordinal values that cannot

00:20:40,130 --> 00:20:44,240
really be associated with numbers like

00:20:42,080 --> 00:20:46,490
in military ranks you cannot say a or

00:20:44,240 --> 00:20:49,910
maybe I'm pretty sure armies can can do

00:20:46,490 --> 00:20:53,270
that you can say a general is a 27 and

00:20:49,910 --> 00:20:54,950
the private is a 3 doesn't really make

00:20:53,270 --> 00:20:57,440
sense yet still you can order this you

00:20:54,950 --> 00:20:59,450
can say a general is higher up in the

00:20:57,440 --> 00:21:01,220
hierarchy than the private and the

00:20:59,450 --> 00:21:04,010
private this may be higher up in the

00:21:01,220 --> 00:21:06,680
hierarchy then I don't know what I'm

00:21:04,010 --> 00:21:11,750
pretty sure there's a lower rank good

00:21:06,680 --> 00:21:13,940
with this this type of type of numbers

00:21:11,750 --> 00:21:16,850
these types of numbers have appeared in

00:21:13,940 --> 00:21:21,350
the in the table before so here for

00:21:16,850 --> 00:21:23,870
instance system of course is type of

00:21:21,350 --> 00:21:25,670
system is is a categorical value that we

00:21:23,870 --> 00:21:28,400
cannot order we can and say a tegra is

00:21:25,670 --> 00:21:30,110
best better than a Raspberry Pi perhaps

00:21:28,400 --> 00:21:32,710
we can in some sense or another but not

00:21:30,110 --> 00:21:35,540
in a statistical sense and we have also

00:21:32,710 --> 00:21:36,620
numbers that are comparable like latency

00:21:35,540 --> 00:21:38,510
is the value that we are actually

00:21:36,620 --> 00:21:42,260
measuring that's a quantitative number

00:21:38,510 --> 00:21:44,000
and formally we are we differentiate

00:21:42,260 --> 00:21:46,280
between of course discrete values and

00:21:44,000 --> 00:21:51,530
continuous always but that's not so

00:21:46,280 --> 00:21:54,560
important for the rest of the talk good

00:21:51,530 --> 00:21:56,300
they are data sets that I'm considering

00:21:54,560 --> 00:21:58,670
that I'm going to play around a little

00:21:56,300 --> 00:22:01,220
bit with is what you typically get is a

00:21:58,670 --> 00:22:04,310
very simple data set and it's what you

00:22:01,220 --> 00:22:05,600
typically get from latency analysis like

00:22:04,310 --> 00:22:06,530
when you're doing when you're running a

00:22:05,600 --> 00:22:09,230
cyclic test

00:22:06,530 --> 00:22:11,660
priam party system so I've done that on

00:22:09,230 --> 00:22:13,940
a system with multiple CPUs I guess it

00:22:11,660 --> 00:22:17,630
had it had four CPUs that are numbering

00:22:13,940 --> 00:22:20,360
from zero to four I'm recording an

00:22:17,630 --> 00:22:22,400
identifier with every measure that I'm

00:22:20,360 --> 00:22:25,160
taking and I'm of course recording the

00:22:22,400 --> 00:22:27,380
measured value which is a latency that

00:22:25,160 --> 00:22:30,110
I'm observing in the real time system so

00:22:27,380 --> 00:22:33,320
very simple data set yet you will see

00:22:30,110 --> 00:22:35,270
that this data set already contains

00:22:33,320 --> 00:22:39,500
quite a lot of information that we can

00:22:35,270 --> 00:22:48,320
get out with some proper exploratory

00:22:39,500 --> 00:22:50,990
visual analysis the typical way how to

00:22:48,320 --> 00:22:55,460
plot such distributions is of course a

00:22:50,990 --> 00:22:57,950
histogram and that is I may have

00:22:55,460 --> 00:22:59,720
mentioned that R and the grammar of

00:22:57,950 --> 00:23:02,660
graphics is a very efficient plot

00:22:59,720 --> 00:23:05,110
mechanism that you can plot with one

00:23:02,660 --> 00:23:07,160
very simple line in R so you specify

00:23:05,110 --> 00:23:11,570
that you want to do a grammar of

00:23:07,160 --> 00:23:14,500
graphics plot using data from a specific

00:23:11,570 --> 00:23:18,170
variable and you say that the

00:23:14,500 --> 00:23:19,490
interesting the interesting that the

00:23:18,170 --> 00:23:24,050
measure that you want to look at is

00:23:19,490 --> 00:23:28,400
contained in latency that's that's a

00:23:24,050 --> 00:23:30,800
column in the data we want that doesn't

00:23:28,400 --> 00:23:33,790
say anything that about how we want the

00:23:30,800 --> 00:23:36,290
data to be plotted that just says what

00:23:33,790 --> 00:23:39,380
subset of what aspect of the data we'd

00:23:36,290 --> 00:23:41,300
like to plot you specify how it looks

00:23:39,380 --> 00:23:43,220
with so called geum's and here I

00:23:41,300 --> 00:23:46,160
specified that I want the histogram and

00:23:43,220 --> 00:23:50,060
yeah well miracles of modern technology

00:23:46,160 --> 00:23:50,870
I do indeed get a histogram that you've

00:23:50,060 --> 00:23:55,400
all seen before

00:23:50,870 --> 00:23:57,940
now that's seems already seems to be a

00:23:55,400 --> 00:24:01,270
point where like 50 percent of

00:23:57,940 --> 00:24:03,860
programmers I happy with but of course I

00:24:01,270 --> 00:24:08,030
mean back to the statisticians that the

00:24:03,860 --> 00:24:09,950
station's know that this is not a that

00:24:08,030 --> 00:24:14,420
this method requires some parameter to

00:24:09,950 --> 00:24:17,030
be chosen namely how wide the individual

00:24:14,420 --> 00:24:19,310
bars should be in that example the

00:24:17,030 --> 00:24:20,540
system cannot know what we want to

00:24:19,310 --> 00:24:22,430
convey what the

00:24:20,540 --> 00:24:25,160
measurement resolution is for instance

00:24:22,430 --> 00:24:28,460
it has chosen a fairly fairly large

00:24:25,160 --> 00:24:30,830
histogram size so there that's one

00:24:28,460 --> 00:24:32,630
that's one of the occasions when you

00:24:30,830 --> 00:24:35,330
need to manually tell the computer

00:24:32,630 --> 00:24:37,130
system something about the data that

00:24:35,330 --> 00:24:40,400
they can better visualize it and here

00:24:37,130 --> 00:24:42,170
I've given an explicit parameter to the

00:24:40,400 --> 00:24:44,120
plotting system that specifies the bin

00:24:42,170 --> 00:24:46,580
width I've measured with a resolution of

00:24:44,120 --> 00:24:49,130
one microsecond and you see if I make

00:24:46,580 --> 00:24:54,070
the bins only one microsecond wide then

00:24:49,130 --> 00:24:56,870
I already get more details into my graph

00:24:54,070 --> 00:24:58,340
of course I've not measured on a single

00:24:56,870 --> 00:25:00,080
core system that's boring single core

00:24:58,340 --> 00:25:01,520
systems are not available anymore these

00:25:00,080 --> 00:25:03,590
days I've measured on a multi-core

00:25:01,520 --> 00:25:06,920
system with four CPUs and I would in

00:25:03,590 --> 00:25:09,410
some way or another like to see how the

00:25:06,920 --> 00:25:12,500
different CPUs in the systems compare to

00:25:09,410 --> 00:25:14,510
each other that's very simple in the

00:25:12,500 --> 00:25:16,250
grammar of graphics or the elements to

00:25:14,510 --> 00:25:20,330
see another aspect of the data that

00:25:16,250 --> 00:25:23,830
aspect is another column in the tiny

00:25:20,330 --> 00:25:28,460
data if you recall that so that's this

00:25:23,830 --> 00:25:30,950
column and of course we've already spent

00:25:28,460 --> 00:25:32,870
our two-dimensional diagram on showing

00:25:30,950 --> 00:25:35,930
the histogram so we need to add another

00:25:32,870 --> 00:25:38,090
dimension to the graph which can for

00:25:35,930 --> 00:25:41,270
instance bacala that i've specified here

00:25:38,090 --> 00:25:43,400
i've said do not just take the latency

00:25:41,270 --> 00:25:47,900
aspect from the data also to consider

00:25:43,400 --> 00:25:52,160
the cpu measurements and adapt the fill

00:25:47,900 --> 00:25:58,970
of the after histogram to the cpu so i

00:25:52,160 --> 00:26:00,680
chooses for ugly colors that then gives

00:25:58,970 --> 00:26:04,610
some extra information about how the

00:26:00,680 --> 00:26:06,710
cpus did that's still not the very good

00:26:04,610 --> 00:26:08,360
ordering so we see here we can we can

00:26:06,710 --> 00:26:11,450
see the shape of the distribution for

00:26:08,360 --> 00:26:13,820
cpu 0 the red cpu but the other cpus are

00:26:11,450 --> 00:26:16,190
just stacked on top of this measurement

00:26:13,820 --> 00:26:20,900
for cpu 0 which gives us reproduce is

00:26:16,190 --> 00:26:22,760
the system global so to speak latency

00:26:20,900 --> 00:26:24,530
measurement so you see this is just a

00:26:22,760 --> 00:26:27,350
colored version of this but what we

00:26:24,530 --> 00:26:33,050
actually like to have is a visualization

00:26:27,350 --> 00:26:34,140
by each cpu and that we can do in some

00:26:33,050 --> 00:26:36,000
ways in the grammar of

00:26:34,140 --> 00:26:38,010
if I could for instance specify that I

00:26:36,000 --> 00:26:39,720
do not want to stack the measured

00:26:38,010 --> 00:26:41,250
results I want to dodge them next to

00:26:39,720 --> 00:26:45,480
each other so we get something like this

00:26:41,250 --> 00:26:51,020
and then you see you see you better see

00:26:45,480 --> 00:26:51,020
the individual recordings for the CPUs a

00:26:51,230 --> 00:26:57,810
slightly preferable way in my opinion to

00:26:53,970 --> 00:27:01,560
deal with the second the second aspect

00:26:57,810 --> 00:27:03,750
to the data is to facet the data into

00:27:01,560 --> 00:27:05,760
different subplots anyone who has done

00:27:03,750 --> 00:27:09,530
that with the mechanism like bootlid

00:27:05,760 --> 00:27:11,430
knows that this can be very painful

00:27:09,530 --> 00:27:13,200
depending on how your plot with the

00:27:11,430 --> 00:27:15,570
grammar of graphics it's really easy I

00:27:13,200 --> 00:27:20,490
tell I want I'd tell the system I want

00:27:15,570 --> 00:27:23,730
to facet my data by the CPU by the

00:27:20,490 --> 00:27:25,620
measured variable CPU and then I get the

00:27:23,730 --> 00:27:28,680
same plot that I specified here in the

00:27:25,620 --> 00:27:31,290
genome repeat it for each value for each

00:27:28,680 --> 00:27:34,800
distinct value of CPU that's present in

00:27:31,290 --> 00:27:37,170
the data and so we get phone ice plots

00:27:34,800 --> 00:27:45,960
that immediately give more clarity to

00:27:37,170 --> 00:27:48,690
what we've measured good those of you

00:27:45,960 --> 00:27:51,000
who do real time work may have realized

00:27:48,690 --> 00:27:54,270
in this graph that this is kind of

00:27:51,000 --> 00:27:56,480
strange that we don't have any values

00:27:54,270 --> 00:27:59,270
here so that we have a very sharp

00:27:56,480 --> 00:28:01,590
initial peak and that we also don't have

00:27:59,270 --> 00:28:04,080
substantial values here and that's a

00:28:01,590 --> 00:28:06,150
problem that often that often occurs in

00:28:04,080 --> 00:28:08,010
box plots that you see from from

00:28:06,150 --> 00:28:09,990
analysis that's because people tend to

00:28:08,010 --> 00:28:12,630
measure for very long time so I didn't I

00:28:09,990 --> 00:28:16,830
didn't even measure to long it's just

00:28:12,630 --> 00:28:18,300
like 60 thousands data points in one bin

00:28:16,830 --> 00:28:23,520
that's not too much but that's already

00:28:18,300 --> 00:28:26,520
way too much for the visual presentation

00:28:23,520 --> 00:28:29,880
to make some details visible invisible

00:28:26,520 --> 00:28:33,870
sorry that are in the data and that can

00:28:29,880 --> 00:28:36,030
be fixed as you all know by adapting or

00:28:33,870 --> 00:28:37,530
by by transforming this axis here one

00:28:36,030 --> 00:28:39,300
typical transformation is a lock

00:28:37,530 --> 00:28:42,300
transformation that you are all familiar

00:28:39,300 --> 00:28:44,220
with from school I guess that's not the

00:28:42,300 --> 00:28:46,160
most clever transformation for data that

00:28:44,220 --> 00:28:47,940
contains zeros because you know that

00:28:46,160 --> 00:28:50,870
taking the logarithm of

00:28:47,940 --> 00:28:52,950
zero is some more challenging so a

00:28:50,870 --> 00:28:55,230
transformation that that is

00:28:52,950 --> 00:28:57,090
recommendable or that works well in this

00:28:55,230 --> 00:28:59,850
case is a square root transformation it

00:28:57,090 --> 00:29:03,840
has no problems with zero occurrences

00:28:59,850 --> 00:29:06,240
but still give some more attention to

00:29:03,840 --> 00:29:08,580
the small values of the data and that in

00:29:06,240 --> 00:29:11,610
fact uncovers some features but they're

00:29:08,580 --> 00:29:13,950
simply not visible in the presentation

00:29:11,610 --> 00:29:16,830
before we have very small latencies

00:29:13,950 --> 00:29:19,409
before the big spike hits and actually

00:29:16,830 --> 00:29:22,830
here we have quite some tail that's

00:29:19,409 --> 00:29:24,929
actually not ending at 75 microseconds

00:29:22,830 --> 00:29:27,149
but I've cut it off here that of course

00:29:24,929 --> 00:29:31,289
is important if you do not just care

00:29:27,149 --> 00:29:32,850
above about the the maximum latency but

00:29:31,289 --> 00:29:35,580
if you really care about temporal

00:29:32,850 --> 00:29:38,070
precision and then you need to somehow

00:29:35,580 --> 00:29:40,019
deal with these early results that don't

00:29:38,070 --> 00:29:42,389
appear very frequently but that do

00:29:40,019 --> 00:29:51,929
appear and that should be visible in the

00:29:42,389 --> 00:29:54,330
way you present data so I had to make

00:29:51,929 --> 00:29:57,179
one choice when I plotted the data one

00:29:54,330 --> 00:30:00,570
manual choice and that was choosing the

00:29:57,179 --> 00:30:02,399
bin width that the system uses to

00:30:00,570 --> 00:30:05,070
present the data and making a manual

00:30:02,399 --> 00:30:08,100
choice is of course bad so let me show

00:30:05,070 --> 00:30:11,669
you another method that can be used to

00:30:08,100 --> 00:30:14,389
visualize such latency data that does

00:30:11,669 --> 00:30:16,289
not need arm that doesn't need any

00:30:14,389 --> 00:30:18,480
presentational choice it on a so-called

00:30:16,289 --> 00:30:20,879
nonparametric method that's the

00:30:18,480 --> 00:30:23,190
cumulative distribution function that's

00:30:20,879 --> 00:30:25,500
shown here this graph contains the very

00:30:23,190 --> 00:30:28,529
same information is this graph in a

00:30:25,500 --> 00:30:31,289
slightly different form so we write down

00:30:28,529 --> 00:30:34,139
the latencies on the x-axis and woods

00:30:31,289 --> 00:30:36,779
then contained in the y-axis a fraction

00:30:34,139 --> 00:30:41,610
from 0 to 1 is how many of the data

00:30:36,779 --> 00:30:43,860
points belong to the range up to 15

00:30:41,610 --> 00:30:47,009
microseconds up to 100 microseconds up

00:30:43,860 --> 00:30:51,360
250 microseconds and so on so you see if

00:30:47,009 --> 00:30:55,379
the data span latencies from 0 to about

00:30:51,360 --> 00:30:57,659
200 microseconds at 200 microseconds we

00:30:55,379 --> 00:31:00,490
have covered a hundred percent of all

00:30:57,659 --> 00:31:02,410
the observed values

00:31:00,490 --> 00:31:05,530
and that smaller latencies we have

00:31:02,410 --> 00:31:07,690
covered smaller fractions of the data

00:31:05,530 --> 00:31:09,750
but we didn't need we didn't need to

00:31:07,690 --> 00:31:13,120
choose any width of the bin or something

00:31:09,750 --> 00:31:16,360
some other manual parameter this always

00:31:13,120 --> 00:31:17,710
looks this so this always this always

00:31:16,360 --> 00:31:21,570
can be produced without human

00:31:17,710 --> 00:31:24,730
intervention it of course requires some

00:31:21,570 --> 00:31:26,230
some familiarity to be interpreted

00:31:24,730 --> 00:31:28,300
directly but when you look at it it's

00:31:26,230 --> 00:31:32,170
not so hard to see here the first

00:31:28,300 --> 00:31:34,390
increase is around we are 55 or so

00:31:32,170 --> 00:31:38,520
microsecond that's of course corresponds

00:31:34,390 --> 00:31:42,160
to this spike here then we have another

00:31:38,520 --> 00:31:45,730
sharp increase here that's about here so

00:31:42,160 --> 00:31:47,470
this is the second spike this on this

00:31:45,730 --> 00:31:49,690
increase in the graph corresponds to

00:31:47,470 --> 00:31:51,610
this guy here around hundred

00:31:49,690 --> 00:31:54,160
microseconds and the atom basically

00:31:51,610 --> 00:32:02,290
flattens out and we covered the majority

00:31:54,160 --> 00:32:05,410
of all values good okay I guess I still

00:32:02,290 --> 00:32:10,810
I still have too many slides even after

00:32:05,410 --> 00:32:13,150
reducing the amount of slides let me let

00:32:10,810 --> 00:32:16,810
me go over that quickly I said in the

00:32:13,150 --> 00:32:18,730
beginning that summary statistics the

00:32:16,810 --> 00:32:21,610
miracle summary statistics are quite

00:32:18,730 --> 00:32:25,510
often fairly inaccurate way of

00:32:21,610 --> 00:32:28,450
describing data yet still you see you

00:32:25,510 --> 00:32:31,090
see in you observe quite often that data

00:32:28,450 --> 00:32:33,370
like scheduling latencies is described

00:32:31,090 --> 00:32:36,430
by two simple numerical summaries namely

00:32:33,370 --> 00:32:39,820
mean value and standard deviation that

00:32:36,430 --> 00:32:41,140
of course comes or that maybe comes from

00:32:39,820 --> 00:32:43,000
the fact that we were all taught in

00:32:41,140 --> 00:32:45,280
school that everything in statistics is

00:32:43,000 --> 00:32:47,350
a Gaussian process some way or another

00:32:45,280 --> 00:32:49,390
and as you all know you can describe a

00:32:47,350 --> 00:32:53,800
Gaussian distribution by exactly these

00:32:49,390 --> 00:32:55,360
two these two summary values mean value

00:32:53,800 --> 00:32:57,300
and standard deviation that suffices to

00:32:55,360 --> 00:32:59,950
reconstruct the whole Gaussian

00:32:57,300 --> 00:33:01,870
unfortunately this whole statement about

00:32:59,950 --> 00:33:03,670
everything in nature being Gaussian is

00:33:01,870 --> 00:33:05,650
may be true for nature in the sense of

00:33:03,670 --> 00:33:08,740
physics or for nature in the sense of

00:33:05,650 --> 00:33:10,600
biology it's unfortunately not true for

00:33:08,740 --> 00:33:13,090
nature in the sense of computer science

00:33:10,600 --> 00:33:16,210
so here you very rarely

00:33:13,090 --> 00:33:18,670
I get to work with with Gaussian

00:33:16,210 --> 00:33:21,340
distributions mean it's immediately

00:33:18,670 --> 00:33:26,140
obvious from a visual inspection of the

00:33:21,340 --> 00:33:28,630
latency later actually I've constructed

00:33:26,140 --> 00:33:31,180
this this Gaussian function here let me

00:33:28,630 --> 00:33:32,860
switch to the switch to the square root

00:33:31,180 --> 00:33:34,780
access scale so that we see the details

00:33:32,860 --> 00:33:37,810
I've constructed this Gaussian

00:33:34,780 --> 00:33:40,390
distribution from these measured values

00:33:37,810 --> 00:33:41,950
I've taken the mean value from the

00:33:40,390 --> 00:33:44,020
measured value of taking the standard

00:33:41,950 --> 00:33:46,120
deviation from the measured value and

00:33:44,020 --> 00:33:48,580
then computed this Gaussian function and

00:33:46,120 --> 00:33:50,260
as I guess is clearly obvious from these

00:33:48,580 --> 00:33:52,330
two graphs these are two very very

00:33:50,260 --> 00:33:57,580
different distributions the Gaussian is

00:33:52,330 --> 00:33:59,050
much wider it's it's less tall and this

00:33:57,580 --> 00:34:01,540
graph should make it clear to you that

00:33:59,050 --> 00:34:03,880
just using these two summary values is

00:34:01,540 --> 00:34:15,310
really not sufficient to describe data

00:34:03,880 --> 00:34:17,290
with some structure good um yeah as an

00:34:15,310 --> 00:34:20,410
example as an example of how to reuse

00:34:17,290 --> 00:34:23,080
such statistics I'm showing the

00:34:20,410 --> 00:34:25,660
corresponding grammar of graph commands

00:34:23,080 --> 00:34:27,940
to produce the cumulative distribution

00:34:25,660 --> 00:34:29,920
function of the empirical cumulative

00:34:27,940 --> 00:34:32,020
distribution function because it's based

00:34:29,920 --> 00:34:35,140
on unmeasured values again it's a very

00:34:32,020 --> 00:34:36,880
very short statement so I'm telling the

00:34:35,140 --> 00:34:39,370
system I want to use some data where I'm

00:34:36,880 --> 00:34:43,300
interested in analyzing the latency now

00:34:39,370 --> 00:34:45,070
if I have two different statistical

00:34:43,300 --> 00:34:48,720
distributions the Gaussian and the

00:34:45,070 --> 00:34:50,950
proper measured distribution that I'm

00:34:48,720 --> 00:34:53,320
denominating by type I'm using a color

00:34:50,950 --> 00:34:56,880
to distinguish between the Gaussian and

00:34:53,320 --> 00:34:59,380
the measured distribution and I want a

00:34:56,880 --> 00:35:02,140
cumulative distribution function that

00:34:59,380 --> 00:35:04,230
simple command line gives me that graph

00:35:02,140 --> 00:35:07,450
so here from this graph again it's

00:35:04,230 --> 00:35:13,120
pretty obvious that the distributions

00:35:07,450 --> 00:35:15,730
are not identical good so playing around

00:35:13,120 --> 00:35:18,490
with this simple data set has already

00:35:15,730 --> 00:35:21,160
consumed quite a lot of time that is

00:35:18,490 --> 00:35:24,280
allocated to this talk but still I

00:35:21,160 --> 00:35:26,980
really wanted to go over that in a very

00:35:24,280 --> 00:35:30,369
great detail because already

00:35:26,980 --> 00:35:32,200
plotting the data is all too often done

00:35:30,369 --> 00:35:34,270
in a way that doesn't really do justice

00:35:32,200 --> 00:35:36,220
to the data despite the fact that as I

00:35:34,270 --> 00:35:37,630
have shown you with the aura commands

00:35:36,220 --> 00:35:43,809
you could do it with very little effort

00:35:37,630 --> 00:35:46,420
in a very very precise and a banana so

00:35:43,809 --> 00:35:48,970
what I've done basically in this last

00:35:46,420 --> 00:35:50,319
slide is already starting the next topic

00:35:48,970 --> 00:35:53,680
that I would like to discuss that's

00:35:50,319 --> 00:35:55,660
about comparing data sets so this

00:35:53,680 --> 00:35:57,700
problem that's the second standard

00:35:55,660 --> 00:36:01,750
problem that occurs in these statistical

00:35:57,700 --> 00:36:03,210
analysis of systems so it occurs for

00:36:01,750 --> 00:36:05,589
instance when you want to track

00:36:03,210 --> 00:36:06,970
behavioral changes of the system after

00:36:05,589 --> 00:36:09,760
you're doing an update say you're

00:36:06,970 --> 00:36:11,770
updating from kernel four point

00:36:09,760 --> 00:36:13,500
something to four point something plus X

00:36:11,770 --> 00:36:15,640
and want to observe are there any

00:36:13,500 --> 00:36:17,950
performance regressions then you take a

00:36:15,640 --> 00:36:19,390
measurement set from the old kernel or

00:36:17,950 --> 00:36:21,339
measurements that from the new kernel

00:36:19,390 --> 00:36:24,910
and somehow need to decide by these

00:36:21,339 --> 00:36:27,069
measurements and by going beyond the the

00:36:24,910 --> 00:36:29,109
mean value and the standard deviation if

00:36:27,069 --> 00:36:30,940
the system's gotten any better if the

00:36:29,109 --> 00:36:32,230
system's gotten any worse if the system

00:36:30,940 --> 00:36:35,530
hasn't changed so I need to prove that

00:36:32,230 --> 00:36:37,930
of course your customer you want you may

00:36:35,530 --> 00:36:39,280
want to use that to evaluate alternative

00:36:37,930 --> 00:36:40,900
choices that you have when you use

00:36:39,280 --> 00:36:43,990
libraries and so on which one performs

00:36:40,900 --> 00:36:47,940
better and there's also lots of lots of

00:36:43,990 --> 00:36:51,609
other use cases to do that as we've seen

00:36:47,940 --> 00:36:53,500
there's one way one way to do that is by

00:36:51,609 --> 00:36:55,540
visual inspection or by explorative

00:36:53,500 --> 00:36:58,480
analysis that's if you do that properly

00:36:55,540 --> 00:37:01,109
very apt for the purpose comparing

00:36:58,480 --> 00:37:04,990
summaries as I have outlined now for the

00:37:01,109 --> 00:37:06,280
27th time I guess is not really a

00:37:04,990 --> 00:37:08,020
sufficient way to do that so just

00:37:06,280 --> 00:37:09,940
comparing mean value and standard

00:37:08,020 --> 00:37:13,829
deviation don't do that don't turn to

00:37:09,940 --> 00:37:15,849
that visual exploration is as simple as

00:37:13,829 --> 00:37:18,549
computing summaries but much more

00:37:15,849 --> 00:37:21,730
effective and there's also some formal

00:37:18,549 --> 00:37:24,940
methods and tests that you can employ

00:37:21,730 --> 00:37:28,180
that can make sense that's why it's

00:37:24,940 --> 00:37:29,790
partly yes but usually in most cases it

00:37:28,180 --> 00:37:32,319
doesn't give you any extra information

00:37:29,790 --> 00:37:35,230
that you get from the visual inspection

00:37:32,319 --> 00:37:37,630
are besides the joy that you can say oh

00:37:35,230 --> 00:37:40,290
I've used some formal method like a

00:37:37,630 --> 00:37:42,480
t-test or wilcoxon rank-sum test or and

00:37:40,290 --> 00:37:56,940
thing else that sounds fancy and that

00:37:42,480 --> 00:38:04,050
may impress your customers as I guess

00:37:56,940 --> 00:38:05,820
I've with the so with the with the with

00:38:04,050 --> 00:38:07,950
this graph type with the community with

00:38:05,820 --> 00:38:09,450
the cumulative distribution function

00:38:07,950 --> 00:38:12,990
I've already given you the appropriate

00:38:09,450 --> 00:38:14,880
command line that you may want to use to

00:38:12,990 --> 00:38:18,180
compare such distributions so I'm not

00:38:14,880 --> 00:38:21,030
going to get into any further details

00:38:18,180 --> 00:38:22,920
with the three examples that I would

00:38:21,030 --> 00:38:25,050
have prepared for this purpose which

00:38:22,920 --> 00:38:27,870
also means I'm going to skip the live

00:38:25,050 --> 00:38:31,320
demo because I need the minutes that I

00:38:27,870 --> 00:38:33,150
have left for for some other stuff which

00:38:31,320 --> 00:38:34,800
at least make sure that the live demo

00:38:33,150 --> 00:38:39,120
doesn't fail you just don't do it then

00:38:34,800 --> 00:38:42,180
it doesn't fail let me end a let me end

00:38:39,120 --> 00:38:44,610
this part of my considerations with what

00:38:42,180 --> 00:38:48,170
can go wrong when you're doing this this

00:38:44,610 --> 00:38:50,760
elementary analysis of statistical data

00:38:48,170 --> 00:38:53,640
like scheduling data like performance

00:38:50,760 --> 00:38:56,940
data and so on and again all the points

00:38:53,640 --> 00:39:00,630
I'm making here do really sound

00:38:56,940 --> 00:39:02,300
extremely obvious but if you look at hi

00:39:00,630 --> 00:39:05,040
Stan in the real world than these

00:39:02,300 --> 00:39:08,130
extremely simple points or these these

00:39:05,040 --> 00:39:14,690
extremely easy to make mistakes are

00:39:08,130 --> 00:39:17,100
being made very often so the from the

00:39:14,690 --> 00:39:18,870
from the list I'm going to pick free of

00:39:17,100 --> 00:39:20,070
course use inappropriate summary

00:39:18,870 --> 00:39:22,920
statistics and may have already

00:39:20,070 --> 00:39:24,870
mentioned that use wrong inadequate bin

00:39:22,920 --> 00:39:26,430
sizes that's the second most often

00:39:24,870 --> 00:39:28,680
occurring thing that you see so if

00:39:26,430 --> 00:39:30,060
you're using a pyramid a parametric

00:39:28,680 --> 00:39:32,880
method make sure that you choose your

00:39:30,060 --> 00:39:36,470
parameter parameters correctly or use a

00:39:32,880 --> 00:39:40,280
nonparametric method like the CDF and

00:39:36,470 --> 00:39:42,990
what also happens quite often is that

00:39:40,280 --> 00:39:45,360
people don't specify the sample sizes

00:39:42,990 --> 00:39:47,570
and basically with our sample sizes it's

00:39:45,360 --> 00:39:50,280
impossible I didn't I didn't go into the

00:39:47,570 --> 00:39:52,140
formal examples that would make use of

00:39:50,280 --> 00:39:54,180
this information but if you don't

00:39:52,140 --> 00:39:56,520
specify your sample size

00:39:54,180 --> 00:39:59,340
and basically you don't know if if

00:39:56,520 --> 00:40:01,770
specific statistical tests will work

00:39:59,340 --> 00:40:03,780
I've said that data in computer science

00:40:01,770 --> 00:40:07,200
are usually not Gaussian distributed

00:40:03,780 --> 00:40:08,730
many statistical tests rely on the fact

00:40:07,200 --> 00:40:12,000
that the data are Gaussian distributed

00:40:08,730 --> 00:40:14,430
except if you have a whole lot of data

00:40:12,000 --> 00:40:16,800
statisticians please close your eyes but

00:40:14,430 --> 00:40:18,510
if you just have if you have millions

00:40:16,800 --> 00:40:20,520
and millions of data points then it

00:40:18,510 --> 00:40:22,230
doesn't really matter if the data are

00:40:20,520 --> 00:40:24,630
Gaussian distributed or not most

00:40:22,230 --> 00:40:27,720
statistical tests will work fairly well

00:40:24,630 --> 00:40:30,150
regardless of the non-normality of the

00:40:27,720 --> 00:40:32,730
data but for that to know you of course

00:40:30,150 --> 00:40:34,500
need to know the sample size and if you

00:40:32,730 --> 00:40:40,400
want to if you need to know the sample

00:40:34,500 --> 00:40:45,300
size needs to be reported good coming

00:40:40,400 --> 00:40:47,310
coming to the final part of the talk

00:40:45,300 --> 00:40:51,540
with only 15 or so slides left for the

00:40:47,310 --> 00:40:53,580
next 10 minutes is one thing that people

00:40:51,540 --> 00:40:55,380
are trying more and more often these

00:40:53,580 --> 00:40:58,170
days and that's to make not just to

00:40:55,380 --> 00:40:59,910
describe data and to learn about the

00:40:58,170 --> 00:41:02,430
system behavior from data but to make

00:40:59,910 --> 00:41:04,530
predictions from measured data how a

00:41:02,430 --> 00:41:06,870
system will behave in the future how a

00:41:04,530 --> 00:41:10,620
system will behave in corner cases that

00:41:06,870 --> 00:41:14,310
have not been explored yet or using

00:41:10,620 --> 00:41:16,260
predictions to to satisfy some

00:41:14,310 --> 00:41:17,730
certification authorities that the

00:41:16,260 --> 00:41:21,170
system they've built is really

00:41:17,730 --> 00:41:25,050
satisfying the criteria that they demand

00:41:21,170 --> 00:41:27,840
so making predictions from statistics is

00:41:25,050 --> 00:41:30,420
essentially very simple process it's two

00:41:27,840 --> 00:41:34,260
easy steps you find a mathematical model

00:41:30,420 --> 00:41:36,060
that describes the data and then you

00:41:34,260 --> 00:41:38,280
just extend the model outside the

00:41:36,060 --> 00:41:40,350
current measured range and that's all

00:41:38,280 --> 00:41:43,170
you need there you go you can predict

00:41:40,350 --> 00:41:44,970
the future of course there are some some

00:41:43,170 --> 00:41:46,770
detail problems like is there such a

00:41:44,970 --> 00:41:48,900
mathematical model that describes the

00:41:46,770 --> 00:41:51,000
system if I have found a model that's it

00:41:48,900 --> 00:41:53,850
really in fact describe my system or

00:41:51,000 --> 00:41:56,600
does it just describe what I want but I

00:41:53,850 --> 00:42:00,710
would like the system to be and so on

00:41:56,600 --> 00:42:03,060
and when you look at how that math

00:42:00,710 --> 00:42:05,220
modeling techniques and prediction

00:42:03,060 --> 00:42:06,880
techniques are currently used in our

00:42:05,220 --> 00:42:08,859
field then you really need

00:42:06,880 --> 00:42:11,710
to be aware of the first rule of

00:42:08,859 --> 00:42:14,380
predictions when you look at statements

00:42:11,710 --> 00:42:16,900
that are made in that respect if things

00:42:14,380 --> 00:42:19,690
sound too good to be true they probably

00:42:16,900 --> 00:42:22,690
are not true so there's lots lots of

00:42:19,690 --> 00:42:24,849
lots of ideas these days that claim that

00:42:22,690 --> 00:42:26,799
you can measure this and that aspect of

00:42:24,849 --> 00:42:29,470
a development process and then prove

00:42:26,799 --> 00:42:32,019
that the system is completely apt for

00:42:29,470 --> 00:42:33,700
for safety-critical deployments and so

00:42:32,019 --> 00:42:36,009
on that you can measure for five minutes

00:42:33,700 --> 00:42:38,109
and then you know that the system will

00:42:36,009 --> 00:42:39,579
satisfy all latency requirements during

00:42:38,109 --> 00:42:45,519
the next ten years or so sounds very

00:42:39,579 --> 00:42:48,130
good but is likely not true how do you

00:42:45,519 --> 00:42:50,380
how do you go about when you model when

00:42:48,130 --> 00:42:52,779
you find a metric mathematical model for

00:42:50,380 --> 00:42:56,289
your data that's of course a problem

00:42:52,779 --> 00:42:59,829
that has been considered for a couple of

00:42:56,289 --> 00:43:02,400
decades and I suppose that many of you

00:42:59,829 --> 00:43:04,900
will have heard about linear regression

00:43:02,400 --> 00:43:07,539
right so most of you have okay great so

00:43:04,900 --> 00:43:09,730
I can be quick about that I'm not

00:43:07,539 --> 00:43:11,799
showing any real data set here I'm just

00:43:09,730 --> 00:43:14,980
I'm showing some made-up dataset that

00:43:11,799 --> 00:43:17,710
connects kilo Idriss Hills whatever that

00:43:14,980 --> 00:43:20,140
is to the number of unicorns and we want

00:43:17,710 --> 00:43:22,450
to find a functional relationship

00:43:20,140 --> 00:43:24,369
between the amount of kilo you draw

00:43:22,450 --> 00:43:27,759
skills I have with the amount of

00:43:24,369 --> 00:43:29,640
unicorns I get again from a visual

00:43:27,759 --> 00:43:32,170
inspection of the data I've done a

00:43:29,640 --> 00:43:34,150
scatter plot of the available measured

00:43:32,170 --> 00:43:37,450
data it seems quite clear that there is

00:43:34,150 --> 00:43:40,059
such a relationship the more you draw

00:43:37,450 --> 00:43:42,490
skills and more unicorns as things are

00:43:40,059 --> 00:43:43,930
in life question is just how can we

00:43:42,490 --> 00:43:45,609
mathematically ascertain this

00:43:43,930 --> 00:43:48,819
relationship and how can we find the

00:43:45,609 --> 00:43:51,609
best possible slope for the for the

00:43:48,819 --> 00:43:54,640
linear relationship how you do that is

00:43:51,609 --> 00:43:56,170
of course you formulate a model a guess

00:43:54,640 --> 00:43:58,440
so I said I'm guessing that there's a

00:43:56,170 --> 00:44:02,170
linear relationship the number of

00:43:58,440 --> 00:44:04,059
unicorns Y is related in this fashion in

00:44:02,170 --> 00:44:06,160
a linear fashion to the number of kilo

00:44:04,059 --> 00:44:09,279
addresses so we have some awesome

00:44:06,160 --> 00:44:12,279
intercept term that shifts this line up

00:44:09,279 --> 00:44:14,950
and down and we have a slope how steep

00:44:12,279 --> 00:44:16,990
the line is the statistical task at hand

00:44:14,950 --> 00:44:20,119
now is of course to estimate the

00:44:16,990 --> 00:44:23,269
coefficients beta 0 beta 1

00:44:20,119 --> 00:44:26,420
slope and intercept and finding out how

00:44:23,269 --> 00:44:30,670
my errors are distributed the beauty

00:44:26,420 --> 00:44:34,189
about that it's a very simple model that

00:44:30,670 --> 00:44:36,349
describes quite quite a lot of processes

00:44:34,189 --> 00:44:38,420
that we are seeing very accurately the

00:44:36,349 --> 00:44:40,789
bad thing about that is you can apply

00:44:38,420 --> 00:44:43,130
linear regression to everything and you

00:44:40,789 --> 00:44:46,969
will always get the result and it will

00:44:43,130 --> 00:44:49,339
always look halfway decent but in most

00:44:46,969 --> 00:44:52,400
cases it will describe the data very

00:44:49,339 --> 00:44:56,599
inaccurately and especially you won't be

00:44:52,400 --> 00:44:58,880
able to draw any to draw any to make any

00:44:56,599 --> 00:45:07,339
predictions from such misspecified

00:44:58,880 --> 00:45:08,930
models so I just realize I haven't shown

00:45:07,339 --> 00:45:10,819
I haven't shown the clue or commands

00:45:08,930 --> 00:45:13,009
that are necessary to produce such a

00:45:10,819 --> 00:45:14,900
model so another one-liner I will

00:45:13,009 --> 00:45:17,809
correct this deficiency in the published

00:45:14,900 --> 00:45:20,150
version of the slides for now let me

00:45:17,809 --> 00:45:23,359
just show you the results the data set

00:45:20,150 --> 00:45:25,699
and the line that describes the

00:45:23,359 --> 00:45:29,179
functional relationship and again

00:45:25,699 --> 00:45:31,670
without going into any details let me

00:45:29,179 --> 00:45:34,900
get the warning that or let me let me

00:45:31,670 --> 00:45:38,119
say that there are very specific

00:45:34,900 --> 00:45:39,229
mathematical tests after you've done the

00:45:38,119 --> 00:45:41,479
model that you need to apply to

00:45:39,229 --> 00:45:45,829
ascertain that the model actually does

00:45:41,479 --> 00:45:50,209
does fit your data nobody ever does that

00:45:45,829 --> 00:45:52,279
of course but it's it's actually a very

00:45:50,209 --> 00:45:54,979
very simple thing to do you need to test

00:45:52,279 --> 00:45:56,630
for basically three for basically four

00:45:54,979 --> 00:45:59,119
you need to test for assumptions that

00:45:56,630 --> 00:46:01,789
you are implicitly making when you come

00:45:59,119 --> 00:46:03,949
up with such models and that is first

00:46:01,789 --> 00:46:05,900
that your errors are normally

00:46:03,949 --> 00:46:07,759
distributed so here we come back to the

00:46:05,900 --> 00:46:10,910
standard Gaussian process we have this

00:46:07,759 --> 00:46:13,160
line of course the measured points are

00:46:10,910 --> 00:46:15,979
not all exactly on this line but they

00:46:13,160 --> 00:46:17,839
are scatter around it in the way they

00:46:15,979 --> 00:46:20,239
scatter around it needs to be in a

00:46:17,839 --> 00:46:24,769
Gaussian fashion you can ascertain that

00:46:20,239 --> 00:46:26,329
with this kind of plot the errors need

00:46:24,769 --> 00:46:28,150
to be uncorrelated that's a more

00:46:26,329 --> 00:46:31,249
mathematical thing that that can also be

00:46:28,150 --> 00:46:33,319
quantitatively ascertain the variance of

00:46:31,249 --> 00:46:35,750
errors needs to be constant and

00:46:33,319 --> 00:46:36,890
from the forth thing the design matrix

00:46:35,750 --> 00:46:39,349
has to have full rank

00:46:36,890 --> 00:46:41,839
I really haven't found any image that

00:46:39,349 --> 00:46:44,150
could explain what this means so I'm

00:46:41,839 --> 00:46:46,880
just stating it without qualifying it

00:46:44,150 --> 00:46:48,349
further but rest assured that you would

00:46:46,880 --> 00:46:49,579
need to test that if you want to make

00:46:48,349 --> 00:46:52,069
sure your models correct

00:46:49,579 --> 00:46:55,010
so from this model from this model all

00:46:52,069 --> 00:46:57,530
these conditions are actually fulfilled

00:46:55,010 --> 00:46:59,210
and unique ones can be quite well

00:46:57,530 --> 00:47:02,299
predict be predicted by Hillary

00:46:59,210 --> 00:47:08,089
Russell's when we look at when we look

00:47:02,299 --> 00:47:10,309
at data that actually occur in computer

00:47:08,089 --> 00:47:11,569
science and that people are using that

00:47:10,309 --> 00:47:15,020
people are using to make predictions

00:47:11,569 --> 00:47:18,740
then we see that life with this kind of

00:47:15,020 --> 00:47:20,809
data is not so simple so as a final

00:47:18,740 --> 00:47:24,680
example of how you should not do things

00:47:20,809 --> 00:47:28,040
let me show you this data set that uses

00:47:24,680 --> 00:47:31,270
but fix commit so that has really been

00:47:28,040 --> 00:47:34,250
used in some safety certification

00:47:31,270 --> 00:47:36,770
efforts to prove to authorities that a

00:47:34,250 --> 00:47:38,510
development process of certain pieces of

00:47:36,770 --> 00:47:41,480
software are good enough to trust on

00:47:38,510 --> 00:47:43,460
your life to it what what people have

00:47:41,480 --> 00:47:46,099
measured here is that way they've taken

00:47:43,460 --> 00:47:48,230
the number of bad fix commits because

00:47:46,099 --> 00:47:50,480
bad fix commits as we all know fix back

00:47:48,230 --> 00:47:53,809
so they don't introduce any problems

00:47:50,480 --> 00:47:58,609
anymore problems never have and they've

00:47:53,809 --> 00:48:00,410
recorded the time that at the time at

00:47:58,609 --> 00:48:02,839
what point these bad fix commits were

00:48:00,410 --> 00:48:05,390
introduced into the system now what you

00:48:02,839 --> 00:48:06,650
can see from the data of course is that

00:48:05,390 --> 00:48:10,490
the number of bad fix

00:48:06,650 --> 00:48:13,040
commits went down over time which is a

00:48:10,490 --> 00:48:15,349
natural thing because either bugs are

00:48:13,040 --> 00:48:17,089
fixed or people start losing interest in

00:48:15,349 --> 00:48:19,819
fixing bugs because there's too many of

00:48:17,089 --> 00:48:23,059
them now the natural thing to do of

00:48:19,819 --> 00:48:25,520
course with this kind of data is to find

00:48:23,059 --> 00:48:28,069
a model that describes the relationship

00:48:25,520 --> 00:48:30,890
between the time when bug fix commits

00:48:28,069 --> 00:48:33,859
are issued and the amount of pet fix of

00:48:30,890 --> 00:48:35,180
these or the time after a piece of

00:48:33,859 --> 00:48:37,790
software has been released and the

00:48:35,180 --> 00:48:40,730
amount of bat fix commits let go into

00:48:37,790 --> 00:48:42,799
the software you do that not with linear

00:48:40,730 --> 00:48:44,809
regression but we form a more

00:48:42,799 --> 00:48:46,850
generalized form of it I'm not going to

00:48:44,809 --> 00:48:49,730
get into details about that but what

00:48:46,850 --> 00:48:52,850
are you end up with in at the end of the

00:48:49,730 --> 00:48:55,010
day is a graph like this so you have a

00:48:52,850 --> 00:48:57,590
functional relationship between time and

00:48:55,010 --> 00:49:01,550
but fix commits and again that looks as

00:48:57,590 --> 00:49:05,360
nice as the graph as the relationship

00:49:01,550 --> 00:49:07,370
between mudras hills and unicorns and so

00:49:05,360 --> 00:49:09,410
you have a natural thing to do is say

00:49:07,370 --> 00:49:12,320
young let's make predictions with that

00:49:09,410 --> 00:49:14,720
because if we know how our bugfixes

00:49:12,320 --> 00:49:17,330
happened in the first 60 whatever time

00:49:14,720 --> 00:49:19,280
units of the product we can just extend

00:49:17,330 --> 00:49:21,950
that predict the future from that which

00:49:19,280 --> 00:49:24,470
is easy to do with statistical software

00:49:21,950 --> 00:49:28,370
and then you can say okay at this point

00:49:24,470 --> 00:49:31,250
we will have essentially reached a

00:49:28,370 --> 00:49:34,960
situation where we don't get any more

00:49:31,250 --> 00:49:37,640
bad fixed commits into the system and

00:49:34,960 --> 00:49:40,970
that means the system is error-free and

00:49:37,640 --> 00:49:43,190
so I can it we can use it in in safety

00:49:40,970 --> 00:49:45,860
and safety critical environments

00:49:43,190 --> 00:49:50,000
now who of you is going to buy this kind

00:49:45,860 --> 00:49:51,950
of argumentation from you no one I'm

00:49:50,000 --> 00:49:55,910
really glad you don't the problem is

00:49:51,950 --> 00:49:58,820
that certification authorities tend to

00:49:55,910 --> 00:50:02,270
buy this kind of argumentation these

00:49:58,820 --> 00:50:04,580
days of course the problem with with

00:50:02,270 --> 00:50:07,280
this approach is clear people did the

00:50:04,580 --> 00:50:10,040
regression did the regression computed

00:50:07,280 --> 00:50:12,470
the model predicted the future from the

00:50:10,040 --> 00:50:14,360
model but the only detailed problem that

00:50:12,470 --> 00:50:16,430
they overlooked is that this model

00:50:14,360 --> 00:50:19,400
doesn't cert if it doesn't satisfy any

00:50:16,430 --> 00:50:22,670
of the requirements that I've mentioned

00:50:19,400 --> 00:50:25,520
before any of these four conditions that

00:50:22,670 --> 00:50:29,150
need to be fulfilled for a model to be

00:50:25,520 --> 00:50:33,140
in yeah correct

00:50:29,150 --> 00:50:38,090
or at least appropriately described the

00:50:33,140 --> 00:50:39,860
data good with that so I'm listing and

00:50:38,090 --> 00:50:42,770
listing the issues here that this model

00:50:39,860 --> 00:50:44,450
had is like errors residuals are all

00:50:42,770 --> 00:50:46,190
completely wrong so we don't have a

00:50:44,450 --> 00:50:47,990
normal distribution of the errors that

00:50:46,190 --> 00:50:51,650
wouldn't be that wouldn't be so bad but

00:50:47,990 --> 00:50:53,600
the accuracy of the base data is is also

00:50:51,650 --> 00:50:56,960
not a given thing the functional

00:50:53,600 --> 00:50:59,330
relationship between time and amount of

00:50:56,960 --> 00:51:00,650
back fix commits is if you look at the

00:50:59,330 --> 00:51:02,450
statistical model

00:51:00,650 --> 00:51:04,160
in detail and if you do actually look at

00:51:02,450 --> 00:51:06,500
the diagnosis that you get from the

00:51:04,160 --> 00:51:10,870
statistical software such that the

00:51:06,500 --> 00:51:13,280
software says okay so the the the

00:51:10,870 --> 00:51:15,350
variables that you are using that you

00:51:13,280 --> 00:51:17,390
are trying to use to predict the amount

00:51:15,350 --> 00:51:21,500
of bad fixed commits are not sufficient

00:51:17,390 --> 00:51:22,970
to given to give an accurate model but

00:51:21,500 --> 00:51:24,890
of course if you don't look at the

00:51:22,970 --> 00:51:28,040
Diagnostics this problem won't bother

00:51:24,890 --> 00:51:31,850
you so that's that's the three things

00:51:28,040 --> 00:51:34,580
that are wrong with this model if people

00:51:31,850 --> 00:51:38,180
would have done the process that I have

00:51:34,580 --> 00:51:40,160
outlined very briefly in this talk isn't

00:51:38,180 --> 00:51:42,890
and we find an appropriate technique fit

00:51:40,160 --> 00:51:45,650
and plot the model and most importantly

00:51:42,890 --> 00:51:48,290
check diagnostic data then they would

00:51:45,650 --> 00:51:51,650
have realized that this model is not

00:51:48,290 --> 00:51:57,530
really appropriate for the task they are

00:51:51,650 --> 00:52:02,000
about they are about to to take on but

00:51:57,530 --> 00:52:03,950
maybe let that be if that's the major

00:52:02,000 --> 00:52:06,530
thing you take home from you from this

00:52:03,950 --> 00:52:09,500
talk namely if you apply statistics and

00:52:06,530 --> 00:52:12,200
please make sure that you've done your

00:52:09,500 --> 00:52:14,180
homework that you've not just computed

00:52:12,200 --> 00:52:15,800
models but that you've checked that your

00:52:14,180 --> 00:52:18,140
model is really accurate then I'm

00:52:15,800 --> 00:52:21,230
already quite happy because that means

00:52:18,140 --> 00:52:22,820
chances statistical chances are having

00:52:21,230 --> 00:52:25,490
proved that I'm not going to die because

00:52:22,820 --> 00:52:28,790
of miss predictions from software

00:52:25,490 --> 00:52:31,130
quality data good thank you very much

00:52:28,790 --> 00:52:34,520
for your interest in this final session

00:52:31,130 --> 00:52:38,000
I know that I've had way too many slides

00:52:34,520 --> 00:52:41,060
these bytes are still not even coming to

00:52:38,000 --> 00:52:45,290
the to the fine points and to the deeper

00:52:41,060 --> 00:52:47,150
points of statistics yeah but still

00:52:45,290 --> 00:52:49,880
again thank you thank you very much for

00:52:47,150 --> 00:52:51,660
for suffering with these simple examples

00:52:49,880 --> 00:52:56,120
and the many slides

00:52:51,660 --> 00:52:56,120

YouTube URL: https://www.youtube.com/watch?v=8671RYrsAj4


