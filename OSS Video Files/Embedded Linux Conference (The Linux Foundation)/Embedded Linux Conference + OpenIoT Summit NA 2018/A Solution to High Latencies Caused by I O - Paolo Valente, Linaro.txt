Title: A Solution to High Latencies Caused by I O - Paolo Valente, Linaro
Publication date: 2018-03-14
Playlist: Embedded Linux Conference + OpenIoT Summit NA 2018
Description: 
	A Solution to High Latencies Caused by I/O - Paolo Valente, Linaro

 The BFQ I/O scheduler has made its way into the Linux kernel, bringing high responsiveness, low latency for time-sensitive applications, and strong fairness. These improvements are particularly beneficial for embedded systems, given the slower speed and the higher latency of the storage devices of these systems. Yet, some year may pass before major OSes for embedded systems, notably Android, will run recent-enough versions of the Linux kernel to enjoy these improvements. In this presentation, we show the benefits to come (or already available in systems running recent kernels), and compare them with the current latency and unfairness problems of Linux-based embedded systems. In particular, we show these facts through demos and numbers for real hardware.

About Paolo Valente
Paolo Valente is an Assistant Professor of Computer Science at the
University of Modena and Reggio Emilia, Italy, and a collaborator of
the Linaro engineering organization. Paolo's main activities focus on
scheduling algorithms for storage devices, transmission links and
CPUs. In this respect, Paolo is the author of the last version of the
BFQ I/O scheduler. BFQ entered the Linux kernel from 4.12, providing
unprecedented low-latency and fairness guarantees. As for transmission links, Paolo is one of the authors of the QFQ packet scheduler, which has been in the Linux kernel until 3.7, after that it has been replaced by QFQ+, a faster variant defined and implemented by Paolo himself. Finally, Paolo has also defined and implemented other algorithms, part of which are now in FreeBSD, and has provided new theoretic results on multiprocessor scheduling.

Paolo has given about forty talks, as an invited speaker or to present
his scientific papers.
Captions: 
	00:00:00,030 --> 00:00:06,960
so I'm Paulo Valenti I'm an assistant

00:00:04,170 --> 00:00:09,120
professor from the University of modern

00:00:06,960 --> 00:00:12,960
era Jamelia but I'm here as a

00:00:09,120 --> 00:00:15,120
collaborator of Lennar oh and this

00:00:12,960 --> 00:00:20,730
presentation is about work

00:00:15,120 --> 00:00:25,170
supported by Lyn ro so the protagonists

00:00:20,730 --> 00:00:30,210
of this presentation are highlighting

00:00:25,170 --> 00:00:33,780
Cisco's by IO and a possible solution to

00:00:30,210 --> 00:00:38,129
this problem the solution is the bfq AIA

00:00:33,780 --> 00:00:41,129
scheduler so to try to make sure that

00:00:38,129 --> 00:00:44,120
nobody lacks the information needed to

00:00:41,129 --> 00:00:47,180
follow what i'm about to show you i'll

00:00:44,120 --> 00:00:50,399
start by spending a few words about our

00:00:47,180 --> 00:00:53,730
schedulers then we will go straight to

00:00:50,399 --> 00:00:56,640
the point I will show you these high

00:00:53,730 --> 00:01:00,570
latencies and I'll show you a bfq at

00:00:56,640 --> 00:01:06,180
work in particular I'll show you demos

00:01:00,570 --> 00:01:11,490
and numbers on the systems listed on the

00:01:06,180 --> 00:01:13,680
on the slide I have we have run and we

00:01:11,490 --> 00:01:17,130
are running tests also on other system

00:01:13,680 --> 00:01:20,759
but I will limit to this presentation to

00:01:17,130 --> 00:01:24,090
the systems listed there this

00:01:20,759 --> 00:01:27,180
presentation might arouse some questions

00:01:24,090 --> 00:01:29,930
so I have already added the answers to

00:01:27,180 --> 00:01:32,939
two possible questions directly to the

00:01:29,930 --> 00:01:36,900
presentation I guess that these

00:01:32,939 --> 00:01:40,079
questions will become clearer after

00:01:36,900 --> 00:01:42,780
showing you what I want to show you so I

00:01:40,079 --> 00:01:45,210
our scheduler an i/o scheduler is a

00:01:42,780 --> 00:01:48,450
component that decides the order in

00:01:45,210 --> 00:01:53,369
which your requests are served by a

00:01:48,450 --> 00:01:58,009
given storage device in particular an

00:01:53,369 --> 00:02:01,860
i/o scheduler does this when there are

00:01:58,009 --> 00:02:06,360
several entities processes groups of

00:02:01,860 --> 00:02:11,129
processes applications that compete for

00:02:06,360 --> 00:02:14,010
the same device this job this reordering

00:02:11,129 --> 00:02:17,879
this control over the order of a

00:02:14,010 --> 00:02:20,670
request is rather important why because

00:02:17,879 --> 00:02:23,549
this is the only way to achieve several

00:02:20,670 --> 00:02:27,180
important goals one is reaching a high

00:02:23,549 --> 00:02:30,030
true boot because storage devices the

00:02:27,180 --> 00:02:33,659
true boot of storage devices is very

00:02:30,030 --> 00:02:36,230
sensitive to the order in which your

00:02:33,659 --> 00:02:39,659
requests are dispatched to to the device

00:02:36,230 --> 00:02:45,359
another important goal is guaranteeing a

00:02:39,659 --> 00:02:48,569
low latency to tasks that need your task

00:02:45,359 --> 00:02:51,269
that to be accomplished need a Oh an

00:02:48,569 --> 00:02:55,549
example is reading frames to playback a

00:02:51,269 --> 00:02:59,639
video another important example are

00:02:55,549 --> 00:03:02,819
interactive tasks such as starting an

00:02:59,639 --> 00:03:08,849
application reading a file saving a file

00:03:02,819 --> 00:03:12,659
and so on if you provide you guarantee a

00:03:08,849 --> 00:03:15,359
low latency to interactive tasks in a

00:03:12,659 --> 00:03:18,359
system then you are guaranteeing a high

00:03:15,359 --> 00:03:22,790
responsiveness to application in the

00:03:18,359 --> 00:03:29,150
system and to the overall system itself

00:03:22,790 --> 00:03:32,280
this in the mobile system community

00:03:29,150 --> 00:03:34,709
responsiveness is usually indicated or

00:03:32,280 --> 00:03:37,290
measured more precisely the opposite or

00:03:34,709 --> 00:03:39,720
responsiveness is usually measured the

00:03:37,290 --> 00:03:42,739
indicated s lag so guaranteeing a high

00:03:39,720 --> 00:03:52,250
responsiveness means guaranteeing a low

00:03:42,739 --> 00:03:57,090
lag in this presentation we will use the

00:03:52,250 --> 00:04:01,379
startup time of applications as a

00:03:57,090 --> 00:04:03,720
measure of responsiveness starting an

00:04:01,379 --> 00:04:06,810
application is an important example of

00:04:03,720 --> 00:04:08,730
an interactive tasks and measuring the

00:04:06,810 --> 00:04:12,419
start of a start time of applications

00:04:08,730 --> 00:04:15,930
also gives an idea of the latency that

00:04:12,419 --> 00:04:18,359
you in general guarantee to any possible

00:04:15,930 --> 00:04:20,760
generic interactive tasks then

00:04:18,359 --> 00:04:23,370
schedulers are scalars have also other

00:04:20,760 --> 00:04:27,810
goals but they are out of the scope of

00:04:23,370 --> 00:04:30,720
this presentation finally where are

00:04:27,810 --> 00:04:34,110
are you schedulers in Linux they are in

00:04:30,720 --> 00:04:37,020
the i/o stack of the operating system of

00:04:34,110 --> 00:04:42,330
the kernel in particular desire stack

00:04:37,020 --> 00:04:45,780
currently comes in two flavors one is

00:04:42,330 --> 00:04:48,330
the legacy block layer BLK the other one

00:04:45,780 --> 00:04:51,870
is the new multi cue block layer

00:04:48,330 --> 00:04:55,889
blocking cube and the sets of schedulers

00:04:51,870 --> 00:04:58,680
that you find in each of these block

00:04:55,889 --> 00:05:01,669
bastions of the block layer differs the

00:04:58,680 --> 00:05:05,550
only scheduler in common is a deadline

00:05:01,669 --> 00:05:09,450
in particular in legacy block you don't

00:05:05,550 --> 00:05:12,960
have bfq to use bfq you have to pick it

00:05:09,450 --> 00:05:17,250
from the it's out of three menu somehow

00:05:12,960 --> 00:05:20,100
to add it to your system while in

00:05:17,250 --> 00:05:24,000
blocking queue fortunately there is also

00:05:20,100 --> 00:05:27,300
be of queue you find also give you okay

00:05:24,000 --> 00:05:30,540
I stopped here about how you schedule

00:05:27,300 --> 00:05:33,210
I'll show you in practice what this i/o

00:05:30,540 --> 00:05:37,490
scheduler can do and above all what they

00:05:33,210 --> 00:05:41,070
cannot do so let's introduce these tests

00:05:37,490 --> 00:05:49,340
all these tests have been run on systems

00:05:41,070 --> 00:05:54,050
using emmc or SD cards and both on

00:05:49,340 --> 00:06:00,539
Android and on some Linux distribution

00:05:54,050 --> 00:06:03,710
as for Android the kernel versions used

00:06:00,539 --> 00:06:06,870
in Android systems are too old to have

00:06:03,710 --> 00:06:10,289
blocking queue support in the MMC

00:06:06,870 --> 00:06:15,600
subsystem so no blocking queue support

00:06:10,289 --> 00:06:17,789
no bfq in this test I will use the out

00:06:15,600 --> 00:06:22,169
of tree version of BQ which is available

00:06:17,789 --> 00:06:24,300
also for the legacy for legacy block on

00:06:22,169 --> 00:06:27,450
the other side on the opposite side for

00:06:24,300 --> 00:06:30,000
linux distributions you do have block

00:06:27,450 --> 00:06:35,010
and queue support also in the MMC

00:06:30,000 --> 00:06:39,900
subsystem from 4.16 onwards and in my

00:06:35,010 --> 00:06:41,060
tests I have used systems with recent

00:06:39,900 --> 00:06:45,070
enough version

00:06:41,060 --> 00:06:48,970
of the can release candidate of the 4.16

00:06:45,070 --> 00:06:53,120
then s for Android the two systems

00:06:48,970 --> 00:06:55,700
tested are the hi keyboard and the

00:06:53,120 --> 00:06:57,830
Google pixel to phone and for Linux

00:06:55,700 --> 00:07:02,419
distributions again yeah keyboard and

00:06:57,830 --> 00:07:07,940
the poco plug series four and then a

00:07:02,419 --> 00:07:11,090
laptop with some SD cards okay I think I

00:07:07,940 --> 00:07:16,370
already told you all I had to so we can

00:07:11,090 --> 00:07:19,070
start with the first demo actually I'll

00:07:16,370 --> 00:07:23,470
show you a treatment version of this

00:07:19,070 --> 00:07:31,419
demo the demo is longer and there are

00:07:23,470 --> 00:07:31,419
other parts okay I will focus only on

00:07:31,660 --> 00:07:44,000
some parts about lag in this demo we

00:07:38,990 --> 00:07:47,320
measure lag by starting the Facebook app

00:07:44,000 --> 00:07:50,600
in two different conditions first with

00:07:47,320 --> 00:07:57,940
nothing in the background so no I /

00:07:50,600 --> 00:08:02,479
cloud in the background and then while

00:07:57,940 --> 00:08:06,070
some updates are in progress

00:08:02,479 --> 00:08:10,250
these updates are performed through

00:08:06,070 --> 00:08:14,930
scripts some in some automatic way and

00:08:10,250 --> 00:08:18,080
in particular they are run controlling

00:08:14,930 --> 00:08:22,310
the download speed to test a wider range

00:08:18,080 --> 00:08:24,770
of scenarios and in this test the

00:08:22,310 --> 00:08:27,710
download speed in what I'm about to show

00:08:24,770 --> 00:08:30,889
you the download speed is around 15

00:08:27,710 --> 00:08:33,110
megabytes per second at this speed as

00:08:30,889 --> 00:08:38,060
I'm about to show you

00:08:33,110 --> 00:08:39,849
starting Facebook becomes starts to be a

00:08:38,060 --> 00:08:42,260
real problem

00:08:39,849 --> 00:08:45,589
I'll show you results only with the

00:08:42,260 --> 00:08:47,870
north but they are the same we'll see a

00:08:45,589 --> 00:08:51,920
few and then dry I remind you that here

00:08:47,870 --> 00:08:55,080
we have the legacy block layer ok I

00:08:51,920 --> 00:08:57,540
think I told you everything so we can

00:08:55,080 --> 00:09:00,750
on the left side you will see the

00:08:57,540 --> 00:09:02,880
startup of Facebook when the system at

00:09:00,750 --> 00:09:06,660
the board is idle while on the right

00:09:02,880 --> 00:09:09,029
hand side the same exact task but this

00:09:06,660 --> 00:09:11,160
time with updates in the background so

00:09:09,029 --> 00:09:15,540
it takes just 6 seconds to start

00:09:11,160 --> 00:09:24,839
Facebook while in the other case you

00:09:15,540 --> 00:09:27,000
have to wait much more much more ok so

00:09:24,839 --> 00:09:30,000
here we have dissipates with a

00:09:27,000 --> 00:09:33,810
controller rate 50 megabytes per second

00:09:30,000 --> 00:09:37,310
if we increase the download speed then

00:09:33,810 --> 00:09:40,290
the time to start Facebook becomes

00:09:37,310 --> 00:09:42,110
infinite you have to wait forever until

00:09:40,290 --> 00:09:49,110
the workload stops

00:09:42,110 --> 00:09:51,149
Facebook does not start ok then there

00:09:49,110 --> 00:09:53,430
are some considerations about how

00:09:51,149 --> 00:09:56,010
realistic these download speeds are but

00:09:53,430 --> 00:09:58,430
I don't want to go into that now because

00:09:56,010 --> 00:10:01,560
I want to show you what happens with bfq

00:09:58,430 --> 00:10:06,839
same identical tests but this time we

00:10:01,560 --> 00:10:10,589
make bsq control da on left side again

00:10:06,839 --> 00:10:12,270
divides either right hand side not in

00:10:10,589 --> 00:10:21,839
the middle you will see what happens

00:10:12,270 --> 00:10:26,640
with bfq and it will take if I remember

00:10:21,839 --> 00:10:29,100
well 10 seconds yeah so it's 4 more

00:10:26,640 --> 00:10:32,339
seconds with respect to the idle case

00:10:29,100 --> 00:10:35,220
and that is due to blocking issues in

00:10:32,339 --> 00:10:36,180
the virtual memory subsystem this this

00:10:35,220 --> 00:10:38,250
is out of control

00:10:36,180 --> 00:10:40,649
for Biff queue but it can be solved with

00:10:38,250 --> 00:10:43,529
bfq but this is a different story

00:10:40,649 --> 00:10:47,130
not for this presentation then in this

00:10:43,529 --> 00:10:51,180
in this video in this demo we went on

00:10:47,130 --> 00:10:53,130
trying with heavier workload and we saw

00:10:51,180 --> 00:10:55,589
that with bfq you can increase the

00:10:53,130 --> 00:10:57,720
download speed but the startup time of

00:10:55,589 --> 00:11:03,360
Facebook remains the same while with the

00:10:57,720 --> 00:11:07,440
other schedule every wait forever in the

00:11:03,360 --> 00:11:09,230
end we saw that one of the heaviest

00:11:07,440 --> 00:11:13,560
workload is just

00:11:09,230 --> 00:11:17,040
transferring a file to the board okay on

00:11:13,560 --> 00:11:18,959
a fast connection so we repeated this

00:11:17,040 --> 00:11:22,050
test in in the video you could see this

00:11:18,959 --> 00:11:23,730
test done also in the other case I don't

00:11:22,050 --> 00:11:26,009
want to show you that too I want to show

00:11:23,730 --> 00:11:27,870
you directly another test

00:11:26,009 --> 00:11:30,240
video playback so what happens with the

00:11:27,870 --> 00:11:32,940
soft real-time application if you have

00:11:30,240 --> 00:11:36,959
this heavy workload in the background as

00:11:32,940 --> 00:11:39,839
before with bsq not sure not just before

00:11:36,959 --> 00:11:42,470
now the the workload for beef queue is

00:11:39,839 --> 00:11:45,089
much heavier because those are two files

00:11:42,470 --> 00:11:47,459
file copies in parallel and believe me

00:11:45,089 --> 00:11:50,519
they are much heavier than just

00:11:47,459 --> 00:11:55,440
transferring one file so again on the

00:11:50,519 --> 00:11:57,120
left side the playback of the movie when

00:11:55,440 --> 00:11:59,660
there is nothing in the background in

00:11:57,120 --> 00:12:03,389
the middle give Q on the right-hand side

00:11:59,660 --> 00:12:05,610
the other scale anova the video starts

00:12:03,389 --> 00:12:08,550
and it is played back smoothly in the

00:12:05,610 --> 00:12:10,769
idle case with vfq it takes a little bit

00:12:08,550 --> 00:12:13,470
more for the reasons that I already told

00:12:10,769 --> 00:12:16,709
you but then when it starts the playback

00:12:13,470 --> 00:12:22,410
is perfectly smooth with nope guess what

00:12:16,709 --> 00:12:24,329
you will wait a lot I don't want to make

00:12:22,410 --> 00:12:26,880
you waste your time waiting so we will

00:12:24,329 --> 00:12:31,860
skip directly to when the video finally

00:12:26,880 --> 00:12:38,009
start and after okay

00:12:31,860 --> 00:12:47,420
still let's do the jump after waiting

00:12:38,009 --> 00:12:53,320
for so long the video video please

00:12:47,420 --> 00:12:53,320
and it will freeze again in a moment

00:12:53,380 --> 00:13:03,820
there okay and and so on so we have a

00:12:59,840 --> 00:13:09,410
problem there is a latency problem and

00:13:03,820 --> 00:13:11,990
vfq seems to solve it first objection

00:13:09,410 --> 00:13:15,950
that I want to do during this

00:13:11,990 --> 00:13:21,440
presentation to that to bfq okay this

00:13:15,950 --> 00:13:26,450
happens because the system the board is

00:13:21,440 --> 00:13:29,630
now a slow one the storage is low if we

00:13:26,450 --> 00:13:32,000
move to a much faster device these

00:13:29,630 --> 00:13:39,260
problems will just disappear

00:13:32,000 --> 00:13:42,470
I did it I tried with a Google pixel to

00:13:39,260 --> 00:13:47,870
which I bought exactly for not only for

00:13:42,470 --> 00:13:51,670
this and this time I wanted to run a

00:13:47,870 --> 00:13:53,570
natural test so no script no

00:13:51,670 --> 00:13:56,000
instrumentation of the code don't

00:13:53,570 --> 00:13:59,180
shaking of the device especially because

00:13:56,000 --> 00:14:04,700
the device is mine and it's not so cheap

00:13:59,180 --> 00:14:06,830
and so I look at for a fast Network to

00:14:04,700 --> 00:14:11,270
do the same I found a simple way to

00:14:06,830 --> 00:14:13,790
trigger update and I tried but

00:14:11,270 --> 00:14:18,050
unfortunately in Italy I couldn't find a

00:14:13,790 --> 00:14:21,320
fast enough network which in a sense is

00:14:18,050 --> 00:14:23,780
good because if you want a vacation

00:14:21,320 --> 00:14:28,160
without lag issues caused by updates

00:14:23,780 --> 00:14:31,370
consider Italy you may kiss

00:14:28,160 --> 00:14:34,940
I needed that problem so I resorted to

00:14:31,370 --> 00:14:39,590
something else a lighter workload that I

00:14:34,940 --> 00:14:41,930
I could generate Jeff some file copies

00:14:39,590 --> 00:14:44,300
one after the other this is lighter than

00:14:41,930 --> 00:14:48,170
an update because it involves less

00:14:44,300 --> 00:14:51,530
resources only storage less hardware and

00:14:48,170 --> 00:14:56,870
software resources but it has in common

00:14:51,530 --> 00:14:59,699
with update the key the key property

00:14:56,870 --> 00:15:02,129
that causes those high leg

00:14:59,699 --> 00:15:05,579
intense I owe because I showed you that

00:15:02,129 --> 00:15:08,519
the problem is IO because bfq solves

00:15:05,579 --> 00:15:12,629
that problem and we have Q works only on

00:15:08,519 --> 00:15:15,869
Io so if we have a high leg will file

00:15:12,629 --> 00:15:20,759
copies then we have a high leg also with

00:15:15,869 --> 00:15:23,119
update on a fascinator let's see what

00:15:20,759 --> 00:15:23,119
happens

00:15:27,449 --> 00:15:37,180
so again I started Facebook Twice first

00:15:32,939 --> 00:15:42,759
while the phone was idle and then with

00:15:37,180 --> 00:15:51,279
some music files being copied in both

00:15:42,759 --> 00:15:54,429
cases I terminated Facebook and cleared

00:15:51,279 --> 00:15:58,769
its data for two reasons

00:15:54,429 --> 00:16:02,050
first to make sure that actually

00:15:58,769 --> 00:16:05,759
Facebook must do I owe to start

00:16:02,050 --> 00:16:08,470
otherwise this wouldn't be an i/o test

00:16:05,759 --> 00:16:11,740
second to put the phone in exactly the

00:16:08,470 --> 00:16:14,230
same conditions in both cases I forgot

00:16:11,740 --> 00:16:19,350
to tell you that of course I did the

00:16:14,230 --> 00:16:24,610
same also in the test for the a key okay

00:16:19,350 --> 00:16:28,869
so first we start Facebook we terminate

00:16:24,610 --> 00:16:31,679
Facebook clear arm okay details on the

00:16:28,869 --> 00:16:34,509
phone brand new pixel to no change made

00:16:31,679 --> 00:16:36,819
anywhere so not bfq this is only to show

00:16:34,509 --> 00:16:37,300
you the problem not not a possible

00:16:36,819 --> 00:16:44,220
solution

00:16:37,300 --> 00:16:44,220
okay so terminating Facebook Cup and

00:16:44,249 --> 00:16:53,069
clearing data and restarting Facebook

00:16:54,410 --> 00:17:03,020
and it will stop very quickly no no no

00:16:59,900 --> 00:17:04,040
no no I mean as far as I know but I I

00:17:03,020 --> 00:17:09,110
don't think so

00:17:04,040 --> 00:17:11,930
yeah it's only Facebook that yeah okay

00:17:09,110 --> 00:17:16,250
then the same is a test but this time I

00:17:11,930 --> 00:17:19,699
will first start some copies how much

00:17:16,250 --> 00:17:23,000
data enough data to make sure that the

00:17:19,699 --> 00:17:26,000
copies go on for all the startup of

00:17:23,000 --> 00:17:30,530
Facebook if actually much more because I

00:17:26,000 --> 00:17:39,080
exaggerated okay and then we do again

00:17:30,530 --> 00:17:43,870
the same we terminate Facebook and clear

00:17:39,080 --> 00:17:43,870
data and

00:17:47,100 --> 00:17:57,160
and it will start very quickly actually

00:17:54,520 --> 00:17:58,900
no you can take a coffee now because

00:17:57,160 --> 00:18:02,350
this is much worse than the other test

00:17:58,900 --> 00:18:05,110
and the reason is that the ayah of the

00:18:02,350 --> 00:18:09,340
copies is sequential and greedy and this

00:18:05,110 --> 00:18:11,290
is what the system like most because it

00:18:09,340 --> 00:18:13,120
gives you the peak rate so Bodi

00:18:11,290 --> 00:18:16,030
operating system and the internally your

00:18:13,120 --> 00:18:18,790
scheduler of the storage device do like

00:18:16,030 --> 00:18:22,000
that IO and they prefer it over the

00:18:18,790 --> 00:18:25,960
mostly random i/o for starting Facebook

00:18:22,000 --> 00:18:28,900
and then you wait 11 times as much as

00:18:25,960 --> 00:18:30,310
you waited before you can find a sink

00:18:28,900 --> 00:18:32,590
consideration that already told you

00:18:30,310 --> 00:18:35,260
there plus a bit of information you get

00:18:32,590 --> 00:18:38,320
the same exact problem if you do a file

00:18:35,260 --> 00:18:39,850
transfer while you are starting facebook

00:18:38,320 --> 00:18:49,570
or myspace so you start Facebook while

00:18:39,850 --> 00:18:55,360
doing a fan transfer ok so I hope I made

00:18:49,570 --> 00:19:00,550
my point here the new objection ok this

00:18:55,360 --> 00:19:05,370
happens because it's Android ok let's

00:19:00,550 --> 00:19:05,370
try with a Linux distribution yeah

00:19:08,580 --> 00:19:13,529
you mean the truth of the device

00:19:21,230 --> 00:19:28,250
I ever slide on that you just give me a

00:19:26,120 --> 00:19:31,370
few minutes and I'll show ya that's

00:19:28,250 --> 00:19:36,130
better yeah I did the same objection to

00:19:31,370 --> 00:19:36,130
me before and it's a good good boy

00:19:46,920 --> 00:19:53,550
and that's another good point according

00:19:50,490 --> 00:19:57,030
to how file copies work I think there

00:19:53,550 --> 00:19:59,340
was no memory no particular memory

00:19:57,030 --> 00:20:02,010
pleasure probably yeah but I did

00:19:59,340 --> 00:20:11,280
actually I didn't check I mean but I

00:20:02,010 --> 00:20:15,540
guess no I guess no yeah that's probably

00:20:11,280 --> 00:20:18,800
yeah yeah yeah anyway I was interested

00:20:15,540 --> 00:20:21,810
into causing a bottleneck on the i/o

00:20:18,800 --> 00:20:25,320
without saturated in the CPU that was

00:20:21,810 --> 00:20:28,740
enough for me yeah okay

00:20:25,320 --> 00:20:31,470
so test on a Linux distribution I used

00:20:28,740 --> 00:20:33,600
the yes benchmark suite for these tests

00:20:31,470 --> 00:20:40,650
and I think I can show you directly the

00:20:33,600 --> 00:20:44,820
demo I can talk on the demo Debian with

00:20:40,650 --> 00:20:51,420
the 4.16 rc2 as you're about to see from

00:20:44,820 --> 00:20:56,360
the video if it is running okay and this

00:20:51,420 --> 00:20:56,360
time the application under test was

00:20:57,470 --> 00:21:04,470
externally with this release candidate I

00:21:02,040 --> 00:21:06,510
had problems with the X server so I

00:21:04,470 --> 00:21:09,060
couldn't run exactly the application so

00:21:06,510 --> 00:21:12,120
I used the future of the s benchmark

00:21:09,060 --> 00:21:14,280
suite which allows you to replay the i/o

00:21:12,120 --> 00:21:17,040
of the application that you want to test

00:21:14,280 --> 00:21:18,900
and believe me results are the same if

00:21:17,040 --> 00:21:24,930
you don't believe me try yourself with

00:21:18,900 --> 00:21:28,230
the suite it's public okay and so we

00:21:24,930 --> 00:21:31,260
start we begin by starting extent with

00:21:28,230 --> 00:21:31,560
nothing in the background we will try

00:21:31,260 --> 00:21:34,500
this

00:21:31,560 --> 00:21:39,560
I guess twice just to have an idea of

00:21:34,500 --> 00:21:43,080
the average as usually I the script

00:21:39,560 --> 00:21:45,870
clear caches before starting the

00:21:43,080 --> 00:21:51,350
application we want to test a yo not

00:21:45,870 --> 00:21:54,410
cache or something else so try twice and

00:21:51,350 --> 00:21:59,430
the average should be something like

00:21:54,410 --> 00:22:00,809
0.32 more or less and after this we

00:21:59,430 --> 00:22:03,179
retry but

00:22:00,809 --> 00:22:04,769
Stan with one file read and one file

00:22:03,179 --> 00:22:07,950
right in the background so we are

00:22:04,769 --> 00:22:09,450
simulate emulating a copy we leave known

00:22:07,950 --> 00:22:11,460
as I your scales know we are in blocking

00:22:09,450 --> 00:22:15,720
queue so there is no known as one of the

00:22:11,460 --> 00:22:19,950
schedulers and we try only once so one

00:22:15,720 --> 00:22:22,769
time one reader one writer known as I

00:22:19,950 --> 00:22:25,970
your scheduler and guess why we try only

00:22:22,769 --> 00:22:29,519
once you can take another coffee and

00:22:25,970 --> 00:22:33,749
call a friend because this time it's

00:22:29,519 --> 00:22:36,509
much longer waiting is much longer again

00:22:33,749 --> 00:22:38,749
the reason is that the reads and the

00:22:36,509 --> 00:22:43,230
rights are sequential are greedy

00:22:38,749 --> 00:22:46,350
everything but bfq later loves the type

00:22:43,230 --> 00:22:49,470
of i/o so it prefers the type of i/o and

00:22:46,350 --> 00:22:53,100
the type of that kind of our data yo

00:22:49,470 --> 00:22:55,710
goes on cutting in front of the mostly

00:22:53,100 --> 00:22:59,779
random I ought to start extend and mix

00:22:55,710 --> 00:23:05,789
their weights not forever but almost

00:22:59,779 --> 00:23:10,860
okay you have to say something guess I

00:23:05,789 --> 00:23:12,659
think a few seconds and it starts okay

00:23:10,860 --> 00:23:14,159
let me anticipate what it's about to

00:23:12,659 --> 00:23:20,519
happen okay

00:23:14,159 --> 00:23:23,519
48 seconds little bit more than 0.3 then

00:23:20,519 --> 00:23:26,100
with vfq again much heavier workload

00:23:23,519 --> 00:23:28,769
five readers five riders this time I

00:23:26,100 --> 00:23:31,470
will repeat it a few times to to have an

00:23:28,769 --> 00:23:38,360
idea of the variability so three

00:23:31,470 --> 00:23:38,360
repetitions five readers five riders bfq

00:23:38,480 --> 00:23:48,629
and this is the result 0.5 but this is a

00:23:45,570 --> 00:23:58,320
lucky iteration on average should be

00:23:48,629 --> 00:24:04,429
something like 0.8 0.9 okay so we have a

00:23:58,320 --> 00:24:04,429
problem also with a Linux distribution

00:24:06,869 --> 00:24:15,609
at this point

00:24:09,700 --> 00:24:17,799
I I mean I need to become more brief

00:24:15,609 --> 00:24:20,109
because with demos it's too long to show

00:24:17,799 --> 00:24:23,019
you a tour of analysis so I will switch

00:24:20,109 --> 00:24:27,279
now to graph only only a few graphs

00:24:23,019 --> 00:24:31,720
don't don't worry graphs of startup

00:24:27,279 --> 00:24:34,029
times and of true but-- about other

00:24:31,720 --> 00:24:36,249
times i tried with several of really

00:24:34,029 --> 00:24:38,169
many applications in particular in this

00:24:36,249 --> 00:24:40,509
last test with external terminal

00:24:38,169 --> 00:24:42,730
libreoffice writer' results are about

00:24:40,509 --> 00:24:44,649
the same we leave office right there

00:24:42,730 --> 00:24:46,539
about the same results as with notarial

00:24:44,649 --> 00:24:49,899
so show view on excel mini on terminal

00:24:46,539 --> 00:24:53,080
and these are results so on the x-axis

00:24:49,899 --> 00:24:56,649
there are the two workloads for which I

00:24:53,080 --> 00:24:59,070
repeated this test 10/10 readers

00:24:56,649 --> 00:25:02,499
sequential turn sequentially this five

00:24:59,070 --> 00:25:04,389
sequentially the +5 sequential writers

00:25:02,499 --> 00:25:07,480
which for the reasons that I already

00:25:04,389 --> 00:25:12,460
told you are the nastiest workload for

00:25:07,480 --> 00:25:15,879
latency on the y-axis the time the

00:25:12,460 --> 00:25:18,279
startup times the bars there is a bar

00:25:15,879 --> 00:25:23,710
for each scheduler and finally there is

00:25:18,279 --> 00:25:26,369
the red line shows the reference that of

00:25:23,710 --> 00:25:29,440
time the one when the device is idle

00:25:26,369 --> 00:25:32,769
there is an X in case the application

00:25:29,440 --> 00:25:37,269
didn't start at all okay which means it

00:25:32,769 --> 00:25:41,409
took more than 120 seconds and timeout

00:25:37,269 --> 00:25:45,399
fired and the test stopped as you can

00:25:41,409 --> 00:25:47,649
see with forex term bfq is a little bit

00:25:45,399 --> 00:25:49,600
better than the other schedulers the

00:25:47,649 --> 00:25:51,879
start time is about the same as if the

00:25:49,600 --> 00:25:54,369
device was either so you don't even

00:25:51,879 --> 00:25:58,480
realize that your system is under load

00:25:54,369 --> 00:26:01,090
this is the nice thing we have your

00:25:58,480 --> 00:26:03,190
application like no terminal the

00:26:01,090 --> 00:26:05,409
situation is very very bad for the other

00:26:03,190 --> 00:26:09,159
schedulers because it doesn't start at

00:26:05,409 --> 00:26:10,960
all only access note that the time scale

00:26:09,159 --> 00:26:13,359
there in the second graph is rather

00:26:10,960 --> 00:26:17,169
smaller than in the first graph and for

00:26:13,359 --> 00:26:19,910
with bfq its start time is about a

00:26:17,169 --> 00:26:23,630
little bit more than what you get when

00:26:19,910 --> 00:26:27,770
the system is idle and finally his

00:26:23,630 --> 00:26:32,179
question so okay this seems to be

00:26:27,770 --> 00:26:37,610
wonderful but what do you make me pay in

00:26:32,179 --> 00:26:41,780
terms of true boot I tested it and the

00:26:37,610 --> 00:26:44,000
answer is apparently nothing this test

00:26:41,780 --> 00:26:46,130
is with the same workloads sequential

00:26:44,000 --> 00:26:48,260
and random sorry the same sequential

00:26:46,130 --> 00:26:51,080
clothes plus random were closed to widen

00:26:48,260 --> 00:26:51,620
the scope within the coverage of this

00:26:51,080 --> 00:26:54,049
test

00:26:51,620 --> 00:26:56,330
as you can see performance is about the

00:26:54,049 --> 00:26:57,830
same with all the scale there are some

00:26:56,330 --> 00:26:59,690
differences that the one to go into

00:26:57,830 --> 00:27:03,620
these details I just wanted to show you

00:26:59,690 --> 00:27:05,900
that you don't pay anything there are

00:27:03,620 --> 00:27:09,380
some axis also here which means

00:27:05,900 --> 00:27:11,929
basically that the system became so

00:27:09,380 --> 00:27:14,780
unresponsive that the script didn't make

00:27:11,929 --> 00:27:19,340
it to stop the workload so results were

00:27:14,780 --> 00:27:22,809
unreliable what yep

00:27:19,340 --> 00:27:25,970
this happens with this steady workloads

00:27:22,809 --> 00:27:29,960
with other more complex for clothes

00:27:25,970 --> 00:27:34,970
there are some regressions like in some

00:27:29,960 --> 00:27:37,010
cases 20% around 20% of throughput loss

00:27:34,970 --> 00:27:38,990
and this is something we are working on

00:27:37,010 --> 00:27:42,220
at trying to understand exactly what

00:27:38,990 --> 00:27:42,220
we're where the problem is

00:27:42,280 --> 00:27:49,669
ok now I'll show you results on this

00:27:47,539 --> 00:27:52,340
other system then on one last system and

00:27:49,669 --> 00:27:56,720
then it will be over

00:27:52,340 --> 00:28:02,600
this system is interesting ok credits

00:27:56,720 --> 00:28:05,990
leanness wale run this test on with this

00:28:02,600 --> 00:28:08,450
poker blog with an SD card plug it into

00:28:05,990 --> 00:28:11,720
it so the test was on that SD card

00:28:08,450 --> 00:28:14,419
so again flash storage this case is

00:28:11,720 --> 00:28:17,179
interesting because bfq does is not that

00:28:14,419 --> 00:28:18,770
good I will show you this all with only

00:28:17,179 --> 00:28:20,780
one graph because with the other

00:28:18,770 --> 00:28:23,120
application results are about the same

00:28:20,780 --> 00:28:26,330
information is about the same so as you

00:28:23,120 --> 00:28:28,159
can see it takes time also with a few a

00:28:26,330 --> 00:28:31,220
lot of time with respect to the idle

00:28:28,159 --> 00:28:32,929
case as usual bfq is much better than

00:28:31,220 --> 00:28:35,230
the other schedule but anyway you have

00:28:32,929 --> 00:28:35,230
to wait

00:28:35,679 --> 00:28:42,679
fortunately the problem was somewhere

00:28:38,000 --> 00:28:48,049
else the bottleneck was not at least not

00:28:42,679 --> 00:28:53,019
only the storage but the CPU also the

00:28:48,049 --> 00:28:56,690
CPU and I will show you test with that

00:28:53,019 --> 00:29:01,190
exact SD card but this time plug it into

00:28:56,690 --> 00:29:07,279
a much faster system a laptop again test

00:29:01,190 --> 00:29:09,620
run by limbs you test run ran this test

00:29:07,279 --> 00:29:12,950
with several cards performance are about

00:29:09,620 --> 00:29:14,960
the same as before it's the same with no

00:29:12,950 --> 00:29:17,750
terminal or a libreoffice writer' so

00:29:14,960 --> 00:29:21,470
I'll show you results only with death SD

00:29:17,750 --> 00:29:24,769
card and 4x terminal tell me not and to

00:29:21,470 --> 00:29:28,909
sum up its the situation is just much

00:29:24,769 --> 00:29:32,240
worse for all the schedulers but bfq

00:29:28,909 --> 00:29:34,009
with vfq again you have about 4 Xterra

00:29:32,240 --> 00:29:36,169
of about the same startup time as if

00:29:34,009 --> 00:29:38,570
this device was either some other

00:29:36,169 --> 00:29:42,919
scheduler makes it with one of the two

00:29:38,570 --> 00:29:49,090
workloads but if we move to non terminal

00:29:42,919 --> 00:29:51,879
its disaster as you can see yeah okay so

00:29:49,090 --> 00:29:56,840
for those of you that are still awake

00:29:51,879 --> 00:30:00,289
the first part is gone and now I want to

00:29:56,840 --> 00:30:03,350
move to the the second one trying to

00:30:00,289 --> 00:30:07,039
anticipate some possible job that you

00:30:03,350 --> 00:30:09,470
may may have so first a question about

00:30:07,039 --> 00:30:12,860
the fact that actually I showed you a

00:30:09,470 --> 00:30:16,340
disaster but where is it and second

00:30:12,860 --> 00:30:19,190
question why does only be a few work so

00:30:16,340 --> 00:30:23,299
as for the first question I showed you

00:30:19,190 --> 00:30:25,879
so this terrible result but we use our

00:30:23,299 --> 00:30:29,629
personal devices and they are perfectly

00:30:25,879 --> 00:30:31,809
responsive almost always why just

00:30:29,629 --> 00:30:35,629
because storage is almost always

00:30:31,809 --> 00:30:38,990
underutilized that's the only reason but

00:30:35,629 --> 00:30:41,929
occasionally we have troubles if the

00:30:38,990 --> 00:30:44,679
system happens to be reading or writing

00:30:41,929 --> 00:30:49,520
some large file compiling

00:30:44,679 --> 00:30:50,820
doing some large compilation compilation

00:30:49,520 --> 00:30:56,710
[Music]

00:30:50,820 --> 00:31:01,149
running some updates and and so on but

00:30:56,710 --> 00:31:04,480
if your use case is above this average

00:31:01,149 --> 00:31:07,899
then you do have problems probably some

00:31:04,480 --> 00:31:09,879
of you know it because the behavior of

00:31:07,899 --> 00:31:12,580
the system changed dramatically and

00:31:09,879 --> 00:31:15,759
exactly what I showed you starts to

00:31:12,580 --> 00:31:19,929
happen it becomes in some cases almost

00:31:15,759 --> 00:31:22,389
impossible to use the system and there

00:31:19,929 --> 00:31:22,720
are no practical solution in nothing

00:31:22,389 --> 00:31:26,679
ready

00:31:22,720 --> 00:31:29,259
apart from B of Q if you have PF q and

00:31:26,679 --> 00:31:34,119
if you know that you have P of Q in your

00:31:29,259 --> 00:31:36,669
system and our perception with respect

00:31:34,119 --> 00:31:39,789
to this is usually that it's somehow

00:31:36,669 --> 00:31:43,210
normal inevitable we just wait do

00:31:39,789 --> 00:31:47,529
something else and wait for the storm to

00:31:43,210 --> 00:31:50,919
go away but it is not inevitable and

00:31:47,529 --> 00:31:54,159
some some companies have turned this

00:31:50,919 --> 00:31:58,379
into a business opportunity because they

00:31:54,159 --> 00:32:02,110
have proposed systems that guarantee

00:31:58,379 --> 00:32:04,509
absence of very low frequency of these

00:32:02,110 --> 00:32:08,039
lag issues and so give the user should

00:32:04,509 --> 00:32:10,830
give the user much better experience

00:32:08,039 --> 00:32:16,240
okay this this was for personal use

00:32:10,830 --> 00:32:19,960
about services again service is provided

00:32:16,240 --> 00:32:24,129
by good service providers don't suffer

00:32:19,960 --> 00:32:26,889
from high latencies usually but the

00:32:24,129 --> 00:32:29,679
systems that provide that implement

00:32:26,889 --> 00:32:33,100
these services are may be subject to

00:32:29,679 --> 00:32:37,600
intense i/o why everything what just

00:32:33,100 --> 00:32:40,570
because the typical solution implemented

00:32:37,600 --> 00:32:46,389
by the engineers on on the other side is

00:32:40,570 --> 00:32:50,559
over provisioning so the idea is just to

00:32:46,389 --> 00:32:54,549
guarantee that the system is always or

00:32:50,559 --> 00:32:56,990
almost always underutilized it works it

00:32:54,549 --> 00:33:00,710
has only a problem and it's

00:32:56,990 --> 00:33:04,330
very very costly because it's costly in

00:33:00,710 --> 00:33:09,260
terms of extra resources energy and

00:33:04,330 --> 00:33:11,870
ultimately money in Italy we have a

00:33:09,260 --> 00:33:16,280
saying that I found written in English

00:33:11,870 --> 00:33:19,700
something like he who is not a good

00:33:16,280 --> 00:33:22,130
brain or to have good legs so in this

00:33:19,700 --> 00:33:25,220
case this is just strength

00:33:22,130 --> 00:33:28,340
what can I say make your legs stronger

00:33:25,220 --> 00:33:32,060
or add more legs if you shouldn't have

00:33:28,340 --> 00:33:35,000
brain it in some cases you add a little

00:33:32,060 --> 00:33:37,250
bit of brain because as an alternative

00:33:35,000 --> 00:33:41,570
as an additional solution there are some

00:33:37,250 --> 00:33:43,940
a dog scheduling or tuning that improves

00:33:41,570 --> 00:33:47,120
things but usually this is very rigid

00:33:43,940 --> 00:33:49,400
and just fails if the workload changes

00:33:47,120 --> 00:33:55,340
with respect to what the solution is

00:33:49,400 --> 00:33:58,400
tightly tailored ok and the very last

00:33:55,340 --> 00:34:02,630
point why

00:33:58,400 --> 00:34:03,920
also all scheduler fail miserably only

00:34:02,630 --> 00:34:09,200
bfq survives

00:34:03,920 --> 00:34:13,720
why because bfq implements a combination

00:34:09,200 --> 00:34:17,450
of three techniques this one first it

00:34:13,720 --> 00:34:19,850
performs proportional sharing of the

00:34:17,450 --> 00:34:25,910
true boot in a rather accurate way so

00:34:19,850 --> 00:34:28,760
every process or group of processes is

00:34:25,910 --> 00:34:31,940
associated with a weight and receives a

00:34:28,760 --> 00:34:35,860
share of the true proportion to to the

00:34:31,940 --> 00:34:40,000
weight to the to its weight first second

00:34:35,860 --> 00:34:43,040
detection of DAO to privilege the

00:34:40,000 --> 00:34:45,560
limited resource here is bandwidth

00:34:43,040 --> 00:34:48,790
bandwidth is limited so if you want to

00:34:45,560 --> 00:34:51,980
guarantee that an application starts

00:34:48,790 --> 00:34:53,570
takes the same time to start as when the

00:34:51,980 --> 00:34:55,850
device was idle we have to give to that

00:34:53,570 --> 00:34:57,650
application the same true boot as when

00:34:55,850 --> 00:35:00,410
the device was either you have to

00:34:57,650 --> 00:35:02,510
privilege that application to do that

00:35:00,410 --> 00:35:05,960
you are first to discover which the

00:35:02,510 --> 00:35:10,440
application is with what is the process

00:35:05,960 --> 00:35:13,589
that must be must receive more too much

00:35:10,440 --> 00:35:15,780
trooper than the others then you have to

00:35:13,589 --> 00:35:18,710
actually guarantee a higher throughput

00:35:15,780 --> 00:35:20,790
to that process so detection and

00:35:18,710 --> 00:35:23,640
enforcing and enforcing is done

00:35:20,790 --> 00:35:30,740
basically by just raising the weight of

00:35:23,640 --> 00:35:35,579
that process 12 third bfq plugs the

00:35:30,740 --> 00:35:39,510
dispatching of I own this is also known

00:35:35,579 --> 00:35:42,780
as device ID link if the process in

00:35:39,510 --> 00:35:47,160
service has no pending uh-oh

00:35:42,780 --> 00:35:51,750
momentarily then bfq does not dispatch

00:35:47,160 --> 00:35:54,270
io from the other competing processes if

00:35:51,750 --> 00:35:58,349
you does so if the processing service

00:35:54,270 --> 00:36:02,250
does sync I own because process deducing

00:35:58,349 --> 00:36:04,589
IO are the typical victim of this of

00:36:02,250 --> 00:36:07,260
this problem because they do I owe

00:36:04,589 --> 00:36:11,280
then wait for the completion of the EO

00:36:07,260 --> 00:36:13,349
and then do new i/o so they frequently

00:36:11,280 --> 00:36:19,349
have some short time intervals during

00:36:13,349 --> 00:36:25,470
which they are deceptively idle okay so

00:36:19,349 --> 00:36:27,690
this plugin prevents low weight yo from

00:36:25,470 --> 00:36:30,900
cutting in front of high weight

00:36:27,690 --> 00:36:38,280
yo that's the idea you have a problem

00:36:30,900 --> 00:36:41,670
when if the low weight of your is how

00:36:38,280 --> 00:36:45,060
can I say a type of a yo that the rest

00:36:41,670 --> 00:36:47,250
of the system likes like sequential at

00:36:45,060 --> 00:36:48,930
your sequential greedier your exam that

00:36:47,250 --> 00:36:50,280
that's the extreme case but there are

00:36:48,930 --> 00:36:53,190
other cases in the middle

00:36:50,280 --> 00:36:56,280
in that case data your if you dispatch

00:36:53,190 --> 00:36:59,069
that your while your high weight process

00:36:56,280 --> 00:37:02,040
is idle then data you will just go on

00:36:59,069 --> 00:37:04,710
cutting in front of the IO of your high

00:37:02,040 --> 00:37:07,440
weight process just because the rest of

00:37:04,710 --> 00:37:10,470
the system wants data your to cut in

00:37:07,440 --> 00:37:16,250
front to achieve a higher true but the

00:37:10,470 --> 00:37:19,589
only way is to plug the dispatch of

00:37:16,250 --> 00:37:22,820
course exactly because of what I just

00:37:19,589 --> 00:37:25,040
said if you don't give

00:37:22,820 --> 00:37:28,520
to the storage device that sequential a

00:37:25,040 --> 00:37:31,640
yo-yo is to lower the truth that's one

00:37:28,520 --> 00:37:34,220
reason the other reason is that modern

00:37:31,640 --> 00:37:37,430
storage devices to reach peak rate want

00:37:34,220 --> 00:37:42,110
to have their internal cues constantly

00:37:37,430 --> 00:37:45,710
non empty if you plug guy up you tend to

00:37:42,110 --> 00:37:47,930
lower and the feeling of the use and so

00:37:45,710 --> 00:37:51,850
also to lower the to boot so the actual

00:37:47,930 --> 00:37:58,040
challenge here is doing this plugging

00:37:51,850 --> 00:37:59,990
for as little time as possible but I

00:37:58,040 --> 00:38:05,500
won't tell you more about this because

00:37:59,990 --> 00:38:05,500
our time is finished the so thank you

00:38:05,850 --> 00:38:14,090
[Applause]

00:38:07,510 --> 00:38:24,470
yeah if you have questions I'll try to

00:38:14,090 --> 00:38:27,620
avoid the tougher one well that easy

00:38:24,470 --> 00:38:31,490
part is just a flag that that it says

00:38:27,620 --> 00:38:35,720
it's somewhere else that that I know is

00:38:31,490 --> 00:38:39,620
flag dancing but there are cases where

00:38:35,720 --> 00:38:42,040
that flag is misleading in most cases it

00:38:39,620 --> 00:38:42,040
is not

00:38:48,130 --> 00:39:02,989
No well now that you are not I mean no

00:39:00,140 --> 00:39:05,679
not yet but yeah why not

00:39:02,989 --> 00:39:05,679
yeah absolutely

00:39:20,480 --> 00:39:33,780
no I didn't run those tests heater yeah

00:39:25,520 --> 00:39:42,810
yeah or if you want to again yes I was

00:39:33,780 --> 00:39:46,500
told to do yeah the question was to be

00:39:42,810 --> 00:39:51,000
tested with any other media besides SD

00:39:46,500 --> 00:39:53,130
and MMC you mean beside that combination

00:39:51,000 --> 00:39:56,520
or in general other mediums in general

00:39:53,130 --> 00:40:02,340
oh yeah absolutely absolutely any

00:39:56,520 --> 00:40:04,980
rotational device raids SSDs whatever I

00:40:02,340 --> 00:40:12,060
found that could store something I tried

00:40:04,980 --> 00:40:18,860
yeah yes actually the new tests are the

00:40:12,060 --> 00:40:23,130
one with MMC because the support for

00:40:18,860 --> 00:40:29,520
blocking Hugh is a new thing in the MMC

00:40:23,130 --> 00:40:33,840
subsystem yes in blocking you the new

00:40:29,520 --> 00:40:38,220
version of of the block layer is

00:40:33,840 --> 00:40:41,220
supported by the MMC subsystem that is

00:40:38,220 --> 00:40:43,260
the subsystem that endows emmc SD card

00:40:41,220 --> 00:40:48,840
and all the rest so this support is

00:40:43,260 --> 00:40:51,780
available only from 4.16 onwards that's

00:40:48,840 --> 00:40:54,450
why this test we have done this test

00:40:51,780 --> 00:40:56,840
only recent like two weeks ago three

00:40:54,450 --> 00:40:56,840
weeks ago

00:41:01,349 --> 00:41:07,789
okay thank you

00:41:04,550 --> 00:41:07,789

YouTube URL: https://www.youtube.com/watch?v=l7j1AqTZKG4


