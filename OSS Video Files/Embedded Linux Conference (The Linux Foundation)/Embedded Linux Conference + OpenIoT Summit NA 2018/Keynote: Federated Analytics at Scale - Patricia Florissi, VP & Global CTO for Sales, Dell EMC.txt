Title: Keynote: Federated Analytics at Scale - Patricia Florissi, VP & Global CTO for Sales, Dell EMC
Publication date: 2018-03-13
Playlist: Embedded Linux Conference + OpenIoT Summit NA 2018
Description: 
	Keynote: Federated Analytics at Scale - Patricia Florissi, VP & Global CTO for Sales, Dell EMC

About Patricia Florissi
Patricia Florissi is Vice President and Global Chief Technology Officer (CTO) for Sales. As Global CTO for Sales, Patricia helps define mid and long term technology strategy, representing the needs of the broader Dell EMC ecosystem in strategic initiatives. Patricia also acts as the liaison between Dell EMC and our customers and partners, to foster stronger alliances and deliver higher value.
Patricia was previously the Vice President and Global CTO for Sales at EMC prior to the merger between Dell and EMC. She also holds the honorary title of EMC Distinguished Engineer, having been nominated in October 2007.

Patricia is the creator, author, narrator, and graphical influencer of the educational video series Dell EMC Big Ideas (http://bit.ly/DellEMCBigIdeas ), on emerging technologies and trends. The Big Ideas animated videos accelerate and expand technical thought leadership in EMC using innovative learning methodologies in a fun, easy way without talking about products to both internal and external audiences. There are over 20 videos in the series, with some videos localized in 10 languages, and with a combined total count of over half a million views. Patricia also writes articles on the impact of Big Data in accelerating innovation (http://www3.weforum.org/docs/WEF_GITR_Newsletter_June_2014.pdf) to the 2014 World Economic Forum (WEF) Global IT Report Newsletters (http://www.weforum.org/issues/global-information-technology).

Patricia joined EMC in February 2005 via the System Management Arts (SMARTS) acquisition as a Distinguished Technologist in the Ionix business unit, and became the CTO for Ionix in November 2005. As CTO, Patricia was responsible for defining, and communicating the medium- to long-term vision EMC would embrace for delivering solutions to automate the management of Information Infrastructure resources. Patricia was appointed Strategic Initiative Leader for Governance, risk and Compliance (GRC) in August 2008, where she was responsible for leading the research, design, execution, and communication of EMC’s GRC vision and strategy. Patricia was appointed Americas CTO for Sales in January 2010, Americas and Europe, Middle East and Africa (EMEA) CTO in March 2011, and Global CTO for Sales in July 2012.

Before joining EMC, Patricia was the Vice President of Advanced Solutions at Smarts in White Plains, New York. In that capacity, Patricia was responsible for researching emerging technologies and for defining the strategy that Smarts would take in bringing solutions to market to address the challenges introduced by these technological advances. Patricia led the research, design, and first release of over half a dozen products. These have been driving millions of dollars in revenue and still remain in the market today.

Patricia is an EMC Distinguished Engineer, holds a Ph. D. in Computer Science from Columbia University in New York, graduated valedictorian with an MBA at the Stern Business School in New York University, and has a Master's and a Bachelor's Degree in Computer Science from the Universidade Federal de Pernambuco, in Brazil. Patricia holds multiple patents, and has published in periodicals including Computer Networks and IEEE Proceedings.

Patricia is a board member of the Columbia School of Engineering Board of Visitors. Founded in 1955, the Columbia Engineering Board of Visitors is approved by the Columbia University Trustees to “advise and assist the Trustees, the Faculty of Engineering, and the Dean in the development of the school.” Patricia is the chairman of the advisory board for the Data Science Graduate Program at Worcester Polytechnic Institute where she will advise and assist in the school’s new program. Patricia is a board member of the Brazilian/American Chamber of Commerce where she assists in fostering relations with Brazil. Additionally, Patricia is also an active member and participant of the Americas Society/Council of the Americas organization.  Patricia also serves as a mentor for several groups both inside and outside of EMC. She sits as mentor and judge for the Boston based Mass Challenge group, as well as the Boston Club for advancing women’s leaders.
Captions: 
	00:00:00,450 --> 00:00:06,210
my name is Patricia Flores let we only

00:00:03,540 --> 00:00:08,730
have half an hour and I have material

00:00:06,210 --> 00:00:11,219
for an hour and a half so number one I

00:00:08,730 --> 00:00:13,019
will bring some clarifications yes I'm

00:00:11,219 --> 00:00:14,790
Brasilia my mother tongue is Portuguese

00:00:13,019 --> 00:00:16,440
day Italian last name like everything

00:00:14,790 --> 00:00:19,740
else in my life is my husband's fault

00:00:16,440 --> 00:00:22,109
and I can prove that number two we are

00:00:19,740 --> 00:00:25,859
going to talk about a new concept so I

00:00:22,109 --> 00:00:28,789
really want to ask you to please buckle

00:00:25,859 --> 00:00:31,679
up on your seats and enjoy the ride and

00:00:28,789 --> 00:00:34,350
be with me all the time so thank you for

00:00:31,679 --> 00:00:36,600
being here we are going we are entering

00:00:34,350 --> 00:00:38,070
cognitive revolution to the road do you

00:00:36,600 --> 00:00:42,030
know when cognitive revolution one

00:00:38,070 --> 00:00:44,129
daughter was I recommend that if you

00:00:42,030 --> 00:00:47,219
don't have anything to do this weekend

00:00:44,129 --> 00:00:49,289
read the book Homo sapiens by avow and

00:00:47,219 --> 00:00:52,770
he talks about the fact that human

00:00:49,289 --> 00:00:54,809
beings actually evolved when Homo

00:00:52,770 --> 00:00:56,699
sapiens came along and the difference

00:00:54,809 --> 00:00:59,039
between Homo sapiens and the previous

00:00:56,699 --> 00:01:01,170
human beings where they didn't have the

00:00:59,039 --> 00:01:04,409
ability to abstract to think of

00:01:01,170 --> 00:01:06,960
structurally to generate patterns and so

00:01:04,409 --> 00:01:09,930
on and he gives a wonderful example how

00:01:06,960 --> 00:01:12,180
we humans actually have this ability to

00:01:09,930 --> 00:01:14,970
believe in entities that we don't see

00:01:12,180 --> 00:01:17,610
like the government the church and laws

00:01:14,970 --> 00:01:19,680
and so on so forth and he say look if

00:01:17,610 --> 00:01:22,110
you have a group of hungry gorillas and

00:01:19,680 --> 00:01:24,869
you show up with a pack of bananas and

00:01:22,110 --> 00:01:26,820
you convince the crew it's impossible to

00:01:24,869 --> 00:01:29,460
actually convince the gorillas that he

00:01:26,820 --> 00:01:32,970
should not eat now because it's Holy

00:01:29,460 --> 00:01:35,520
Friday but if he doesn't then he will

00:01:32,970 --> 00:01:38,130
have a lot of bananas and infinite

00:01:35,520 --> 00:01:40,680
number of bananas when he dies to hold

00:01:38,130 --> 00:01:45,420
the eternity but we humans are actually

00:01:40,680 --> 00:01:47,729
capable of of believing in abstract

00:01:45,420 --> 00:01:49,890
concepts and that differentiates us and

00:01:47,729 --> 00:01:51,930
that was cognitive revolution too on

00:01:49,890 --> 00:01:54,720
door oh and then I said if that was

00:01:51,930 --> 00:01:57,450
cognitive revolution one auto cognitive

00:01:54,720 --> 00:02:00,990
revolution Todaro is what is happening

00:01:57,450 --> 00:02:03,780
with artificial intelligence evolution

00:02:00,990 --> 00:02:05,729
so you all know about AI machine

00:02:03,780 --> 00:02:07,500
learning many of you have not learned

00:02:05,729 --> 00:02:10,739
about representation learn it was a

00:02:07,500 --> 00:02:11,849
marketing staff the area that never took

00:02:10,739 --> 00:02:13,859
off and now

00:02:11,849 --> 00:02:17,269
we are in deep learning and why is deep

00:02:13,859 --> 00:02:21,420
learning different because before in the

00:02:17,269 --> 00:02:23,129
first area era of AI you actually

00:02:21,420 --> 00:02:25,200
focused on teaching the computer

00:02:23,129 --> 00:02:27,389
something that was very simple for you

00:02:25,200 --> 00:02:30,000
to do like playing chess but you

00:02:27,389 --> 00:02:32,310
couldn't is scale to actually analyze

00:02:30,000 --> 00:02:35,069
all the possibilities and all the moves

00:02:32,310 --> 00:02:38,250
ahead of you now we are trying to let

00:02:35,069 --> 00:02:40,530
the computer learn how to do things that

00:02:38,250 --> 00:02:42,450
are very simple for us we take it for

00:02:40,530 --> 00:02:45,180
great like recognizing the face of our

00:02:42,450 --> 00:02:48,629
parents and our siblings but we cannot

00:02:45,180 --> 00:02:49,949
express easily rules how we actually do

00:02:48,629 --> 00:02:52,379
that to be honest with you we don't

00:02:49,949 --> 00:02:55,439
understand ourselves we take it for

00:02:52,379 --> 00:02:58,409
granted so now what we are doing is that

00:02:55,439 --> 00:03:00,599
we are giving the computer a lot of data

00:02:58,409 --> 00:03:03,150
and that is generating what is called

00:03:00,599 --> 00:03:07,349
they are reasonable effectiveness of

00:03:03,150 --> 00:03:09,750
data and what you have here is a graph a

00:03:07,349 --> 00:03:12,659
very important graph that shows for

00:03:09,750 --> 00:03:14,760
algorithms or for different analytics

00:03:12,659 --> 00:03:17,939
algorithms to accomplish the same task

00:03:14,760 --> 00:03:20,340
and what you have in the x-axis is

00:03:17,939 --> 00:03:22,409
actually the number of words that you

00:03:20,340 --> 00:03:25,199
are feeding the algorithm with measured

00:03:22,409 --> 00:03:27,479
in millions and all the Y acts you

00:03:25,199 --> 00:03:29,400
actually have the precision of the

00:03:27,479 --> 00:03:32,400
algorithm the accuracy if you have any

00:03:29,400 --> 00:03:35,099
of you have used Lexi Lexia Alexei

00:03:32,400 --> 00:03:37,169
whatever that is that actually has

00:03:35,099 --> 00:03:38,729
interrupted the numerous phone calls of

00:03:37,169 --> 00:03:41,370
mine by the way I don't know what

00:03:38,729 --> 00:03:43,979
triggers that thing but then you have

00:03:41,370 --> 00:03:46,500
the accuracy measured you are looking

00:03:43,979 --> 00:03:49,049
for precision in translations also forth

00:03:46,500 --> 00:03:52,109
you observed from this graph two

00:03:49,049 --> 00:03:54,540
important lessons it doesn't matter how

00:03:52,109 --> 00:03:58,799
good the algorithm is they only achieve

00:03:54,540 --> 00:04:01,109
95% accuracy when they crossed a hundred

00:03:58,799 --> 00:04:04,469
million number of words that's number

00:04:01,109 --> 00:04:07,349
one number two after they did cross the

00:04:04,469 --> 00:04:09,599
number 100 million of words they

00:04:07,349 --> 00:04:12,180
performance between them the difference

00:04:09,599 --> 00:04:14,879
is negligible yes there is a difference

00:04:12,180 --> 00:04:18,449
but sometimes one performs better than

00:04:14,879 --> 00:04:20,170
the other so on so forth and even though

00:04:18,449 --> 00:04:22,180
there are many

00:04:20,170 --> 00:04:25,870
learning algorithms and there will be a

00:04:22,180 --> 00:04:28,840
quiz afterwards on these names what

00:04:25,870 --> 00:04:32,080
there is one thing that they do share in

00:04:28,840 --> 00:04:35,200
common they perform better the more data

00:04:32,080 --> 00:04:37,450
they analyze and then you actually see

00:04:35,200 --> 00:04:40,780
the emergency what I hope it becomes

00:04:37,450 --> 00:04:44,410
Moore's law for data that's very very

00:04:40,780 --> 00:04:48,220
deep simple algorithms with lots of data

00:04:44,410 --> 00:04:52,930
will outperform sophisticated algorithms

00:04:48,220 --> 00:04:54,190
with less data so far so good okay now

00:04:52,930 --> 00:04:56,740
why do you care

00:04:54,190 --> 00:04:59,200
you actually care because with the

00:04:56,740 --> 00:05:01,090
number of IOT devices estimated to be

00:04:59,200 --> 00:05:03,130
one trillion by the end of this century

00:05:01,090 --> 00:05:06,550
and I'm very glad I'm not alive to clean

00:05:03,130 --> 00:05:09,670
up that mess you actually have all the

00:05:06,550 --> 00:05:12,760
data in the world that you can think of

00:05:09,670 --> 00:05:16,300
the problem now then becomes how you

00:05:12,760 --> 00:05:20,020
actually scale now we need to design

00:05:16,300 --> 00:05:22,530
today for the future not for the past

00:05:20,020 --> 00:05:24,850
now if you actually look at what

00:05:22,530 --> 00:05:28,150
historically has happened in years that

00:05:24,850 --> 00:05:32,080
has a hypnotic effect what happens is

00:05:28,150 --> 00:05:33,820
that over time we moved from centralized

00:05:32,080 --> 00:05:35,860
to distributed centralized or

00:05:33,820 --> 00:05:39,040
distributed when we went to the cloud we

00:05:35,860 --> 00:05:40,720
thought hey we are done we are done we

00:05:39,040 --> 00:05:42,910
can send something everything to this

00:05:40,720 --> 00:05:46,420
magic place and boom there we get a

00:05:42,910 --> 00:05:49,060
result and then came a IOT now let's

00:05:46,420 --> 00:05:51,580
take a look at what spectrum or what

00:05:49,060 --> 00:05:55,180
side of the spectrum should we take with

00:05:51,580 --> 00:05:57,310
IOT if we go all centralized I will give

00:05:55,180 --> 00:05:59,920
you five reasons why that will be

00:05:57,310 --> 00:06:02,550
challenging reason number one with IOT

00:05:59,920 --> 00:06:05,170
data is inherently distributed at

00:06:02,550 --> 00:06:07,840
hard-to-reach places remember we put

00:06:05,170 --> 00:06:10,750
sensors to begin with on people that

00:06:07,840 --> 00:06:12,940
human place that humans could reach it

00:06:10,750 --> 00:06:15,670
was all born you could say we for you a

00:06:12,940 --> 00:06:17,850
guess and we know he gets plataform you

00:06:15,670 --> 00:06:20,920
oh you want to put as a man and a dog

00:06:17,850 --> 00:06:23,770
why do you need the dog to make sure

00:06:20,920 --> 00:06:26,530
that no humans touches the buttons why

00:06:23,770 --> 00:06:29,470
do you need the man to feed the dog so

00:06:26,530 --> 00:06:31,729
now we are saying hey we put sensors in

00:06:29,470 --> 00:06:34,999
those hard-to-reach places but

00:06:31,729 --> 00:06:37,550
hey now we can bring them over well

00:06:34,999 --> 00:06:39,710
welcome 5g but we are not there yet

00:06:37,550 --> 00:06:42,889
the second problem is regulatory

00:06:39,710 --> 00:06:45,620
compliance that's creating really really

00:06:42,889 --> 00:06:48,110
hard boundaries on data movement across

00:06:45,620 --> 00:06:50,300
geo political and social boundary then

00:06:48,110 --> 00:06:53,930
you have the effect of multi-cloud the

00:06:50,300 --> 00:06:56,479
techno silo of IOT the attack villain of

00:06:53,930 --> 00:06:58,400
IOT is the silo now you have half of

00:06:56,479 --> 00:07:00,259
your data in Salesforce another half on

00:06:58,400 --> 00:07:02,569
workday another half on Amazon and other

00:07:00,259 --> 00:07:04,580
half on Microsoft Azure and another half

00:07:02,569 --> 00:07:06,319
from virtual stream and where is the

00:07:04,580 --> 00:07:08,479
data how are you going to bring this

00:07:06,319 --> 00:07:10,639
data to a centralized place then you

00:07:08,479 --> 00:07:13,729
have your traditional data systems like

00:07:10,639 --> 00:07:16,279
your ERP us APU Oracle databases your

00:07:13,729 --> 00:07:19,580
data warehouses that is locking in data

00:07:16,279 --> 00:07:21,529
but also has a lot of value and then

00:07:19,580 --> 00:07:24,110
last but not least you have been duyst

00:07:21,529 --> 00:07:27,199
constraints in terms of speed capacity

00:07:24,110 --> 00:07:29,779
and real time Ness of actually getting

00:07:27,199 --> 00:07:31,669
the data where you need to analyze so

00:07:29,779 --> 00:07:34,370
that's if you want to centralize and

00:07:31,669 --> 00:07:37,159
what if you want to actually distribute

00:07:34,370 --> 00:07:39,830
this is how your world looks like you

00:07:37,159 --> 00:07:42,770
actually have now if you follow mist is

00:07:39,830 --> 00:07:45,319
not only the edge to form the edge to

00:07:42,770 --> 00:07:47,479
core to cloud but mist also came

00:07:45,319 --> 00:07:50,029
recently with the notion of the mist and

00:07:47,479 --> 00:07:51,770
the notion of the fog the mist is close

00:07:50,029 --> 00:07:54,050
to the edge the fog is close to the

00:07:51,770 --> 00:07:56,870
cloud and I'm glad the weather system

00:07:54,050 --> 00:08:00,110
doesn't have many more terms that we

00:07:56,870 --> 00:08:02,240
could leverage but in any case this is

00:08:00,110 --> 00:08:04,639
how the architecture looks like so

00:08:02,240 --> 00:08:06,620
you're going to distributed where where

00:08:04,639 --> 00:08:08,360
you're going to put the analysis of the

00:08:06,620 --> 00:08:10,219
data and if you are actually going to

00:08:08,360 --> 00:08:13,490
put the analysis of the data at the edge

00:08:10,219 --> 00:08:15,740
you get my optic view remember the whole

00:08:13,490 --> 00:08:18,949
point of deep learning of the cognitive

00:08:15,740 --> 00:08:21,110
revolution Todaro is what simple

00:08:18,949 --> 00:08:23,839
algorithmic modes of data is going to

00:08:21,110 --> 00:08:26,360
outperform sophisticated algorithms with

00:08:23,839 --> 00:08:28,639
less data so I don't care what a Yoda

00:08:26,360 --> 00:08:30,589
plan you put at the edge if you don't

00:08:28,639 --> 00:08:34,519
fit enough data won't be able to draw

00:08:30,589 --> 00:08:36,440
patterns correct so now I'm all of my

00:08:34,519 --> 00:08:39,829
walk is inspired in the shoulders of

00:08:36,440 --> 00:08:42,260
bright women and some men but I will

00:08:39,829 --> 00:08:43,800
leave you to actually research these is

00:08:42,260 --> 00:08:46,559
Pythagoras wife

00:08:43,800 --> 00:08:48,540
and she came up with the notion of a

00:08:46,559 --> 00:08:51,300
golden mean and I take that to the heart

00:08:48,540 --> 00:08:53,939
and the golden mean is the ideal median

00:08:51,300 --> 00:08:57,300
between two extremes so in the area of

00:08:53,939 --> 00:08:59,999
IOT perhaps our answer is not going

00:08:57,300 --> 00:09:02,489
fully centralized perhaps our answer is

00:08:59,999 --> 00:09:05,999
not going to fully distributed but

00:09:02,489 --> 00:09:07,889
actually is to find the golden mean that

00:09:05,999 --> 00:09:10,559
will give you the scale that you need

00:09:07,889 --> 00:09:12,449
without requiring data to be centralized

00:09:10,559 --> 00:09:15,929
and without the side effects or full

00:09:12,449 --> 00:09:18,720
anarchy of a fully distributed data so

00:09:15,929 --> 00:09:21,360
let me tell you if you were to take an

00:09:18,720 --> 00:09:24,449
x-ray of your edge for cloud core

00:09:21,360 --> 00:09:27,569
continuum then you actually would be

00:09:24,449 --> 00:09:30,209
able to detect what we call pockets of

00:09:27,569 --> 00:09:33,600
data these are represented by the Globes

00:09:30,209 --> 00:09:36,179
here and we call them data zones in a

00:09:33,600 --> 00:09:38,999
data zone is a logical boundary where

00:09:36,179 --> 00:09:42,540
data resides and together with data you

00:09:38,999 --> 00:09:45,089
have processing capacity so you the data

00:09:42,540 --> 00:09:47,309
is being collected in that neighborhood

00:09:45,089 --> 00:09:49,980
the data can be stored in that

00:09:47,309 --> 00:09:52,170
neighborhood the data can be analyzed in

00:09:49,980 --> 00:09:54,869
that neighborhood but you really have

00:09:52,170 --> 00:09:57,660
severe constraints on how to move the

00:09:54,869 --> 00:09:57,899
data outside the data zone so far so

00:09:57,660 --> 00:10:02,279
good

00:09:57,899 --> 00:10:05,549
and our goal is to actually enable

00:10:02,279 --> 00:10:08,429
another I'm sorry our goal is to

00:10:05,549 --> 00:10:12,209
actually enable analytics of data in

00:10:08,429 --> 00:10:15,329
place in parallel at worldwide scale

00:10:12,209 --> 00:10:18,329
without giving data scientists they are

00:10:15,329 --> 00:10:21,929
extraction that there is a virtual

00:10:18,329 --> 00:10:25,230
fabric connecting all of them without

00:10:21,929 --> 00:10:28,769
making we make the data scientist aware

00:10:25,230 --> 00:10:32,519
that the data is not centralized but yet

00:10:28,769 --> 00:10:35,610
we are locked away from him or her all

00:10:32,519 --> 00:10:38,399
the details and hard walk around heavy

00:10:35,610 --> 00:10:41,129
data analyzed in a distributed fashion

00:10:38,399 --> 00:10:43,589
so let me give you example suppose that

00:10:41,129 --> 00:10:46,199
you want to calculate a histogram by the

00:10:43,589 --> 00:10:48,600
way we do do deep learning but it's much

00:10:46,199 --> 00:10:50,939
harder to explain half an hour how you

00:10:48,600 --> 00:10:53,060
do deep learning in a federated way so

00:10:50,939 --> 00:10:55,850
let's stick with the histogram

00:10:53,060 --> 00:10:57,980
the histogram is a graph where you have

00:10:55,850 --> 00:11:00,110
these slices or the columns and you want

00:10:57,980 --> 00:11:02,780
to count the number of elements in each

00:11:00,110 --> 00:11:07,580
column and you want to calculate let's

00:11:02,780 --> 00:11:11,030
say the worldwide histogram on anything

00:11:07,580 --> 00:11:13,640
it can be on the vibration sensor

00:11:11,030 --> 00:11:16,040
measurements that are being taken your

00:11:13,640 --> 00:11:18,920
factory across factories across the

00:11:16,040 --> 00:11:21,470
world it can be actually the age of

00:11:18,920 --> 00:11:24,080
survival for patients with diabetes a

00:11:21,470 --> 00:11:26,360
true use case we have worked on that you

00:11:24,080 --> 00:11:28,970
actually are taking a particular type of

00:11:26,360 --> 00:11:31,910
insulin and you want to know how many of

00:11:28,970 --> 00:11:35,300
those are actually taking a particular

00:11:31,910 --> 00:11:37,550
interval or dosage and one way in

00:11:35,300 --> 00:11:40,670
decentralised you have to bring all the

00:11:37,550 --> 00:11:43,100
data to a central location correct in

00:11:40,670 --> 00:11:46,130
the federated analytics what we do is

00:11:43,100 --> 00:11:49,910
that we orchestrate the execution of

00:11:46,130 --> 00:11:52,610
moco histograms in place close to where

00:11:49,910 --> 00:11:54,350
the data is so each of the data zones of

00:11:52,610 --> 00:11:58,070
interest are going to calculate the

00:11:54,350 --> 00:12:00,830
histograms and they will share only the

00:11:58,070 --> 00:12:04,250
histograms with the initiating node and

00:12:00,830 --> 00:12:06,770
we will then be able to calculate after

00:12:04,250 --> 00:12:09,380
we receive all of them a global

00:12:06,770 --> 00:12:11,510
histogram now again I'm giving you a

00:12:09,380 --> 00:12:13,160
very simple example please believe me if

00:12:11,510 --> 00:12:15,110
you want let's go and have a

00:12:13,160 --> 00:12:17,690
conversation of how you do regression

00:12:15,110 --> 00:12:20,180
analysis and other kind of analytics in

00:12:17,690 --> 00:12:23,120
that fashion now I want you to think

00:12:20,180 --> 00:12:26,780
about two things that just happened the

00:12:23,120 --> 00:12:29,300
first one we reduced a number of data

00:12:26,780 --> 00:12:32,030
space that could be infinite in the

00:12:29,300 --> 00:12:34,370
trillions billions of entries that you

00:12:32,030 --> 00:12:37,310
could have and because all we want to

00:12:34,370 --> 00:12:40,120
calculate as a histogram the number of

00:12:37,310 --> 00:12:45,620
samples that are actually sent is

00:12:40,120 --> 00:12:47,570
actually finite it depends on the number

00:12:45,620 --> 00:12:50,240
of it slices that you want for the

00:12:47,570 --> 00:12:52,880
histogram and all you want is a count of

00:12:50,240 --> 00:12:54,980
course we actually have what we call it

00:12:52,880 --> 00:12:57,830
the federated analytics wander all

00:12:54,980 --> 00:13:00,410
packaged 101 package and you actually

00:12:57,830 --> 00:13:03,440
get me max averages standard deviation

00:13:00,410 --> 00:13:05,330
percent I all the good stuff but in one

00:13:03,440 --> 00:13:06,710
shot and you actually have more data

00:13:05,330 --> 00:13:09,920
samples but it is

00:13:06,710 --> 00:13:12,590
to you finite so you conserve bandwidth

00:13:09,920 --> 00:13:14,900
you don't waste number two because the

00:13:12,590 --> 00:13:17,630
amount of data is very very short can

00:13:14,900 --> 00:13:20,420
you actually do analytics in place do

00:13:17,630 --> 00:13:23,030
you get the time to learn or the time to

00:13:20,420 --> 00:13:25,670
insight as much shorter because you only

00:13:23,030 --> 00:13:28,640
need to transmit a short amount of data

00:13:25,670 --> 00:13:31,850
and number three which I think it's a

00:13:28,640 --> 00:13:34,580
very very positive residue is the fact

00:13:31,850 --> 00:13:36,920
that is privacy preserving it has

00:13:34,580 --> 00:13:39,530
impossible to reverse-engineer the

00:13:36,920 --> 00:13:42,140
individual values if all you have is a

00:13:39,530 --> 00:13:44,930
histogram of course we put provision in

00:13:42,140 --> 00:13:47,510
the algorithm that if a site only has

00:13:44,930 --> 00:13:49,370
five samples or less we don't let that

00:13:47,510 --> 00:13:51,530
site participate in the federated

00:13:49,370 --> 00:13:54,710
analytics because you could reverse

00:13:51,530 --> 00:13:56,690
engineer so this is what we call we

00:13:54,710 --> 00:13:58,910
define as federated analytics is

00:13:56,690 --> 00:14:02,270
analytics in place close to the data

00:13:58,910 --> 00:14:04,220
source where you analyze data very very

00:14:02,270 --> 00:14:06,920
near to the place of collection in

00:14:04,220 --> 00:14:09,800
near-real-time and you share the results

00:14:06,920 --> 00:14:12,080
and you let those results be merged so

00:14:09,800 --> 00:14:14,930
why is this challenging couldn't we be

00:14:12,080 --> 00:14:18,260
done yesterday well I have to tell you

00:14:14,930 --> 00:14:21,140
if I were smart enough we would have

00:14:18,260 --> 00:14:23,330
started from the right to the left but

00:14:21,140 --> 00:14:25,640
we started from the left and then we saw

00:14:23,330 --> 00:14:28,790
that where we had started was the easy

00:14:25,640 --> 00:14:30,770
place and then we started facing other

00:14:28,790 --> 00:14:34,100
challenges that I want to share with you

00:14:30,770 --> 00:14:35,840
there are four components on federated

00:14:34,100 --> 00:14:38,630
analytics that you need to take into

00:14:35,840 --> 00:14:41,330
consideration the first one where we

00:14:38,630 --> 00:14:44,570
start was how you actually orchestrate

00:14:41,330 --> 00:14:46,490
analytics how you push code close to the

00:14:44,570 --> 00:14:48,740
data source how you push code to the

00:14:46,490 --> 00:14:51,830
Gateway and how do you actually collect

00:14:48,740 --> 00:14:54,410
the results especially in scale where

00:14:51,830 --> 00:14:57,410
the numbers actually grow but the real

00:14:54,410 --> 00:15:01,540
challenge number one is how you actually

00:14:57,410 --> 00:15:04,820
ensure trust traceability repeatability

00:15:01,540 --> 00:15:07,130
transparency in that process one of the

00:15:04,820 --> 00:15:09,350
biggest challenges in IOT is security

00:15:07,130 --> 00:15:11,540
how can you actually validate there is a

00:15:09,350 --> 00:15:14,450
legitimate source how can you actually

00:15:11,540 --> 00:15:15,870
validate that you actually got analytics

00:15:14,450 --> 00:15:19,260
that was done on real

00:15:15,870 --> 00:15:21,060
data and on factual data help you trash

00:15:19,260 --> 00:15:24,240
the sauce how do you know that the data

00:15:21,060 --> 00:15:27,000
was not papered along the way from the

00:15:24,240 --> 00:15:29,700
edge to the core to the cloud the second

00:15:27,000 --> 00:15:30,930
point that we actually really didn't

00:15:29,700 --> 00:15:33,839
know what you were getting ourselves

00:15:30,930 --> 00:15:36,390
into was the fact that when you

00:15:33,839 --> 00:15:39,150
calculate in a federated manner it

00:15:36,390 --> 00:15:40,529
completely changes data science so let's

00:15:39,150 --> 00:15:43,140
say you want to calculate the average

00:15:40,529 --> 00:15:44,850
you cannot ask each data zone to

00:15:43,140 --> 00:15:46,710
calculate the average and then you

00:15:44,850 --> 00:15:49,230
calculate the average of average because

00:15:46,710 --> 00:15:51,330
any five-year-old will tell you the

00:15:49,230 --> 00:15:53,580
average of average has no significant

00:15:51,330 --> 00:15:58,200
value and the average of average is not

00:15:53,580 --> 00:16:00,990
a global beverages Kitsch site to send

00:15:58,200 --> 00:16:03,240
you the sum and the number of items and

00:16:00,990 --> 00:16:05,070
then you calculate the sum of the sum

00:16:03,240 --> 00:16:06,510
and the sum of the number of items and

00:16:05,070 --> 00:16:09,230
then you calculate the global average

00:16:06,510 --> 00:16:12,390
right in an average that is very

00:16:09,230 --> 00:16:14,670
interesting and easy try to do that to

00:16:12,390 --> 00:16:16,980
calculate the medium or the percentile

00:16:14,670 --> 00:16:20,510
and then we found that that I actually

00:16:16,980 --> 00:16:23,880
there is a group of scholars working on

00:16:20,510 --> 00:16:26,010
privates preserving our it algorithms

00:16:23,880 --> 00:16:28,440
which I call data science to doto

00:16:26,010 --> 00:16:31,589
when data science one daughter was not

00:16:28,440 --> 00:16:34,170
even at age of procreation yet but we

00:16:31,589 --> 00:16:37,800
actually have to start thinking in a

00:16:34,170 --> 00:16:40,440
totally different way what if I want to

00:16:37,800 --> 00:16:42,750
do deep learning when all the data is

00:16:40,440 --> 00:16:46,230
not in a central location how I break

00:16:42,750 --> 00:16:48,750
the analytics process completely into

00:16:46,230 --> 00:16:51,240
smaller steps where some steps are done

00:16:48,750 --> 00:16:54,660
at the edge others in the path and

00:16:51,240 --> 00:16:56,970
others where the destination is and then

00:16:54,660 --> 00:16:58,800
last but not least which for me was one

00:16:56,970 --> 00:17:02,420
of the most interesting findings is that

00:16:58,800 --> 00:17:05,579
if we actually stop to think what is the

00:17:02,420 --> 00:17:08,100
abstraction that we computer scientists

00:17:05,579 --> 00:17:10,650
have actually used over the last three

00:17:08,100 --> 00:17:15,329
or four decades to actually locate

00:17:10,650 --> 00:17:18,120
address and access data is fire right we

00:17:15,329 --> 00:17:20,250
based everything in say you give me the

00:17:18,120 --> 00:17:22,410
name of a database or you give me the

00:17:20,250 --> 00:17:23,790
name of fire system give me the name of

00:17:22,410 --> 00:17:27,120
files and then

00:17:23,790 --> 00:17:30,420
you ingest this data in IOT when you

00:17:27,120 --> 00:17:33,570
have one trillion one trillion devices

00:17:30,420 --> 00:17:37,710
how are you going to manage one trillion

00:17:33,570 --> 00:17:39,780
names of files and these sensors are

00:17:37,710 --> 00:17:41,880
also a femoral you have a sensor here

00:17:39,780 --> 00:17:44,370
the sensor brakes you put another one is

00:17:41,880 --> 00:17:47,040
this a new file name it will put a new

00:17:44,370 --> 00:17:49,770
brand is this a new file name and so

00:17:47,040 --> 00:17:54,270
forth so these are the four things that

00:17:49,770 --> 00:17:57,290
we actually set out to investigate how

00:17:54,270 --> 00:17:59,840
you distribute computing how you

00:17:57,290 --> 00:18:02,850
federated data addressing how you

00:17:59,840 --> 00:18:06,570
federated analytics and learning and how

00:18:02,850 --> 00:18:09,690
you ensure transparency and this is the

00:18:06,570 --> 00:18:12,270
name of the project worldwide hood if

00:18:09,690 --> 00:18:15,330
analytics at the beginning was

00:18:12,270 --> 00:18:17,250
represented by Hadoop in one her dupe is

00:18:15,330 --> 00:18:19,380
one elephant if we want to have a

00:18:17,250 --> 00:18:22,710
collection you end up with a herd and

00:18:19,380 --> 00:18:26,190
that's where the name got started is a

00:18:22,710 --> 00:18:29,040
worldwide hood and worldwide herd

00:18:26,190 --> 00:18:31,860
addresses those four in the following

00:18:29,040 --> 00:18:34,410
way number one we integrate with

00:18:31,860 --> 00:18:37,040
blockchain so that gives us trust

00:18:34,410 --> 00:18:40,350
transparency traceability repeatability

00:18:37,040 --> 00:18:42,630
every time a data zone actually does a

00:18:40,350 --> 00:18:46,230
transaction on behalf of a federated

00:18:42,630 --> 00:18:49,380
analytics it enters an entry in a ledger

00:18:46,230 --> 00:18:52,410
number two we actually make provisions

00:18:49,380 --> 00:18:57,210
in the framework so that analytics can

00:18:52,410 --> 00:18:59,400
be broken down into iterative parts that

00:18:57,210 --> 00:19:01,230
are done at the edge and at the cloud at

00:18:59,400 --> 00:19:03,660
the edge and at the cloud so that

00:19:01,230 --> 00:19:07,260
learning gets into a continuous learning

00:19:03,660 --> 00:19:10,110
mode we address the issue of file names

00:19:07,260 --> 00:19:12,720
and fire abstractions by actually

00:19:10,110 --> 00:19:14,700
creating a metadata layer and I'm going

00:19:12,720 --> 00:19:17,580
to talk a little bit about later how we

00:19:14,700 --> 00:19:21,350
integrate that with our own gateways and

00:19:17,580 --> 00:19:23,820
HX foundry because we are giving data

00:19:21,350 --> 00:19:26,250
scientists for the first time the

00:19:23,820 --> 00:19:29,490
ability to say I want to do this

00:19:26,250 --> 00:19:32,190
computation on all the measurements of

00:19:29,490 --> 00:19:36,290
vibration sensors I want to do this

00:19:32,190 --> 00:19:39,480
computation on all the measurements of

00:19:36,290 --> 00:19:42,210
genomic data and the way that it

00:19:39,480 --> 00:19:45,330
actually works is that each data zone

00:19:42,210 --> 00:19:48,090
does a local search which is completely

00:19:45,330 --> 00:19:51,540
self-contained nobody has outside has

00:19:48,090 --> 00:19:54,420
visibility decides which data sources

00:19:51,540 --> 00:19:56,880
should be eligible to participate on a

00:19:54,420 --> 00:20:00,240
federated analytics from some trusted

00:19:56,880 --> 00:20:04,080
sources attaches metadata tag to those

00:20:00,240 --> 00:20:07,679
data sources and when data scientist

00:20:04,080 --> 00:20:10,320
says I want to actually calculate these

00:20:07,679 --> 00:20:13,580
on a vibration sensor measure we push

00:20:10,320 --> 00:20:16,200
computation to all of the gateways and

00:20:13,580 --> 00:20:18,690
when the computation gets into the

00:20:16,200 --> 00:20:22,380
Gateway it connects to Ajax foundry or

00:20:18,690 --> 00:20:24,920
any other system that already obstructs

00:20:22,380 --> 00:20:28,470
the data source into some higher-level

00:20:24,920 --> 00:20:30,540
type of information and then we actually

00:20:28,470 --> 00:20:33,030
connect to that data source and do the

00:20:30,540 --> 00:20:36,059
calculation and last but not least we

00:20:33,030 --> 00:20:37,920
actually dell technologies one of the

00:20:36,059 --> 00:20:41,220
members of the dell technologies family

00:20:37,920 --> 00:20:44,130
as vmware and we have extended vmware we

00:20:41,220 --> 00:20:47,190
realize platform so now you can deploy

00:20:44,130 --> 00:20:49,580
containers anywhere in the world it can

00:20:47,190 --> 00:20:54,120
be docker containers it can be vmware

00:20:49,580 --> 00:20:56,880
containers at any place in the world i

00:20:54,120 --> 00:20:59,700
would like to actually talk a little bit

00:20:56,880 --> 00:21:02,850
about a couple of things why are your

00:20:59,700 --> 00:21:06,390
tia is unique why do we have to think

00:21:02,850 --> 00:21:09,510
different about iot and I come up with

00:21:06,390 --> 00:21:13,500
what I call the three SS they scale the

00:21:09,510 --> 00:21:17,190
system of systems and they streaming the

00:21:13,500 --> 00:21:20,880
first aspect the system they scale is

00:21:17,190 --> 00:21:24,210
based on the symbiotic relationship this

00:21:20,880 --> 00:21:27,300
positive feedback that exists is between

00:21:24,210 --> 00:21:30,330
IOT the ability to scale analyze and

00:21:27,300 --> 00:21:33,780
derive value you actually start with

00:21:30,330 --> 00:21:36,330
some very very small sensors and then

00:21:33,780 --> 00:21:38,730
you analyze the data you demonstrate

00:21:36,330 --> 00:21:41,760
value when you demonstrate value

00:21:38,730 --> 00:21:43,800
generate revenue generate return on

00:21:41,760 --> 00:21:45,990
investment which will allow you to

00:21:43,800 --> 00:21:48,540
increase the number of sensors which

00:21:45,990 --> 00:21:49,330
will increase the scale and as you

00:21:48,540 --> 00:21:51,400
escape

00:21:49,330 --> 00:21:53,650
you'll be able to do more defense than a

00:21:51,400 --> 00:21:56,710
latex to the type that you'll be able to

00:21:53,650 --> 00:21:59,650
truly do pattern recognition and deep

00:21:56,710 --> 00:22:03,340
learning with data that is generated at

00:21:59,650 --> 00:22:05,770
the edge so we need it's our duty your

00:22:03,340 --> 00:22:09,490
duty and - computer scientists to

00:22:05,770 --> 00:22:12,630
actually accelerate this journey we must

00:22:09,490 --> 00:22:15,790
accelerate this feedback loop to create

00:22:12,630 --> 00:22:18,310
a scale that is required for deep

00:22:15,790 --> 00:22:20,740
learning the second problem is the

00:22:18,310 --> 00:22:23,620
second characteristic I wouldn't say a

00:22:20,740 --> 00:22:26,350
problem of IOT is this notion of system

00:22:23,620 --> 00:22:30,420
of systems that we cannot think of IOT

00:22:26,350 --> 00:22:34,840
at the sensor level why because that

00:22:30,420 --> 00:22:37,510
absolute value of the data is absolutely

00:22:34,840 --> 00:22:40,300
irrelevant in my humble opinion that

00:22:37,510 --> 00:22:43,090
data needs context needs business

00:22:40,300 --> 00:22:45,850
society needs to have an ecosystem of

00:22:43,090 --> 00:22:49,000
other datas that make the other data

00:22:45,850 --> 00:22:52,540
that makes that data relevant is the

00:22:49,000 --> 00:22:55,060
idea that we will need to analyze IOT

00:22:52,540 --> 00:22:58,090
vibration sensor data in relation to

00:22:55,060 --> 00:23:00,520
weather system data in relation to the

00:22:58,090 --> 00:23:02,830
business data in relation to the supply

00:23:00,520 --> 00:23:05,650
chain and the performance of whatever

00:23:02,830 --> 00:23:08,890
device you're building as it is on the

00:23:05,650 --> 00:23:12,070
floor so we have to learn how we create

00:23:08,890 --> 00:23:14,800
a system an ecosystem where data can be

00:23:12,070 --> 00:23:16,630
analyzed by multiple parties so the

00:23:14,800 --> 00:23:19,570
whole concept of this federated

00:23:16,630 --> 00:23:23,110
analytics is that you make your data

00:23:19,570 --> 00:23:26,470
available for analysis by other entities

00:23:23,110 --> 00:23:29,830
as long as you can also analyze other

00:23:26,470 --> 00:23:32,380
entities data and actually derive

00:23:29,830 --> 00:23:34,840
insight so please I think different and

00:23:32,380 --> 00:23:37,510
the last but not least is the extremely

00:23:34,840 --> 00:23:41,140
nature of the data we are not talking

00:23:37,510 --> 00:23:47,140
about discrete data we come I come I

00:23:41,140 --> 00:23:49,000
part but I punched cars and I used the

00:23:47,140 --> 00:23:50,500
mainframe so I'm fully qualified to say

00:23:49,000 --> 00:23:53,950
that I was from the previous generation

00:23:50,500 --> 00:23:56,980
not a cloud native I have to tell you I

00:23:53,950 --> 00:23:59,290
was a poor native where the data center

00:23:56,980 --> 00:24:01,870
was across the pond for security reasons

00:23:59,290 --> 00:24:04,450
but you

00:24:01,870 --> 00:24:07,300
have the extreme nature of the data that

00:24:04,450 --> 00:24:10,960
is something that we need you need to

00:24:07,300 --> 00:24:13,510
learn IOT you need to think different

00:24:10,960 --> 00:24:16,150
there is an IOT evolution that is this

00:24:13,510 --> 00:24:18,460
scale there is this dreaming alleged but

00:24:16,150 --> 00:24:20,770
not least the system of systems so how

00:24:18,460 --> 00:24:23,740
do we address all of that with worldwide

00:24:20,770 --> 00:24:25,780
herd so first of all we consider that

00:24:23,740 --> 00:24:29,320
these data zones are connected to

00:24:25,780 --> 00:24:33,130
gateways one gateway can be a data Zone

00:24:29,320 --> 00:24:35,590
one gateway can also be an aggregator an

00:24:33,130 --> 00:24:38,080
appliance that is connected to multiple

00:24:35,590 --> 00:24:41,980
data zones where each data zone is a

00:24:38,080 --> 00:24:45,760
gateway you also have the ability of

00:24:41,980 --> 00:24:49,270
actually data flowing as streams so one

00:24:45,760 --> 00:24:52,720
important thing here is that if we had

00:24:49,270 --> 00:24:56,080
the intention as data comes in a stream

00:24:52,720 --> 00:24:59,200
fashion to just propagate the stream all

00:24:56,080 --> 00:25:02,820
the way out we couldn't scale so what

00:24:59,200 --> 00:25:06,520
did we do we found ways of actually

00:25:02,820 --> 00:25:09,720
decreasing the density of de streams by

00:25:06,520 --> 00:25:13,150
dividing it number one in batches

00:25:09,720 --> 00:25:16,780
analyze the batches one at a time those

00:25:13,150 --> 00:25:19,330
are windows of the stream and actually

00:25:16,780 --> 00:25:23,220
sending just the results of the data and

00:25:19,330 --> 00:25:27,400
that's how we scale to two extreme so

00:25:23,220 --> 00:25:30,940
let me explain to you here when you have

00:25:27,400 --> 00:25:34,360
a stream of data we split at each data

00:25:30,940 --> 00:25:37,030
zone they stream in two batches we

00:25:34,360 --> 00:25:39,900
analyze that batch and we feed the

00:25:37,030 --> 00:25:44,230
results of that batch and we synchronize

00:25:39,900 --> 00:25:49,210
batches along the way so this is our

00:25:44,230 --> 00:25:53,470
vision this is what we want to create we

00:25:49,210 --> 00:25:55,510
also actually look how we integrated I

00:25:53,470 --> 00:25:58,570
don't know if you know Dell technologies

00:25:55,510 --> 00:26:00,429
was one of the pioneers on Ajax foundry

00:25:58,570 --> 00:26:04,210
and we take that very very seriously

00:26:00,429 --> 00:26:07,090
sold our federated analytics is fully

00:26:04,210 --> 00:26:10,540
integrated with Ajax foundry it runs on

00:26:07,090 --> 00:26:12,940
the Gateway it uses a lazy approach that

00:26:10,540 --> 00:26:15,070
it asks all the information to Ajax

00:26:12,940 --> 00:26:20,170
foundry at the moment that the analytics

00:26:15,070 --> 00:26:22,590
is done we actually have a we run on the

00:26:20,170 --> 00:26:26,760
Gateway but also we have appliances for

00:26:22,590 --> 00:26:30,640
for streaming and as I mentioned we also

00:26:26,760 --> 00:26:33,280
integrated with with blockchain this is

00:26:30,640 --> 00:26:36,010
actually very very important every time

00:26:33,280 --> 00:26:38,170
we do a computation we actually register

00:26:36,010 --> 00:26:41,080
into the ledger who requested the

00:26:38,170 --> 00:26:43,660
computation was what was the analytics

00:26:41,080 --> 00:26:47,020
that was done what was the code that was

00:26:43,660 --> 00:26:50,020
executed what were the results who or

00:26:47,020 --> 00:26:52,480
else what are the data zones have we

00:26:50,020 --> 00:26:54,820
requested that most importantly we keep

00:26:52,480 --> 00:26:56,710
a hash of the data now keep in mind we

00:26:54,820 --> 00:26:58,030
are also a data storage company so we

00:26:56,710 --> 00:27:01,929
encourage you to keep a copy of

00:26:58,030 --> 00:27:04,210
everything so but what do we do is that

00:27:01,929 --> 00:27:07,480
we look at the data we calculate a hash

00:27:04,210 --> 00:27:11,500
and we put that hash into the ladder so

00:27:07,480 --> 00:27:13,900
should that type of analytics be subject

00:27:11,500 --> 00:27:16,990
to litigation let's say in data sciences

00:27:13,900 --> 00:27:20,170
genomics or clinical trial you can

00:27:16,990 --> 00:27:24,640
actually go back and repeat the results

00:27:20,170 --> 00:27:25,590
by doing that let me give you some final

00:27:24,640 --> 00:27:29,169
thoughts

00:27:25,590 --> 00:27:32,260
WWH the worldwide herd of federated

00:27:29,169 --> 00:27:34,360
analytics was actually inspired on solid

00:27:32,260 --> 00:27:37,870
foundations first it comes from Greek

00:27:34,360 --> 00:27:42,429
methodology not Greek mythology and it

00:27:37,870 --> 00:27:44,080
is inspired and I urge you to look up

00:27:42,429 --> 00:27:47,050
her name

00:27:44,080 --> 00:27:50,470
it's a Pythagoras wife on the concept of

00:27:47,050 --> 00:27:52,200
golden mean we are inspired on the World

00:27:50,470 --> 00:27:55,360
Wide Web

00:27:52,200 --> 00:27:57,700
principles this is our foundation this

00:27:55,360 --> 00:28:01,000
is our 10 commandments that were reduced

00:27:57,700 --> 00:28:04,120
to 5 because we economize and do we take

00:28:01,000 --> 00:28:05,980
them seriously I don't know if you heard

00:28:04,120 --> 00:28:09,580
but the World Economic Forum

00:28:05,980 --> 00:28:12,880
actually had a charter for 2018 was how

00:28:09,580 --> 00:28:16,870
to create a shared world or shared

00:28:12,880 --> 00:28:20,650
economy or shared future in a fractured

00:28:16,870 --> 00:28:21,470
world that was this speech and I like to

00:28:20,650 --> 00:28:24,530
say that

00:28:21,470 --> 00:28:28,130
the rated analytics was inspired by that

00:28:24,530 --> 00:28:33,169
how can you actually do analytics and

00:28:28,130 --> 00:28:34,669
learning at worldwide scale he when data

00:28:33,169 --> 00:28:37,700
is fractured

00:28:34,669 --> 00:28:40,429
throughout now some final final thoughts

00:28:37,700 --> 00:28:43,250
I promise I will finish I would like to

00:28:40,429 --> 00:28:45,799
finish by telling you a story as I

00:28:43,250 --> 00:28:48,620
mentioned I'm Brazilian and at the end

00:28:45,799 --> 00:28:50,690
of 2014 there was a documentary in the

00:28:48,620 --> 00:28:52,520
salt of the earth about a famous

00:28:50,690 --> 00:28:55,280
Brazilian photographer his name is

00:28:52,520 --> 00:28:58,760
Sebastien Salgado and his oldest son

00:28:55,280 --> 00:29:01,010
turned 21 and actually did a documentary

00:28:58,760 --> 00:29:03,200
of his father actually going on

00:29:01,010 --> 00:29:05,929
exploration and there is a very famous

00:29:03,200 --> 00:29:09,679
scene in the movie where they are about

00:29:05,929 --> 00:29:13,700
to shoot sea lions in Alaska very very

00:29:09,679 --> 00:29:15,880
cold and then white bear comes along so

00:29:13,700 --> 00:29:19,250
the Sun turns to the father and says

00:29:15,880 --> 00:29:21,620
daddy please take a picture we have to

00:29:19,250 --> 00:29:24,429
run and the father looks at their white

00:29:21,620 --> 00:29:27,020
bear which by the way is in no hurry and

00:29:24,429 --> 00:29:29,419
walking very calmly but was their

00:29:27,020 --> 00:29:29,780
direction and he says no I will not do

00:29:29,419 --> 00:29:33,320
it

00:29:29,780 --> 00:29:35,480
we'll find a tent we'll hide and to make

00:29:33,320 --> 00:29:37,690
a long story short ten hours later they

00:29:35,480 --> 00:29:41,809
came back and the father did his job

00:29:37,690 --> 00:29:45,350
after the the bear head Kamala had head

00:29:41,809 --> 00:29:47,870
left so the son asks the father why

00:29:45,350 --> 00:29:51,830
didn't you take a picture when I told

00:29:47,870 --> 00:29:53,630
you to do to do so and the father looks

00:29:51,830 --> 00:29:57,289
at the son and says I don't take

00:29:53,630 --> 00:30:01,210
pictures I really don't and the son says

00:29:57,289 --> 00:30:05,690
what do you do for a living he said I

00:30:01,210 --> 00:30:08,270
actually when you take a picture you are

00:30:05,690 --> 00:30:11,510
actually capturing a moment or you're

00:30:08,270 --> 00:30:15,140
documenting history to take a picture is

00:30:11,510 --> 00:30:18,559
to capture a moment or document history

00:30:15,140 --> 00:30:21,200
I actually photograph and photograph

00:30:18,559 --> 00:30:26,230
comes from the Greek word photo is light

00:30:21,200 --> 00:30:32,120
and graph is to draw or design I design

00:30:26,230 --> 00:30:34,440
I draw I invent with light and these are

00:30:32,120 --> 00:30:38,340
examples of the pictures that

00:30:34,440 --> 00:30:40,289
actually the father takes right or maybe

00:30:38,340 --> 00:30:43,649
he doesn't take picture that's what he

00:30:40,289 --> 00:30:46,470
designs with light and when I actually

00:30:43,649 --> 00:30:48,389
saw that movie I told my husband now I

00:30:46,470 --> 00:30:50,610
finally can explain digital

00:30:48,389 --> 00:30:54,149
transformation now he knows better than

00:30:50,610 --> 00:30:59,250
argue so he agreed and moved on but I

00:30:54,149 --> 00:31:00,779
actually heard out of that movie and I

00:30:59,250 --> 00:31:05,429
put this together

00:31:00,779 --> 00:31:09,240
look if photograph is to draw or design

00:31:05,429 --> 00:31:12,840
with light then I Patricia can come up

00:31:09,240 --> 00:31:16,169
with that techno graph tech for

00:31:12,840 --> 00:31:18,659
technology and graph to draw the deal in

00:31:16,169 --> 00:31:23,129
design and tech toe graph is this

00:31:18,659 --> 00:31:26,490
ability to draw or design whatever you

00:31:23,129 --> 00:31:29,490
want with technology and if you agree on

00:31:26,490 --> 00:31:32,059
that definition then I will give another

00:31:29,490 --> 00:31:36,029
definition for digital transformation

00:31:32,059 --> 00:31:40,080
digital transformation is their the way

00:31:36,029 --> 00:31:43,620
that we are going to imagine our future

00:31:40,080 --> 00:31:47,779
we are going to check no graph the way

00:31:43,620 --> 00:31:51,480
we walk live and think in different ways

00:31:47,779 --> 00:31:54,870
there is a very famous movie ferries for

00:31:51,480 --> 00:31:57,299
boomer day off have you seen that well

00:31:54,870 --> 00:32:00,710
maybe some of you for the young ones

00:31:57,299 --> 00:32:04,049
don't waste your time but there is a

00:32:00,710 --> 00:32:08,940
there is a very famous saying where he

00:32:04,049 --> 00:32:11,100
says life moves pretty fast correct if

00:32:08,940 --> 00:32:14,610
you don't stop and look around once in a

00:32:11,100 --> 00:32:17,100
while you could miss it and this is what

00:32:14,610 --> 00:32:19,429
I fear every day ladies and gentlemen

00:32:17,100 --> 00:32:21,720
there are three types of people on earth

00:32:19,429 --> 00:32:24,149
people that make it happen

00:32:21,720 --> 00:32:29,639
people that watch it happen and people

00:32:24,149 --> 00:32:31,590
that do not know what happened we are in

00:32:29,639 --> 00:32:35,399
the dawn of a new era

00:32:31,590 --> 00:32:38,250
we are the dawn of the digital era in

00:32:35,399 --> 00:32:41,460
the dawn of this new era what do you

00:32:38,250 --> 00:32:44,710
want to do do you want to take a picture

00:32:41,460 --> 00:32:48,130
a digital picture of your present

00:32:44,710 --> 00:32:52,150
in your past which is even worse as it

00:32:48,130 --> 00:32:56,740
is and make it repeatable by automating

00:32:52,150 --> 00:33:00,190
it or do you want in this digital era to

00:32:56,740 --> 00:33:02,380
imagine and techno graph your future let

00:33:00,190 --> 00:33:04,200
our digital transformation begin thank

00:33:02,380 --> 00:33:10,290
you very much for your time today

00:33:04,200 --> 00:33:10,290

YouTube URL: https://www.youtube.com/watch?v=x9haDnOaNzg


