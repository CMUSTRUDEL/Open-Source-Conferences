Title: Combatting Bias in Machine Learning   DrupalCon Global 2020
Publication date: 2020-08-25
Playlist: DrupalCon Global 2020
Description: 
	By giving examples from recent research attendees can expect clarification on what bias means in machine learning.
Captions: 
	00:00:02,000 --> 00:00:05,200
welcome back everyone

00:00:03,600 --> 00:00:06,799
and thanks for the story lisa that was

00:00:05,200 --> 00:00:08,960
great um

00:00:06,799 --> 00:00:09,920
i'm really excited about our next

00:00:08,960 --> 00:00:12,960
session

00:00:09,920 --> 00:00:15,920
combating bias in machine learning our

00:00:12,960 --> 00:00:18,000
speaker io delhi odubela is the data

00:00:15,920 --> 00:00:20,080
scientist for samba safety

00:00:18,000 --> 00:00:21,760
and we're very pleased to welcome her to

00:00:20,080 --> 00:00:24,800
the drupalcon stage

00:00:21,760 --> 00:00:27,199
hey everyone thank you so much suzanne

00:00:24,800 --> 00:00:28,480
i am really really excited to be talking

00:00:27,199 --> 00:00:32,079
about this today

00:00:28,480 --> 00:00:34,559
um i think that bias in ml has gotten

00:00:32,079 --> 00:00:35,840
a lot of more press recently which i

00:00:34,559 --> 00:00:38,000
think is great but

00:00:35,840 --> 00:00:38,960
um i'm here to present some solutions as

00:00:38,000 --> 00:00:41,120
well so

00:00:38,960 --> 00:00:42,800
calling out some of the issues and being

00:00:41,120 --> 00:00:43,520
a little bit more specific about what we

00:00:42,800 --> 00:00:47,920
mean about

00:00:43,520 --> 00:00:50,000
bias in ml so to start us off i will

00:00:47,920 --> 00:00:51,920
kind of define what this means so

00:00:50,000 --> 00:00:54,480
we do have different meanings in data

00:00:51,920 --> 00:00:56,719
science and machine learning for bias

00:00:54,480 --> 00:00:58,239
that differ from what we talk or talking

00:00:56,719 --> 00:01:01,199
about in society and

00:00:58,239 --> 00:01:02,879
um actual discrimination so in data

00:01:01,199 --> 00:01:05,040
science um you might see

00:01:02,879 --> 00:01:07,360
bias referenced as the difference

00:01:05,040 --> 00:01:09,920
between models predicted values and

00:01:07,360 --> 00:01:10,799
actual values so on the left here you

00:01:09,920 --> 00:01:12,880
see um

00:01:10,799 --> 00:01:14,479
you know a little bit of the bias

00:01:12,880 --> 00:01:17,520
variance trade-off

00:01:14,479 --> 00:01:19,040
um a model that has low bias means

00:01:17,520 --> 00:01:20,720
essentially that the

00:01:19,040 --> 00:01:22,320
there's not a huge difference between

00:01:20,720 --> 00:01:23,840
our predicted values and the actual

00:01:22,320 --> 00:01:27,680
values which is a good thing

00:01:23,840 --> 00:01:29,759
so high bias models are tend to predict

00:01:27,680 --> 00:01:30,720
uh or they don't didn't learn very well

00:01:29,759 --> 00:01:33,920
from the

00:01:30,720 --> 00:01:34,320
actual training and in machine learning

00:01:33,920 --> 00:01:36,960
um

00:01:34,320 --> 00:01:38,400
bias is also a term that we use when

00:01:36,960 --> 00:01:40,720
we're talking about

00:01:38,400 --> 00:01:42,320
the data that's added to a neural

00:01:40,720 --> 00:01:44,799
network in order to avoid

00:01:42,320 --> 00:01:46,399
activation functions um multiplying by

00:01:44,799 --> 00:01:48,560
zero essentially so

00:01:46,399 --> 00:01:50,799
imagine if your training inputs are both

00:01:48,560 --> 00:01:52,240
zero if you multiply that by any kind of

00:01:50,799 --> 00:01:52,880
weight you're still going to get zero so

00:01:52,240 --> 00:01:57,600
we do use

00:01:52,880 --> 00:01:59,600
bias as an additional feature there

00:01:57,600 --> 00:02:01,200
but when we're talking about bias in

00:01:59,600 --> 00:02:03,360
this context

00:02:01,200 --> 00:02:04,240
i really like the psychology today

00:02:03,360 --> 00:02:07,360
actual um

00:02:04,240 --> 00:02:10,560
definition of bias so it talks about how

00:02:07,360 --> 00:02:11,520
we are how we um have these cognitive

00:02:10,560 --> 00:02:14,319
biases

00:02:11,520 --> 00:02:15,840
against specific groups so um there's

00:02:14,319 --> 00:02:19,840
two main kinds there is

00:02:15,840 --> 00:02:21,840
implicit and implicit bias so um

00:02:19,840 --> 00:02:23,040
implicit bias essentially it's so much

00:02:21,840 --> 00:02:26,319
easier to spot

00:02:23,040 --> 00:02:28,720
um these are things that we think but

00:02:26,319 --> 00:02:31,120
explicit buys it's really difficult for

00:02:28,720 --> 00:02:33,040
us to get to the root of this so

00:02:31,120 --> 00:02:34,720
i'm going to be digging into how these

00:02:33,040 --> 00:02:36,000
biases really exist

00:02:34,720 --> 00:02:37,680
when we're talking about the data

00:02:36,000 --> 00:02:40,400
science process and creating machine

00:02:37,680 --> 00:02:43,840
learning models

00:02:40,400 --> 00:02:45,840
so the problem with bias is that

00:02:43,840 --> 00:02:47,680
it can be encoded in our algorithms and

00:02:45,840 --> 00:02:48,720
things that we build without us meaning

00:02:47,680 --> 00:02:50,400
to

00:02:48,720 --> 00:02:52,640
so if we just have a look at the

00:02:50,400 --> 00:02:56,000
different headlines we see up here

00:02:52,640 --> 00:02:58,400
um a lot of these tools in ai

00:02:56,000 --> 00:03:00,239
they're just using our trading data and

00:02:58,400 --> 00:03:02,879
trying to project that onto

00:03:00,239 --> 00:03:04,640
future results or future events but the

00:03:02,879 --> 00:03:05,599
problem is if our training data is

00:03:04,640 --> 00:03:09,040
flawed

00:03:05,599 --> 00:03:10,640
and if we have flaws in how we approach

00:03:09,040 --> 00:03:12,159
solving these problems

00:03:10,640 --> 00:03:14,239
or maybe we're not aware of these

00:03:12,159 --> 00:03:17,280
problems to get to begin with

00:03:14,239 --> 00:03:18,319
it's really easy for these biases that

00:03:17,280 --> 00:03:22,640
are in society

00:03:18,319 --> 00:03:22,640
to be just perpetuated by our models

00:03:23,840 --> 00:03:27,200
so something i like to ask and everyone

00:03:26,080 --> 00:03:29,599
and really think about

00:03:27,200 --> 00:03:31,280
i'll just give everyone a moment this

00:03:29,599 --> 00:03:33,360
doesn't apply to all of us

00:03:31,280 --> 00:03:34,640
but many of us are systematically

00:03:33,360 --> 00:03:37,680
marginalized in

00:03:34,640 --> 00:03:40,720
many ways um this can be from housing

00:03:37,680 --> 00:03:43,920
to um if you're someone who has had fire

00:03:40,720 --> 00:03:44,319
prior felonies there's a lot of ways

00:03:43,920 --> 00:03:46,480
that

00:03:44,319 --> 00:03:48,000
our systems kind of encode these

00:03:46,480 --> 00:03:51,680
imperfect processes

00:03:48,000 --> 00:03:52,080
in our society and then extrapolate that

00:03:51,680 --> 00:03:56,080
over

00:03:52,080 --> 00:03:56,080
millions and millions of rows of data

00:03:58,080 --> 00:04:02,239
so the first part really um to

00:03:59,920 --> 00:04:05,040
understanding this is awareness so

00:04:02,239 --> 00:04:06,560
um as much as we like to put our trust

00:04:05,040 --> 00:04:09,439
in math and stats

00:04:06,560 --> 00:04:10,560
but computers and data neither of those

00:04:09,439 --> 00:04:12,959
are neutral

00:04:10,560 --> 00:04:14,000
the way that zeros and ones might be

00:04:12,959 --> 00:04:17,280
neutral in their

00:04:14,000 --> 00:04:20,560
like definition however the way that we

00:04:17,280 --> 00:04:21,280
use computers to extrapolate from this

00:04:20,560 --> 00:04:24,479
data

00:04:21,280 --> 00:04:27,120
is not neutral the data in the way is

00:04:24,479 --> 00:04:28,880
data is biased in the way it's collected

00:04:27,120 --> 00:04:31,199
in the way it's manipulated

00:04:28,880 --> 00:04:32,479
and a lot of that comes from what data

00:04:31,199 --> 00:04:34,080
scientists and machine learning

00:04:32,479 --> 00:04:36,479
engineers do when we're building these

00:04:34,080 --> 00:04:36,479
models

00:04:37,520 --> 00:04:41,840
the first part is really just

00:04:39,919 --> 00:04:43,360
understanding that we have to be

00:04:41,840 --> 00:04:44,720
uncomfortable to fix some of these

00:04:43,360 --> 00:04:46,479
problems

00:04:44,720 --> 00:04:47,919
i'm going to outline a couple examples

00:04:46,479 --> 00:04:51,280
but there

00:04:47,919 --> 00:04:54,000
are there are products out there that

00:04:51,280 --> 00:04:54,800
can essentially draw your face shape or

00:04:54,000 --> 00:04:57,680
imagine

00:04:54,800 --> 00:05:00,560
or create an image of what you look like

00:04:57,680 --> 00:05:04,000
based off of the sound of your voice

00:05:00,560 --> 00:05:05,919
initially this doesn't seem ominous

00:05:04,000 --> 00:05:07,520
necessarily but we have to understand

00:05:05,919 --> 00:05:10,960
that a product like this

00:05:07,520 --> 00:05:13,199
is very clearly transphobic um

00:05:10,960 --> 00:05:14,639
we're in almost every single facial

00:05:13,199 --> 00:05:17,199
recognition case

00:05:14,639 --> 00:05:17,680
only we only have two categories for

00:05:17,199 --> 00:05:19,840
gender

00:05:17,680 --> 00:05:22,080
male and female and if we know that

00:05:19,840 --> 00:05:23,440
there are trans people that exist

00:05:22,080 --> 00:05:25,199
we have to understand that these are

00:05:23,440 --> 00:05:27,120
going to be this data is going to be

00:05:25,199 --> 00:05:29,280
biased by that from the jump

00:05:27,120 --> 00:05:30,800
so we have to be willing to be

00:05:29,280 --> 00:05:34,320
uncomfortable when we're talking about

00:05:30,800 --> 00:05:34,320
these problems in order to fix them

00:05:35,280 --> 00:05:39,199
so outlining a couple different kinds of

00:05:38,080 --> 00:05:41,120
bias that really

00:05:39,199 --> 00:05:42,240
exists when we're creating these

00:05:41,120 --> 00:05:45,440
frameworks

00:05:42,240 --> 00:05:46,000
um first being prejudice wise so let's

00:05:45,440 --> 00:05:48,720
say you

00:05:46,000 --> 00:05:50,639
are training a computer vision system to

00:05:48,720 --> 00:05:53,199
recognize people at work

00:05:50,639 --> 00:05:53,919
um again it doesn't seem insidious

00:05:53,199 --> 00:05:55,919
however

00:05:53,919 --> 00:05:57,919
if you're going off of internet images

00:05:55,919 --> 00:06:00,639
which the vast majority of

00:05:57,919 --> 00:06:02,639
us would be um you know you're using

00:06:00,639 --> 00:06:05,440
some kind of data scraper

00:06:02,639 --> 00:06:06,000
to pull images and you may not think

00:06:05,440 --> 00:06:07,759
that

00:06:06,000 --> 00:06:09,440
these are going to encode gender bias

00:06:07,759 --> 00:06:12,000
but they will um

00:06:09,440 --> 00:06:14,160
there are of course images of women as

00:06:12,000 --> 00:06:16,720
doctors on the internet there are

00:06:14,160 --> 00:06:17,440
images of women in traditionally male

00:06:16,720 --> 00:06:20,080
rules and

00:06:17,440 --> 00:06:21,759
vice versa however we have to understand

00:06:20,080 --> 00:06:23,280
this data is going to be drastically

00:06:21,759 --> 00:06:25,120
skewed so

00:06:23,280 --> 00:06:28,800
this prejudice bias really just comes

00:06:25,120 --> 00:06:28,800
from those cultural stereotypes

00:06:28,880 --> 00:06:33,039
we're talking about ways to reduce or

00:06:31,520 --> 00:06:35,280
fix this problem

00:06:33,039 --> 00:06:36,479
the first is really having stratified

00:06:35,280 --> 00:06:39,120
data samples so

00:06:36,479 --> 00:06:39,680
paying attention to our racial and our

00:06:39,120 --> 00:06:42,319
gender

00:06:39,680 --> 00:06:43,520
uh distributions and actually creating

00:06:42,319 --> 00:06:46,000
training data

00:06:43,520 --> 00:06:47,120
and using training data that matches

00:06:46,000 --> 00:06:50,240
real life

00:06:47,120 --> 00:06:52,800
or at least provide some of that balance

00:06:50,240 --> 00:06:55,280
and a more even distribution of these

00:06:52,800 --> 00:06:55,280
samples

00:06:56,560 --> 00:06:59,919
there's also measurement bias so this is

00:06:58,960 --> 00:07:02,400
one that i

00:06:59,919 --> 00:07:03,919
i think is really important to go over

00:07:02,400 --> 00:07:05,360
especially because

00:07:03,919 --> 00:07:07,759
many technologists don't want to

00:07:05,360 --> 00:07:10,319
acknowledge this um

00:07:07,759 --> 00:07:10,800
there is often bias when we're talking

00:07:10,319 --> 00:07:13,840
about

00:07:10,800 --> 00:07:14,400
devices that are used to measure data

00:07:13,840 --> 00:07:16,960
like

00:07:14,400 --> 00:07:18,639
sensors that can be heart rate monitors

00:07:16,960 --> 00:07:21,919
this can be tsa

00:07:18,639 --> 00:07:24,160
body scanners most of these tools

00:07:21,919 --> 00:07:26,639
despite the fact that they're out and in

00:07:24,160 --> 00:07:30,240
the wild and we use them consistently

00:07:26,639 --> 00:07:32,639
on a large scale most of them have this

00:07:30,240 --> 00:07:35,840
bias towards darker skin

00:07:32,639 --> 00:07:36,800
um and towards every group that these

00:07:35,840 --> 00:07:38,880
weren't trained on

00:07:36,800 --> 00:07:40,160
so on the right you can see a couple

00:07:38,880 --> 00:07:43,280
different examples

00:07:40,160 --> 00:07:44,000
um really of this measurement bias not

00:07:43,280 --> 00:07:47,360
working as well

00:07:44,000 --> 00:07:50,960
for people with darker skin um

00:07:47,360 --> 00:07:54,319
we can only really infer that this is

00:07:50,960 --> 00:07:56,560
partially an oversight of the developers

00:07:54,319 --> 00:07:57,520
by not training these things on darker

00:07:56,560 --> 00:07:58,960
skin but

00:07:57,520 --> 00:08:00,800
you know like the apple watch has a

00:07:58,960 --> 00:08:01,440
combination of cameras an infrared

00:08:00,800 --> 00:08:04,479
camera

00:08:01,440 --> 00:08:07,520
and a color uh sensor um

00:08:04,479 --> 00:08:10,479
sensors on them and both of these things

00:08:07,520 --> 00:08:11,440
just don't work as well for dark skin so

00:08:10,479 --> 00:08:13,919
but these products

00:08:11,440 --> 00:08:15,599
are widespread and they're used in a lot

00:08:13,919 --> 00:08:18,319
of different environments

00:08:15,599 --> 00:08:19,120
so we have to understand that some of

00:08:18,319 --> 00:08:22,720
these problems

00:08:19,120 --> 00:08:22,720
are problems within the hardware

00:08:22,800 --> 00:08:26,240
so one of the ways to reduce this

00:08:25,280 --> 00:08:30,319
measurement bias

00:08:26,240 --> 00:08:32,479
is essentially to stratify our data and

00:08:30,319 --> 00:08:33,760
use different data when we're training

00:08:32,479 --> 00:08:35,919
so

00:08:33,760 --> 00:08:38,800
both of the examples with the heart rate

00:08:35,919 --> 00:08:41,440
monitors and the tsa scanners

00:08:38,800 --> 00:08:42,640
those can be fixed if we have wider

00:08:41,440 --> 00:08:46,080
ranges of people who

00:08:42,640 --> 00:08:48,959
these products are being tested on

00:08:46,080 --> 00:08:50,720
going into sample bias um i talked about

00:08:48,959 --> 00:08:53,279
it very briefly but

00:08:50,720 --> 00:08:55,040
this is this typically happens if our

00:08:53,279 --> 00:08:57,279
data that we're using for training

00:08:55,040 --> 00:08:58,880
doesn't accurately represent the real

00:08:57,279 --> 00:09:02,000
life environment so

00:08:58,880 --> 00:09:03,040
again facial recognition is one here a

00:09:02,000 --> 00:09:04,880
lot of these

00:09:03,040 --> 00:09:07,440
facial recognition models are trained on

00:09:04,880 --> 00:09:08,640
things like the imdb data set

00:09:07,440 --> 00:09:10,320
but from what we know about the

00:09:08,640 --> 00:09:11,279
demographics of people working in

00:09:10,320 --> 00:09:13,120
hollywood

00:09:11,279 --> 00:09:15,200
we know that there are going to be more

00:09:13,120 --> 00:09:17,120
white men than any other group

00:09:15,200 --> 00:09:18,880
um and that is one of the biggest

00:09:17,120 --> 00:09:22,160
problems with sample bias

00:09:18,880 --> 00:09:23,600
um essentially at least it is it is

00:09:22,160 --> 00:09:27,519
somewhat easier to fix

00:09:23,600 --> 00:09:30,240
so um with sample sampling bias

00:09:27,519 --> 00:09:30,720
we can stratify these training samples

00:09:30,240 --> 00:09:33,680
um

00:09:30,720 --> 00:09:34,959
again that just takes developer uh

00:09:33,680 --> 00:09:37,920
consideration

00:09:34,959 --> 00:09:38,800
into which different groups you have in

00:09:37,920 --> 00:09:40,480
your data

00:09:38,800 --> 00:09:42,080
which groups are going to be using your

00:09:40,480 --> 00:09:43,920
products in real life

00:09:42,080 --> 00:09:45,839
and taking that into consideration how

00:09:43,920 --> 00:09:46,959
you build these samples and how we train

00:09:45,839 --> 00:09:49,680
these models

00:09:46,959 --> 00:09:51,440
a lot of machine learning problems that

00:09:49,680 --> 00:09:54,720
have sampling bias

00:09:51,440 --> 00:09:56,959
can fairly easily be fixed by

00:09:54,720 --> 00:09:59,760
be fixed with new data or be fixed by

00:09:56,959 --> 00:09:59,760
resampling

00:10:00,959 --> 00:10:04,000
so i think part of this is also a

00:10:03,440 --> 00:10:06,880
mindset

00:10:04,000 --> 00:10:08,320
change right so we understand that data

00:10:06,880 --> 00:10:11,279
can have issues

00:10:08,320 --> 00:10:14,240
but we have to understand that our

00:10:11,279 --> 00:10:16,320
mindsets in creating these projects

00:10:14,240 --> 00:10:18,880
have to change we have to assume that

00:10:16,320 --> 00:10:19,839
it's going to encode racist and sexist

00:10:18,880 --> 00:10:22,160
norms

00:10:19,839 --> 00:10:23,519
until we take the steps to specifically

00:10:22,160 --> 00:10:26,000
address them

00:10:23,519 --> 00:10:27,600
and the reason i say that is because we

00:10:26,000 --> 00:10:30,160
as humans are biased

00:10:27,600 --> 00:10:32,160
we are going to be looking at problems

00:10:30,160 --> 00:10:35,760
with our perspective

00:10:32,160 --> 00:10:37,839
and with every um you know regardless of

00:10:35,760 --> 00:10:40,720
what that perspective is

00:10:37,839 --> 00:10:41,440
we have a hard time separating our work

00:10:40,720 --> 00:10:44,079
from

00:10:41,440 --> 00:10:45,040
uh how we view the world so we do have

00:10:44,079 --> 00:10:48,480
that observer

00:10:45,040 --> 00:10:51,519
observer bias as well um things that

00:10:48,480 --> 00:10:52,320
make sense to us or kind of uphold the

00:10:51,519 --> 00:10:56,800
status quo

00:10:52,320 --> 00:10:56,800
to us are easier to just ignore

00:10:57,440 --> 00:11:01,600
so that goes into what our role really

00:10:59,839 --> 00:11:04,800
is um

00:11:01,600 --> 00:11:06,399
i think we have to see these machine

00:11:04,800 --> 00:11:08,640
learning problems in the same way we do

00:11:06,399 --> 00:11:11,760
we see a lot of tech you know there's

00:11:08,640 --> 00:11:14,560
the idiom about garbage and garbage out

00:11:11,760 --> 00:11:15,839
the same with bias if we're working on a

00:11:14,560 --> 00:11:17,920
policing problem and

00:11:15,839 --> 00:11:19,920
we're taking data from pretty much any

00:11:17,920 --> 00:11:20,640
country but specifically the united

00:11:19,920 --> 00:11:22,399
states

00:11:20,640 --> 00:11:24,399
we have to understand that that data is

00:11:22,399 --> 00:11:27,760
going to be biased by society

00:11:24,399 --> 00:11:30,720
and what i mean by that is how policing

00:11:27,760 --> 00:11:31,360
developed and is enforced around this

00:11:30,720 --> 00:11:34,240
country

00:11:31,360 --> 00:11:34,560
all of that is going to be perpetuated

00:11:34,240 --> 00:11:36,800
by

00:11:34,560 --> 00:11:38,720
whatever model we use if it's based off

00:11:36,800 --> 00:11:40,800
of that kind of data

00:11:38,720 --> 00:11:42,240
i think we're also in a really unique

00:11:40,800 --> 00:11:44,640
position because

00:11:42,240 --> 00:11:45,360
we tend to be masters of our own demise

00:11:44,640 --> 00:11:47,440
so

00:11:45,360 --> 00:11:48,560
um data science is was not my first

00:11:47,440 --> 00:11:51,680
career field

00:11:48,560 --> 00:11:54,240
um i was initially in marketing and

00:11:51,680 --> 00:11:55,440
i've never worked or have heard of other

00:11:54,240 --> 00:11:57,519
positions where

00:11:55,440 --> 00:11:59,440
we're able to choose our own the metrics

00:11:57,519 --> 00:12:01,920
that we're measured against

00:11:59,440 --> 00:12:03,120
so data scientists are able to choose

00:12:01,920 --> 00:12:06,880
between using r

00:12:03,120 --> 00:12:10,160
squared or the f1 statistic or accuracy

00:12:06,880 --> 00:12:12,560
or precision precision or recall

00:12:10,160 --> 00:12:13,680
there are so many evaluation metrics we

00:12:12,560 --> 00:12:15,680
can use

00:12:13,680 --> 00:12:17,839
but people often forget that we are the

00:12:15,680 --> 00:12:20,639
ones who get to choose that as well

00:12:17,839 --> 00:12:22,959
um if you're someone of not necessarily

00:12:20,639 --> 00:12:26,000
high moral integrity it is easier

00:12:22,959 --> 00:12:29,040
and you can choose the metric that makes

00:12:26,000 --> 00:12:29,839
your model essentially look good which

00:12:29,040 --> 00:12:33,120
sometimes

00:12:29,839 --> 00:12:33,920
and is frequently accuracy um but that

00:12:33,120 --> 00:12:36,000
doesn't give

00:12:33,920 --> 00:12:38,079
the best glimpse into how your model

00:12:36,000 --> 00:12:40,560
actually performs

00:12:38,079 --> 00:12:42,480
and on top of that um the dominant

00:12:40,560 --> 00:12:44,560
culture tends to dominate

00:12:42,480 --> 00:12:45,920
and what i mean is that as tech has

00:12:44,560 --> 00:12:47,040
developed in the united states

00:12:45,920 --> 00:12:50,800
especially

00:12:47,040 --> 00:12:53,680
um we have seen a leaning towards more

00:12:50,800 --> 00:12:54,000
white men in these fields and that's not

00:12:53,680 --> 00:12:56,639
the

00:12:54,000 --> 00:12:58,480
problem it's that if you're not in a

00:12:56,639 --> 00:13:00,320
marginalized group it's very hard to

00:12:58,480 --> 00:13:03,200
recognize the problems that people

00:13:00,320 --> 00:13:04,399
in those groups deal with so we have to

00:13:03,200 --> 00:13:06,320
understand

00:13:04,399 --> 00:13:07,600
and be willing to consult and both

00:13:06,320 --> 00:13:09,519
listen um

00:13:07,600 --> 00:13:10,959
when people who are in these groups are

00:13:09,519 --> 00:13:12,079
telling us hey there's a problem with

00:13:10,959 --> 00:13:15,120
your software

00:13:12,079 --> 00:13:17,920
um this has happened with people with

00:13:15,120 --> 00:13:19,600
projects at major companies um google

00:13:17,920 --> 00:13:20,079
from google photos labeling to people

00:13:19,600 --> 00:13:24,079
with two

00:13:20,079 --> 00:13:25,040
as gorillas to other major gaffes that

00:13:24,079 --> 00:13:28,320
people don't

00:13:25,040 --> 00:13:29,200
want to talk about but there is a lack

00:13:28,320 --> 00:13:32,079
of

00:13:29,200 --> 00:13:33,839
insight into what it's like to be pat to

00:13:32,079 --> 00:13:34,880
have these models perpetuate bias

00:13:33,839 --> 00:13:36,880
against you

00:13:34,880 --> 00:13:38,639
and that's why we not like that's why

00:13:36,880 --> 00:13:41,839
diversity is important but

00:13:38,639 --> 00:13:44,000
um it's really about having people who

00:13:41,839 --> 00:13:48,000
can spot these issues and then listening

00:13:44,000 --> 00:13:49,920
to these people as well

00:13:48,000 --> 00:13:51,360
so a couple solutions because i don't

00:13:49,920 --> 00:13:52,160
like bringing up problems without

00:13:51,360 --> 00:13:54,720
talking about

00:13:52,160 --> 00:13:55,920
any ways to deal with them um starting

00:13:54,720 --> 00:13:59,360
with documentation

00:13:55,920 --> 00:14:01,040
so there are a lot of different aspects

00:13:59,360 --> 00:14:02,720
to data science and machine learning

00:14:01,040 --> 00:14:04,959
that we need to

00:14:02,720 --> 00:14:06,720
make more scientific and one of those

00:14:04,959 --> 00:14:10,240
pieces of documentation

00:14:06,720 --> 00:14:13,120
um data sheets for data sets it was an

00:14:10,240 --> 00:14:15,680
awesome paper that really just outlined

00:14:13,120 --> 00:14:18,240
ways to document our data sets so

00:14:15,680 --> 00:14:19,920
i talked about data being biased because

00:14:18,240 --> 00:14:21,680
of how it was collected

00:14:19,920 --> 00:14:24,000
a lot of our training data that people

00:14:21,680 --> 00:14:25,040
in industry use we have no idea how it

00:14:24,000 --> 00:14:28,560
was collected or

00:14:25,040 --> 00:14:30,399
sampled or if there are any um

00:14:28,560 --> 00:14:31,839
issues with how those things were

00:14:30,399 --> 00:14:34,160
conducted and

00:14:31,839 --> 00:14:36,399
by creating a data sheet for data sets

00:14:34,160 --> 00:14:38,320
that you use and you build

00:14:36,399 --> 00:14:40,160
you can make your models like you can

00:14:38,320 --> 00:14:43,440
make all of your resource

00:14:40,160 --> 00:14:45,279
research more reproducible that way

00:14:43,440 --> 00:14:46,480
another solution i really like is model

00:14:45,279 --> 00:14:49,839
cards so

00:14:46,480 --> 00:14:51,040
um google actually has these model card

00:14:49,839 --> 00:14:52,639
examples up for

00:14:51,040 --> 00:14:54,959
face detection as well as object

00:14:52,639 --> 00:14:57,440
detection but it just outlines the

00:14:54,959 --> 00:14:59,519
limitations of their modeling

00:14:57,440 --> 00:15:00,560
the features held with the performance

00:14:59,519 --> 00:15:03,199
it gives

00:15:00,560 --> 00:15:05,040
a really solid way to just understand

00:15:03,199 --> 00:15:07,199
what their framework does

00:15:05,040 --> 00:15:08,800
and where their framework is lacking so

00:15:07,199 --> 00:15:10,480
the first that's a

00:15:08,800 --> 00:15:12,079
good first step as far as being

00:15:10,480 --> 00:15:14,000
transparent about how

00:15:12,079 --> 00:15:17,680
we build our models and really

00:15:14,000 --> 00:15:17,680
highlighting these limitations as well

00:15:18,320 --> 00:15:22,000
the next aspect is model explainability

00:15:21,360 --> 00:15:24,079
so

00:15:22,000 --> 00:15:25,279
this is really important for

00:15:24,079 --> 00:15:28,000
understanding

00:15:25,279 --> 00:15:30,560
how our models can be biased so the

00:15:28,000 --> 00:15:34,000
compass algorithm which is a recidivism

00:15:30,560 --> 00:15:36,320
algorithm used by county judges

00:15:34,000 --> 00:15:38,320
all across the u.s essentially um

00:15:36,320 --> 00:15:41,199
they're able to

00:15:38,320 --> 00:15:41,759
put a person's information in and try

00:15:41,199 --> 00:15:44,320
and give

00:15:41,759 --> 00:15:45,279
judges a better way to decide uh how

00:15:44,320 --> 00:15:47,279
long to decide

00:15:45,279 --> 00:15:48,560
or how long of prison term someone

00:15:47,279 --> 00:15:51,680
should get

00:15:48,560 --> 00:15:54,079
so this algorithm is found out to be

00:15:51,680 --> 00:15:55,680
extremely biased based off of race but

00:15:54,079 --> 00:15:56,639
when you looked at the really important

00:15:55,680 --> 00:15:58,079
features

00:15:56,639 --> 00:15:59,759
a lot of the features that went into

00:15:58,079 --> 00:16:02,160
this algorithm

00:15:59,759 --> 00:16:03,279
and determined someone's risk score were

00:16:02,160 --> 00:16:06,399
things about them that they

00:16:03,279 --> 00:16:09,519
couldn't change so a lot were

00:16:06,399 --> 00:16:11,519
if you had a high school diploma or if

00:16:09,519 --> 00:16:12,320
you have a mental illness did you grow

00:16:11,519 --> 00:16:15,600
up in a

00:16:12,320 --> 00:16:19,040
poor or low-income neighborhood using fe

00:16:15,600 --> 00:16:22,079
using um frameworks like shapely values

00:16:19,040 --> 00:16:25,519
uh tcav and lime are really good for

00:16:22,079 --> 00:16:27,120
outlining the important features so um

00:16:25,519 --> 00:16:29,279
all of the things listed on on the

00:16:27,120 --> 00:16:31,440
screen or on this slide

00:16:29,279 --> 00:16:33,600
are ways to deal with problems in your

00:16:31,440 --> 00:16:37,120
machine learning models so

00:16:33,600 --> 00:16:39,440
adversarial ml really helps us when

00:16:37,120 --> 00:16:41,360
we kind of have oversight into how our

00:16:39,440 --> 00:16:44,720
models could be used negatively

00:16:41,360 --> 00:16:45,279
so um considering bad actors and people

00:16:44,720 --> 00:16:47,600
who

00:16:45,279 --> 00:16:49,360
want to use our models maliciously

00:16:47,600 --> 00:16:50,560
especially um you know if your company

00:16:49,360 --> 00:16:53,440
like google and you have

00:16:50,560 --> 00:16:54,399
have a public api it's really important

00:16:53,440 --> 00:16:56,079
to

00:16:54,399 --> 00:16:57,759
employ some of these tactics to

00:16:56,079 --> 00:17:00,320
understand what kind of harm

00:16:57,759 --> 00:17:01,600
is going to be perpetuated if someone

00:17:00,320 --> 00:17:05,839
chooses to

00:17:01,600 --> 00:17:05,839
use your algorithm for evil essentially

00:17:06,720 --> 00:17:10,720
and then going into best practices um

00:17:09,360 --> 00:17:14,160
it's really important to

00:17:10,720 --> 00:17:16,640
test with your edge cases first so

00:17:14,160 --> 00:17:18,079
a lot of these problems that with

00:17:16,640 --> 00:17:21,039
sensors or the

00:17:18,079 --> 00:17:22,720
measurement bias that i mentioned um

00:17:21,039 --> 00:17:24,640
tend to come from

00:17:22,720 --> 00:17:25,760
these think these products were not

00:17:24,640 --> 00:17:27,919
tested on

00:17:25,760 --> 00:17:30,240
people who work with have darker skin

00:17:27,919 --> 00:17:33,280
they were typically not tested on women

00:17:30,240 --> 00:17:34,640
um we have to instead of excluding these

00:17:33,280 --> 00:17:36,160
groups and only having

00:17:34,640 --> 00:17:38,400
a small proportion of them in our

00:17:36,160 --> 00:17:39,120
training data we have to test on this

00:17:38,400 --> 00:17:42,240
first

00:17:39,120 --> 00:17:43,919
so as you can assume you can

00:17:42,240 --> 00:17:46,160
test something on someone with darker

00:17:43,919 --> 00:17:48,160
skin and it's more likely to work on

00:17:46,160 --> 00:17:49,919
someone with fair skin as well

00:17:48,160 --> 00:17:52,160
and it doesn't work the opposite way

00:17:49,919 --> 00:17:54,240
around especially when we're considering

00:17:52,160 --> 00:17:56,799
things like camera hardware

00:17:54,240 --> 00:17:59,440
if you can detect dark tones it's easier

00:17:56,799 --> 00:18:01,679
to detect those light tones as well

00:17:59,440 --> 00:18:02,640
we also just need to be extremely

00:18:01,679 --> 00:18:05,280
skeptic about

00:18:02,640 --> 00:18:07,200
not only our data but how we're building

00:18:05,280 --> 00:18:10,559
these models with our data so

00:18:07,200 --> 00:18:13,760
um like i said paying attention to the

00:18:10,559 --> 00:18:14,960
groups that are in our subsets

00:18:13,760 --> 00:18:18,080
essentially so

00:18:14,960 --> 00:18:20,400
um going over these racial and gender uh

00:18:18,080 --> 00:18:21,280
subsets to make sure we have distributed

00:18:20,400 --> 00:18:23,600
samples

00:18:21,280 --> 00:18:24,799
um that's one of the biggest problems

00:18:23,600 --> 00:18:26,880
when we're

00:18:24,799 --> 00:18:28,080
creating these models and i think one of

00:18:26,880 --> 00:18:30,559
the biggest reasons why

00:18:28,080 --> 00:18:32,240
we have so many issues with products

00:18:30,559 --> 00:18:33,600
only working for some people and not

00:18:32,240 --> 00:18:35,679
others

00:18:33,600 --> 00:18:37,600
lastly we have to consult with the

00:18:35,679 --> 00:18:39,039
people who are impacted by this model

00:18:37,600 --> 00:18:42,320
bias so

00:18:39,039 --> 00:18:43,919
um even for me personally before

00:18:42,320 --> 00:18:46,160
pre-pandemic when i was flying

00:18:43,919 --> 00:18:47,919
frequently um every single time i would

00:18:46,160 --> 00:18:50,160
go to the airport it would take me 10 to

00:18:47,919 --> 00:18:53,120
15 minutes longer than everyone else

00:18:50,160 --> 00:18:53,440
and that's because the rf scanners in

00:18:53,120 --> 00:18:56,240
the

00:18:53,440 --> 00:18:56,880
tsa body scanners have a really tough

00:18:56,240 --> 00:18:59,679
time

00:18:56,880 --> 00:19:00,080
like understanding black women's hair i

00:18:59,679 --> 00:19:02,080
almost

00:19:00,080 --> 00:19:04,960
always have to be patted down there has

00:19:02,080 --> 00:19:06,240
been i can count on one hand

00:19:04,960 --> 00:19:08,400
since they've how many times since

00:19:06,240 --> 00:19:09,679
they've implemented body scanners that i

00:19:08,400 --> 00:19:13,440
have been able to

00:19:09,679 --> 00:19:15,679
make it through successfully or you know

00:19:13,440 --> 00:19:17,520
have that product fulfill its goal of

00:19:15,679 --> 00:19:18,320
making things faster and easier for

00:19:17,520 --> 00:19:20,960
people

00:19:18,320 --> 00:19:21,360
we always have to question who who is

00:19:20,960 --> 00:19:25,840
this

00:19:21,360 --> 00:19:25,840
product easier for

00:19:26,400 --> 00:19:30,880
and then there is human intervention so

00:19:28,960 --> 00:19:33,120
we need to create

00:19:30,880 --> 00:19:35,679
machine learning has a problem with

00:19:33,120 --> 00:19:36,640
giving finite examples or giving finite

00:19:35,679 --> 00:19:39,120
outputs

00:19:36,640 --> 00:19:41,120
and human interpreters just kind of take

00:19:39,120 --> 00:19:43,760
those and run with them

00:19:41,120 --> 00:19:46,160
we see them for fact we don't question

00:19:43,760 --> 00:19:48,000
or criticize why we were given an answer

00:19:46,160 --> 00:19:50,240
but we need more human in the loop

00:19:48,000 --> 00:19:52,240
systems where you know

00:19:50,240 --> 00:19:53,280
these models have an explanation

00:19:52,240 --> 00:19:56,240
interface

00:19:53,280 --> 00:19:56,960
and they can say uh you know why they

00:19:56,240 --> 00:20:00,160
were wrong

00:19:56,960 --> 00:20:03,200
they can at least explain to users um

00:20:00,160 --> 00:20:05,120
why certain decisions were made and how

00:20:03,200 --> 00:20:06,960
the model failed so

00:20:05,120 --> 00:20:09,280
at least users that are working with

00:20:06,960 --> 00:20:10,880
these human in the loop systems

00:20:09,280 --> 00:20:13,440
are able to take some of that

00:20:10,880 --> 00:20:16,400
information and they're essentially

00:20:13,440 --> 00:20:17,840
comparing that um to the situation at

00:20:16,400 --> 00:20:18,720
hand and seeing if it's the right

00:20:17,840 --> 00:20:21,200
solution

00:20:18,720 --> 00:20:22,159
so actually having computer input and

00:20:21,200 --> 00:20:25,280
not just

00:20:22,159 --> 00:20:26,799
um computer input to their decision

00:20:25,280 --> 00:20:30,880
making process and not

00:20:26,799 --> 00:20:32,840
just trusting the computer's uh input

00:20:30,880 --> 00:20:34,000
and saying that that's the end of all be

00:20:32,840 --> 00:20:36,400
all

00:20:34,000 --> 00:20:37,840
another aspect here is really making our

00:20:36,400 --> 00:20:40,720
models open source

00:20:37,840 --> 00:20:41,120
and having our data public so a lot of

00:20:40,720 --> 00:20:42,640
people

00:20:41,120 --> 00:20:44,720
and a lot of companies don't like this

00:20:42,640 --> 00:20:46,480
because we see this

00:20:44,720 --> 00:20:48,000
modeling and machine learning as our

00:20:46,480 --> 00:20:50,159
secret sauce it's our competitive

00:20:48,000 --> 00:20:51,840
advantage but we have to understand if

00:20:50,159 --> 00:20:53,039
we're using systems

00:20:51,840 --> 00:20:55,520
and we're working with the government

00:20:53,039 --> 00:20:58,000
like the compass algorithm

00:20:55,520 --> 00:20:59,039
any kind of policing algorithm these are

00:20:58,000 --> 00:21:02,000
impacting people

00:20:59,039 --> 00:21:03,280
in a very public sphere there's no way

00:21:02,000 --> 00:21:05,919
to opt out

00:21:03,280 --> 00:21:07,919
and they're owned by private companies

00:21:05,919 --> 00:21:09,679
controlled by private companies and the

00:21:07,919 --> 00:21:12,320
public has no access

00:21:09,679 --> 00:21:14,400
to how these are built um we have to

00:21:12,320 --> 00:21:16,480
start considering this when we have

00:21:14,400 --> 00:21:18,240
products that have high impact on

00:21:16,480 --> 00:21:20,320
people's lives and have a high potential

00:21:18,240 --> 00:21:22,640
to cause harm

00:21:20,320 --> 00:21:24,000
and like i mentioned earlier it we do

00:21:22,640 --> 00:21:27,120
have to change our mindset

00:21:24,000 --> 00:21:28,559
think about this as a science um data

00:21:27,120 --> 00:21:29,600
science has often been lumped in with

00:21:28,559 --> 00:21:32,640
engineering

00:21:29,600 --> 00:21:33,360
thinking um and moving fast and breaking

00:21:32,640 --> 00:21:36,159
things

00:21:33,360 --> 00:21:37,760
might work for products but it doesn't

00:21:36,159 --> 00:21:40,240
work the same way when we do

00:21:37,760 --> 00:21:40,960
have products that use machine learning

00:21:40,240 --> 00:21:42,880
so

00:21:40,960 --> 00:21:44,559
essentially take whatever bias was there

00:21:42,880 --> 00:21:46,640
and perpetuate it more

00:21:44,559 --> 00:21:48,000
but in these really really harmful areas

00:21:46,640 --> 00:21:52,159
like hiring

00:21:48,000 --> 00:21:54,960
hiring employees as well as

00:21:52,159 --> 00:21:55,520
policing as well as deciding who gets to

00:21:54,960 --> 00:21:58,720
get a

00:21:55,520 --> 00:22:01,840
apartment or who gets to get a mortgage

00:21:58,720 --> 00:22:03,679
these are highly impactful areas and we

00:22:01,840 --> 00:22:06,799
can't just take the move fast and break

00:22:03,679 --> 00:22:06,799
things approach to those

00:22:08,640 --> 00:22:12,799
and just to reiterate this our edge

00:22:10,720 --> 00:22:15,039
cases should be our baseline

00:22:12,799 --> 00:22:16,240
not implemented in v2 not an

00:22:15,039 --> 00:22:19,120
afterthought

00:22:16,240 --> 00:22:21,919
um this is something that's happened in

00:22:19,120 --> 00:22:24,480
very large very public products so

00:22:21,919 --> 00:22:25,679
um i believe the apple health app it was

00:22:24,480 --> 00:22:26,000
like for the first year and a half

00:22:25,679 --> 00:22:29,039
didn't

00:22:26,000 --> 00:22:29,760
have cycle tracking and then you know

00:22:29,039 --> 00:22:32,159
you look at

00:22:29,760 --> 00:22:33,120
the percentage of iphone users and it's

00:22:32,159 --> 00:22:35,760
like 51

00:22:33,120 --> 00:22:37,120
female we have to understand these

00:22:35,760 --> 00:22:39,440
development teams

00:22:37,120 --> 00:22:41,039
are probably not always thinking about

00:22:39,440 --> 00:22:43,360
these marginalized groups

00:22:41,039 --> 00:22:45,200
but that's why we should be the baseline

00:22:43,360 --> 00:22:47,120
that should be the benchmark that it's

00:22:45,200 --> 00:22:50,080
checked against and not

00:22:47,120 --> 00:22:50,080
an afterthought

00:22:51,440 --> 00:22:55,039
so thinking about just in general

00:22:53,600 --> 00:22:58,880
development priorities

00:22:55,039 --> 00:23:01,520
um why delay modeling um

00:22:58,880 --> 00:23:03,200
you know why delay dealing with issues

00:23:01,520 --> 00:23:06,320
in our machine learning models

00:23:03,200 --> 00:23:09,039
if it's not just because we don't care

00:23:06,320 --> 00:23:10,559
if we if we have the time and the money

00:23:09,039 --> 00:23:13,120
there is no reason to

00:23:10,559 --> 00:23:14,320
delay this pro these uh problem dealing

00:23:13,120 --> 00:23:16,000
with these problems

00:23:14,320 --> 00:23:17,919
so what you see on the screen right now

00:23:16,000 --> 00:23:19,360
is a shirley card so um

00:23:17,919 --> 00:23:21,039
the major problem with facial

00:23:19,360 --> 00:23:23,919
recognition right now

00:23:21,039 --> 00:23:25,760
is that camera hardware is really

00:23:23,919 --> 00:23:27,520
horrible at reading dark skin

00:23:25,760 --> 00:23:29,760
it's happened on almost every zoom call

00:23:27,520 --> 00:23:30,640
i've been on it's happened in facetime

00:23:29,760 --> 00:23:33,360
calls

00:23:30,640 --> 00:23:35,280
um it affects me personally it happens

00:23:33,360 --> 00:23:37,440
if i'm trying to take photos at night

00:23:35,280 --> 00:23:38,480
camera hardware was trained to recognize

00:23:37,440 --> 00:23:40,400
fair skin

00:23:38,480 --> 00:23:41,600
and even though we've digitized a lot of

00:23:40,400 --> 00:23:43,760
this hardware

00:23:41,600 --> 00:23:45,440
if we're using a flawed foundation we're

00:23:43,760 --> 00:23:48,240
only going to create more products that

00:23:45,440 --> 00:23:48,240
are flawed as well

00:23:48,960 --> 00:23:54,720
so speaking on facial recognition um

00:23:52,000 --> 00:23:56,080
as of just last month the acm has

00:23:54,720 --> 00:23:58,159
actually called for

00:23:56,080 --> 00:24:00,240
essentially a ban of using facial

00:23:58,159 --> 00:24:04,080
recognition in all government

00:24:00,240 --> 00:24:04,880
um situations so it is extremely flawed

00:24:04,080 --> 00:24:06,960
and it's been

00:24:04,880 --> 00:24:08,320
very well documented that we do need to

00:24:06,960 --> 00:24:11,039
ban facial recognition

00:24:08,320 --> 00:24:12,320
it is cameras right now do not have the

00:24:11,039 --> 00:24:15,039
capability

00:24:12,320 --> 00:24:16,640
to be as effective recognizing dark skin

00:24:15,039 --> 00:24:18,960
as they do fair skin

00:24:16,640 --> 00:24:21,039
we can't use this and this has already

00:24:18,960 --> 00:24:23,919
happened people have been arrested

00:24:21,039 --> 00:24:26,240
based off of algorithms but we're going

00:24:23,919 --> 00:24:28,320
to be perpetuating this same bias

00:24:26,240 --> 00:24:30,840
and leaving out these same groups of

00:24:28,320 --> 00:24:35,279
people that have continued to be

00:24:30,840 --> 00:24:37,840
marginalized so if you're thinking about

00:24:35,279 --> 00:24:39,919
working in machine learning um or using

00:24:37,840 --> 00:24:41,520
machine learning in your projects

00:24:39,919 --> 00:24:43,679
you just have to think about ethics

00:24:41,520 --> 00:24:44,960
first think keep it in mind when you're

00:24:43,679 --> 00:24:47,600
building

00:24:44,960 --> 00:24:49,279
the foundation for this so really

00:24:47,600 --> 00:24:50,640
thinking about if this solution even

00:24:49,279 --> 00:24:52,559
fits your problem

00:24:50,640 --> 00:24:54,159
that's one of the biggest issues in data

00:24:52,559 --> 00:24:57,520
science in industry

00:24:54,159 --> 00:24:59,919
is that we can find data and

00:24:57,520 --> 00:25:01,520
try and approximate the uh the problem

00:24:59,919 --> 00:25:03,679
we're trying to solve for

00:25:01,520 --> 00:25:05,360
but we very rarely have data that

00:25:03,679 --> 00:25:07,679
matches this perfectly

00:25:05,360 --> 00:25:09,840
so really taking a lot of consideration

00:25:07,679 --> 00:25:12,320
into should i really be doing this with

00:25:09,840 --> 00:25:15,279
the information i have available

00:25:12,320 --> 00:25:17,200
um we also need to be examining for and

00:25:15,279 --> 00:25:20,159
also removing biased proxies

00:25:17,200 --> 00:25:21,279
um things like zip code can be a proxy

00:25:20,159 --> 00:25:24,080
for race

00:25:21,279 --> 00:25:25,039
and if you're making if you're applying

00:25:24,080 --> 00:25:26,480
for a credit card

00:25:25,039 --> 00:25:28,480
you're more likely to get approved if

00:25:26,480 --> 00:25:30,480
your zip code is 90210

00:25:28,480 --> 00:25:32,720
regardless of your own personal credit

00:25:30,480 --> 00:25:35,440
card history

00:25:32,720 --> 00:25:36,159
we also need to just in general as an

00:25:35,440 --> 00:25:38,880
industry

00:25:36,159 --> 00:25:40,320
develop plans to mitigate harm what do

00:25:38,880 --> 00:25:42,159
we do if it does

00:25:40,320 --> 00:25:44,159
happen what do we do if someone doesn't

00:25:42,159 --> 00:25:45,200
get a job and we find out that the

00:25:44,159 --> 00:25:46,960
biggest feature

00:25:45,200 --> 00:25:49,679
and to the models that help to decide

00:25:46,960 --> 00:25:52,240
this were proxies for their race

00:25:49,679 --> 00:25:53,760
are we actually going to take that those

00:25:52,240 --> 00:25:56,400
accountability steps

00:25:53,760 --> 00:25:59,360
or do we kind of shove it under the rug

00:25:56,400 --> 00:25:59,360
like it didn't happen

00:26:01,120 --> 00:26:04,240
so a couple questions you should ask

00:26:03,200 --> 00:26:06,400
yourself

00:26:04,240 --> 00:26:07,840
if you're gearing up for a project that

00:26:06,400 --> 00:26:10,559
uses machine learning

00:26:07,840 --> 00:26:11,760
is really does does my product need ml

00:26:10,559 --> 00:26:15,279
will it be better

00:26:11,760 --> 00:26:17,760
if it's going to be drastically better

00:26:15,279 --> 00:26:19,600
consider how it impacts your users how

00:26:17,760 --> 00:26:21,279
it impacts the kind of updates you're

00:26:19,600 --> 00:26:23,120
going to have to make how your model

00:26:21,279 --> 00:26:26,720
gets those updates

00:26:23,120 --> 00:26:27,520
as well as privacy and a huge aspect

00:26:26,720 --> 00:26:29,919
here as well

00:26:27,520 --> 00:26:32,080
is really commit collecting feedback and

00:26:29,919 --> 00:26:34,320
then giving your model feedback so

00:26:32,080 --> 00:26:35,600
this is a step that the vast majority of

00:26:34,320 --> 00:26:38,880
people don't do

00:26:35,600 --> 00:26:40,720
but we should be doing take a case like

00:26:38,880 --> 00:26:43,200
the compass algorithm where

00:26:40,720 --> 00:26:46,240
these models have predicted people will

00:26:43,200 --> 00:26:48,159
recommit crimes and they don't

00:26:46,240 --> 00:26:50,000
these models are not currently getting

00:26:48,159 --> 00:26:50,960
updated about every single time they

00:26:50,000 --> 00:26:53,279
were wrong

00:26:50,960 --> 00:26:55,200
so regardless of if you don't think the

00:26:53,279 --> 00:26:55,679
training date is biased it's still going

00:26:55,200 --> 00:26:58,240
to be

00:26:55,679 --> 00:27:00,159
perpetuating the same issues it's not

00:26:58,240 --> 00:27:01,760
going to be getting updated

00:27:00,159 --> 00:27:03,360
and it's hard to say that we can put a

00:27:01,760 --> 00:27:06,000
lot of trust into a model that

00:27:03,360 --> 00:27:09,600
doesn't get you know guidance on where

00:27:06,000 --> 00:27:09,600
it has made mistakes in the past

00:27:11,760 --> 00:27:14,880
so if you are interested um in reading

00:27:14,480 --> 00:27:17,360
more

00:27:14,880 --> 00:27:18,559
uh here's my personal favorite list of

00:27:17,360 --> 00:27:21,279
um

00:27:18,559 --> 00:27:22,559
books that cover a lot of the issues um

00:27:21,279 --> 00:27:24,799
if you were

00:27:22,559 --> 00:27:25,679
looking for especially uh some heart

00:27:24,799 --> 00:27:28,559
issues on

00:27:25,679 --> 00:27:30,399
uh some hard data on how these models

00:27:28,559 --> 00:27:32,399
impact people i really suggest uh

00:27:30,399 --> 00:27:34,000
weapons of math destruction by kathy

00:27:32,399 --> 00:27:35,840
o'neill

00:27:34,000 --> 00:27:37,360
these books really help to set my

00:27:35,840 --> 00:27:39,760
foundational understanding of these

00:27:37,360 --> 00:27:39,760
issues

00:27:40,880 --> 00:27:49,840
all right any questions

00:28:40,159 --> 00:28:48,159
yes i'm hearing if you want to see

00:28:44,080 --> 00:28:51,360
more of this reading list

00:28:48,159 --> 00:28:53,760
feel free to jot those down um yeah i

00:28:51,360 --> 00:28:54,399
i really love every single book on this

00:28:53,760 --> 00:28:56,240
list

00:28:54,399 --> 00:28:57,760
um each one kind of hits a different

00:28:56,240 --> 00:29:00,799
point in

00:28:57,760 --> 00:29:00,799
these issues in ml

00:29:01,840 --> 00:29:05,919
oh i do have a question is there any

00:29:04,000 --> 00:29:07,279
ethics body that helps work on these

00:29:05,919 --> 00:29:10,399
types of issues

00:29:07,279 --> 00:29:13,600
there are so i know of three groups

00:29:10,399 --> 00:29:15,840
first is ai for all uh second is

00:29:13,600 --> 00:29:17,840
data for black lives they have a lot of

00:29:15,840 --> 00:29:19,279
volunteer data scientists machine

00:29:17,840 --> 00:29:21,200
learning engineers

00:29:19,279 --> 00:29:23,279
recently they've been digging into a lot

00:29:21,200 --> 00:29:24,799
of coven 19 data

00:29:23,279 --> 00:29:26,320
and then there is the algorithmic

00:29:24,799 --> 00:29:29,440
justice league so

00:29:26,320 --> 00:29:32,399
um that is run by joypolamwini um

00:29:29,440 --> 00:29:33,360
she's an awesome researcher in ai ethics

00:29:32,399 --> 00:29:35,600
uh produced the

00:29:33,360 --> 00:29:37,520
gender shades project which really

00:29:35,600 --> 00:29:40,559
outlined the ways in

00:29:37,520 --> 00:29:42,000
much like the face plus plus amazon and

00:29:40,559 --> 00:29:44,080
microsoft

00:29:42,000 --> 00:29:46,799
facial recognition apis are biased

00:29:44,080 --> 00:29:48,640
specifically towards black women

00:29:46,799 --> 00:29:50,880
i know she's done a lot of work with

00:29:48,640 --> 00:29:53,279
talking to congress as well as

00:29:50,880 --> 00:29:56,720
helping to get cities like boston to

00:29:53,279 --> 00:29:56,720
stop using facial recognition

00:29:57,279 --> 00:30:01,840
yeah uh next question we have do you

00:29:59,440 --> 00:30:03,679
have any suggestions for how software

00:30:01,840 --> 00:30:05,120
and website developers can recognize

00:30:03,679 --> 00:30:07,760
their own bias and change their

00:30:05,120 --> 00:30:11,760
processes to include more perspectives

00:30:07,760 --> 00:30:14,399
absolutely so um first thing really is

00:30:11,760 --> 00:30:16,240
consider consulting with other people

00:30:14,399 --> 00:30:16,960
consider consulting with the people who

00:30:16,240 --> 00:30:20,640
are going to be

00:30:16,960 --> 00:30:22,559
using your sites who are going to

00:30:20,640 --> 00:30:23,760
probably be in these marginalized

00:30:22,559 --> 00:30:25,600
categories

00:30:23,760 --> 00:30:26,960
and it really does just take being

00:30:25,600 --> 00:30:30,080
genuine and asking

00:30:26,960 --> 00:30:31,760
how how can they illuminate the areas of

00:30:30,080 --> 00:30:33,520
bias you don't see

00:30:31,760 --> 00:30:37,360
i think that's one of the best first

00:30:33,520 --> 00:30:40,240
steps you really can take

00:30:37,360 --> 00:30:42,399
yeah um and yes i will make these slides

00:30:40,240 --> 00:30:45,200
uh publicly available after the

00:30:42,399 --> 00:30:45,200
after the session

00:30:48,840 --> 00:30:51,520
awesome um

00:30:51,679 --> 00:30:55,600
another question we have have you found

00:30:53,760 --> 00:30:57,679
that business analysts can help write

00:30:55,600 --> 00:30:59,200
requirements to capture the edge cases

00:30:57,679 --> 00:31:01,120
to get ahead of biases during

00:30:59,200 --> 00:31:04,559
development absolutely

00:31:01,120 --> 00:31:04,960
so i think this is where we can rely on

00:31:04,559 --> 00:31:07,919
our

00:31:04,960 --> 00:31:08,799
analysts a little bit more um to

00:31:07,919 --> 00:31:10,640
understand

00:31:08,799 --> 00:31:12,799
where these outliers are and instead of

00:31:10,640 --> 00:31:14,720
just you know removing them

00:31:12,799 --> 00:31:15,919
but really paying special attention to

00:31:14,720 --> 00:31:17,679
them

00:31:15,919 --> 00:31:19,039
i understand especially in a business

00:31:17,679 --> 00:31:23,279
it's really really

00:31:19,039 --> 00:31:26,320
difficult to get an idea of how to deal

00:31:23,279 --> 00:31:28,320
with some of these edge cases um

00:31:26,320 --> 00:31:29,440
but i i think that's an area where we

00:31:28,320 --> 00:31:33,039
can

00:31:29,440 --> 00:31:35,440
have business analysts really focus on

00:31:33,039 --> 00:31:36,799
and then make sure that everyone else is

00:31:35,440 --> 00:31:39,679
aware and considering

00:31:36,799 --> 00:31:41,039
that these outliers or these edge cases

00:31:39,679 --> 00:31:47,120
do exist when we're

00:31:41,039 --> 00:32:01,440
making our products

00:31:47,120 --> 00:32:03,760
some anything else

00:32:01,440 --> 00:32:03,760
cool

00:32:05,279 --> 00:32:10,399
it doesn't look like we have any other

00:32:07,120 --> 00:32:12,960
questions um actually just kidding we do

00:32:10,399 --> 00:32:14,159
um do you think the truly impartial data

00:32:12,960 --> 00:32:15,679
as possible

00:32:14,159 --> 00:32:17,360
what advice would you give to those who

00:32:15,679 --> 00:32:19,200
are trying to examine data with a

00:32:17,360 --> 00:32:22,000
critical eye for biases

00:32:19,200 --> 00:32:23,360
um i would say for the first part is

00:32:22,000 --> 00:32:26,240
truly impartial date

00:32:23,360 --> 00:32:27,200
impartial data possible the harsh answer

00:32:26,240 --> 00:32:30,480
is no

00:32:27,200 --> 00:32:31,360
um every piece of data is going to have

00:32:30,480 --> 00:32:33,279
bias

00:32:31,360 --> 00:32:34,640
um that that is a way we have to

00:32:33,279 --> 00:32:37,600
consider them as not

00:32:34,640 --> 00:32:40,320
neutral but the way we deal with that is

00:32:37,600 --> 00:32:41,440
just being very explicit about this bias

00:32:40,320 --> 00:32:44,720
so

00:32:41,440 --> 00:32:46,480
um outlining and i i know people hate

00:32:44,720 --> 00:32:48,960
this but like if you're in data you have

00:32:46,480 --> 00:32:52,320
to put caveats to your project and say

00:32:48,960 --> 00:32:54,559
this works under these conditions or um

00:32:52,320 --> 00:32:55,360
acknowledge that the data you use is

00:32:54,559 --> 00:32:58,080
going to be

00:32:55,360 --> 00:33:00,000
biased by those systems there are ways

00:32:58,080 --> 00:33:01,679
you can mitigate against that but

00:33:00,000 --> 00:33:04,399
it's really about acknowledging and

00:33:01,679 --> 00:33:08,240
making that very very clear to people

00:33:04,399 --> 00:33:10,399
so for facial recognition apis um

00:33:08,240 --> 00:33:11,760
had these companies that create them

00:33:10,399 --> 00:33:14,159
said

00:33:11,760 --> 00:33:15,519
here's an api you can use for facial

00:33:14,159 --> 00:33:17,440
recognition

00:33:15,519 --> 00:33:19,440
clearly you need to understand that

00:33:17,440 --> 00:33:20,000
because of camera hardware or training

00:33:19,440 --> 00:33:22,880
data

00:33:20,000 --> 00:33:24,000
x y and z it is not going to work as

00:33:22,880 --> 00:33:26,960
well for darker people

00:33:24,000 --> 00:33:28,960
as it is for fair-skinned people um and

00:33:26,960 --> 00:33:31,120
for the second half of that

00:33:28,960 --> 00:33:32,640
what advice would i give to those who

00:33:31,120 --> 00:33:34,640
are trying to examine data with a

00:33:32,640 --> 00:33:37,760
critical eye for biases

00:33:34,640 --> 00:33:40,880
um it's really about

00:33:37,760 --> 00:33:43,760
looking at the classes in your data so

00:33:40,880 --> 00:33:45,279
um when you're saying with a critical

00:33:43,760 --> 00:33:48,480
eye for bias

00:33:45,279 --> 00:33:50,080
it the bias typically comes in one or

00:33:48,480 --> 00:33:51,039
two forms if you're looking at a new

00:33:50,080 --> 00:33:53,919
data set

00:33:51,039 --> 00:33:56,000
um either the distribution between like

00:33:53,919 --> 00:33:59,279
genders or racial classes

00:33:56,000 --> 00:34:00,960
and then the other aspect is the

00:33:59,279 --> 00:34:03,200
the heart it's harder to spot but the

00:34:00,960 --> 00:34:05,200
bias that kind of exists in our world so

00:34:03,200 --> 00:34:07,600
like i mentioned with facial recognition

00:34:05,200 --> 00:34:09,599
only including male or female

00:34:07,600 --> 00:34:10,960
google very recently actually just

00:34:09,599 --> 00:34:13,839
removed their

00:34:10,960 --> 00:34:15,760
uh male and female labeling from their

00:34:13,839 --> 00:34:19,440
cloud vision api so

00:34:15,760 --> 00:34:21,599
um you can only see someone as a person

00:34:19,440 --> 00:34:23,440
i think those are sometimes harder to

00:34:21,599 --> 00:34:25,599
spot and where we might have to rely on

00:34:23,440 --> 00:34:29,520
people who are marginalized to

00:34:25,599 --> 00:34:29,520
you know illuminate that aspect for us

00:34:30,399 --> 00:34:34,240
um another question we have here do you

00:34:33,040 --> 00:34:36,720
feel the burden

00:34:34,240 --> 00:34:38,639
falls disproportionately on you to be a

00:34:36,720 --> 00:34:42,079
champion of this kind of change

00:34:38,639 --> 00:34:43,040
um i do in a way because it has impacted

00:34:42,079 --> 00:34:44,639
me so

00:34:43,040 --> 00:34:46,800
um i was talking actually to someone

00:34:44,639 --> 00:34:47,599
yesterday i have a lot of skin in the

00:34:46,800 --> 00:34:49,280
game

00:34:47,599 --> 00:34:51,040
i have been discriminated against in

00:34:49,280 --> 00:34:53,280
housing algorithms

00:34:51,040 --> 00:34:54,960
just in facial recognition algorithms

00:34:53,280 --> 00:34:56,240
and going to the airport and trying to

00:34:54,960 --> 00:34:58,320
fly somewhere

00:34:56,240 --> 00:34:59,839
as someone who feels this and

00:34:58,320 --> 00:35:02,640
understands how

00:34:59,839 --> 00:35:04,880
this cost me money this cost me time in

00:35:02,640 --> 00:35:06,240
my day this cost me frustration

00:35:04,880 --> 00:35:08,400
and it's supposed to be a product that

00:35:06,240 --> 00:35:11,760
makes our lives easier

00:35:08,400 --> 00:35:14,000
um i definitely feel that it's it's not

00:35:11,760 --> 00:35:16,320
necessarily the burden falls on me

00:35:14,000 --> 00:35:17,440
um i'm coming from it from a different

00:35:16,320 --> 00:35:20,480
perspective

00:35:17,440 --> 00:35:22,480
and that's one of i have been subject to

00:35:20,480 --> 00:35:22,880
these like harmful algorithms so i can

00:35:22,480 --> 00:35:25,599
speak

00:35:22,880 --> 00:35:27,680
to not only the technical piece but what

00:35:25,599 --> 00:35:29,119
it does to your life and i think that

00:35:27,680 --> 00:35:30,640
a lot of times technologists we don't

00:35:29,119 --> 00:35:33,359
get that perspective and we don't get to

00:35:30,640 --> 00:35:33,359
see it often

00:35:35,920 --> 00:35:45,839
any other questions

00:36:02,640 --> 00:36:06,880
hope i think um yeah the only other

00:36:06,160 --> 00:36:08,720
thing so

00:36:06,880 --> 00:36:10,400
last question anything you want to share

00:36:08,720 --> 00:36:12,000
in closing um

00:36:10,400 --> 00:36:14,560
i am writing a book that will be out

00:36:12,000 --> 00:36:15,359
next year on uncovering bias in machine

00:36:14,560 --> 00:36:18,079
learning i

00:36:15,359 --> 00:36:19,680
am really excited about it i have the

00:36:18,079 --> 00:36:21,359
opportunity to talk to you so many

00:36:19,680 --> 00:36:23,359
amazing researchers

00:36:21,359 --> 00:36:27,839
um and just look out for that it should

00:36:23,359 --> 00:36:27,839
be out september next year

00:36:30,800 --> 00:36:35,599
awesome thank you so much everyone i am

00:36:33,760 --> 00:36:41,839
i'm so glad that i had the chance to

00:36:35,599 --> 00:36:41,839

YouTube URL: https://www.youtube.com/watch?v=AMFX8TJyyfI


