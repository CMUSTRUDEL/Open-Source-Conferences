Title: DrupalCon Los Angeles 2015: PHP Containers at Scale: 5K Containers per Server
Publication date: 2015-05-13
Playlist: DrupalCon Los Angeles - May 2015
Description: 
	At Pantheon, we run millions of production containers. Let's discussion techniques we've learned that you can apply to your own container use.

Topics include:

Disk, CPU, and network resource isolation
Making HA possible for at every project scale
Avoiding server upgrades: how to migrate containers to fresh hosts (without site downtime)
Security isolation
Capacity planning: what resources run out for what parts of the Drupal stack
Captions: 
	00:00:00,000 --> 00:00:06,720
okay I think I'll start on time just to

00:00:03,090 --> 00:00:10,260
get through all the material so I'm

00:00:06,720 --> 00:00:13,410
David Strauss i'm here with pantheon and

00:00:10,260 --> 00:00:16,260
we run a lot of containers all for PHP

00:00:13,410 --> 00:00:18,119
because we exclusively work with Drupal

00:00:16,260 --> 00:00:20,850
and WordPress and there are a lot of

00:00:18,119 --> 00:00:22,949
challenges running infrastructure at the

00:00:20,850 --> 00:00:25,320
density that we have and I want to kind

00:00:22,949 --> 00:00:29,519
of provide the story of the lead-up of

00:00:25,320 --> 00:00:32,730
why these kind of at least previously

00:00:29,519 --> 00:00:34,860
exotic approaches are necessary and what

00:00:32,730 --> 00:00:37,530
we're delivering with them today and how

00:00:34,860 --> 00:00:40,850
we're delivering it and it all kind of

00:00:37,530 --> 00:00:43,800
starts with the growth of the web the

00:00:40,850 --> 00:00:47,700
exponential growth of web sites over

00:00:43,800 --> 00:00:50,910
time it's gone from some major companies

00:00:47,700 --> 00:00:54,149
and some usergroups having static HTML

00:00:50,910 --> 00:00:58,280
sites up on the web to almost every

00:00:54,149 --> 00:01:01,980
single organization division product

00:00:58,280 --> 00:01:04,409
event conference etc having a website

00:01:01,980 --> 00:01:06,270
and that shows the numbers here but

00:01:04,409 --> 00:01:09,090
they've also gotten much more focused in

00:01:06,270 --> 00:01:12,060
their audience they've gotten much more

00:01:09,090 --> 00:01:14,189
personal in terms of the niche that each

00:01:12,060 --> 00:01:17,850
website is targeting the specific goals

00:01:14,189 --> 00:01:20,009
of each group and project that people

00:01:17,850 --> 00:01:21,900
are building which is part of what is

00:01:20,009 --> 00:01:24,570
fueling this because websites are

00:01:21,900 --> 00:01:26,369
expected to cater to every part of an

00:01:24,570 --> 00:01:29,340
organization's functions not just say

00:01:26,369 --> 00:01:31,200
post or press releases anymore and this

00:01:29,340 --> 00:01:34,079
has manifested itself in terms of

00:01:31,200 --> 00:01:36,840
demands on these websites in the form of

00:01:34,079 --> 00:01:39,060
the stacks that they use and what is

00:01:36,840 --> 00:01:41,790
necessary to deliver those so we're not

00:01:39,060 --> 00:01:45,210
just looking at exponential growth of

00:01:41,790 --> 00:01:47,880
the websites as in their count itself

00:01:45,210 --> 00:01:49,500
we're looking at an exponential growth

00:01:47,880 --> 00:01:52,170
and the complexity and the resources

00:01:49,500 --> 00:01:54,030
necessary to support the development

00:01:52,170 --> 00:01:56,399
testing and deployment of these websites

00:01:54,030 --> 00:01:58,469
where we've gone from you could have a

00:01:56,399 --> 00:02:02,189
single production website in the 90s

00:01:58,469 --> 00:02:03,719
push up HTML files and you would be in a

00:02:02,189 --> 00:02:06,119
situation where if you made a mistake

00:02:03,719 --> 00:02:08,489
you'd mess up one page and you the

00:02:06,119 --> 00:02:09,989
stakes were low enough that you didn't

00:02:08,489 --> 00:02:11,250
necessarily need to have any kind of a

00:02:09,989 --> 00:02:12,720
staging environment or even a

00:02:11,250 --> 00:02:13,890
development environment you could pretty

00:02:12,720 --> 00:02:16,880
much make changes

00:02:13,890 --> 00:02:20,250
them up fix mistakes no one would care

00:02:16,880 --> 00:02:23,220
and then that that evolves into greater

00:02:20,250 --> 00:02:26,790
demands on sites in the kind of early

00:02:23,220 --> 00:02:29,459
aughts with the idea of dynamic websites

00:02:26,790 --> 00:02:31,980
as PHP rose to prominence projects like

00:02:29,459 --> 00:02:33,240
WordPress and Drupal got founded people

00:02:31,980 --> 00:02:36,050
started building personal sites

00:02:33,240 --> 00:02:38,900
increasingly corporations adopted

00:02:36,050 --> 00:02:41,910
websites with these dynamic features and

00:02:38,900 --> 00:02:43,890
this marked an important change in the

00:02:41,910 --> 00:02:46,560
way people built the sites because you

00:02:43,890 --> 00:02:48,900
could make a change of starting neto are

00:02:46,560 --> 00:02:50,730
starting then that would break an entire

00:02:48,900 --> 00:02:53,340
site you could make a syntax error or

00:02:50,730 --> 00:02:56,489
say in your bootstrap part of the

00:02:53,340 --> 00:02:59,670
website or a module and that syntax

00:02:56,489 --> 00:03:01,350
error would crash the entire site so you

00:02:59,670 --> 00:03:03,330
needed to have somewhere to actually

00:03:01,350 --> 00:03:05,400
test your development before pushing it

00:03:03,330 --> 00:03:07,950
out to a live environment so this

00:03:05,400 --> 00:03:09,870
started mostly focused on say desktop

00:03:07,950 --> 00:03:12,030
development where you could install a

00:03:09,870 --> 00:03:13,950
product like wamp or MAMP and you would

00:03:12,030 --> 00:03:15,840
have everything you needed to develop

00:03:13,950 --> 00:03:17,820
and test your website on your local

00:03:15,840 --> 00:03:19,980
machine and then you could push it up to

00:03:17,820 --> 00:03:21,269
a production environment as soon as you

00:03:19,980 --> 00:03:23,430
felt like things were working well

00:03:21,269 --> 00:03:25,950
enough on your local box and that got us

00:03:23,430 --> 00:03:28,320
past the idea of making silly mistakes

00:03:25,950 --> 00:03:30,750
on say syntax or calling functions that

00:03:28,320 --> 00:03:31,980
weren't defined or other basic things

00:03:30,750 --> 00:03:35,489
that you go through as you're iterating

00:03:31,980 --> 00:03:37,260
on development it was also a difference

00:03:35,489 --> 00:03:38,910
in terms of the devices people were

00:03:37,260 --> 00:03:41,160
using to access these sites we're

00:03:38,910 --> 00:03:43,230
viewing it on your desktop and maybe a

00:03:41,160 --> 00:03:46,530
couple browsers was also sufficient to

00:03:43,230 --> 00:03:49,290
perform quality testing of the

00:03:46,530 --> 00:03:50,930
deployments you were making so you could

00:03:49,290 --> 00:03:53,670
handle all of that on your local machine

00:03:50,930 --> 00:03:56,280
but we've evolved considerably past that

00:03:53,670 --> 00:03:58,140
now where we now have really advanced

00:03:56,280 --> 00:04:00,840
stacks for delivering websites that's

00:03:58,140 --> 00:04:03,090
almost every major site that people are

00:04:00,840 --> 00:04:04,620
delivering from this conference are

00:04:03,090 --> 00:04:07,049
people from people who are attending

00:04:04,620 --> 00:04:09,060
this conference is running behind

00:04:07,049 --> 00:04:12,810
something like varnish running with a

00:04:09,060 --> 00:04:15,570
CDN using some type of full-text index

00:04:12,810 --> 00:04:18,870
whether an external service that is

00:04:15,570 --> 00:04:21,120
spidering the site or a back-end like

00:04:18,870 --> 00:04:22,770
elastic search or solar that is able to

00:04:21,120 --> 00:04:24,900
index it so that you don't have to do

00:04:22,770 --> 00:04:27,360
crazy queries on the database we're

00:04:24,900 --> 00:04:30,150
using external caches for the objects

00:04:27,360 --> 00:04:32,490
in a sense of things like redness and

00:04:30,150 --> 00:04:33,719
memcache and that's just kind of table

00:04:32,490 --> 00:04:36,180
stakes at this point for doing

00:04:33,719 --> 00:04:38,699
professional websites you can't install

00:04:36,180 --> 00:04:40,770
lamp or have on your desktop and be able

00:04:38,699 --> 00:04:43,770
to get that kind of environment and

00:04:40,770 --> 00:04:45,509
you're also in a situation where there's

00:04:43,770 --> 00:04:46,949
massively increased mobile use of the

00:04:45,509 --> 00:04:48,900
Internet as well which means you need to

00:04:46,949 --> 00:04:50,669
be able to view the websites on a mobile

00:04:48,900 --> 00:04:53,039
device and while you could probably hack

00:04:50,669 --> 00:04:55,379
it through some Wi-Fi thing of viewing a

00:04:53,039 --> 00:04:57,360
site that's running off your desktop by

00:04:55,379 --> 00:04:59,729
and large the answer for testing a site

00:04:57,360 --> 00:05:01,919
on mobile devices is deploying some sort

00:04:59,729 --> 00:05:03,930
of pre production environment viewing it

00:05:01,919 --> 00:05:05,939
on those devices making sure things are

00:05:03,930 --> 00:05:08,279
working right so we've moved past the

00:05:05,939 --> 00:05:10,139
idea that you can just simply do the

00:05:08,279 --> 00:05:12,960
development on your desktop which is

00:05:10,139 --> 00:05:15,330
another multiplying effect that not only

00:05:12,960 --> 00:05:17,490
are there now that many millions of

00:05:15,330 --> 00:05:19,169
websites but any of them with any

00:05:17,490 --> 00:05:21,629
sophistication require additional

00:05:19,169 --> 00:05:23,460
deployed resources often in cloud

00:05:21,629 --> 00:05:24,979
environments to be able to support the

00:05:23,460 --> 00:05:28,259
development maintenance of those sites

00:05:24,979 --> 00:05:30,240
so what we deal what we end up with is a

00:05:28,259 --> 00:05:33,330
massive massive number of stacks that

00:05:30,240 --> 00:05:34,949
need supporting and let's look at some

00:05:33,330 --> 00:05:39,330
of the ways that we've we've deployed

00:05:34,949 --> 00:05:40,770
these things in the past the three kind

00:05:39,330 --> 00:05:43,589
of architectures that are traditional

00:05:40,770 --> 00:05:45,629
are shared single VM and clusters of

00:05:43,589 --> 00:05:47,009
course single BM it's I don't know if

00:05:45,629 --> 00:05:50,460
I'd call it quite traditional because it

00:05:47,009 --> 00:05:54,270
rose with the evolution of amazon web

00:05:50,460 --> 00:05:56,909
services but the but by and large this

00:05:54,270 --> 00:06:00,089
is the kind of breakdown all you'll

00:05:56,909 --> 00:06:01,979
notice I'm have some kind of I have

00:06:00,089 --> 00:06:03,539
thickness of lines kind of denoting how

00:06:01,979 --> 00:06:05,099
much isolation there is there where

00:06:03,539 --> 00:06:07,770
physical machines have these really

00:06:05,099 --> 00:06:09,900
thick borders and virtual machines have

00:06:07,770 --> 00:06:12,020
thinner ones and then local UNIX users

00:06:09,900 --> 00:06:13,979
that the kind of ones used to deploy

00:06:12,020 --> 00:06:16,680
traditional shared hosting you know

00:06:13,979 --> 00:06:18,389
dreamhost I'll have the like permeable

00:06:16,680 --> 00:06:21,029
borders because that's really really

00:06:18,389 --> 00:06:25,169
lightweight and let's step through these

00:06:21,029 --> 00:06:27,569
so with the old-style shared hosting one

00:06:25,169 --> 00:06:28,949
of the benefits was that it was dirt

00:06:27,569 --> 00:06:31,560
cheap and it continues to be dirt cheap

00:06:28,949 --> 00:06:33,599
and the value it provides continues to

00:06:31,560 --> 00:06:35,189
feed dirt cheap in the sense that it

00:06:33,599 --> 00:06:37,020
doesn't it's really efficient but it

00:06:35,189 --> 00:06:39,060
provides almost no isolation there's a

00:06:37,020 --> 00:06:40,610
famous number of security

00:06:39,060 --> 00:06:43,610
vulnerabilities that have been caught

00:06:40,610 --> 00:06:47,539
by bad configurations on these types of

00:06:43,610 --> 00:06:49,189
infrastructures and they often are there

00:06:47,539 --> 00:06:50,900
often single points of failure if the

00:06:49,189 --> 00:06:52,639
machine you're on goes down or even

00:06:50,900 --> 00:06:54,620
another website on the machine you're on

00:06:52,639 --> 00:06:56,990
gets really popular takes or your site

00:06:54,620 --> 00:06:58,520
down and also there was usually limited

00:06:56,990 --> 00:07:00,409
ability to scale up with web traffic

00:06:58,520 --> 00:07:02,210
that the resources you got on the

00:07:00,409 --> 00:07:05,150
machine you're on was pretty much it so

00:07:02,210 --> 00:07:06,860
not great and there are very few major

00:07:05,150 --> 00:07:07,909
professional projects that I see being

00:07:06,860 --> 00:07:11,539
done in this kind of infrastructure

00:07:07,909 --> 00:07:14,000
today more common I see a medium

00:07:11,539 --> 00:07:15,710
websites of targeting kind of single VMs

00:07:14,000 --> 00:07:18,139
like initially the first version of

00:07:15,710 --> 00:07:19,939
Pantheon actually did this and it was

00:07:18,139 --> 00:07:22,069
nice because it was familiar it was like

00:07:19,939 --> 00:07:23,599
the same tools you would use on a single

00:07:22,069 --> 00:07:26,090
physical machine you would deploy it a

00:07:23,599 --> 00:07:27,949
place like Rackspace you got good

00:07:26,090 --> 00:07:29,539
isolation like there were no other

00:07:27,949 --> 00:07:32,569
customers that were going to step on you

00:07:29,539 --> 00:07:35,509
from a security or a resource

00:07:32,569 --> 00:07:39,650
perspective but it was hella expensive

00:07:35,509 --> 00:07:41,569
to run this because you every single box

00:07:39,650 --> 00:07:43,069
that would be able to credibly pack all

00:07:41,569 --> 00:07:44,629
these elements of the stack into one

00:07:43,069 --> 00:07:46,250
machine would be at least a hundred

00:07:44,629 --> 00:07:49,460
bucks a month in terms of memory

00:07:46,250 --> 00:07:51,949
necessary and then it so it couldn't go

00:07:49,460 --> 00:07:54,409
down scale because 100 bucks a month is

00:07:51,949 --> 00:07:56,150
is pretty expensive on a per site basis

00:07:54,409 --> 00:07:59,000
especially you spin up additional ones

00:07:56,150 --> 00:08:00,409
for development and testing and it also

00:07:59,000 --> 00:08:03,440
had no ability to really scale up

00:08:00,409 --> 00:08:05,719
because you couldn't spread it across a

00:08:03,440 --> 00:08:07,250
cluster if you were just consolidating

00:08:05,719 --> 00:08:09,800
all the resources onto a single machine

00:08:07,250 --> 00:08:13,039
and the approach is inherent in this

00:08:09,800 --> 00:08:16,610
limited that and then we kind of have

00:08:13,039 --> 00:08:19,219
the traditional big choice for a cluster

00:08:16,610 --> 00:08:20,990
choice for big websites where it was

00:08:19,219 --> 00:08:23,779
even more expensive to deploy one of

00:08:20,990 --> 00:08:26,089
these for a site and the biggest

00:08:23,779 --> 00:08:27,560
problems with this is that of as I

00:08:26,089 --> 00:08:29,240
talked about before you really need

00:08:27,560 --> 00:08:32,180
great development resources to maintain

00:08:29,240 --> 00:08:34,610
and develop these sites because you need

00:08:32,180 --> 00:08:36,339
a representative stack you can't debug

00:08:34,610 --> 00:08:39,709
varnish if you don't have varnish

00:08:36,339 --> 00:08:41,419
available you can't debug how your

00:08:39,709 --> 00:08:44,000
indexing to something like solar if you

00:08:41,419 --> 00:08:45,470
don't have solar available and more

00:08:44,000 --> 00:08:47,420
importantly with clusters you can't

00:08:45,470 --> 00:08:50,149
debug latency issues between your

00:08:47,420 --> 00:08:51,709
database and application or latency and

00:08:50,149 --> 00:08:54,390
performance is used with the file system

00:08:51,709 --> 00:08:57,360
unless you have those latency

00:08:54,390 --> 00:08:59,820
a representative latency in your tests

00:08:57,360 --> 00:09:01,470
or development environment so that left

00:08:59,820 --> 00:09:04,350
us was through left us with three

00:09:01,470 --> 00:09:06,030
choices here we're for doing development

00:09:04,350 --> 00:09:07,890
on this type of thing you could clone

00:09:06,030 --> 00:09:10,500
the whole stack and have a separate

00:09:07,890 --> 00:09:11,690
development or testing cluster which is

00:09:10,500 --> 00:09:14,340
really expensive you're basically

00:09:11,690 --> 00:09:16,650
creating a multiplier maybe less than a

00:09:14,340 --> 00:09:19,200
hundred percent multiplier but still a

00:09:16,650 --> 00:09:20,670
big one on the stack you could set

00:09:19,200 --> 00:09:22,470
something up that was not representative

00:09:20,670 --> 00:09:24,450
elsewhere you could say do development

00:09:22,470 --> 00:09:26,400
on single virtual machines which means

00:09:24,450 --> 00:09:29,870
that you're not actually able to test

00:09:26,400 --> 00:09:33,570
things like latency issues or

00:09:29,870 --> 00:09:35,370
performance of the architecture when we

00:09:33,570 --> 00:09:36,900
outside the production environment or

00:09:35,370 --> 00:09:38,850
you could do Co deployment which is

00:09:36,900 --> 00:09:41,850
where you have this production cluster

00:09:38,850 --> 00:09:43,920
and you're also doing the kind of QA or

00:09:41,850 --> 00:09:46,050
development on that same cluster which

00:09:43,920 --> 00:09:47,490
is perfectly representative but puts

00:09:46,050 --> 00:09:49,500
your production environment in danger

00:09:47,490 --> 00:09:51,570
because if you make mistakes and

00:09:49,500 --> 00:09:54,570
development you could suck up a bunch of

00:09:51,570 --> 00:09:56,550
resources and and undermine the

00:09:54,570 --> 00:09:59,190
stability of the production site none of

00:09:56,550 --> 00:10:04,200
these are great choices of for cost or

00:09:59,190 --> 00:10:06,150
accuracy and so we have to think about

00:10:04,200 --> 00:10:08,490
like what we're doing here with all

00:10:06,150 --> 00:10:10,290
these sites as we have more development

00:10:08,490 --> 00:10:12,630
testing environments as we have more and

00:10:10,290 --> 00:10:14,880
more websites are we going to kind of

00:10:12,630 --> 00:10:16,710
rubber stamp this out a million times

00:10:14,880 --> 00:10:18,570
over and I'm think I can make the case

00:10:16,710 --> 00:10:21,600
of why we shouldn't actually do that

00:10:18,570 --> 00:10:24,090
because it's ridiculously inefficient

00:10:21,600 --> 00:10:25,890
not just from a cost perspective but

00:10:24,090 --> 00:10:30,030
it's ridiculously environmentally

00:10:25,890 --> 00:10:34,320
damaging as well just a little bit more

00:10:30,030 --> 00:10:35,910
background on me I've I have a bit of an

00:10:34,320 --> 00:10:38,340
interesting kind of combination of

00:10:35,910 --> 00:10:40,440
things that I work on because my roots

00:10:38,340 --> 00:10:41,610
go back all the way in Drupal I think I

00:10:40,440 --> 00:10:45,600
deployed my first Drupal site around

00:10:41,610 --> 00:10:47,760
2006 and around 2007 I started doing

00:10:45,600 --> 00:10:50,550
infrastructure work for Drupal and then

00:10:47,760 --> 00:10:52,080
as time went on i joined security team

00:10:50,550 --> 00:10:54,150
i've done a bunch of scalability and

00:10:52,080 --> 00:10:56,610
performance work more recently I've

00:10:54,150 --> 00:11:00,330
gotten involved in the kind of core

00:10:56,610 --> 00:11:01,920
tooling of how we deploy systems with my

00:11:00,330 --> 00:11:04,080
work on system d which is the first

00:11:01,920 --> 00:11:05,820
thing that starts after the kernel and

00:11:04,080 --> 00:11:07,590
almost every major distribution now if

00:11:05,820 --> 00:11:08,370
it starts all the services it starts

00:11:07,590 --> 00:11:12,700
this process

00:11:08,370 --> 00:11:15,100
and I've mostly focused that work on the

00:11:12,700 --> 00:11:18,130
needs for deploying things at density

00:11:15,100 --> 00:11:20,410
managing a resource management building

00:11:18,130 --> 00:11:21,910
resource management tools handling the

00:11:20,410 --> 00:11:24,130
aggregation of events in the

00:11:21,910 --> 00:11:25,810
infrastructure and making sure that

00:11:24,130 --> 00:11:27,910
everything can kind of launch on demand

00:11:25,810 --> 00:11:31,060
as I'll talk about and then I've fed all

00:11:27,910 --> 00:11:33,400
of this into kind of why I started

00:11:31,060 --> 00:11:34,420
Pantheon with my co-founders all of

00:11:33,400 --> 00:11:38,860
which are kind of here at the conference

00:11:34,420 --> 00:11:40,840
and we've dedicated our company just

00:11:38,860 --> 00:11:42,430
solving this problem of like of how

00:11:40,840 --> 00:11:43,300
developers build sites and how we can

00:11:42,430 --> 00:11:46,720
deploy all this infrastructure

00:11:43,300 --> 00:11:48,540
efficiently and we do that for over a

00:11:46,720 --> 00:11:50,860
hundred thousand websites my

00:11:48,540 --> 00:11:52,930
back-of-the-envelope calculation shows

00:11:50,860 --> 00:11:54,880
that about one out of a thousand pages

00:11:52,930 --> 00:11:58,030
on the internet loads from Pantheon

00:11:54,880 --> 00:11:59,740
servers in some way and I don't know

00:11:58,030 --> 00:12:01,030
that if that's exactly right but it's

00:11:59,740 --> 00:12:02,980
probably the right order of magnitude

00:12:01,030 --> 00:12:05,370
there there's no really great

00:12:02,980 --> 00:12:08,710
comprehensive measure of all web traffic

00:12:05,370 --> 00:12:10,390
and we not only run a hundred thousand

00:12:08,710 --> 00:12:11,560
websites for every one of those we

00:12:10,390 --> 00:12:14,830
provide a development and testing

00:12:11,560 --> 00:12:16,840
environment if not another stack and to

00:12:14,830 --> 00:12:18,970
do that we could have deployed 400,000

00:12:16,840 --> 00:12:21,610
VMs which would have made is very

00:12:18,970 --> 00:12:23,530
popular with rackspace or Amazon but it

00:12:21,610 --> 00:12:25,030
would have made us very unpopular with

00:12:23,530 --> 00:12:27,730
anyone using the service because we

00:12:25,030 --> 00:12:29,530
would have had to pass that cost on so

00:12:27,730 --> 00:12:31,420
instead we've developed some novel

00:12:29,530 --> 00:12:33,640
approaches and we did containers before

00:12:31,420 --> 00:12:34,870
they were really call the containers and

00:12:33,640 --> 00:12:36,370
if you look at the source code to

00:12:34,870 --> 00:12:38,620
Pantheon it's called we call them

00:12:36,370 --> 00:12:41,590
bindings because they associate a

00:12:38,620 --> 00:12:43,390
service with a website or an environment

00:12:41,590 --> 00:12:45,370
Leonard on our infrastructure well call

00:12:43,390 --> 00:12:47,880
them containers for the sake of popular

00:12:45,370 --> 00:12:50,200
language now that everyone calls it that

00:12:47,880 --> 00:12:53,650
and we think this can be a choice for

00:12:50,200 --> 00:12:55,090
all websites there are various elements

00:12:53,650 --> 00:12:57,220
of how you would deploy it differently

00:12:55,090 --> 00:12:58,720
for different sites but we think it

00:12:57,220 --> 00:13:01,450
captures almost all the advantages

00:12:58,720 --> 00:13:04,120
without capturing most of the problems

00:13:01,450 --> 00:13:06,430
and since that it provides the most

00:13:04,120 --> 00:13:08,410
reliable resource isolation that you can

00:13:06,430 --> 00:13:10,990
get outside of separate machines or

00:13:08,410 --> 00:13:12,670
separate virtual machines it provides a

00:13:10,990 --> 00:13:13,750
representative environment that you can

00:13:12,670 --> 00:13:15,580
actually have clustered with

00:13:13,750 --> 00:13:18,970
representative Layton sees for every

00:13:15,580 --> 00:13:21,610
single stack and it's really low cost of

00:13:18,970 --> 00:13:21,930
in terms of the resources you have to

00:13:21,610 --> 00:13:23,760
deploy

00:13:21,930 --> 00:13:25,110
for each one because you're not

00:13:23,760 --> 00:13:26,610
deploying an operating system you're not

00:13:25,110 --> 00:13:29,610
deploying a hypervisor you're just

00:13:26,610 --> 00:13:31,710
deploying the runtime and the I think

00:13:29,610 --> 00:13:34,170
the cons we can engineer our way out of

00:13:31,710 --> 00:13:36,089
in a sense of how familiar it is how

00:13:34,170 --> 00:13:39,140
complex it can be to configure and

00:13:36,089 --> 00:13:41,940
orchestrate containers and dealing with

00:13:39,140 --> 00:13:43,830
the different types of isolation whether

00:13:41,940 --> 00:13:46,290
you're using kind of a hard isolation of

00:13:43,830 --> 00:13:48,720
specific assigned resources or sort of a

00:13:46,290 --> 00:13:50,190
proportional isolation that provides

00:13:48,720 --> 00:13:53,970
something more like a power grid where

00:13:50,190 --> 00:13:56,580
you turn on more power stage power

00:13:53,970 --> 00:13:59,520
plants as demand goes up and then you

00:13:56,580 --> 00:14:01,320
control the distribution of that which

00:13:59,520 --> 00:14:02,370
is another thing that we've done a lot

00:14:01,320 --> 00:14:04,890
of engineering work that I'll talk about

00:14:02,370 --> 00:14:08,339
on here and also it gets down to the

00:14:04,890 --> 00:14:10,830
fact that of Nyx t allow who's our

00:14:08,339 --> 00:14:13,740
director of engineering likes to say

00:14:10,830 --> 00:14:17,730
most sore most servers no problems in

00:14:13,740 --> 00:14:19,709
the sense that it's also a cost issue of

00:14:17,730 --> 00:14:22,260
just having the number of people to deal

00:14:19,709 --> 00:14:24,600
with the amount of percentages of

00:14:22,260 --> 00:14:26,550
failure of physical hardware or virtual

00:14:24,600 --> 00:14:28,110
machines where for every machine you

00:14:26,550 --> 00:14:30,060
have deployed a certain percentage of

00:14:28,110 --> 00:14:32,279
them will fail over a certain amount of

00:14:30,060 --> 00:14:33,660
time and you have to have a certain

00:14:32,279 --> 00:14:35,130
amount of human staff whether you're

00:14:33,660 --> 00:14:37,110
contracting it through a vendor or

00:14:35,130 --> 00:14:39,600
directly employing it to be able to deal

00:14:37,110 --> 00:14:41,610
with that amazon pretty much has people

00:14:39,600 --> 00:14:43,709
constantly going through the aisles and

00:14:41,610 --> 00:14:46,140
their data centers unwrapping damaged

00:14:43,709 --> 00:14:47,670
servers racking new ones there's a lot

00:14:46,140 --> 00:14:53,550
of human overhead beyond just the power

00:14:47,670 --> 00:14:56,190
for this stuff and it ultimately goes to

00:14:53,550 --> 00:14:57,330
the goals of kind of doing any kind of

00:14:56,190 --> 00:14:59,520
computing I don't think that really

00:14:57,330 --> 00:15:03,300
delivering Drupal websites is a

00:14:59,520 --> 00:15:04,200
particularly unique problem because the

00:15:03,300 --> 00:15:05,459
goals that we're all trying to

00:15:04,200 --> 00:15:08,760
accomplish here is making something work

00:15:05,459 --> 00:15:10,230
and making an efficient which is about

00:15:08,760 --> 00:15:12,060
making it efficient for the developers

00:15:10,230 --> 00:15:14,339
the human element making an efficient

00:15:12,060 --> 00:15:16,410
for the underlying infrastructure and

00:15:14,339 --> 00:15:17,760
that's pretty much it if you accomplish

00:15:16,410 --> 00:15:20,190
your goal and you do it efficiently

00:15:17,760 --> 00:15:22,890
there's really nothing there there's no

00:15:20,190 --> 00:15:24,930
other factor in there and I think that

00:15:22,890 --> 00:15:27,620
that's really this is really the core of

00:15:24,930 --> 00:15:30,000
why we build the infrastructure we built

00:15:27,620 --> 00:15:32,400
to give an idea of the magnitude of the

00:15:30,000 --> 00:15:34,260
problem in 2012 which is quite a long

00:15:32,400 --> 00:15:35,400
time ago at this point and it's only

00:15:34,260 --> 00:15:36,990
gotten worse since

00:15:35,400 --> 00:15:41,970
data centers took two percent of all

00:15:36,990 --> 00:15:43,920
power in the US that's a lot the when I

00:15:41,970 --> 00:15:46,620
was looking up the numbers on this i was

00:15:43,920 --> 00:15:49,410
expecting some fraction of a percent but

00:15:46,620 --> 00:15:50,820
the we run a lot of data centers and

00:15:49,410 --> 00:15:53,010
they're really power of hungry because

00:15:50,820 --> 00:15:56,400
every watt that a computer uses is

00:15:53,010 --> 00:15:58,170
basically x three because there's the

00:15:56,400 --> 00:16:00,810
watt going into the computer there's the

00:15:58,170 --> 00:16:02,730
what and then there are two wats going

00:16:00,810 --> 00:16:04,410
there's a lot going into the computer

00:16:02,730 --> 00:16:07,200
almost all of which is released as heat

00:16:04,410 --> 00:16:09,420
and then there's about two wats going to

00:16:07,200 --> 00:16:11,070
cooling that heat out of the data center

00:16:09,420 --> 00:16:14,310
because air conditioning is at best

00:16:11,070 --> 00:16:16,680
fifty percent efficient so most of data

00:16:14,310 --> 00:16:18,330
center engineering revolves around power

00:16:16,680 --> 00:16:21,690
management and cooling management

00:16:18,330 --> 00:16:24,390
because managing that at density is is

00:16:21,690 --> 00:16:25,770
tough and part of the reason why that's

00:16:24,390 --> 00:16:27,450
so tough and we've created such a

00:16:25,770 --> 00:16:30,900
problem is we're really bad at doing it

00:16:27,450 --> 00:16:33,270
efficiently so let's so this was a neat

00:16:30,900 --> 00:16:38,250
study where they took advantage of

00:16:33,270 --> 00:16:42,300
scheduling various scheduling anomalies

00:16:38,250 --> 00:16:45,390
that you can that you can monitor in

00:16:42,300 --> 00:16:47,730
intel processors to identify how much

00:16:45,390 --> 00:16:51,090
utilization was happening on machines on

00:16:47,730 --> 00:16:53,940
Amazon's ec2 infrastructure based on how

00:16:51,090 --> 00:16:56,400
the CPU behaved with the other

00:16:53,940 --> 00:16:58,080
co-resident virtual machines and so what

00:16:56,400 --> 00:17:00,900
they basically did is spun up a ton of

00:16:58,080 --> 00:17:02,580
tiny virtual machines and then asked the

00:17:00,900 --> 00:17:04,290
hypervisor certain things that go back

00:17:02,580 --> 00:17:06,180
to the core CPU where they could

00:17:04,290 --> 00:17:09,510
determine what the overall load of the

00:17:06,180 --> 00:17:11,910
box running the hypervisor was and even

00:17:09,510 --> 00:17:14,459
on an infrastructure with like ec2 where

00:17:11,910 --> 00:17:15,990
you can literally the provision boxes on

00:17:14,459 --> 00:17:17,970
demand and tear them down when you're

00:17:15,990 --> 00:17:19,410
done with them which is massively easier

00:17:17,970 --> 00:17:21,870
than it ever was a physical hardware

00:17:19,410 --> 00:17:24,300
they only found about a 7.3 percent

00:17:21,870 --> 00:17:27,030
average utilization of the CPUs on those

00:17:24,300 --> 00:17:28,980
machines which isn't super shocking it's

00:17:27,030 --> 00:17:30,990
and it's probably far worse for most

00:17:28,980 --> 00:17:33,240
physical hardware that people have a

00:17:30,990 --> 00:17:35,670
high overhead bureaucratically for

00:17:33,240 --> 00:17:38,450
provisioning and tearing down and this

00:17:35,670 --> 00:17:41,580
is pretty awful so two percent of power

00:17:38,450 --> 00:17:44,370
in the US and we're actually making

00:17:41,580 --> 00:17:45,810
about 7.3 percent efficient use of it

00:17:44,370 --> 00:17:49,080
which is I think better than an

00:17:45,810 --> 00:17:53,730
incandescent bulb but not mass

00:17:49,080 --> 00:17:58,590
sively um and so this is not a new story

00:17:53,730 --> 00:18:00,390
of the timeshare computing to

00:17:58,590 --> 00:18:01,860
efficiently make use of computational

00:18:00,390 --> 00:18:04,019
resources has been around for a long

00:18:01,860 --> 00:18:06,659
time as long as this guy and the screen

00:18:04,019 --> 00:18:08,519
has been around and ultimately it comes

00:18:06,659 --> 00:18:11,970
back to that problem of how do you

00:18:08,519 --> 00:18:14,130
effectively take the CPU resources and

00:18:11,970 --> 00:18:16,289
other resources we have and efficiently

00:18:14,130 --> 00:18:18,960
break them into units so that we can

00:18:16,289 --> 00:18:21,630
make use of them and not make use of

00:18:18,960 --> 00:18:23,760
them at 7.3 percent but much much closer

00:18:21,630 --> 00:18:25,860
to one hundred percent as much as

00:18:23,760 --> 00:18:28,250
possible without compromising the

00:18:25,860 --> 00:18:33,360
quality and reliability of the result

00:18:28,250 --> 00:18:35,100
and the it started this has always been

00:18:33,360 --> 00:18:37,679
a problem because something is always

00:18:35,100 --> 00:18:39,960
scarce about doing computation whether

00:18:37,679 --> 00:18:42,510
it's the hardware being scarce or the

00:18:39,960 --> 00:18:44,070
power being scarce or the space to be

00:18:42,510 --> 00:18:45,929
able to deploy the systems being scarce

00:18:44,070 --> 00:18:48,120
this has always been something we

00:18:45,929 --> 00:18:50,639
focused on and it's always on some part

00:18:48,120 --> 00:18:52,620
of the books and so it's obviously had a

00:18:50,639 --> 00:18:54,059
long history of people trying to

00:18:52,620 --> 00:18:56,279
confront this problem starting in the

00:18:54,059 --> 00:18:58,260
50s of batch processing going into the

00:18:56,279 --> 00:19:01,139
70s with terminals and even virtual

00:18:58,260 --> 00:19:03,659
machines on mainframes the developing a

00:19:01,139 --> 00:19:05,549
client-server architecture so that not

00:19:03,659 --> 00:19:10,320
everything had to be consolidated into a

00:19:05,549 --> 00:19:12,450
single cluster or mainframe machine to

00:19:10,320 --> 00:19:14,490
the point where they had to consolidate

00:19:12,450 --> 00:19:15,870
all the processing power because of like

00:19:14,490 --> 00:19:19,559
how dense they could actually deploy

00:19:15,870 --> 00:19:21,659
that computation and then kind of a

00:19:19,559 --> 00:19:23,730
reversal of that where we reconsolidate

00:19:21,659 --> 00:19:26,460
the hardware to be able to have those

00:19:23,730 --> 00:19:28,940
systems run more efficiently but but

00:19:26,460 --> 00:19:31,139
ultimately all of these things are

00:19:28,940 --> 00:19:33,389
culminating on the idea of we're trying

00:19:31,139 --> 00:19:35,190
to push around the computation burden in

00:19:33,389 --> 00:19:37,010
various ways and slice it in various

00:19:35,190 --> 00:19:40,289
ways so that we can do it efficiently

00:19:37,010 --> 00:19:43,559
for whatever goals we have because

00:19:40,289 --> 00:19:45,360
computation isn't a is not an end in

00:19:43,559 --> 00:19:47,159
itself and always serve some other end

00:19:45,360 --> 00:19:48,960
and it always has to be on the books of

00:19:47,159 --> 00:19:54,029
something else to accomplish something

00:19:48,960 --> 00:19:56,399
else but why do people use these these

00:19:54,029 --> 00:19:58,230
inefficient architectures at least in

00:19:56,399 --> 00:20:00,299
terms of virtual machines and things

00:19:58,230 --> 00:20:02,950
like ec2 and why do they end up only

00:20:00,299 --> 00:20:05,150
utilizing them about 7.3

00:20:02,950 --> 00:20:07,730
and it's because people like virtual

00:20:05,150 --> 00:20:10,190
machines when I went back to the

00:20:07,730 --> 00:20:12,380
efficiency I mentioned two kinds of

00:20:10,190 --> 00:20:14,360
efficiency computational and the human

00:20:12,380 --> 00:20:15,680
side and one thing that virtual machines

00:20:14,360 --> 00:20:18,050
have been great about is the human

00:20:15,680 --> 00:20:20,210
efficiency that the amount of extra

00:20:18,050 --> 00:20:22,580
things you need to learn to work on our

00:20:20,210 --> 00:20:24,020
virtual machine or redeploy an

00:20:22,580 --> 00:20:25,670
application to our virtual machine is

00:20:24,020 --> 00:20:27,200
really low because it looks just like

00:20:25,670 --> 00:20:30,890
physical hardware and that's the point

00:20:27,200 --> 00:20:32,750
that it's it's very human efficient even

00:20:30,890 --> 00:20:37,310
if we're paying the price in terms of

00:20:32,750 --> 00:20:39,440
the the utilization of power grid and

00:20:37,310 --> 00:20:42,770
they've also been great about reducing

00:20:39,440 --> 00:20:45,260
the human element of running these

00:20:42,770 --> 00:20:46,760
systems where you used to have to have a

00:20:45,260 --> 00:20:48,320
physical machine for each level of

00:20:46,760 --> 00:20:50,690
isolation you wanted for all these

00:20:48,320 --> 00:20:52,220
different systems and by consolidating

00:20:50,690 --> 00:20:53,900
them onto virtual machines there are

00:20:52,220 --> 00:20:55,550
fewer people pulling carts through the

00:20:53,900 --> 00:20:57,890
data center racking and unwrapping

00:20:55,550 --> 00:20:59,600
systems because they're having issues or

00:20:57,890 --> 00:21:01,520
that they need to be provisioned so

00:20:59,600 --> 00:21:03,290
that's another area of efficiency not

00:21:01,520 --> 00:21:08,230
even the developer just the person with

00:21:03,290 --> 00:21:10,460
the cart through the data center and the

00:21:08,230 --> 00:21:13,160
but this is almost a kind of

00:21:10,460 --> 00:21:15,170
computational skeuomorphic design in the

00:21:13,160 --> 00:21:18,200
sense that it akin this element that's

00:21:15,170 --> 00:21:20,000
familiar and we've reapplied it on to

00:21:18,200 --> 00:21:23,450
another context and emulated that

00:21:20,000 --> 00:21:25,070
context it's kind of like when used the

00:21:23,450 --> 00:21:27,260
previous generations of the iphone

00:21:25,070 --> 00:21:28,610
design where they would have or iOS

00:21:27,260 --> 00:21:29,930
where they would have the calendar

00:21:28,610 --> 00:21:32,420
application it would look like physical

00:21:29,930 --> 00:21:34,790
pieces of paper or a notepad and then it

00:21:32,420 --> 00:21:36,080
was designed to evoke this idea of you

00:21:34,790 --> 00:21:37,520
know how to write on a notepad you know

00:21:36,080 --> 00:21:39,560
how to write things onto a scheduler

00:21:37,520 --> 00:21:42,080
here's on a familiar interface for you

00:21:39,560 --> 00:21:43,670
to work with and virtual machines are a

00:21:42,080 --> 00:21:45,650
much much more technical interface

00:21:43,670 --> 00:21:47,810
that's being replicated but it's

00:21:45,650 --> 00:21:51,110
ultimately for the same purpose the idea

00:21:47,810 --> 00:21:54,860
of increased similarity but that

00:21:51,110 --> 00:21:57,710
similarity that confined that constraint

00:21:54,860 --> 00:21:59,480
of similarity also means that we can't

00:21:57,710 --> 00:22:01,490
improve the design very much you can't

00:21:59,480 --> 00:22:03,500
make virtual machines stop looking like

00:22:01,490 --> 00:22:06,380
physical machines otherwise you lose the

00:22:03,500 --> 00:22:11,960
point of virtual machines but I think

00:22:06,380 --> 00:22:14,990
that we need to make this leap the there

00:22:11,960 --> 00:22:16,770
are all sorts of great things I p.m. I'm

00:22:14,990 --> 00:22:22,770
sorry just tracking time here

00:22:16,770 --> 00:22:24,210
the and what we need to do is we need to

00:22:22,770 --> 00:22:25,860
actually make a leap and basically say

00:22:24,210 --> 00:22:29,970
we should abandon some of the old ways

00:22:25,860 --> 00:22:32,510
and look at containers as these as the

00:22:29,970 --> 00:22:34,860
efficiency Paragons they actually can be

00:22:32,510 --> 00:22:37,170
and this was just a funny anything in

00:22:34,860 --> 00:22:38,460
San Francisco where VMware was like you

00:22:37,170 --> 00:22:40,620
know what virtualization did for

00:22:38,460 --> 00:22:42,720
computing well I stopped there and it

00:22:40,620 --> 00:22:44,370
was funny because vmware has actually

00:22:42,720 --> 00:22:48,030
published a lot of kind of fud pieces on

00:22:44,370 --> 00:22:51,090
containers and they're they're focused

00:22:48,030 --> 00:22:53,250
as a company on virtualization but

00:22:51,090 --> 00:22:56,070
they've they know that they've the

00:22:53,250 --> 00:22:57,840
market for virtualization as our product

00:22:56,070 --> 00:22:59,670
is has reached the point of maturity

00:22:57,840 --> 00:23:00,810
that there's not much further to go so

00:22:59,670 --> 00:23:03,480
they're looking at other opportunities

00:23:00,810 --> 00:23:05,190
and their act this was several years ago

00:23:03,480 --> 00:23:06,450
that they had this up and they're now

00:23:05,190 --> 00:23:08,370
actually introducing a container

00:23:06,450 --> 00:23:13,050
infrastructure for data centers as part

00:23:08,370 --> 00:23:16,020
of vmware's cloud product and containers

00:23:13,050 --> 00:23:18,870
actually are amazing at achieving

00:23:16,020 --> 00:23:20,910
efficiency not just for the underlying

00:23:18,870 --> 00:23:22,800
infrastructure but if developers start

00:23:20,910 --> 00:23:25,470
working on that working with them the

00:23:22,800 --> 00:23:28,230
right ways we can achieve efficiency for

00:23:25,470 --> 00:23:30,240
developers and the data centers jointly

00:23:28,230 --> 00:23:33,890
this is just a neat kind of comparison

00:23:30,240 --> 00:23:36,480
here of what happened in shipping when

00:23:33,890 --> 00:23:40,070
containers got introduced the idea being

00:23:36,480 --> 00:23:42,240
that instead of working with non-uniform

00:23:40,070 --> 00:23:44,010
environments that people would be

00:23:42,240 --> 00:23:47,520
deploying applications to custom server

00:23:44,010 --> 00:23:52,380
clusters custom virtual machine clusters

00:23:47,520 --> 00:23:54,540
of custom isolation layers for managing

00:23:52,380 --> 00:23:56,490
the resources of applications they can

00:23:54,540 --> 00:23:57,780
deal with a uniform interface and I'm

00:23:56,490 --> 00:24:00,630
not suggesting that we're going to gain

00:23:57,780 --> 00:24:04,130
the exact same efficiency by moving the

00:24:00,630 --> 00:24:06,420
containers on our infrastructure but

00:24:04,130 --> 00:24:08,760
dealing with uniform environments is

00:24:06,420 --> 00:24:11,790
actually important and having developers

00:24:08,760 --> 00:24:15,530
use those uniform environments of can

00:24:11,790 --> 00:24:17,900
provide them the same flexibility and

00:24:15,530 --> 00:24:20,220
familiarity that they can get from

00:24:17,900 --> 00:24:23,820
virtual machines today just in a

00:24:20,220 --> 00:24:25,860
different stack the so containers

00:24:23,820 --> 00:24:30,470
actually have their own long history as

00:24:25,860 --> 00:24:32,750
well as as building blocks for

00:24:30,470 --> 00:24:36,049
sure starting in about nineteen eighty

00:24:32,750 --> 00:24:37,760
six now I've had a couple engineers from

00:24:36,049 --> 00:24:41,000
IBM dispute this that they're not quite

00:24:37,760 --> 00:24:43,880
the same thing but what I'm defining a

00:24:41,000 --> 00:24:47,150
container as is a mechanism for managing

00:24:43,880 --> 00:24:49,309
resources and security partitions on a

00:24:47,150 --> 00:24:50,990
system in a way where you're not only

00:24:49,309 --> 00:24:52,610
running one thing at a time like batch

00:24:50,990 --> 00:24:54,770
processing has been around forever where

00:24:52,610 --> 00:24:55,880
you stop doing one thing and you start

00:24:54,770 --> 00:24:57,740
doing another and you do it first

00:24:55,880 --> 00:24:59,510
different units of time but can you

00:24:57,740 --> 00:25:02,120
actually juggle them like can you

00:24:59,510 --> 00:25:04,280
actually give one thing thirty percent

00:25:02,120 --> 00:25:06,409
of the system and another thing seventy

00:25:04,280 --> 00:25:08,929
percent of the system and have that be

00:25:06,409 --> 00:25:11,960
persistent that that both applications

00:25:08,929 --> 00:25:14,630
get to run and the closest I could find

00:25:11,960 --> 00:25:17,000
was workload partitions back on the

00:25:14,630 --> 00:25:18,919
mainframe environment of AIX where you

00:25:17,000 --> 00:25:20,539
could actually define slices of the

00:25:18,919 --> 00:25:22,309
system in terms of resources that would

00:25:20,539 --> 00:25:24,700
be devoted to one application or another

00:25:22,309 --> 00:25:28,090
application so this goes back a while

00:25:24,700 --> 00:25:30,890
and then if you see my original post on

00:25:28,090 --> 00:25:32,169
the linux journal there's an angry

00:25:30,890 --> 00:25:35,260
comment from someone who works on the

00:25:32,169 --> 00:25:37,400
freebsd kernel about how long ago

00:25:35,260 --> 00:25:39,710
freebsd has been doing this and why is

00:25:37,400 --> 00:25:42,380
it so popular now that merely Linux has

00:25:39,710 --> 00:25:45,710
gotten it he's the venerable author of

00:25:42,380 --> 00:25:49,010
our our prized varnish reverse proxy

00:25:45,710 --> 00:25:52,190
cache Paul an in camp and he's he's a

00:25:49,010 --> 00:25:54,350
great guy actually but the bsd has

00:25:52,190 --> 00:25:56,450
admittedly been doing this as long as

00:25:54,350 --> 00:25:59,720
are longer than anyone else in the open

00:25:56,450 --> 00:26:02,150
source world at least and then solaris

00:25:59,720 --> 00:26:04,309
actually introduced it a little later

00:26:02,150 --> 00:26:08,059
kind of back into the enterprise with

00:26:04,309 --> 00:26:10,400
zones in 2005 the idea of splitting out

00:26:08,059 --> 00:26:12,380
the resources that machines were using

00:26:10,400 --> 00:26:14,750
without emulating full-on virtual

00:26:12,380 --> 00:26:18,230
machines and then the rest is kind of

00:26:14,750 --> 00:26:20,390
more recent history where we've seen

00:26:18,230 --> 00:26:22,610
that google has built their entire

00:26:20,390 --> 00:26:25,190
infrastructure on containerization and

00:26:22,610 --> 00:26:27,620
they push their work on see groups which

00:26:25,190 --> 00:26:30,080
is designed to split out resources back

00:26:27,620 --> 00:26:31,940
into the Linux kernel and 2010 systemd

00:26:30,080 --> 00:26:34,100
got introduced which provides a direct

00:26:31,940 --> 00:26:36,110
way of configuring see groups in a

00:26:34,100 --> 00:26:38,179
straightforward way for user

00:26:36,110 --> 00:26:41,270
applications and then we actually

00:26:38,179 --> 00:26:43,830
started to see the word a container be

00:26:41,270 --> 00:26:46,140
really rising in the

00:26:43,830 --> 00:26:50,610
world in 2013 with docker and core OS

00:26:46,140 --> 00:26:53,100
the idea of a unit of software that you

00:26:50,610 --> 00:26:56,309
could basically put into a digital box

00:26:53,100 --> 00:26:58,649
and drop onto a system and then have it

00:26:56,309 --> 00:27:01,919
get wired into the kernel on the network

00:26:58,649 --> 00:27:05,340
layer as as the box required for the

00:27:01,919 --> 00:27:07,320
purpose of deployment I mean it's almost

00:27:05,340 --> 00:27:09,240
analogous to some of the refrigeration

00:27:07,320 --> 00:27:10,649
containers you see on ships where they

00:27:09,240 --> 00:27:12,990
have a standardized way of plugging

00:27:10,649 --> 00:27:14,760
power into them and monitoring into them

00:27:12,990 --> 00:27:16,320
so that they know that it's achieving

00:27:14,760 --> 00:27:18,149
the right temperature and it's getting

00:27:16,320 --> 00:27:21,380
it's running the refrigeration here and

00:27:18,149 --> 00:27:27,149
it fits in a place that's been defined

00:27:21,380 --> 00:27:29,610
and then it even more mainstream use at

00:27:27,149 --> 00:27:32,010
least for for production environments

00:27:29,610 --> 00:27:34,889
with alexi 1 point 0 and kuber Nettie's

00:27:32,010 --> 00:27:36,809
in 2014 kuber Nettie's is Google getting

00:27:34,889 --> 00:27:39,570
into the work the kind of open source

00:27:36,809 --> 00:27:42,059
orchestration game they've long used an

00:27:39,570 --> 00:27:44,190
analogous system internally to schedule

00:27:42,059 --> 00:27:46,590
how their containers get distributed but

00:27:44,190 --> 00:27:48,840
they they've only released that

00:27:46,590 --> 00:27:50,190
relatively recently and then now we're

00:27:48,840 --> 00:27:52,950
actually starting to see this kind of

00:27:50,190 --> 00:27:54,960
app containerisation spec from the folks

00:27:52,950 --> 00:27:57,360
at core OS and even public container

00:27:54,960 --> 00:27:59,039
clouds the idea that you could supply

00:27:57,360 --> 00:28:01,080
this unit that needs to get plugged in a

00:27:59,039 --> 00:28:03,510
certain way up to a provider and they

00:28:01,080 --> 00:28:06,000
would you know attach it and run it

00:28:03,510 --> 00:28:08,100
which is these are all signs of

00:28:06,000 --> 00:28:13,470
increasing maturity in that mark in that

00:28:08,100 --> 00:28:15,120
space and it provides all of this

00:28:13,470 --> 00:28:18,600
without having to duplicate the OS

00:28:15,120 --> 00:28:20,159
without having to to run a hypervisor

00:28:18,600 --> 00:28:21,899
without having to waste a bunch of

00:28:20,159 --> 00:28:26,490
resources this way and there's small

00:28:21,899 --> 00:28:28,950
units the they're useful in small units

00:28:26,490 --> 00:28:31,230
this is this is a remarkable thing here

00:28:28,950 --> 00:28:34,320
where it used to be that you could get

00:28:31,230 --> 00:28:37,049
256 megabytes on a lot of the public

00:28:34,320 --> 00:28:38,850
clouds but providers basically decided

00:28:37,049 --> 00:28:40,740
that those were useless because by the

00:28:38,850 --> 00:28:43,080
time you've started the operating system

00:28:40,740 --> 00:28:45,750
and done and are running basic services

00:28:43,080 --> 00:28:47,700
on the system 256 megabytes get gobbled

00:28:45,750 --> 00:28:49,860
up and you have almost no room left to

00:28:47,700 --> 00:28:51,299
run a real application load so that's

00:28:49,860 --> 00:28:53,700
that's kind of the overhead of each

00:28:51,299 --> 00:28:55,980
slice for a virtual machine that 256

00:28:53,700 --> 00:28:57,510
megabytes is a useless slice it's

00:28:55,980 --> 00:28:59,760
basically all of your cake

00:28:57,510 --> 00:29:01,740
going to the overhead before you even

00:28:59,760 --> 00:29:05,210
get to take a bite and that's

00:29:01,740 --> 00:29:09,210
fortunately not the case with containers

00:29:05,210 --> 00:29:11,100
they're lightweight you're just which

00:29:09,210 --> 00:29:13,740
also allows you to distribute the

00:29:11,100 --> 00:29:16,110
computing load much more densely that

00:29:13,740 --> 00:29:17,760
one of the fears of deploying machines

00:29:16,110 --> 00:29:20,370
and having them run it's a ninety

00:29:17,760 --> 00:29:22,440
percent utilization is what happens when

00:29:20,370 --> 00:29:24,180
it goes beyond that what happens when it

00:29:22,440 --> 00:29:25,530
hits ninety-nine percent utilization a

00:29:24,180 --> 00:29:28,020
hundred percent what happens when it

00:29:25,530 --> 00:29:30,390
starts swapping how do you respond to

00:29:28,020 --> 00:29:32,460
those events and the answer for physical

00:29:30,390 --> 00:29:33,870
hardware is really real really hard like

00:29:32,460 --> 00:29:36,240
upgrading physical hardware typically

00:29:33,870 --> 00:29:37,860
requires taking down the box installing

00:29:36,240 --> 00:29:39,600
more RAM and you can only do that to a

00:29:37,860 --> 00:29:41,130
certain point upgrading a virtual

00:29:39,600 --> 00:29:44,550
machine usually requires at least

00:29:41,130 --> 00:29:46,740
rebooting the instance because you

00:29:44,550 --> 00:29:48,240
you're probably provisioning it onto new

00:29:46,740 --> 00:29:50,820
hardware you might be migrating a bunch

00:29:48,240 --> 00:29:52,920
of data at a minimum you're having to

00:29:50,820 --> 00:29:55,020
pull all of the OS all of the

00:29:52,920 --> 00:29:57,360
application runtime and the actually

00:29:55,020 --> 00:29:59,960
application persistent data with that

00:29:57,360 --> 00:30:02,820
instance in order to rebalance load

00:29:59,960 --> 00:30:04,710
containers by contrast are really really

00:30:02,820 --> 00:30:06,450
lightweight because you might have the

00:30:04,710 --> 00:30:08,280
application run time depending on what

00:30:06,450 --> 00:30:09,390
container system you use and you just

00:30:08,280 --> 00:30:11,880
have the persistent data with the

00:30:09,390 --> 00:30:14,130
application so you can run boxes hotter

00:30:11,880 --> 00:30:18,540
because you're able to rebalance more

00:30:14,130 --> 00:30:19,950
efficiently the and you're able to also

00:30:18,540 --> 00:30:21,660
rebalance more efficiently because the

00:30:19,950 --> 00:30:23,790
provisioning time is massively smaller

00:30:21,660 --> 00:30:26,190
where you're going from putting in a

00:30:23,790 --> 00:30:28,890
request where if you had the best

00:30:26,190 --> 00:30:30,390
physical of kind of traditional data

00:30:28,890 --> 00:30:32,010
center you could ever work with you

00:30:30,390 --> 00:30:34,350
could probably get a machine in 8 to 24

00:30:32,010 --> 00:30:35,940
hours that's really optimistic and my

00:30:34,350 --> 00:30:37,440
experience with a provider like

00:30:35,940 --> 00:30:40,440
Rackspace it typically takes a week or

00:30:37,440 --> 00:30:43,950
two you go to a cloud virtual machine

00:30:40,440 --> 00:30:46,050
and like the last few that I've deployed

00:30:43,950 --> 00:30:47,670
on say ec2 it takes five to ten minutes

00:30:46,050 --> 00:30:49,500
that's really not very agile for

00:30:47,670 --> 00:30:52,320
responding to load for an application

00:30:49,500 --> 00:30:54,030
and then you go to a container where if

00:30:52,320 --> 00:30:56,310
you're actually really pessimistic about

00:30:54,030 --> 00:30:59,520
the container deploying it takes 5 to 15

00:30:56,310 --> 00:31:01,680
seconds because it's a relatively

00:30:59,520 --> 00:31:03,510
lightweight of data lightweight set of

00:31:01,680 --> 00:31:06,570
data and it requires basically no boot

00:31:03,510 --> 00:31:11,250
time and no stand up of really advanced

00:31:06,570 --> 00:31:12,600
user land in some cases where you're

00:31:11,250 --> 00:31:14,960
just actually launching the container

00:31:12,600 --> 00:31:17,790
local OS not migrating data at all

00:31:14,960 --> 00:31:20,760
you're looking at a tiny tiny fraction

00:31:17,790 --> 00:31:22,200
of a system a second your start you're

00:31:20,760 --> 00:31:23,880
able to start applications this is

00:31:22,200 --> 00:31:27,870
actually kind of booting Debian on

00:31:23,880 --> 00:31:32,850
debian here and it takes less time than

00:31:27,870 --> 00:31:34,770
drupal bootstrapping so enough about the

00:31:32,850 --> 00:31:35,850
theory of why this is important let's

00:31:34,770 --> 00:31:38,970
get into what we're doing with

00:31:35,850 --> 00:31:42,480
containers of pantheon we have we run

00:31:38,970 --> 00:31:46,580
them really hot and go proud of that we

00:31:42,480 --> 00:31:49,050
have 30 about 30 gig servers and we put

00:31:46,580 --> 00:31:52,590
conservatively 150 containers each on

00:31:49,050 --> 00:31:55,050
them we're actually closer to 500 on

00:31:52,590 --> 00:31:56,880
some of our densest deployments and that

00:31:55,050 --> 00:32:02,250
means we're using every bit of those

00:31:56,880 --> 00:32:04,860
machines the if you look at the it looks

00:32:02,250 --> 00:32:06,390
like the databases here are a bit less

00:32:04,860 --> 00:32:09,510
dense than we're able to deploy the

00:32:06,390 --> 00:32:11,220
actual PHP run times but the the fact is

00:32:09,510 --> 00:32:13,020
is we're able to deploy them just at the

00:32:11,220 --> 00:32:14,760
overhead of php-fpm and just at the

00:32:13,020 --> 00:32:16,500
overhead of maury DB in terms of the

00:32:14,760 --> 00:32:18,840
memory that those things take and the

00:32:16,500 --> 00:32:22,140
CPU that those things take and so our

00:32:18,840 --> 00:32:25,170
average amount of actual memory per

00:32:22,140 --> 00:32:27,240
container is 205 megabytes which is

00:32:25,170 --> 00:32:29,430
smaller than the instances that got

00:32:27,240 --> 00:32:31,500
discontinued on rackspace because they

00:32:29,430 --> 00:32:33,690
would get dog gobbled up by the OS

00:32:31,500 --> 00:32:35,040
before you would have any resources and

00:32:33,690 --> 00:32:36,770
that makes us at least twice as

00:32:35,040 --> 00:32:42,030
efficient as running virtual machines

00:32:36,770 --> 00:32:45,420
just looking at that and on average on

00:32:42,030 --> 00:32:47,970
pantheon we actually deploy containers

00:32:45,420 --> 00:32:50,370
all the way from requesting them in the

00:32:47,970 --> 00:32:52,620
API to finding the box that they should

00:32:50,370 --> 00:32:54,150
run on to running the jobs on the box to

00:32:52,620 --> 00:32:55,710
provisionally container to running chef

00:32:54,150 --> 00:32:57,720
to actually configure the things in the

00:32:55,710 --> 00:32:59,400
container to downloading the data that

00:32:57,720 --> 00:33:02,940
should be in the container on an average

00:32:59,400 --> 00:33:04,980
about 20 seconds and that's including

00:33:02,940 --> 00:33:08,580
quite a lot of stateful data moving

00:33:04,980 --> 00:33:10,740
around and we are able to achieve this

00:33:08,580 --> 00:33:13,170
by having that kind of featherweight

00:33:10,740 --> 00:33:16,050
design for things even beyond what

00:33:13,170 --> 00:33:18,570
regular public container projects use

00:33:16,050 --> 00:33:20,910
one of the things we do that is I think

00:33:18,570 --> 00:33:22,530
unique is we run containers only when

00:33:20,910 --> 00:33:24,700
they're actually needed we use a

00:33:22,530 --> 00:33:26,290
technique called socket activation that

00:33:24,700 --> 00:33:27,930
waits for a network for us to come in

00:33:26,290 --> 00:33:30,480
before we actually start the container

00:33:27,930 --> 00:33:34,180
that actually takes care of starting

00:33:30,480 --> 00:33:36,640
engine X php-fpm and Marie D be in the

00:33:34,180 --> 00:33:38,080
background as the request comes in we

00:33:36,640 --> 00:33:39,970
take care of shutting things down in a

00:33:38,080 --> 00:33:42,190
way where that only adds a few seconds

00:33:39,970 --> 00:33:44,260
on the first request and this means that

00:33:42,190 --> 00:33:46,750
when we deploy all those developments

00:33:44,260 --> 00:33:48,280
tax all of those testing stacks all of

00:33:46,750 --> 00:33:50,410
those kind of development environment

00:33:48,280 --> 00:33:52,360
stacks that we have which constitute

00:33:50,410 --> 00:33:54,310
more than two-thirds of the actual

00:33:52,360 --> 00:33:56,830
stacks on our infrastructure if no one's

00:33:54,310 --> 00:33:59,470
using them they're not running there's

00:33:56,830 --> 00:34:02,080
no cpu going to them there's no memory

00:33:59,470 --> 00:34:04,540
going to them and that allows us to

00:34:02,080 --> 00:34:07,810
provide those stacks at really good

00:34:04,540 --> 00:34:11,110
efficiency the other things that we do

00:34:07,810 --> 00:34:13,720
that are also a typical is we make

00:34:11,110 --> 00:34:15,790
mutual use of the kind of base resources

00:34:13,720 --> 00:34:17,860
on the system if you look at a lot of

00:34:15,790 --> 00:34:20,440
container projects like docker you see

00:34:17,860 --> 00:34:21,730
them using their putting the whole run

00:34:20,440 --> 00:34:23,050
time and often a lot of the shared

00:34:21,730 --> 00:34:25,900
libraries into the container itself

00:34:23,050 --> 00:34:28,120
where what we do is we deploy every

00:34:25,900 --> 00:34:30,910
flavor of runtime to the base OS on the

00:34:28,120 --> 00:34:33,040
system into the base file system and

00:34:30,910 --> 00:34:35,140
then we allow the container to access

00:34:33,040 --> 00:34:36,850
those resources so that they only get

00:34:35,140 --> 00:34:39,370
mapped into memory once because one

00:34:36,850 --> 00:34:41,530
really neat thing that happens on Linux

00:34:39,370 --> 00:34:44,320
systems it's probably pretty common off

00:34:41,530 --> 00:34:45,820
of Linux systems too is if 100 things

00:34:44,320 --> 00:34:47,560
are using the same shared library or

00:34:45,820 --> 00:34:50,140
runtime it only goes into memory once

00:34:47,560 --> 00:34:52,090
and so that saves us about 10 gigabytes

00:34:50,140 --> 00:34:54,340
of RAM per server just not loading the

00:34:52,090 --> 00:34:56,920
PHP runtime and all the necessary

00:34:54,340 --> 00:34:58,180
libraries for every single site that is

00:34:56,920 --> 00:35:01,390
running a container on that

00:34:58,180 --> 00:35:03,430
infrastructure so we get the ninety

00:35:01,390 --> 00:35:05,110
percent savings there we get the mutual

00:35:03,430 --> 00:35:06,880
use of base system resources so we're

00:35:05,110 --> 00:35:08,590
not even deploying the runtimes multiple

00:35:06,880 --> 00:35:10,750
times and then we're not deploying the

00:35:08,590 --> 00:35:13,060
base unless at all multiple times within

00:35:10,750 --> 00:35:16,000
those containers all kind of multiplying

00:35:13,060 --> 00:35:20,590
out to make our infrastructure super

00:35:16,000 --> 00:35:22,840
efficient and then we also keep agile

00:35:20,590 --> 00:35:25,240
with this design by using a kind of Lego

00:35:22,840 --> 00:35:26,530
like architecture where there does all

00:35:25,240 --> 00:35:27,880
these blocks are designed to fit

00:35:26,530 --> 00:35:29,710
together and they're all designed to be

00:35:27,880 --> 00:35:31,810
interchangeable so when you deploy a

00:35:29,710 --> 00:35:35,110
large site to the platform we don't

00:35:31,810 --> 00:35:37,210
deploy huge containers on huge machines

00:35:35,110 --> 00:35:38,050
we actually would rather deploy 30

00:35:37,210 --> 00:35:39,900
containers

00:35:38,050 --> 00:35:42,250
that are all relatively small and

00:35:39,900 --> 00:35:44,440
comparable in size to the ones you would

00:35:42,250 --> 00:35:46,330
get even for a free site and then we

00:35:44,440 --> 00:35:49,890
just load balance among them and that

00:35:46,330 --> 00:35:52,300
works great for things like PHP and

00:35:49,890 --> 00:35:53,980
there's a neat analogy of kind of pets

00:35:52,300 --> 00:35:57,130
versus livestock of the idea of like

00:35:53,980 --> 00:35:59,080
what happens when one gets sick and that

00:35:57,130 --> 00:36:00,940
we treat our containers like livestock

00:35:59,080 --> 00:36:03,610
in the sense that this there's this huge

00:36:00,940 --> 00:36:06,880
fleet of resources we have and some of

00:36:03,610 --> 00:36:08,740
them may get sick in various ways and we

00:36:06,880 --> 00:36:10,980
deal with that as a kind of systemic

00:36:08,740 --> 00:36:13,690
problem rather than fretting over

00:36:10,980 --> 00:36:15,250
healing one thing and taking it to the

00:36:13,690 --> 00:36:16,960
doctor a bunch of times which is kind of

00:36:15,250 --> 00:36:20,020
the traditional way that people work

00:36:16,960 --> 00:36:22,540
with servers we also achieve this

00:36:20,020 --> 00:36:23,560
through scheduling which is we run our

00:36:22,540 --> 00:36:26,230
servers at about ninety percent

00:36:23,560 --> 00:36:27,610
utilization constantly rebalancing them

00:36:26,230 --> 00:36:30,280
with something we call the migration

00:36:27,610 --> 00:36:32,230
dragon the migration dragon is something

00:36:30,280 --> 00:36:34,960
that runs on some of the hottest boxes

00:36:32,230 --> 00:36:37,240
based on our scoring and then accuse

00:36:34,960 --> 00:36:38,860
those containers for migration and they

00:36:37,240 --> 00:36:42,070
automatically get rescheduled onto the

00:36:38,860 --> 00:36:44,140
least loaded boxes so all we have to do

00:36:42,070 --> 00:36:46,810
to rebalance load or take away the heat

00:36:44,140 --> 00:36:48,730
from a heavy server is to deploy an

00:36:46,810 --> 00:36:52,390
empty server and then it kind of

00:36:48,730 --> 00:36:54,310
naturally migrates off of there this all

00:36:52,390 --> 00:36:55,660
and then we also avoid resource

00:36:54,310 --> 00:36:57,610
saturation by running several

00:36:55,660 --> 00:37:01,120
representative workloads on the machines

00:36:57,610 --> 00:37:03,850
to that inform that scoring so if memory

00:37:01,120 --> 00:37:05,860
or CPU or disk i/o or network i/o or

00:37:03,850 --> 00:37:08,290
getting saturated we kind of score those

00:37:05,860 --> 00:37:09,910
down this is often handled in open

00:37:08,290 --> 00:37:12,670
source projects by projects like Cooper

00:37:09,910 --> 00:37:15,040
Nettie's where you it informs the

00:37:12,670 --> 00:37:16,690
scheduler about decisions for

00:37:15,040 --> 00:37:19,450
distributing the containers and this all

00:37:16,690 --> 00:37:20,940
manifests as an average container age of

00:37:19,450 --> 00:37:24,160
about 50 days on our infrastructure

00:37:20,940 --> 00:37:27,310
which allows us to do really neat things

00:37:24,160 --> 00:37:28,840
like when Rackspace introduced SSDs we

00:37:27,310 --> 00:37:30,250
just deployed a bunch of SSD machines

00:37:28,840 --> 00:37:33,810
waited for the containers to migrate

00:37:30,250 --> 00:37:36,490
onto them and then when the other can

00:37:33,810 --> 00:37:38,110
march the other servers as deprecated

00:37:36,490 --> 00:37:39,790
which migrated the rest of the

00:37:38,110 --> 00:37:42,730
containers off of them even beyond just

00:37:39,790 --> 00:37:44,260
even balancing and then we retired them

00:37:42,730 --> 00:37:48,100
and that allowed us to migrate our

00:37:44,260 --> 00:37:50,410
entire customer base to a complete SSD

00:37:48,100 --> 00:37:51,700
fleet in about two months without

00:37:50,410 --> 00:37:54,550
actually contacting

00:37:51,700 --> 00:37:57,609
anyone or introducing any downtime from

00:37:54,550 --> 00:38:00,070
the migration and so this will please

00:37:57,609 --> 00:38:02,950
Zak who's in the audience our venerable

00:38:00,070 --> 00:38:04,810
CEO he wanted me to do a real-time demo

00:38:02,950 --> 00:38:06,670
and honestly I give them a hard time

00:38:04,810 --> 00:38:08,079
about it but it is a really neat kind of

00:38:06,670 --> 00:38:11,980
demonstration about how some of our

00:38:08,079 --> 00:38:14,200
stuff works so if i go to we created

00:38:11,980 --> 00:38:16,810
these test sites that have something

00:38:14,200 --> 00:38:23,980
called the platform demo on them in the

00:38:16,810 --> 00:38:27,820
plateau no resolution so let me just do

00:38:23,980 --> 00:38:31,030
that so we have the platform demo here

00:38:27,820 --> 00:38:33,910
and what this is is this is a an

00:38:31,030 --> 00:38:35,589
instance it's just a web regular website

00:38:33,910 --> 00:38:38,200
on Pantheon and it's running this thing

00:38:35,589 --> 00:38:41,349
that's just index HTML with some

00:38:38,200 --> 00:38:43,359
JavaScript and accessing a PHP resource

00:38:41,349 --> 00:38:45,010
on the backend that does introspection

00:38:43,359 --> 00:38:48,130
theoretically you could just deploy this

00:38:45,010 --> 00:38:51,520
code to any project on Pantheon and what

00:38:48,130 --> 00:38:53,320
this does is it queries our core API to

00:38:51,520 --> 00:38:56,710
ask what resources are deployed to this

00:38:53,320 --> 00:38:59,079
environment for the stack and the neat

00:38:56,710 --> 00:39:02,260
thing is as I scale it we can see it's

00:38:59,079 --> 00:39:06,130
kayla real time so oops if i go back

00:39:02,260 --> 00:39:08,500
over here and i go into the settings for

00:39:06,130 --> 00:39:10,180
the site and I bump it up to a like a

00:39:08,500 --> 00:39:12,400
business plan level which means that we

00:39:10,180 --> 00:39:14,230
start deploying active active

00:39:12,400 --> 00:39:15,849
application servers instead of active

00:39:14,230 --> 00:39:19,349
passive which is the case for the

00:39:15,849 --> 00:39:22,060
professional level and I hop over here

00:39:19,349 --> 00:39:23,680
what's going to happen is we get a

00:39:22,060 --> 00:39:25,720
second container that's actually

00:39:23,680 --> 00:39:28,420
displaying it real time it was literally

00:39:25,720 --> 00:39:29,950
configured in that time and even these

00:39:28,420 --> 00:39:31,810
requests that are going through of

00:39:29,950 --> 00:39:32,980
refreshing this thing are getting routed

00:39:31,810 --> 00:39:35,109
through that same container

00:39:32,980 --> 00:39:40,869
infrastructure that is in flight right

00:39:35,109 --> 00:39:44,050
now on the base baseline and then I just

00:39:40,869 --> 00:39:46,930
for fun I brought up a thing here where

00:39:44,050 --> 00:39:51,880
I can actually set the application

00:39:46,930 --> 00:39:54,010
servers here let's hope that went

00:39:51,880 --> 00:39:57,670
through there we go so I can set it to a

00:39:54,010 --> 00:39:59,530
goal number and then as things what

00:39:57,670 --> 00:40:02,140
happens is the core API realizes that

00:39:59,530 --> 00:40:05,050
there's a disparity between the goal of

00:40:02,140 --> 00:40:05,650
configuration for the system and what is

00:40:05,050 --> 00:40:06,849
actually

00:40:05,650 --> 00:40:09,490
available for the system and then it

00:40:06,849 --> 00:40:11,230
fills that in so if we were it gets a

00:40:09,490 --> 00:40:15,609
little unstable with the graphing thing

00:40:11,230 --> 00:40:17,920
when you start having a lot and so what

00:40:15,609 --> 00:40:19,559
this does is I can basically say that's

00:40:17,920 --> 00:40:22,150
the goal and it doesn't just work for

00:40:19,559 --> 00:40:25,599
stateless resources like application

00:40:22,150 --> 00:40:27,700
servers I can actually go in to here and

00:40:25,599 --> 00:40:29,529
create database replicas as well and

00:40:27,700 --> 00:40:31,210
with it whatever replication topology

00:40:29,529 --> 00:40:32,349
that I want I'm not sure if that's going

00:40:31,210 --> 00:40:34,510
to come up too quick because it actually

00:40:32,349 --> 00:40:37,299
has to do that the transactional

00:40:34,510 --> 00:40:39,609
snapshot loaded into the replica server

00:40:37,299 --> 00:40:41,230
and then bring it online with the

00:40:39,609 --> 00:40:44,470
replica but we'll see if it comes up in

00:40:41,230 --> 00:40:46,210
a moment while I'm talking but the idea

00:40:44,470 --> 00:40:48,010
here is that instead of treating

00:40:46,210 --> 00:40:50,799
infrastructure as something where we

00:40:48,010 --> 00:40:52,990
treat individual instances with lots of

00:40:50,799 --> 00:40:54,880
care and dedication we treat it as a

00:40:52,990 --> 00:40:57,309
declarative thing where we just want

00:40:54,880 --> 00:40:59,500
this much and when there's not that much

00:40:57,309 --> 00:41:01,299
we provision that much and that also

00:40:59,500 --> 00:41:02,829
means that we can do things like retire

00:41:01,299 --> 00:41:04,660
one of these servers it will realize

00:41:02,829 --> 00:41:07,109
that the infrastructure of this site is

00:41:04,660 --> 00:41:09,880
out of alignment with the goals for it

00:41:07,109 --> 00:41:11,770
like the ways that we handle databases

00:41:09,880 --> 00:41:15,309
and migration is will actually keep a

00:41:11,770 --> 00:41:18,039
master server and a replica and the way

00:41:15,309 --> 00:41:19,720
that we retired this instance is we

00:41:18,039 --> 00:41:22,029
basically just say this box is going

00:41:19,720 --> 00:41:24,549
offline which reap which promotes this

00:41:22,029 --> 00:41:25,660
to the master and then it realizes wait

00:41:24,549 --> 00:41:28,150
i'm supposed to have at least one

00:41:25,660 --> 00:41:30,430
replica and then it says okay well i'll

00:41:28,150 --> 00:41:32,829
create one off the current master and it

00:41:30,430 --> 00:41:34,480
will create it there and what this means

00:41:32,829 --> 00:41:36,930
is that we can set that goal to anything

00:41:34,480 --> 00:41:41,140
they want like we get set it to have

00:41:36,930 --> 00:41:42,490
three replica instances and whenever one

00:41:41,140 --> 00:41:45,250
of them goes away or one of them gets

00:41:42,490 --> 00:41:47,710
promoted it recreates the hierarchy with

00:41:45,250 --> 00:41:52,390
enough active provisioned instances to

00:41:47,710 --> 00:41:54,339
meet the goal there and then um to hop

00:41:52,390 --> 00:41:57,460
back to kind of like a little bit more

00:41:54,339 --> 00:41:58,930
gritty shell stuff I wanted to show off

00:41:57,460 --> 00:42:03,730
some of the kind of socket activation

00:41:58,930 --> 00:42:07,180
stuff that we do so here so here I have

00:42:03,730 --> 00:42:09,789
the development environment for this

00:42:07,180 --> 00:42:15,099
site that I was working on and let me

00:42:09,789 --> 00:42:18,480
just do this more real time so the state

00:42:15,099 --> 00:42:22,660
of oops no it's been reconfigured

00:42:18,480 --> 00:42:25,650
what the that's okay uh so here's the

00:42:22,660 --> 00:42:27,790
engine X instance for the actual

00:42:25,650 --> 00:42:32,460
environment that I'm talking about here

00:42:27,790 --> 00:42:37,390
and the that engine X instance is

00:42:32,460 --> 00:42:40,540
actually listed as dependent on the

00:42:37,390 --> 00:42:42,190
php-fpm instance which is another

00:42:40,540 --> 00:42:46,120
service that runs within that container

00:42:42,190 --> 00:42:49,210
and so what I can and then this is all

00:42:46,120 --> 00:42:51,850
listening on something that is a socket

00:42:49,210 --> 00:42:54,190
for socket activation where it's

00:42:51,850 --> 00:42:57,580
listening on port 26 607 on this machine

00:42:54,190 --> 00:42:59,680
on v4 and v6 and as soon as a request

00:42:57,580 --> 00:43:03,280
comes in what's going to happen is this

00:42:59,680 --> 00:43:04,510
socket will get it systemd will say what

00:43:03,280 --> 00:43:06,760
is the service associated with the

00:43:04,510 --> 00:43:08,980
socket oh it's engine X I'll start that

00:43:06,760 --> 00:43:10,510
I'll pass in the socket which is a neat

00:43:08,980 --> 00:43:12,370
thing you can do on unit systems with

00:43:10,510 --> 00:43:14,620
file descriptors it hands it off to

00:43:12,370 --> 00:43:18,460
engine acts engine X takes over that and

00:43:14,620 --> 00:43:20,290
then actually before engine X comes

00:43:18,460 --> 00:43:21,910
online it realizes that php-fpm is a

00:43:20,290 --> 00:43:25,750
dependency of engine X it starts up

00:43:21,910 --> 00:43:28,150
php-fpm waits tell php-fpm is ready then

00:43:25,750 --> 00:43:30,010
starts engine X then engine X gets the

00:43:28,150 --> 00:43:31,660
socket and then engine X can actually

00:43:30,010 --> 00:43:33,940
service the request and stay online and

00:43:31,660 --> 00:43:35,770
then we monitor this with a process

00:43:33,940 --> 00:43:37,390
called the Reaper that is looking for

00:43:35,770 --> 00:43:39,640
currently running containers that are

00:43:37,390 --> 00:43:41,020
not actively servicing requests like

00:43:39,640 --> 00:43:43,540
they've been idle for hours at a time

00:43:41,020 --> 00:43:45,280
and then we can spin them down and

00:43:43,540 --> 00:43:47,860
that's actually a neat kind of almost

00:43:45,280 --> 00:43:50,410
transactional atomic thing because if we

00:43:47,860 --> 00:43:52,960
queue a spin down of it and a request

00:43:50,410 --> 00:43:54,520
comes in system system d realizes that a

00:43:52,960 --> 00:43:56,140
request has come in for a service that

00:43:54,520 --> 00:43:57,640
is even kind of in the process of

00:43:56,140 --> 00:44:00,940
getting shut down and it will

00:43:57,640 --> 00:44:03,220
immediately schedule a following start

00:44:00,940 --> 00:44:05,500
of the service so there's no time at

00:44:03,220 --> 00:44:07,930
which this should interrupt availability

00:44:05,500 --> 00:44:10,480
of instances by using this kind of

00:44:07,930 --> 00:44:16,840
approach to resources and now if I go

00:44:10,480 --> 00:44:19,090
over to the here and hop to my

00:44:16,840 --> 00:44:21,010
development site you'll notice it takes

00:44:19,090 --> 00:44:22,510
a few moments because it's actually

00:44:21,010 --> 00:44:27,040
spending up those resources in the back

00:44:22,510 --> 00:44:30,090
end and if I go here you'll actually

00:44:27,040 --> 00:44:34,530
notice that whoops that's the

00:44:30,090 --> 00:44:37,350
file stennis so it's actually running

00:44:34,530 --> 00:44:39,330
that now it started the master process

00:44:37,350 --> 00:44:46,740
and the worker process for engine X it

00:44:39,330 --> 00:44:48,990
is also started my php-fpm pool for this

00:44:46,740 --> 00:44:50,820
as well which is actually quite small

00:44:48,990 --> 00:44:56,160
because we're using kind of dynamic

00:44:50,820 --> 00:44:57,900
ratios for that the and then they don't

00:44:56,160 --> 00:44:59,640
now it's come up and in a case it was

00:44:57,900 --> 00:45:01,200
taking long enough that I can tell like

00:44:59,640 --> 00:45:03,120
as someone who's worked on this was

00:45:01,200 --> 00:45:04,980
actually pulling up the Raby be instance

00:45:03,120 --> 00:45:07,290
in the background as well which we have

00:45:04,980 --> 00:45:08,520
less aggressive shutdown goals for those

00:45:07,290 --> 00:45:11,640
because they take a little longer to

00:45:08,520 --> 00:45:13,740
come online but the but the really cool

00:45:11,640 --> 00:45:16,200
thing here is that we're able to deploy

00:45:13,740 --> 00:45:18,960
out all these stacks kind of make it all

00:45:16,200 --> 00:45:20,930
so liberally available and democratized

00:45:18,960 --> 00:45:23,460
in terms of everyone having access to it

00:45:20,930 --> 00:45:30,390
without actually taking very many

00:45:23,460 --> 00:45:33,690
resources and then the hopping back to

00:45:30,390 --> 00:45:36,300
here let's talk about some of the kind

00:45:33,690 --> 00:45:38,910
of bones that make up the stuff of one

00:45:36,300 --> 00:45:40,290
of the when people talk about containers

00:45:38,910 --> 00:45:41,840
they're not there's no concept of

00:45:40,290 --> 00:45:44,610
containers in the linux kernel at all

00:45:41,840 --> 00:45:47,100
and that's why i have the kernels in the

00:45:44,610 --> 00:45:50,610
background here because unlike solaris

00:45:47,100 --> 00:45:52,950
and bsd kernels of containers are

00:45:50,610 --> 00:45:54,840
basically a synthesis of many control

00:45:52,950 --> 00:45:56,820
systems that are present in the kernel

00:45:54,840 --> 00:45:58,380
and when we say that there's a container

00:45:56,820 --> 00:46:00,750
it's basically like we've flipped on all

00:45:58,380 --> 00:46:03,930
the isolation switches and the two key

00:46:00,750 --> 00:46:07,760
ones are see groups in namespaces which

00:46:03,930 --> 00:46:07,760
are actually things in the linux kernel

00:46:08,450 --> 00:46:14,580
see groups is ultimately a hierarchy of

00:46:11,580 --> 00:46:17,250
processes where you can basically divide

00:46:14,580 --> 00:46:19,380
out resources either rigidly by saying

00:46:17,250 --> 00:46:21,450
that there are specific limits or

00:46:19,380 --> 00:46:23,360
proportionately saying that someone gets

00:46:21,450 --> 00:46:26,700
75% of the pie and someone gets

00:46:23,360 --> 00:46:29,190
twenty-five percent of the pie and this

00:46:26,700 --> 00:46:32,070
also happens hierarchically where you

00:46:29,190 --> 00:46:34,530
could divide things that are 75 percent

00:46:32,070 --> 00:46:37,920
for this kind of dress and production

00:46:34,530 --> 00:46:40,590
php-fpm and maybe you are running drush

00:46:37,920 --> 00:46:42,060
as development process and you're using

00:46:40,590 --> 00:46:43,470
something like our sink to be able to

00:46:42,060 --> 00:46:45,180
pull around files or freshman

00:46:43,470 --> 00:46:47,250
development copy you could give that

00:46:45,180 --> 00:46:49,680
considerably fewer resources this could

00:46:47,250 --> 00:46:54,080
be block I oh this could be CPU this

00:46:49,680 --> 00:46:56,700
could be other memory on the system and

00:46:54,080 --> 00:46:58,950
what it does is it actually promotes no

00:46:56,700 --> 00:47:01,170
limits on the activity unless there's

00:46:58,950 --> 00:47:03,540
contention for those resources so as

00:47:01,170 --> 00:47:05,609
long as our sink is not putting much as

00:47:03,540 --> 00:47:07,770
long as our sinks disk load is not

00:47:05,609 --> 00:47:10,349
actually affecting drush and php-fpm it

00:47:07,770 --> 00:47:12,119
can have all at once to eat it's only

00:47:10,349 --> 00:47:13,680
when it starts undermining the

00:47:12,119 --> 00:47:15,540
performance of the production system

00:47:13,680 --> 00:47:17,340
does the colonel get involved and start

00:47:15,540 --> 00:47:21,150
saying you get this much and you get

00:47:17,340 --> 00:47:27,270
this much and you cannot you can take it

00:47:21,150 --> 00:47:29,220
to extremes of the and we don't quite do

00:47:27,270 --> 00:47:31,260
it to this extreme we're more like this

00:47:29,220 --> 00:47:33,599
for like enterprise versus development

00:47:31,260 --> 00:47:36,780
on pantheon but you could take it to an

00:47:33,599 --> 00:47:38,340
extreme and there are realistic

00:47:36,780 --> 00:47:40,170
implications of using something like

00:47:38,340 --> 00:47:41,369
this where if you're running a backup

00:47:40,170 --> 00:47:43,920
you might be perfectly comfortable

00:47:41,369 --> 00:47:45,420
giving it two percent because the

00:47:43,920 --> 00:47:46,710
majority of the time it's not actually

00:47:45,420 --> 00:47:48,150
going to be confined to that because

00:47:46,710 --> 00:47:50,010
it's not actually going to have resource

00:47:48,150 --> 00:47:51,810
contention but when it does you can

00:47:50,010 --> 00:47:54,060
ensure that those things are not

00:47:51,810 --> 00:47:56,609
undermining your production deployments

00:47:54,060 --> 00:47:58,410
and there are a ton of controllers for

00:47:56,609 --> 00:48:00,180
cgroups all of which affect the

00:47:58,410 --> 00:48:02,910
hierarchy of processes that are under

00:48:00,180 --> 00:48:05,010
them and this is just a few of them the

00:48:02,910 --> 00:48:11,720
ones we use at Pantheon or mostly the

00:48:05,010 --> 00:48:18,300
cpu block i/o and memory ones and um

00:48:11,720 --> 00:48:22,640
let's see skip a few things here it's

00:48:18,300 --> 00:48:26,520
boring but so this is a kind of neat

00:48:22,640 --> 00:48:28,080
demonstration here where what you can do

00:48:26,520 --> 00:48:29,369
is you can actually set hard limits for

00:48:28,080 --> 00:48:34,290
something where you could give something

00:48:29,369 --> 00:48:36,510
100 bytes for even your current process

00:48:34,290 --> 00:48:38,280
like your current shell what this is

00:48:36,510 --> 00:48:40,410
doing here is it's creating a see group

00:48:38,280 --> 00:48:43,950
for just yourself with number named

00:48:40,410 --> 00:48:48,660
teensy and setting a 100 bite limit for

00:48:43,950 --> 00:48:51,510
it and then when you actually make that

00:48:48,660 --> 00:48:53,520
take effect where echo dollar dollar is

00:48:51,510 --> 00:48:56,110
your current process ID and if you

00:48:53,520 --> 00:48:59,230
assign your current process ID to

00:48:56,110 --> 00:49:00,790
the task or like PID list of that C

00:48:59,230 --> 00:49:02,560
group and then you try to run something

00:49:00,790 --> 00:49:05,560
even as lightweight as LS it'll just

00:49:02,560 --> 00:49:06,670
kill you and so these these really do

00:49:05,560 --> 00:49:09,970
work and they're actually pretty

00:49:06,670 --> 00:49:13,810
lightweight to implement here's another

00:49:09,970 --> 00:49:17,260
case where I created a a Python process

00:49:13,810 --> 00:49:18,880
that was designed to just eat CPU um you

00:49:17,260 --> 00:49:21,700
could argue every Python process each

00:49:18,880 --> 00:49:23,530
CPU but the this one was specifically

00:49:21,700 --> 00:49:26,860
designed to do that and python is an

00:49:23,530 --> 00:49:30,490
excellent language for that the with the

00:49:26,860 --> 00:49:32,230
global interpreter lock the and so what

00:49:30,490 --> 00:49:36,550
this does is I was just running these

00:49:32,230 --> 00:49:39,850
one in C group AAA and one in C Group BB

00:49:36,550 --> 00:49:41,560
and I gave one of them 100 CPU shares

00:49:39,850 --> 00:49:43,180
and one of them 10 CPU share someone

00:49:41,560 --> 00:49:45,850
that does in the system is it just adds

00:49:43,180 --> 00:49:48,160
those two so we have a pie of 100 CPU

00:49:45,850 --> 00:49:50,950
shares and one gets 111th of it and the

00:49:48,160 --> 00:49:53,200
other one gets 10 eleventh of it and you

00:49:50,950 --> 00:49:55,090
can see it manifest as not quite

00:49:53,200 --> 00:49:56,830
precisely that but pretty close you're

00:49:55,090 --> 00:49:58,720
seeing you know an order of magnitude of

00:49:56,830 --> 00:50:00,910
of the different resources available

00:49:58,720 --> 00:50:03,550
because when I run those concurrently on

00:50:00,910 --> 00:50:05,200
the box they are ones getting six

00:50:03,550 --> 00:50:06,310
percent of CPU and ones getting sixty

00:50:05,200 --> 00:50:10,060
percent and they're the exact same

00:50:06,310 --> 00:50:12,520
program there are also namespaces like

00:50:10,060 --> 00:50:15,070
the other half two containers these are

00:50:12,520 --> 00:50:17,140
not designed strictly for security

00:50:15,070 --> 00:50:18,400
isolation and you would really wouldn't

00:50:17,140 --> 00:50:21,100
want to rely on that but they can

00:50:18,400 --> 00:50:22,720
provide the illusion of an isolated

00:50:21,100 --> 00:50:25,630
system which is very useful for

00:50:22,720 --> 00:50:27,970
developer productivity and I say that

00:50:25,630 --> 00:50:30,490
because the one of the key things that

00:50:27,970 --> 00:50:32,020
developers loved about VMs and continue

00:50:30,490 --> 00:50:34,570
to love about VMs is they look just like

00:50:32,020 --> 00:50:36,820
you've got your own box and everyone

00:50:34,570 --> 00:50:38,770
knows how to work with a single box for

00:50:36,820 --> 00:50:41,260
installing services and configuring

00:50:38,770 --> 00:50:42,970
things and namespaces are really

00:50:41,260 --> 00:50:45,610
effective way of providing that illusion

00:50:42,970 --> 00:50:47,620
for containers not it's not quite to the

00:50:45,610 --> 00:50:49,060
level of what virtual machines can

00:50:47,620 --> 00:50:50,890
provide with their own kind of colonel

00:50:49,060 --> 00:50:53,020
but it's getting really really close

00:50:50,890 --> 00:50:55,570
because now you can have your own p ID

00:50:53,020 --> 00:50:58,510
numbers your own user IDs your own

00:50:55,570 --> 00:50:59,680
mounts your own network devices all

00:50:58,510 --> 00:51:04,240
these things that used to require

00:50:59,680 --> 00:51:06,550
running a virtual machine and the

00:51:04,240 --> 00:51:08,590
interface for namespaces is called

00:51:06,550 --> 00:51:09,670
unshare whether you're doing it with a

00:51:08,590 --> 00:51:14,859
shell or whether your call

00:51:09,670 --> 00:51:16,960
the kernel of syscalls and the you base

00:51:14,859 --> 00:51:19,059
so the way that everything starts in the

00:51:16,960 --> 00:51:21,220
system when you start it on Linux is you

00:51:19,059 --> 00:51:22,720
just start in the namespaces of your

00:51:21,220 --> 00:51:24,460
parent which if you're on the base

00:51:22,720 --> 00:51:27,119
system you start with basically no names

00:51:24,460 --> 00:51:29,410
base or at all like namespace 0 and

00:51:27,119 --> 00:51:32,250
Linux basically forces you to turn on

00:51:29,410 --> 00:51:36,730
all these individual isolation isolation

00:51:32,250 --> 00:51:39,369
isolation switches individually and when

00:51:36,730 --> 00:51:41,440
you use a tool like docker all that is

00:51:39,369 --> 00:51:42,940
really doing is basically going through

00:51:41,440 --> 00:51:45,130
the switches and turning them all on for

00:51:42,940 --> 00:51:47,260
you but you can do them individually and

00:51:45,130 --> 00:51:49,390
this is a case here where this would

00:51:47,260 --> 00:51:52,030
create a new network name space for a

00:51:49,390 --> 00:51:54,160
shell that you could start and what

00:51:52,030 --> 00:51:56,770
that'll do is you'll just any if you do

00:51:54,160 --> 00:51:59,470
with something like AF ifconfig you'll

00:51:56,770 --> 00:52:01,630
just list a local host adapter and

00:51:59,470 --> 00:52:03,579
nothing else and then you can pull in

00:52:01,630 --> 00:52:05,530
other devices into the new name space

00:52:03,579 --> 00:52:09,339
but you have to have a handle to the old

00:52:05,530 --> 00:52:10,599
one and then I just wanted to kind of

00:52:09,339 --> 00:52:12,010
provide a little bit of a survey like

00:52:10,599 --> 00:52:13,930
what's out there if you want to work

00:52:12,010 --> 00:52:15,520
with containers because we've rolled a

00:52:13,930 --> 00:52:17,500
lot of our own kind of custom stuff but

00:52:15,520 --> 00:52:20,770
there are increasingly good options in

00:52:17,500 --> 00:52:23,799
the community now the old old standby

00:52:20,770 --> 00:52:26,380
which is still not a bad tool is Alexi

00:52:23,799 --> 00:52:27,460
it's fairly lightweight and the sense

00:52:26,380 --> 00:52:29,079
that it doesn't have any kind of image

00:52:27,460 --> 00:52:31,599
format but it does allow you to turn all

00:52:29,079 --> 00:52:33,280
the switches on really fast and it has a

00:52:31,599 --> 00:52:35,260
lot of language bindings if you want to

00:52:33,280 --> 00:52:39,460
invoke it through something like Python

00:52:35,260 --> 00:52:40,660
or Lua or Ruby or go and this is before

00:52:39,460 --> 00:52:42,460
a lot of these other container

00:52:40,660 --> 00:52:45,490
frameworks existed this was kind of the

00:52:42,460 --> 00:52:46,930
standard tool to work with google

00:52:45,490 --> 00:52:51,569
launched a project called let me contain

00:52:46,930 --> 00:52:55,059
that for you which i think what's of oh

00:52:51,569 --> 00:52:57,040
I think was mostly launched because they

00:52:55,059 --> 00:52:59,230
wanted to be like me too oh we've been

00:52:57,040 --> 00:53:01,210
doing this for like five years when you

00:52:59,230 --> 00:53:03,940
youngins were just like thinking about

00:53:01,210 --> 00:53:05,500
the idea of containers and it has all

00:53:03,940 --> 00:53:06,880
the signs that it's that kind of project

00:53:05,500 --> 00:53:08,890
because it was like thrown over the wall

00:53:06,880 --> 00:53:12,220
on github and I don't think it's had a

00:53:08,890 --> 00:53:13,630
single commit since 2014 but but it

00:53:12,220 --> 00:53:15,549
definitely demonstrated that Google had

00:53:13,630 --> 00:53:17,740
been doing these things for years and

00:53:15,549 --> 00:53:19,390
proved it to the world so I wouldn't use

00:53:17,740 --> 00:53:20,890
it for anything but it's interesting to

00:53:19,390 --> 00:53:22,870
browse if you're curious how Google's

00:53:20,890 --> 00:53:24,280
doing it and then

00:53:22,870 --> 00:53:26,530
systemd in spawn which is part of the

00:53:24,280 --> 00:53:28,390
system d project now whose virtue is

00:53:26,530 --> 00:53:33,160
being on everyone's distribution whether

00:53:28,390 --> 00:53:34,510
you like it or not and it's the closest

00:53:33,160 --> 00:53:36,900
to what pantheon uses because we

00:53:34,510 --> 00:53:40,210
actually use system d services and

00:53:36,900 --> 00:53:41,830
system DS own in spawn tool allows you

00:53:40,210 --> 00:53:44,050
to flip on all those switches as well

00:53:41,830 --> 00:53:45,580
we're like services and system d you

00:53:44,050 --> 00:53:47,170
still flip them on individually and

00:53:45,580 --> 00:53:50,380
spawn is like you know going up against

00:53:47,170 --> 00:53:52,420
the wall and turning them all on and it

00:53:50,380 --> 00:53:55,390
started off as a toy that Leonard wrote

00:53:52,420 --> 00:53:56,950
just to like test system d and but it's

00:53:55,390 --> 00:54:01,030
now actually part of production tools

00:53:56,950 --> 00:54:02,440
like core OSS rocket run time so I we're

00:54:01,030 --> 00:54:04,600
actually going to remove the thing about

00:54:02,440 --> 00:54:08,830
it being not usable for production

00:54:04,600 --> 00:54:10,150
purposes and then it's super easy to use

00:54:08,830 --> 00:54:13,780
so you can actually do something like

00:54:10,150 --> 00:54:15,790
vagrant ssh that bootstrap and like

00:54:13,780 --> 00:54:18,640
install Debian unstable and then you can

00:54:15,790 --> 00:54:20,110
actually launch Debian on fedora by just

00:54:18,640 --> 00:54:22,540
doing system the insulin and giving it a

00:54:20,110 --> 00:54:25,450
directory and it's inherently designed

00:54:22,540 --> 00:54:26,590
not actually have any kind of Damon back

00:54:25,450 --> 00:54:29,710
end it's just designed to be a

00:54:26,590 --> 00:54:31,870
foreground tool and then rocket has been

00:54:29,710 --> 00:54:33,610
built on system d and spawn and is

00:54:31,870 --> 00:54:35,290
actually my favorite new container tool

00:54:33,610 --> 00:54:37,630
that we have yet to deploy at Pantheon

00:54:35,290 --> 00:54:39,850
but I'm really excited about because it

00:54:37,630 --> 00:54:41,650
uses the app container spec which is a

00:54:39,850 --> 00:54:43,180
kind of independently maintained

00:54:41,650 --> 00:54:46,840
specification for what should go in a

00:54:43,180 --> 00:54:49,420
container and it uses my favorite system

00:54:46,840 --> 00:54:52,390
d tools so i'm going to kind of you know

00:54:49,420 --> 00:54:54,370
advertise that project but google is

00:54:52,390 --> 00:54:56,440
actually behind the image format as well

00:54:54,370 --> 00:54:58,570
as core OS and several other companies

00:54:56,440 --> 00:54:59,860
so i think it's going to take off and

00:54:58,570 --> 00:55:02,470
we're probably going to pull a few more

00:54:59,860 --> 00:55:03,940
elements of it into system d and then

00:55:02,470 --> 00:55:07,600
there's the the venerable daughter which

00:55:03,940 --> 00:55:11,230
everyone's heard of the it's definitely

00:55:07,600 --> 00:55:12,970
more focused on developer integration

00:55:11,230 --> 00:55:14,950
and developer tools than production

00:55:12,970 --> 00:55:17,140
deployments but no discussion of

00:55:14,950 --> 00:55:20,050
containers would be complete without

00:55:17,140 --> 00:55:22,420
mentioning it they have their own de

00:55:20,050 --> 00:55:24,970
facto image format and a very very wide

00:55:22,420 --> 00:55:27,490
variety of images that are available for

00:55:24,970 --> 00:55:30,460
deployment of to assemble you know Lego

00:55:27,490 --> 00:55:32,440
stacks of every kind of software you

00:55:30,460 --> 00:55:35,080
could imagine including everything for

00:55:32,440 --> 00:55:36,369
Drupal and beyond and they kind of have

00:55:35,080 --> 00:55:39,700
a spectrum of complex

00:55:36,369 --> 00:55:41,259
city & and completeness all the way from

00:55:39,700 --> 00:55:43,150
let me container eyes that for you which

00:55:41,259 --> 00:55:45,430
barely does anything all the way up to

00:55:43,150 --> 00:55:50,650
docker with its own defined

00:55:45,430 --> 00:55:52,720
infrastructure and de facto definition

00:55:50,650 --> 00:55:55,359
of how containers should work and kind

00:55:52,720 --> 00:55:57,640
of most complete set of requirements so

00:55:55,359 --> 00:55:59,499
like Alexi has a more comprehensive

00:55:57,640 --> 00:56:01,450
controls over resources than let me

00:55:59,499 --> 00:56:02,650
container eyes that for you in spawn

00:56:01,450 --> 00:56:04,240
actually has the ability to start

00:56:02,650 --> 00:56:06,339
working with image formats but doesn't

00:56:04,240 --> 00:56:07,809
have any Damon rock it actually starts

00:56:06,339 --> 00:56:09,430
the idea of having a daemon that's

00:56:07,809 --> 00:56:11,079
available for configuring it in addition

00:56:09,430 --> 00:56:12,849
to everything to the right of it and

00:56:11,079 --> 00:56:15,249
then docker adds in some of the

00:56:12,849 --> 00:56:16,390
orchestration features where you're not

00:56:15,249 --> 00:56:18,150
just dealing with images but an

00:56:16,390 --> 00:56:22,569
infrastructure for running the images

00:56:18,150 --> 00:56:25,089
and so if you all these tools are

00:56:22,569 --> 00:56:27,670
legitimate to use except for possibly

00:56:25,089 --> 00:56:29,680
let me container eyes that for you but

00:56:27,670 --> 00:56:32,259
it's important to look at containers as

00:56:29,680 --> 00:56:33,999
the spectrum it's not that they're not

00:56:32,259 --> 00:56:36,519
some they shouldn't be synonymous with

00:56:33,999 --> 00:56:38,829
any single one of these technologies and

00:56:36,519 --> 00:56:41,890
orchestration is the new hotness which

00:56:38,829 --> 00:56:44,980
our friends at core OS are pushing a lot

00:56:41,890 --> 00:56:46,989
of neat things on where in turn

00:56:44,980 --> 00:56:48,910
orchestration being there are these

00:56:46,989 --> 00:56:50,769
great tools for managing containers on

00:56:48,910 --> 00:56:52,809
individual machines but then you start

00:56:50,769 --> 00:56:54,759
dealing with a data center or at least

00:56:52,809 --> 00:56:56,829
multiple machines how do you control

00:56:54,759 --> 00:56:58,989
where those containers are deploying how

00:56:56,829 --> 00:57:00,759
the images get distributed the security

00:56:58,989 --> 00:57:02,829
relationships between those machines and

00:57:00,759 --> 00:57:05,039
they're there I'd say at the forefront

00:57:02,829 --> 00:57:08,349
of some of the most innovative designs

00:57:05,039 --> 00:57:13,739
for for dealing with that problem and

00:57:08,349 --> 00:57:13,739
I'll open the floor for questions yes

00:57:18,799 --> 00:57:26,369
the point during the deploy process and

00:57:21,569 --> 00:57:28,470
we want to move away from that ah it's

00:57:26,369 --> 00:57:30,299
not super slow it's more about

00:57:28,470 --> 00:57:32,670
repeatability where it would be really

00:57:30,299 --> 00:57:34,710
nice to be able to bake the images of QA

00:57:32,670 --> 00:57:38,130
them and then push them out rather than

00:57:34,710 --> 00:57:39,630
put configuring in place and that's why

00:57:38,130 --> 00:57:42,180
I'm pretty excited about the app

00:57:39,630 --> 00:57:43,650
container spec it provides a way to to

00:57:42,180 --> 00:57:47,039
provide a uniform way to bake a

00:57:43,650 --> 00:57:57,599
container that is not necessarily tied

00:57:47,039 --> 00:57:59,849
to any one deployment environment we

00:57:57,599 --> 00:58:03,390
don't actually run chef server we run

00:57:59,849 --> 00:58:04,680
plugins to chef I'm actually if you

00:58:03,390 --> 00:58:07,260
could go to the microphone that would

00:58:04,680 --> 00:58:09,299
that would be helpful okay yeah we don't

00:58:07,260 --> 00:58:11,490
run we don't run chef server we run

00:58:09,299 --> 00:58:13,109
plugins to chef that use our public key

00:58:11,490 --> 00:58:15,420
infrastructure to talk to our core API

00:58:13,109 --> 00:58:18,569
and that tells them what should be going

00:58:15,420 --> 00:58:20,940
anyplace like every container has a uuid

00:58:18,569 --> 00:58:23,490
and then it knows that the uuid it talks

00:58:20,940 --> 00:58:25,589
to our API and says what do I deploy in

00:58:23,490 --> 00:58:29,160
here for this uuid and it knows the

00:58:25,589 --> 00:58:30,900
flavor of container and then that that

00:58:29,160 --> 00:58:33,029
combined with the metadata and access to

00:58:30,900 --> 00:58:37,160
some of the resources is what allows it

00:58:33,029 --> 00:58:39,119
to configure it in place microphone now

00:58:37,160 --> 00:58:41,640
thank you for your talk those really

00:58:39,119 --> 00:58:45,029
enlightening so I noticed in your

00:58:41,640 --> 00:58:47,839
diagram that your load balancing among a

00:58:45,029 --> 00:58:50,579
large number of containers per per site

00:58:47,839 --> 00:58:52,049
you host a lot of sites so I was

00:58:50,579 --> 00:58:54,420
wondering how you deal with load on the

00:58:52,049 --> 00:58:57,059
load balancers themselves we have

00:58:54,420 --> 00:58:59,490
several clusters of load balancers our

00:58:57,059 --> 00:59:01,170
in-house load balancer we call sticks

00:58:59,490 --> 00:59:05,910
because it takes requests to their final

00:59:01,170 --> 00:59:08,069
destination the it's written in go and

00:59:05,910 --> 00:59:09,930
when it does is it queries the container

00:59:08,069 --> 00:59:12,000
back ends for a given hostname

00:59:09,930 --> 00:59:13,950
dynamically does a kind of lazy look up

00:59:12,000 --> 00:59:16,470
so as soon as the request comes into our

00:59:13,950 --> 00:59:18,480
edge it checks its local cache for

00:59:16,470 --> 00:59:19,950
routes to see if it knows what

00:59:18,480 --> 00:59:22,289
containers should be serving that

00:59:19,950 --> 00:59:23,970
environment or that hostname if it

00:59:22,289 --> 00:59:25,920
doesn't have any data on that minute

00:59:23,970 --> 00:59:28,920
talks to our API asks what containers

00:59:25,920 --> 00:59:32,100
are servicing this environment and then

00:59:28,920 --> 00:59:33,930
it uses that to in the completely

00:59:32,100 --> 00:59:36,300
naive case just randomly choose a

00:59:33,930 --> 00:59:39,000
container to single load to but it also

00:59:36,300 --> 00:59:40,350
learns passively about the response

00:59:39,000 --> 00:59:44,220
times of those containers and their

00:59:40,350 --> 00:59:47,340
availability and we'll kind of it will

00:59:44,220 --> 00:59:48,900
kind of add like demerit points to the

00:59:47,340 --> 00:59:51,090
containers that are performing poorly or

00:59:48,900 --> 00:59:53,010
not responding and then that way it

00:59:51,090 --> 00:59:55,050
doesn't turn out to be even load

00:59:53,010 --> 00:59:56,640
distribution what happens is we have

00:59:55,050 --> 00:59:58,110
these small queues and engine X for it

00:59:56,640 --> 00:59:59,820
to refuse the requests if there are too

00:59:58,110 --> 01:00:01,680
many in a container and then that

00:59:59,820 --> 01:00:03,750
actually effectively causes it to make

01:00:01,680 --> 01:00:05,880
maximum use of as many containers as

01:00:03,750 --> 01:00:10,170
possible thank you that's really

01:00:05,880 --> 01:00:14,220
interesting hi great talk by the way

01:00:10,170 --> 01:00:17,190
thanks um so I work at a media company

01:00:14,220 --> 01:00:21,570
and we're really looking into possibly

01:00:17,190 --> 01:00:23,100
using container technologies and so when

01:00:21,570 --> 01:00:25,850
I when I saw you in your talk you were

01:00:23,100 --> 01:00:29,520
saying that you would deploy 150 or so

01:00:25,850 --> 01:00:32,010
number of containers and so I'm I can't

01:00:29,520 --> 01:00:35,030
read this from a physical perspective

01:00:32,010 --> 01:00:38,520
let's just say one ec2 instance that is

01:00:35,030 --> 01:00:40,410
on m3 or something so are you actually

01:00:38,520 --> 01:00:42,870
deploying multiple containers in that

01:00:40,410 --> 01:00:44,730
one ec2 instance well we're so we're

01:00:42,870 --> 01:00:47,520
using Rackspace cloud about 30 gig

01:00:44,730 --> 01:00:50,250
instances and then we deploy about five

01:00:47,520 --> 01:00:52,560
thousand containers per box and then we

01:00:50,250 --> 01:00:56,010
run somewhere between 100 and 50 to 500

01:00:52,560 --> 01:00:59,070
of them at once so let me ask this if

01:00:56,010 --> 01:01:01,130
let's say we were a company that had one

01:00:59,070 --> 01:01:05,430
website and we are looking into saying

01:01:01,130 --> 01:01:08,520
to being able to let's say scale out on

01:01:05,430 --> 01:01:11,040
demand so we would would happen we would

01:01:08,520 --> 01:01:14,130
have one large ec2 instance with only a

01:01:11,040 --> 01:01:16,260
single a single container launched and

01:01:14,130 --> 01:01:17,880
then as demand grows we would fire up

01:01:16,260 --> 01:01:19,530
more oh I wouldn't recommend that I

01:01:17,880 --> 01:01:21,990
think it's not going to get high

01:01:19,530 --> 01:01:24,600
availability for systems so our

01:01:21,990 --> 01:01:26,640
scheduler looks at two things other than

01:01:24,600 --> 01:01:28,140
some kind of hard and fast constraints

01:01:26,640 --> 01:01:30,630
but the main two things that makes the

01:01:28,140 --> 01:01:34,380
decision on are the first priority is

01:01:30,630 --> 01:01:37,580
ha4 sites so that it will almost never

01:01:34,380 --> 01:01:40,800
schedule a container to be on the same

01:01:37,580 --> 01:01:43,480
Conejos as an existing container for the

01:01:40,800 --> 01:01:45,130
site unless it has absolutely no other

01:01:43,480 --> 01:01:47,530
option which is typically only the case

01:01:45,130 --> 01:01:50,290
in our development environments and what

01:01:47,530 --> 01:01:52,750
happens there is that typically black

01:01:50,290 --> 01:01:54,910
lists a number of servers more than a

01:01:52,750 --> 01:01:57,580
handful of servers if you have say 30

01:01:54,910 --> 01:01:59,790
containers but it certainly spreads it

01:01:57,580 --> 01:02:01,480
out and then the second priority is

01:01:59,790 --> 01:02:04,000
distributing the container to the

01:02:01,480 --> 01:02:06,310
machine with lowest load so what I would

01:02:04,000 --> 01:02:09,010
recommend is don't deploy one machine

01:02:06,310 --> 01:02:11,140
deploy at least two and then what you

01:02:09,010 --> 01:02:14,740
can do is you can use a tool like Cooper

01:02:11,140 --> 01:02:16,720
Nettie's to maintain additional machines

01:02:14,740 --> 01:02:18,820
and incorporate them into your

01:02:16,720 --> 01:02:21,310
infrastructure kuber Nettie's is by the

01:02:18,820 --> 01:02:23,800
way built into core alessa stuff too and

01:02:21,310 --> 01:02:25,450
then what will happen is that it has

01:02:23,800 --> 01:02:26,890
similar constraints around trying to

01:02:25,450 --> 01:02:28,660
distribute stuff and then what you

01:02:26,890 --> 01:02:31,090
should do is say I need to application

01:02:28,660 --> 01:02:32,980
servers for this one site and then

01:02:31,090 --> 01:02:35,710
that'll ensure that it has a footprint

01:02:32,980 --> 01:02:42,280
on both machines Thanks does that make

01:02:35,710 --> 01:02:45,520
sense yes okay hi very informative how

01:02:42,280 --> 01:02:47,410
finally do you slice the services on a

01:02:45,520 --> 01:02:49,960
machine in the different containers and

01:02:47,410 --> 01:02:52,390
what's the general rule finally we slice

01:02:49,960 --> 01:02:55,240
them yeah like engine eggs versus edge

01:02:52,390 --> 01:02:57,520
NX and and PHP or those are all running

01:02:55,240 --> 01:03:00,880
and so everything is containerized in

01:02:57,520 --> 01:03:04,270
its own unique we're piling we do pile

01:03:00,880 --> 01:03:06,280
engine X the most kind of co-located

01:03:04,270 --> 01:03:10,030
thing is our application server and that

01:03:06,280 --> 01:03:11,770
has engine X php-fpm and the client for

01:03:10,030 --> 01:03:13,930
our distributed file system all piled

01:03:11,770 --> 01:03:16,930
into the same security context in a

01:03:13,930 --> 01:03:18,640
container most of our other containers

01:03:16,930 --> 01:03:20,859
are single process like our reticent a

01:03:18,640 --> 01:03:26,250
nurs only container etis armory DB

01:03:20,859 --> 01:03:26,250
containers only contain more ad be ok

01:03:28,260 --> 01:03:32,950
hey David thanks for your talk I just

01:03:31,390 --> 01:03:34,090
have a quick question so it was

01:03:32,950 --> 01:03:35,290
impressive to see how quick the

01:03:34,090 --> 01:03:38,170
containers come up and I was just

01:03:35,290 --> 01:03:40,660
curious as to how the Drupal application

01:03:38,170 --> 01:03:42,400
layer makes it or the code itself makes

01:03:40,660 --> 01:03:44,650
it inside the container is it it's the

01:03:42,400 --> 01:03:46,990
code running inside that container when

01:03:44,650 --> 01:03:49,420
you when you bring one up to to add more

01:03:46,990 --> 01:03:51,300
capacity or is it that container just

01:03:49,420 --> 01:03:54,910
connecting to a distributed file system

01:03:51,300 --> 01:03:56,530
the the runt the actual application code

01:03:54,910 --> 01:03:57,250
is not running on distributed file

01:03:56,530 --> 01:04:00,070
system

01:03:57,250 --> 01:04:01,600
we pull that through get and if you're

01:04:00,070 --> 01:04:04,660
deploying the test or live environment

01:04:01,600 --> 01:04:06,010
its associated with a get tagged and if

01:04:04,660 --> 01:04:08,260
you're looking at any other environment

01:04:06,010 --> 01:04:10,630
its associated if they get the tip of a

01:04:08,260 --> 01:04:12,820
get branch and what happens is that

01:04:10,630 --> 01:04:14,620
every container gets a certificate

01:04:12,820 --> 01:04:16,420
deployed to it from our public key

01:04:14,620 --> 01:04:18,190
infrastructure and that gives it

01:04:16,420 --> 01:04:20,710
authorization to access the resources

01:04:18,190 --> 01:04:23,260
for that environment and so what that

01:04:20,710 --> 01:04:26,020
means is that we need when I deploy a

01:04:23,260 --> 01:04:28,300
new application server for the live

01:04:26,020 --> 01:04:29,770
environment it knows it's an application

01:04:28,300 --> 01:04:31,300
server for the live environment and it

01:04:29,770 --> 01:04:33,310
has a certificate that it can use to

01:04:31,300 --> 01:04:35,320
talk to the get server and pull down the

01:04:33,310 --> 01:04:37,030
code for that is tagged for the live

01:04:35,320 --> 01:04:40,450
environment for the latest release and

01:04:37,030 --> 01:04:42,640
then it reports back up to our API with

01:04:40,450 --> 01:04:43,960
the actual hash that it's on and then we

01:04:42,640 --> 01:04:46,270
make sure that those are consistent

01:04:43,960 --> 01:04:49,030
across these the containers deployed to

01:04:46,270 --> 01:04:51,130
an environment okay so the the initial

01:04:49,030 --> 01:04:53,770
start of a container is going to do a

01:04:51,130 --> 01:04:55,690
git clone in order to get that practice

01:04:53,770 --> 01:04:57,580
of code okay and we do take advantage of

01:04:55,690 --> 01:05:02,800
get shallow clone to avoid pulling the

01:04:57,580 --> 01:05:04,450
history end oh all right thanks thanks

01:05:02,800 --> 01:05:06,850
for the talk my question is related do

01:05:04,450 --> 01:05:09,820
you use what back-end to use to store

01:05:06,850 --> 01:05:12,220
the Maria DB databases that your Rio de

01:05:09,820 --> 01:05:15,460
Vie containers are using oh they're

01:05:12,220 --> 01:05:17,320
using local storage of the all of the

01:05:15,460 --> 01:05:20,860
Rackspace pm's that we deploy to have

01:05:17,320 --> 01:05:23,440
local raid 10 at a minimum of SSD and at

01:05:20,860 --> 01:05:25,720
a maximum they'll have fusion-io follow

01:05:23,440 --> 01:05:27,370
up would you mind sharing like a little

01:05:25,720 --> 01:05:29,890
bit about how you migrate the databases

01:05:27,370 --> 01:05:32,650
but yeah that's really nice um so what

01:05:29,890 --> 01:05:34,990
we do for the database migrations is we

01:05:32,650 --> 01:05:37,450
basically treat it as a thing where we

01:05:34,990 --> 01:05:39,940
temporarily create a replica and then we

01:05:37,450 --> 01:05:41,140
basically promote the replica there's

01:05:39,940 --> 01:05:42,700
actually no difference in our

01:05:41,140 --> 01:05:46,240
infrastructure between migrating and

01:05:42,700 --> 01:05:47,740
database and then failing over we just

01:05:46,240 --> 01:05:49,360
treated as different windows of time so

01:05:47,740 --> 01:05:52,960
if you're migrating a database for

01:05:49,360 --> 01:05:56,080
machine ATM machine be it's just a very

01:05:52,960 --> 01:05:59,020
rapid progression through transactional

01:05:56,080 --> 01:06:01,090
snapshot to replication set up to

01:05:59,020 --> 01:06:03,370
replication catching up with the master

01:06:01,090 --> 01:06:06,100
server to failover like there's just no

01:06:03,370 --> 01:06:08,050
delay between the replica catching up

01:06:06,100 --> 01:06:10,920
and nuts doing failover whereas if we

01:06:08,050 --> 01:06:12,589
maintain a replica persistently it's

01:06:10,920 --> 01:06:15,750
that we kind of hold it at that point

01:06:12,589 --> 01:06:17,339
before doing failover and then that

01:06:15,750 --> 01:06:20,099
allows us to do the migration with

01:06:17,339 --> 01:06:21,829
minimal downtime and weary target all of

01:06:20,099 --> 01:06:24,480
the application servers to the new

01:06:21,829 --> 01:06:34,079
master as well as populating updated

01:06:24,480 --> 01:06:35,910
information about the replicas hi David

01:06:34,079 --> 01:06:37,980
hey same question is last time do you

01:06:35,910 --> 01:06:40,260
guys have plans to release your

01:06:37,980 --> 01:06:45,119
distributed file system not at the

01:06:40,260 --> 01:06:47,280
moment the to give a little background

01:06:45,119 --> 01:06:51,510
we wrote our own distributed file system

01:06:47,280 --> 01:06:53,430
for pantheon because if you look at a

01:06:51,510 --> 01:06:56,940
lot of competing architectures you have

01:06:53,430 --> 01:06:59,280
this kind of split over scale where

01:06:56,940 --> 01:07:01,410
every you either deal with single

01:06:59,280 --> 01:07:02,819
machines and local file system or you're

01:07:01,410 --> 01:07:08,069
dealing with a setup of something like

01:07:02,819 --> 01:07:09,890
cluster FS NFS or a tool like some other

01:07:08,069 --> 01:07:12,119
distributed file system back end and

01:07:09,890 --> 01:07:14,970
when you're dealing with media content

01:07:12,119 --> 01:07:17,819
for Drupal compiled CSS JavaScript you

01:07:14,970 --> 01:07:19,829
need to be able to have a least very

01:07:17,819 --> 01:07:21,390
close to consistent file system for it

01:07:19,829 --> 01:07:23,010
and we didn't want to switch

01:07:21,390 --> 01:07:25,589
technologies in the middle of the stack

01:07:23,010 --> 01:07:27,900
for scaling up we one of our kind of

01:07:25,589 --> 01:07:29,220
mantras is smooth scaling we're like the

01:07:27,900 --> 01:07:31,290
performance of the application doesn't

01:07:29,220 --> 01:07:34,500
change from the free environment up to

01:07:31,290 --> 01:07:35,880
the large enterprise ones and so what we

01:07:34,500 --> 01:07:38,940
did is build our own distributed file

01:07:35,880 --> 01:07:40,799
system that is content addressed meaning

01:07:38,940 --> 01:07:44,190
whenever you put something into it it

01:07:40,799 --> 01:07:46,109
hashes the content and then references

01:07:44,190 --> 01:07:50,099
that content using the hash of it it

01:07:46,109 --> 01:07:52,920
uses a sha 512 and then every single

01:07:50,099 --> 01:07:54,809
application server has a client to our

01:07:52,920 --> 01:07:57,690
distributed file system that is

01:07:54,809 --> 01:07:59,700
accessing the metadata clusters and is

01:07:57,690 --> 01:08:02,369
accessing the kind of longer tail

01:07:59,700 --> 01:08:04,530
storage on s3 mm that allows us to

01:08:02,369 --> 01:08:06,119
deploy the exact same file system for

01:08:04,530 --> 01:08:08,910
every project on the platform and

01:08:06,119 --> 01:08:11,160
massively scale it out there are only

01:08:08,910 --> 01:08:12,900
six production servers for the file

01:08:11,160 --> 01:08:16,430
system supporting all pantheon right now

01:08:12,900 --> 01:08:16,430
2n two separate clusters

01:08:22,850 --> 01:08:25,850

YouTube URL: https://www.youtube.com/watch?v=hFqEsqRFB9s


