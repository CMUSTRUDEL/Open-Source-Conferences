Title: DrupalCon Nashville 2018: Exciting Ethical Engineering
Publication date: 2018-04-11
Playlist: DrupalCon Nashville - April 2018
Description: 
	There is a lot of discussion about what ethical engineering is and how to "be an ethical engineer," but for folks working on and with open source software, there is not a lot of advice on how to be an ethical technology maker (including designer, programmer, content strategist, site-builder) in an open source context.

This lightning talk session will introduce audience members to several types of ethical frameworks, present case studies of ethical engineering failures (and successes), and discuss some guidelines that technology makers can use to think critically and ethically about their work.  If you think that sounds boring, don't worry! 

Similar to last year’s 100 Ways to be an Ally, this talk will be fast, action-packed and full of exciting philosophers, still-pretty-edgy ethical conclusions and epic tales of great engineering disasters.  



Attendees will leave with:

An understanding of several popular ethical frameworks

Guidelines they can use when determining whether or not a technical activity falls within their ethical guidelines

Resources for developing technical ethical literacy

An understanding of why ethical software engineering is important
Captions: 
	00:00:00,000 --> 00:00:06,569
hi my phone says it's 2:15 so we'll go

00:00:05,040 --> 00:00:08,429
ahead and get started I'm sorry that the

00:00:06,569 --> 00:00:13,490
podium is so far back I would rather be

00:00:08,429 --> 00:00:16,289
kind of closer and out there near you

00:00:13,490 --> 00:00:18,840
yeah this is exciting engineering ethics

00:00:16,289 --> 00:00:23,850
and not only is it alliterative it's

00:00:18,840 --> 00:00:25,500
true so I am Nikki I'm a tech lead at

00:00:23,850 --> 00:00:26,820
canopy Studios which is why I'm wearing

00:00:25,500 --> 00:00:28,410
their hoodie also they're full of nice

00:00:26,820 --> 00:00:31,050
people I'd probably wear it anyway

00:00:28,410 --> 00:00:32,820
I'm also incidentally a PhD student at

00:00:31,050 --> 00:00:35,239
Arizona State University thinking about

00:00:32,820 --> 00:00:38,489
engineering ethics as part of that work

00:00:35,239 --> 00:00:40,410
I'm on Twitter as dr. Nikki which I've

00:00:38,489 --> 00:00:45,480
had for years before I went to PhD

00:00:40,410 --> 00:00:47,219
school I'm not really a doctor yet quick

00:00:45,480 --> 00:00:49,020
trigger warning we are gonna be talking

00:00:47,219 --> 00:00:50,730
a little bit just in passing about the

00:00:49,020 --> 00:00:52,289
philosophy of death and killing versus

00:00:50,730 --> 00:00:54,809
letting dies if that stuff that stresses

00:00:52,289 --> 00:00:57,090
you out just know that it's coming this

00:00:54,809 --> 00:00:59,160
is one a lightning talk so we're gonna

00:00:57,090 --> 00:01:01,230
go real fast hopefully maybe even end a

00:00:59,160 --> 00:01:02,640
minute or two early through my 50 slides

00:01:01,230 --> 00:01:05,100
and second of all a

00:01:02,640 --> 00:01:07,080
choose-your-own-adventure type situation

00:01:05,100 --> 00:01:08,939
I'm not gonna tell you in this talk what

00:01:07,080 --> 00:01:10,560
to believe or how to believe it I'm just

00:01:08,939 --> 00:01:12,570
gonna present some things and encourage

00:01:10,560 --> 00:01:13,530
you to maybe think about them I'm also

00:01:12,570 --> 00:01:16,259
not going to tell you what I believe

00:01:13,530 --> 00:01:17,729
except that everything we do is kind of

00:01:16,259 --> 00:01:20,729
laden with our beliefs but I won't make

00:01:17,729 --> 00:01:23,369
it too explicit so not gonna tell you

00:01:20,729 --> 00:01:25,140
where to draw the lines just encourage

00:01:23,369 --> 00:01:27,509
you to do so

00:01:25,140 --> 00:01:29,220
so when people think about ethics a lot

00:01:27,509 --> 00:01:31,380
of what happens is they get ethics and

00:01:29,220 --> 00:01:32,430
values and morality all kind of mixed up

00:01:31,380 --> 00:01:34,560
and they're not sure which one they're

00:01:32,430 --> 00:01:37,020
talking about so I just want to get some

00:01:34,560 --> 00:01:40,530
clarity on those the first is just

00:01:37,020 --> 00:01:42,240
defining what our values are and values

00:01:40,530 --> 00:01:44,520
are just literally that things that we

00:01:42,240 --> 00:01:47,310
value so some of us value family over

00:01:44,520 --> 00:01:50,490
work we value free time over money or

00:01:47,310 --> 00:01:53,460
the opposite we value health over beauty

00:01:50,490 --> 00:01:56,880
and we cats over dogs our values are

00:01:53,460 --> 00:01:58,680
just that system that we have and it's

00:01:56,880 --> 00:02:01,799
important here to point out that nothing

00:01:58,680 --> 00:02:04,610
is value neutral literally not a single

00:02:01,799 --> 00:02:07,799
thing in the world is devoid of values

00:02:04,610 --> 00:02:09,420
we are all people with values and all

00:02:07,799 --> 00:02:11,879
the things that we make our values go

00:02:09,420 --> 00:02:12,970
into that process and so I think anyone

00:02:11,879 --> 00:02:15,220
who's ever sat in a

00:02:12,970 --> 00:02:17,590
colder room negotiating homepage real

00:02:15,220 --> 00:02:19,420
estate knows that even something as

00:02:17,590 --> 00:02:21,610
simple as a website is really political

00:02:19,420 --> 00:02:23,440
right and then the product that we make

00:02:21,610 --> 00:02:25,090
at the end has all those politics in it

00:02:23,440 --> 00:02:26,890
even if we think oh it's not that

00:02:25,090 --> 00:02:29,380
important it's just an ad site it's just

00:02:26,890 --> 00:02:32,620
of this there's still a political value

00:02:29,380 --> 00:02:35,200
Laden process and the objects in our

00:02:32,620 --> 00:02:38,500
lives also have values associated with

00:02:35,200 --> 00:02:40,210
them so things like seat boats we value

00:02:38,500 --> 00:02:42,790
your safety over your freedom to move

00:02:40,210 --> 00:02:44,710
about the car right even these objects

00:02:42,790 --> 00:02:48,550
that exist in our lives and these values

00:02:44,710 --> 00:02:50,620
are everywhere morality is if

00:02:48,550 --> 00:02:52,600
something's good or bad you can keep it

00:02:50,620 --> 00:02:55,600
pretty simple and morality and ethics

00:02:52,600 --> 00:02:58,870
are often used synonymously in common

00:02:55,600 --> 00:03:01,209
vernacular and we maybe will to hear but

00:02:58,870 --> 00:03:03,970
morality is the goodness or badness of a

00:03:01,209 --> 00:03:05,830
thing and ethics is the systems of

00:03:03,970 --> 00:03:08,740
goodness or badness systems of morality

00:03:05,830 --> 00:03:12,010
that people use when people talk about

00:03:08,740 --> 00:03:13,480
ethics in academic situations they talk

00:03:12,010 --> 00:03:15,610
about three kinds of ethics and we won't

00:03:13,480 --> 00:03:17,110
get too far into them but I want to make

00:03:15,610 --> 00:03:18,760
sure that you know about them the first

00:03:17,110 --> 00:03:20,470
one is meta ethics and people think

00:03:18,760 --> 00:03:22,510
about like what is the meaning of

00:03:20,470 --> 00:03:24,239
goodness what is the meaning of

00:03:22,510 --> 00:03:27,160
rightness what is the meaning of truth

00:03:24,239 --> 00:03:29,170
and lots of smart people don't even take

00:03:27,160 --> 00:03:30,459
those words for granted let me say like

00:03:29,170 --> 00:03:33,010
well what's the right thing to do well

00:03:30,459 --> 00:03:36,070
what is right even mean and meta ethics

00:03:33,010 --> 00:03:38,709
helps us to think about that there's a

00:03:36,070 --> 00:03:40,269
field called normative ethics which is

00:03:38,709 --> 00:03:43,660
based on the word norms like what is

00:03:40,269 --> 00:03:46,959
what are the expected things in a

00:03:43,660 --> 00:03:49,870
situation so it helps us ask questions

00:03:46,959 --> 00:03:52,269
like how should we ask how should we act

00:03:49,870 --> 00:03:55,360
in the situation what are the standards

00:03:52,269 --> 00:03:57,730
of right behavior these normative sorts

00:03:55,360 --> 00:03:58,900
of claims and the one that we're really

00:03:57,730 --> 00:04:01,720
going to be talking about today is

00:03:58,900 --> 00:04:03,820
applied ethics which helps us answer

00:04:01,720 --> 00:04:07,420
questions which I think we think about a

00:04:03,820 --> 00:04:11,610
lot what should we do when what should

00:04:07,420 --> 00:04:18,310
we do if in these kinds of situations so

00:04:11,610 --> 00:04:19,600
why are we even here one because I think

00:04:18,310 --> 00:04:21,160
it's important to know that other people

00:04:19,600 --> 00:04:23,680
are thinking about this stuff it's easy

00:04:21,160 --> 00:04:25,270
to feel like oh my gosh I'm the only one

00:04:23,680 --> 00:04:25,580
worried about big data I'm the only one

00:04:25,270 --> 00:04:27,650
thinking

00:04:25,580 --> 00:04:29,330
about maybe if you're not on Twitter you

00:04:27,650 --> 00:04:30,830
think you're the only one right but it's

00:04:29,330 --> 00:04:32,210
like we're there's a whole community of

00:04:30,830 --> 00:04:34,490
people who are thinking about how to be

00:04:32,210 --> 00:04:36,439
ethical technologists and it's important

00:04:34,490 --> 00:04:39,710
to connect with each other a smart

00:04:36,439 --> 00:04:41,150
philosopher says that if you don't have

00:04:39,710 --> 00:04:43,370
someone to talk about ethics with it's

00:04:41,150 --> 00:04:45,080
really hard to be ethical you need other

00:04:43,370 --> 00:04:47,930
people to bounce those things off of and

00:04:45,080 --> 00:04:50,810
to help you situate yourself in what the

00:04:47,930 --> 00:04:52,039
good and the right is um another way

00:04:50,810 --> 00:04:55,310
reason that we're talking about this is

00:04:52,039 --> 00:04:56,780
so that you have a door into all of the

00:04:55,310 --> 00:04:58,129
smart writing that exists about this

00:04:56,780 --> 00:04:59,659
because I know when I started reading

00:04:58,129 --> 00:05:01,699
about ethics I was like this is for this

00:04:59,659 --> 00:05:03,469
is old and boring and doesn't apply to

00:05:01,699 --> 00:05:06,729
my life at all and while that might be

00:05:03,469 --> 00:05:09,379
true it's not a hundred percent true

00:05:06,729 --> 00:05:11,330
smart guy named Hickman said if we can't

00:05:09,379 --> 00:05:14,300
talk about our experiences we can't do

00:05:11,330 --> 00:05:17,389
ethics at all so all of us already have

00:05:14,300 --> 00:05:18,620
inroads into ethical thinking and we're

00:05:17,389 --> 00:05:20,509
gonna come back to this in a minute but

00:05:18,620 --> 00:05:22,219
ethics is not the same as justice and

00:05:20,509 --> 00:05:23,599
just because we might be doing something

00:05:22,219 --> 00:05:27,259
ethically it does not mean we're being

00:05:23,599 --> 00:05:29,840
just so here's another equation but

00:05:27,259 --> 00:05:32,330
knowledge equals power this is both a

00:05:29,840 --> 00:05:35,210
like after school the more you know sort

00:05:32,330 --> 00:05:37,300
of thing but also something I think that

00:05:35,210 --> 00:05:40,250
we all know to be intrinsically true

00:05:37,300 --> 00:05:42,919
right people fight for education and by

00:05:40,250 --> 00:05:45,139
virtue of having education we have more

00:05:42,919 --> 00:05:46,520
power to do things by virtue of going to

00:05:45,139 --> 00:05:50,120
school civil engineers learn how to

00:05:46,520 --> 00:05:52,460
build bridges by virtue of having that

00:05:50,120 --> 00:05:55,370
power we then also have more

00:05:52,460 --> 00:05:57,289
responsibility because civil engineers

00:05:55,370 --> 00:06:00,400
know how to build bridges they have the

00:05:57,289 --> 00:06:02,330
responsibility to build safe bridges and

00:06:00,400 --> 00:06:05,300
associations of civil engineers have

00:06:02,330 --> 00:06:07,039
codes like seeing exactly this because

00:06:05,300 --> 00:06:08,960
you know these things you're on the hook

00:06:07,039 --> 00:06:13,039
for doing them well and for taking care

00:06:08,960 --> 00:06:15,110
of people so because we know how to make

00:06:13,039 --> 00:06:20,029
technology and we have the power to do

00:06:15,110 --> 00:06:23,270
so we have a greater responsibility and

00:06:20,029 --> 00:06:26,419
that by extension asks this question of

00:06:23,270 --> 00:06:28,520
what is our ethical responsibility do we

00:06:26,419 --> 00:06:31,639
even do we have one obviously I'm saying

00:06:28,520 --> 00:06:33,409
yes we do so here's a famous thought

00:06:31,639 --> 00:06:35,060
experiment that I like a lot it's called

00:06:33,409 --> 00:06:37,940
the trolley problem you might have seen

00:06:35,060 --> 00:06:39,199
it in a meme got really popular a couple

00:06:37,940 --> 00:06:41,689
of years ago

00:06:39,199 --> 00:06:43,400
so you are a trolley operator you're

00:06:41,689 --> 00:06:47,210
that guy with the unhappy face and

00:06:43,400 --> 00:06:48,710
here's a trolley coming and on one side

00:06:47,210 --> 00:06:51,379
of the track the direction you were

00:06:48,710 --> 00:06:53,569
already going there are five people tied

00:06:51,379 --> 00:06:56,389
to the trolley track so if you do

00:06:53,569 --> 00:06:58,460
nothing five people will be hit and

00:06:56,389 --> 00:07:01,460
killed by the trolley if you divert it

00:06:58,460 --> 00:07:03,620
only one person will be killed I'm not

00:07:01,460 --> 00:07:05,120
gonna ask you what you would choose but

00:07:03,620 --> 00:07:08,599
if you know which one you would choose

00:07:05,120 --> 00:07:10,490
raise your hand okay that's about fifty

00:07:08,599 --> 00:07:13,999
fifty sixty percent of you were like yes

00:07:10,490 --> 00:07:16,069
I know which one I would pick would your

00:07:13,999 --> 00:07:19,749
answer change if on either one of those

00:07:16,069 --> 00:07:22,249
sides it was your child or loved one or

00:07:19,749 --> 00:07:25,219
mother right how many of you would

00:07:22,249 --> 00:07:27,560
change your answer based on okay okay a

00:07:25,219 --> 00:07:28,639
couple of you yeah it's totally normal

00:07:27,560 --> 00:07:31,279
there's nothing right or wrong about

00:07:28,639 --> 00:07:34,219
that but this is a classic ethical

00:07:31,279 --> 00:07:36,050
problem saying well what what do our

00:07:34,219 --> 00:07:38,870
rules say we should do when we're in

00:07:36,050 --> 00:07:41,870
this sticky artificial but sticky

00:07:38,870 --> 00:07:43,729
situation and one of the things that

00:07:41,870 --> 00:07:46,669
that taps into is this notion of active

00:07:43,729 --> 00:07:48,409
killing versus letting die and we're

00:07:46,669 --> 00:07:49,909
gonna understand maybe why we're talking

00:07:48,409 --> 00:07:51,770
about this in a minute right but one

00:07:49,909 --> 00:07:53,389
direction we're actively killing when

00:07:51,770 --> 00:07:55,009
we're hitting that one and the other

00:07:53,389 --> 00:07:56,779
when we're hitting the five we're just

00:07:55,009 --> 00:08:00,740
not doing anything and oops

00:07:56,779 --> 00:08:02,240
some people get hit so lots of smart

00:08:00,740 --> 00:08:03,860
people have thought about the answer to

00:08:02,240 --> 00:08:05,719
the trolley problem and have opinions

00:08:03,860 --> 00:08:09,620
about what the right answer to the

00:08:05,719 --> 00:08:11,870
trolley problem is and we are both

00:08:09,620 --> 00:08:14,360
physically and ideologically in a system

00:08:11,870 --> 00:08:17,120
of Western thought so a lot of our most

00:08:14,360 --> 00:08:19,069
popular ethical theories are Western but

00:08:17,120 --> 00:08:21,680
before we get into some of those there

00:08:19,069 --> 00:08:24,110
are great books about Buddhist ethics

00:08:21,680 --> 00:08:26,529
great books about black women aesthetics

00:08:24,110 --> 00:08:29,180
great books about Islamic ethics the

00:08:26,529 --> 00:08:31,460
famous Islamic thinkers started medical

00:08:29,180 --> 00:08:33,529
ethics actually great books about queer

00:08:31,460 --> 00:08:36,079
ethics about Jewish ethics about African

00:08:33,529 --> 00:08:39,079
ethics both in the Diaspora and specific

00:08:36,079 --> 00:08:42,169
countries so if you identify with any of

00:08:39,079 --> 00:08:44,540
those over the stuff we're about to talk

00:08:42,169 --> 00:08:48,350
about go check it out there's lots of

00:08:44,540 --> 00:08:50,540
good stuff there when people here in the

00:08:48,350 --> 00:08:51,980
West and North America talk about ethics

00:08:50,540 --> 00:08:53,040
they always start with Aristotle I

00:08:51,980 --> 00:08:56,339
always go back to the

00:08:53,040 --> 00:08:59,279
Greeks Aristotle said lots of stuff

00:08:56,339 --> 00:09:01,560
the main one was that he advocated a

00:08:59,279 --> 00:09:03,870
system of virtue ethics which means that

00:09:01,560 --> 00:09:05,399
we don't need to we don't need to decide

00:09:03,870 --> 00:09:07,170
ok when you have one where you have five

00:09:05,399 --> 00:09:10,920
you always choose five here's the rules

00:09:07,170 --> 00:09:13,949
rather he said just be a good person and

00:09:10,920 --> 00:09:16,470
he outlined some virtues prudence

00:09:13,949 --> 00:09:17,970
temperance courage justice and he said

00:09:16,470 --> 00:09:20,670
you know if you're a person who has

00:09:17,970 --> 00:09:25,589
these you're gonna do all right more or

00:09:20,670 --> 00:09:28,889
less then another guy said hang on we

00:09:25,589 --> 00:09:30,569
need to do our duty and these are called

00:09:28,889 --> 00:09:34,430
deontological ethics I think because

00:09:30,569 --> 00:09:34,430
calling things Duty based sounds funny

00:09:35,060 --> 00:09:39,240
so they call them deontological ethics

00:09:37,500 --> 00:09:42,480
which is just a fancy word for duty or

00:09:39,240 --> 00:09:44,819
rules and a guy named Khan - you might

00:09:42,480 --> 00:09:47,069
have heard of he was a big fan of this

00:09:44,819 --> 00:09:49,949
and he said his kind of main thing was

00:09:47,069 --> 00:09:51,750
like only if you're gonna act in a

00:09:49,949 --> 00:09:53,399
certain way it should be a way that's

00:09:51,750 --> 00:09:56,670
good enough that everyone could act in

00:09:53,399 --> 00:09:57,870
it so only act in a way that everyone

00:09:56,670 --> 00:10:00,089
could do the same thing and the world

00:09:57,870 --> 00:10:03,449
would be good it's kind of the core of

00:10:00,089 --> 00:10:05,040
that idea and the focus here so the

00:10:03,449 --> 00:10:06,870
focus on virtue ethics is on the

00:10:05,040 --> 00:10:10,199
character of the person and the furch

00:10:06,870 --> 00:10:12,120
the focus here is on the character of

00:10:10,199 --> 00:10:13,260
the behavior is your behavior the

00:10:12,120 --> 00:10:16,410
character of your behavior so good that

00:10:13,260 --> 00:10:18,120
everyone could do it and the focus here

00:10:16,410 --> 00:10:19,889
on consequentialism is on the outcome

00:10:18,120 --> 00:10:24,420
all that matters is the outcome of your

00:10:19,889 --> 00:10:26,430
action the consequences so in that case

00:10:24,420 --> 00:10:29,610
of the trolley problem which

00:10:26,430 --> 00:10:31,949
consequences are better and a subset of

00:10:29,610 --> 00:10:33,120
that is utilitarianism and I know that

00:10:31,949 --> 00:10:35,300
we're getting we're getting back to the

00:10:33,120 --> 00:10:37,079
exciting part what hang on

00:10:35,300 --> 00:10:39,209
utilitarianism where we just need to

00:10:37,079 --> 00:10:41,339
maximize a value so in that Charlie

00:10:39,209 --> 00:10:44,430
problem one dead person is better than

00:10:41,339 --> 00:10:46,560
five dead people no matter what so we

00:10:44,430 --> 00:10:50,130
always got to choose that one based on

00:10:46,560 --> 00:10:51,420
this system and part of the reason that

00:10:50,130 --> 00:10:53,519
we're talking about these systems is

00:10:51,420 --> 00:10:55,290
because I think as technologists we are

00:10:53,519 --> 00:10:57,420
used to thinking in systems even if we

00:10:55,290 --> 00:10:59,339
don't agree with them we're gonna use

00:10:57,420 --> 00:11:00,810
react as a JavaScript framework we all

00:10:59,339 --> 00:11:02,819
know the rules even if we don't like

00:11:00,810 --> 00:11:04,639
them we're gonna work within them and

00:11:02,819 --> 00:11:05,990
then leave it

00:11:04,639 --> 00:11:07,490
and they were gonna try on another

00:11:05,990 --> 00:11:09,110
system and another framework and we're

00:11:07,490 --> 00:11:11,509
gonna work within that so we're already

00:11:09,110 --> 00:11:13,790
primed to do this we've been doing this

00:11:11,509 --> 00:11:15,470
for our careers trying on frameworks

00:11:13,790 --> 00:11:17,300
thinking through whether or not they fit

00:11:15,470 --> 00:11:19,550
us and setting them aside or keeping

00:11:17,300 --> 00:11:21,230
them and so there here's up there's a

00:11:19,550 --> 00:11:23,600
ton of frameworks that you can pick up

00:11:21,230 --> 00:11:27,649
saying do I follow rules do I follow

00:11:23,600 --> 00:11:31,279
virtue do I choose consequences so

00:11:27,649 --> 00:11:33,110
here's a question for you is Facebook

00:11:31,279 --> 00:11:41,180
ethical don't you'll have to answer yes

00:11:33,110 --> 00:11:43,550
or no don't everyone rush to answer but

00:11:41,180 --> 00:11:46,819
if you have an opinion on it raise your

00:11:43,550 --> 00:11:54,110
hand oh okay almost all the hands have

00:11:46,819 --> 00:11:56,410
gone up great what is there a difference

00:11:54,110 --> 00:12:00,649
then between actively tracking someone

00:11:56,410 --> 00:12:06,800
and inserting a company's tracking pixel

00:12:00,649 --> 00:12:08,449
on a client site is there a difference

00:12:06,800 --> 00:12:09,680
between actively killing someone and

00:12:08,449 --> 00:12:11,000
letting someone die is there a

00:12:09,680 --> 00:12:13,009
difference between active participation

00:12:11,000 --> 00:12:17,980
and something that might be unethical

00:12:13,009 --> 00:12:23,449
and bystander or sideline participation

00:12:17,980 --> 00:12:26,019
maybe but the choices that we make are

00:12:23,449 --> 00:12:28,689
part of our ethical framework there

00:12:26,019 --> 00:12:33,170
here's another question for you our

00:12:28,689 --> 00:12:34,730
driverless cars ethical and if you

00:12:33,170 --> 00:12:36,379
haven't seen one I live in Phoenix where

00:12:34,730 --> 00:12:39,259
they are everywhere because Arizona is

00:12:36,379 --> 00:12:42,139
the testing bed for driverless cars and

00:12:39,259 --> 00:12:44,209
they are cars that have human people in

00:12:42,139 --> 00:12:45,819
them but those human people are not

00:12:44,209 --> 00:12:48,679
controlling the vehicle at that moment

00:12:45,819 --> 00:12:53,360
the car is being driven by software

00:12:48,679 --> 00:12:54,920
written by humans in a previous time so

00:12:53,360 --> 00:12:56,620
if you have an opinion on whether or not

00:12:54,920 --> 00:13:01,429
driverless cars are ethical

00:12:56,620 --> 00:13:06,589
raise your hand oh just shut up okay so

00:13:01,429 --> 00:13:08,600
fewer of you I would guess that whether

00:13:06,589 --> 00:13:11,899
or not you think driverless cars are

00:13:08,600 --> 00:13:16,639
ethical affects your life less maybe

00:13:11,899 --> 00:13:17,690
than Facebook recently an article came

00:13:16,639 --> 00:13:19,910
out that says

00:13:17,690 --> 00:13:23,630
that's titled autonomous vehicles must

00:13:19,910 --> 00:13:26,030
be programmed to kill and that's both a

00:13:23,630 --> 00:13:30,800
clickbait title but also a true

00:13:26,030 --> 00:13:32,690
statement and here is Nick the Charlie

00:13:30,800 --> 00:13:34,850
problem which is hypothetical artificial

00:13:32,690 --> 00:13:39,550
most of us maybe won't ever be trolley

00:13:34,850 --> 00:13:42,950
drivers I probably won't but who knows

00:13:39,550 --> 00:13:46,460
all of us have written or participated

00:13:42,950 --> 00:13:48,410
in the writing of software or the design

00:13:46,460 --> 00:13:51,680
right all of us are here because this is

00:13:48,410 --> 00:13:54,080
what we're doing I think so at some

00:13:51,680 --> 00:13:56,480
point all of us have been a participant

00:13:54,080 --> 00:13:57,380
in the creation of an algorithm and for

00:13:56,480 --> 00:13:58,970
those of you who don't know what that is

00:13:57,380 --> 00:14:01,190
an algorithm is just steps to solve a

00:13:58,970 --> 00:14:04,610
problem just a bunch of steps to get to

00:14:01,190 --> 00:14:06,830
a destination so this image which I took

00:14:04,610 --> 00:14:10,970
from that article which is titled above

00:14:06,830 --> 00:14:14,060
shows three scenarios and in the first

00:14:10,970 --> 00:14:16,190
we are swerving to hit a group of people

00:14:14,060 --> 00:14:18,320
and in as a result choosing to hit one

00:14:16,190 --> 00:14:21,680
in the middle we're swerving to avoid

00:14:18,320 --> 00:14:24,260
one and hitting nobody and on the far

00:14:21,680 --> 00:14:27,580
right we're just swerving to avoid a

00:14:24,260 --> 00:14:30,230
group an algorithm made this choice

00:14:27,580 --> 00:14:32,150
because a person made this choice and

00:14:30,230 --> 00:14:34,540
because a person made that choice they

00:14:32,150 --> 00:14:37,880
looked at their own ethical code and

00:14:34,540 --> 00:14:47,000
they put those into the software codes a

00:14:37,880 --> 00:14:49,820
team maybe that makes sense so so we

00:14:47,000 --> 00:14:52,370
can't pretend that we do not do

00:14:49,820 --> 00:14:57,050
political things and that we do not make

00:14:52,370 --> 00:14:59,810
things that affect real people here is

00:14:57,050 --> 00:15:01,670
another exciting but unfortunate thing

00:14:59,810 --> 00:15:03,460
for those of you who maybe don't know

00:15:01,670 --> 00:15:05,660
this is a picture of the Challenger in

00:15:03,460 --> 00:15:07,100
1986 the Challenger went up and the

00:15:05,660 --> 00:15:08,690
Challenger came back down about 80

00:15:07,100 --> 00:15:12,560
seconds later and everyone on it was

00:15:08,690 --> 00:15:15,080
killed but before that happened a guy

00:15:12,560 --> 00:15:17,300
named Roger boo Jala knew that it was

00:15:15,080 --> 00:15:18,710
going to happen and this is a classic

00:15:17,300 --> 00:15:20,089
case in engineering ethics

00:15:18,710 --> 00:15:21,230
Roger boo Julie knew that it was going

00:15:20,089 --> 00:15:24,110
to happen it was a problem with the

00:15:21,230 --> 00:15:26,600
o-rings which our fastener kinds of

00:15:24,110 --> 00:15:28,280
things on the on the shuttle and he said

00:15:26,600 --> 00:15:29,810
hey y'all if we launch when it's cold

00:15:28,280 --> 00:15:31,970
those aren't going to work and there

00:15:29,810 --> 00:15:32,959
might be problems and he said something

00:15:31,970 --> 00:15:34,099
and he said something and he said

00:15:32,959 --> 00:15:37,279
something and he said something and

00:15:34,099 --> 00:15:38,209
eventually the vendor and NASA pressured

00:15:37,279 --> 00:15:40,459
him to be like you know what it's gonna

00:15:38,209 --> 00:15:42,769
be fine and we all know as a result that

00:15:40,459 --> 00:15:44,659
it wasn't fine so he did something that

00:15:42,769 --> 00:15:45,679
was ethical and he did something that I

00:15:44,659 --> 00:15:47,419
think most of us would say like yeah

00:15:45,679 --> 00:15:50,679
that's the right thing to do like like

00:15:47,419 --> 00:15:53,029
take a stand but it happened anyway

00:15:50,679 --> 00:15:55,999
whose fault was that

00:15:53,029 --> 00:15:57,679
don't answer not really one answer here

00:15:55,999 --> 00:16:00,409
but it's a question I think that we ask

00:15:57,679 --> 00:16:02,779
ourselves well who who's to blame who's

00:16:00,409 --> 00:16:04,639
to blame for the autonomous vehicle that

00:16:02,779 --> 00:16:06,829
hit someone in Arizona two or three

00:16:04,639 --> 00:16:09,009
weeks ago who's to blame for Facebook I

00:16:06,829 --> 00:16:11,989
bet you have opinions about that one

00:16:09,009 --> 00:16:14,629
whose is it is it Roger Blue Jays fault

00:16:11,989 --> 00:16:19,099
that the Challenger exploded or is it

00:16:14,629 --> 00:16:20,359
the vendors fault Roger blue jewel a

00:16:19,099 --> 00:16:22,669
nowadays we would call him a

00:16:20,359 --> 00:16:24,229
whistleblower and I think that's a lot

00:16:22,669 --> 00:16:25,609
of what ethics is tied really closely

00:16:24,229 --> 00:16:27,649
when we have an ethical problem and you

00:16:25,609 --> 00:16:28,789
say okay well I'm gonna take a stand I'm

00:16:27,649 --> 00:16:30,229
gonna stand up for something that I

00:16:28,789 --> 00:16:32,029
believe is correct and I'm gonna protect

00:16:30,229 --> 00:16:34,879
people and then we're like well there's

00:16:32,029 --> 00:16:36,470
gonna be repercussions and there are

00:16:34,879 --> 00:16:39,919
some repercussions for whistleblowers

00:16:36,470 --> 00:16:41,239
but very few and it's not um it's

00:16:39,919 --> 00:16:46,429
irresponsible to pretend like that's not

00:16:41,239 --> 00:16:47,779
the case so dr. Gina rego is a very

00:16:46,429 --> 00:16:49,909
smart woman and when was one of the

00:16:47,779 --> 00:16:53,299
whistleblowers exposing psychologists

00:16:49,909 --> 00:16:54,889
involvement and military torture and she

00:16:53,299 --> 00:16:56,389
recently said that in order to be a

00:16:54,889 --> 00:16:59,749
whistleblower nowadays you need to have

00:16:56,389 --> 00:17:03,559
a second career and an offshore bank

00:16:59,749 --> 00:17:06,409
account and she was already an affluent

00:17:03,559 --> 00:17:08,959
adult when she exposed when she whistle

00:17:06,409 --> 00:17:12,139
blew so here we come back to this idea

00:17:08,959 --> 00:17:13,339
that ethics is not justice but just

00:17:12,139 --> 00:17:14,809
because you're doing something ethical

00:17:13,339 --> 00:17:19,879
it doesn't necessarily mean you're doing

00:17:14,809 --> 00:17:21,350
something just and the real truth that

00:17:19,879 --> 00:17:27,110
justice is not compatible with

00:17:21,350 --> 00:17:29,299
capitalism I see some faces back there

00:17:27,110 --> 00:17:32,480
they're like I don't know what to do

00:17:29,299 --> 00:17:34,970
with that but I think that it's true

00:17:32,480 --> 00:17:36,740
because if justice were compatible with

00:17:34,970 --> 00:17:39,620
capitalism the Challenger wouldn't have

00:17:36,740 --> 00:17:41,169
gone up that ethical the the driverless

00:17:39,620 --> 00:17:43,220
car wouldn't have hit somebody

00:17:41,169 --> 00:17:45,380
capitalism is driving a lot of these

00:17:43,220 --> 00:17:48,200
choices and capitalism acts as a bear

00:17:45,380 --> 00:17:51,650
year two a lot of our freedom to make

00:17:48,200 --> 00:17:52,370
ethical to take ethical stands and I

00:17:51,650 --> 00:17:54,620
think that's something we have to

00:17:52,370 --> 00:17:57,560
acknowledge we can't just pretend that

00:17:54,620 --> 00:17:58,970
we all have the freedom to be like I'm

00:17:57,560 --> 00:18:00,530
going to say no to this and walk away

00:17:58,970 --> 00:18:03,080
and not have career repercussions

00:18:00,530 --> 00:18:07,070
financial repercussions other sorts of

00:18:03,080 --> 00:18:08,000
challenges yet there's a little footnote

00:18:07,070 --> 00:18:10,810
down there

00:18:08,000 --> 00:18:14,360
that's like yet here we are doing our

00:18:10,810 --> 00:18:17,450
darndest to engage ethically and to

00:18:14,360 --> 00:18:23,390
engage with justice on our minds in the

00:18:17,450 --> 00:18:27,350
ways that we make technology so I have a

00:18:23,390 --> 00:18:29,120
couple of questions the first is to ask

00:18:27,350 --> 00:18:31,160
what's most important to you and this is

00:18:29,120 --> 00:18:33,170
a value-based question and what most

00:18:31,160 --> 00:18:38,750
people answer this with my family my

00:18:33,170 --> 00:18:41,210
children my pets my career whatever you

00:18:38,750 --> 00:18:43,520
can take that answer doing good there's

00:18:41,210 --> 00:18:46,880
a lot of one a common answer I want to

00:18:43,520 --> 00:18:50,180
do good and say okay well here's the

00:18:46,880 --> 00:18:51,950
thing that's most important to me does

00:18:50,180 --> 00:18:55,460
it support the code of ethics I already

00:18:51,950 --> 00:18:57,740
have because all of us already have one

00:18:55,460 --> 00:19:00,770
we just haven't art a lot of us haven't

00:18:57,740 --> 00:19:02,510
articulated it a lot of us are already

00:19:00,770 --> 00:19:05,900
acting as though money is the most

00:19:02,510 --> 00:19:08,660
important thing or safety or social

00:19:05,900 --> 00:19:10,490
change or whatever and I think it's

00:19:08,660 --> 00:19:13,280
absolutely worth taking a moment to

00:19:10,490 --> 00:19:15,560
articulate that some engineering schools

00:19:13,280 --> 00:19:17,690
make engineers take a small like

00:19:15,560 --> 00:19:20,900
business card sized piece of paper and

00:19:17,690 --> 00:19:23,690
write their ethics on it three or four

00:19:20,900 --> 00:19:26,000
bullet points because you already know

00:19:23,690 --> 00:19:27,770
them I see a lot of stressed-out faces

00:19:26,000 --> 00:19:31,040
like I can't do that but you already

00:19:27,770 --> 00:19:32,870
know what goes on that card and I think

00:19:31,040 --> 00:19:34,850
what people find is that they don't

00:19:32,870 --> 00:19:37,790
either don't like what they honestly see

00:19:34,850 --> 00:19:42,320
on that card or that having it written

00:19:37,790 --> 00:19:45,380
down helps them make better choices when

00:19:42,320 --> 00:19:47,570
the moment comes up and this is

00:19:45,380 --> 00:19:49,970
something that we advocate in a lot of

00:19:47,570 --> 00:19:52,190
context if you decide before you need to

00:19:49,970 --> 00:19:52,970
then when you need to it makes the

00:19:52,190 --> 00:19:56,360
moment easier

00:19:52,970 --> 00:19:57,470
so if here while at DrupalCon hopefully

00:19:56,360 --> 00:19:58,909
you don't have any major ethical

00:19:57,470 --> 00:20:01,940
conundrums the

00:19:58,909 --> 00:20:04,669
week maybe you will if you do I would

00:20:01,940 --> 00:20:06,259
love to chat about them but decide here

00:20:04,669 --> 00:20:10,129
at the con okay I'm gonna be someone

00:20:06,259 --> 00:20:12,830
hypothetically who values social justice

00:20:10,129 --> 00:20:15,080
first money second and family third

00:20:12,830 --> 00:20:18,409
great then when you're in a situation

00:20:15,080 --> 00:20:20,090
you can use that rubric to make that

00:20:18,409 --> 00:20:21,619
choice I'm someone who believes that the

00:20:20,090 --> 00:20:23,570
end justifies the means which is a

00:20:21,619 --> 00:20:26,419
consequentialist or utilitarian based

00:20:23,570 --> 00:20:28,700
approach that approach gets a lot of bad

00:20:26,419 --> 00:20:30,499
rap people use it all the time I'm gonna

00:20:28,700 --> 00:20:33,889
do whatever it takes to feed my kids

00:20:30,499 --> 00:20:37,639
that's a consequentialist approach and

00:20:33,889 --> 00:20:40,849
then you have that and then a couple of

00:20:37,639 --> 00:20:43,879
important reminders the first is that

00:20:40,849 --> 00:20:46,309
you are not value neutral either just

00:20:43,879 --> 00:20:48,739
like the technology that you make your

00:20:46,309 --> 00:20:50,599
values are with you all the time and

00:20:48,739 --> 00:20:52,999
it's easy to forget as we are encouraged

00:20:50,599 --> 00:20:55,399
to be production machines creation

00:20:52,999 --> 00:20:56,899
machines we're here in a community that

00:20:55,399 --> 00:20:58,669
tends to value technological

00:20:56,899 --> 00:21:01,099
contributions over human contributions

00:20:58,669 --> 00:21:04,190
it's easy to forget that we are humans

00:21:01,099 --> 00:21:06,739
contributing human value which sometimes

00:21:04,190 --> 00:21:09,349
takes the form of code but not even

00:21:06,739 --> 00:21:12,470
maybe most of the time for most of us so

00:21:09,349 --> 00:21:14,799
we are not value neutral and that

00:21:12,470 --> 00:21:18,139
creating technology is a political act

00:21:14,799 --> 00:21:20,090
every single time we do it for Cillian

00:21:18,139 --> 00:21:21,529
reasons its political because you're the

00:21:20,090 --> 00:21:23,149
only person like you're doing it it's

00:21:21,529 --> 00:21:24,529
political because you're political and

00:21:23,149 --> 00:21:26,690
it's political because that technology

00:21:24,529 --> 00:21:30,379
goes into the world and becomes

00:21:26,690 --> 00:21:31,849
political the websites you make are used

00:21:30,379 --> 00:21:34,399
to make money that's a political thing

00:21:31,849 --> 00:21:37,970
or they're used for literally politics

00:21:34,399 --> 00:21:39,590
that's also political so every day that

00:21:37,970 --> 00:21:41,599
we go to work we are doing political

00:21:39,590 --> 00:21:43,399
things and we have the option to be

00:21:41,599 --> 00:21:44,989
political in ways that support our

00:21:43,399 --> 00:21:47,330
ethical frameworks or ways that do not

00:21:44,989 --> 00:21:49,220
support our ethical frameworks all of

00:21:47,330 --> 00:21:51,349
the things that I cited all of the

00:21:49,220 --> 00:21:53,629
references will be on my website in a

00:21:51,349 --> 00:21:55,849
couple of days I'm on Twitter as dr.

00:21:53,629 --> 00:21:57,200
Nicki and before I stop talking I'm

00:21:55,849 --> 00:21:58,519
going to remind you that the

00:21:57,200 --> 00:22:00,679
contributions friends are Friday

00:21:58,519 --> 00:22:02,899
sprinting is so fun sprinting is also

00:22:00,679 --> 00:22:05,659
not value neutral it is a huge value

00:22:02,899 --> 00:22:07,519
added activity and there's a bunch of

00:22:05,659 --> 00:22:09,320
ways that you can get involved and if

00:22:07,519 --> 00:22:12,999
you have feedback you can do that over

00:22:09,320 --> 00:22:12,999
there and that's it thank you very much

00:22:17,149 --> 00:22:23,909
with three minutes to spare I made it

00:22:20,759 --> 00:22:25,320
through 50 slides if anyone has any

00:22:23,909 --> 00:22:27,119
questions we have three minutes if not

00:22:25,320 --> 00:22:34,649
take a break come to the mic please

00:22:27,119 --> 00:22:38,759
because they're recording it you said I

00:22:34,649 --> 00:22:40,440
think this is not just not justice not

00:22:38,759 --> 00:22:42,990
always not always yeah

00:22:40,440 --> 00:22:45,899
you mean justice like human justice or

00:22:42,990 --> 00:22:48,269
justice like legal justice I guess I

00:22:45,899 --> 00:22:50,490
mean both academically we talk about

00:22:48,269 --> 00:22:51,899
three kinds of justice like distributive

00:22:50,490 --> 00:22:54,419
justice getting everybody what they need

00:22:51,899 --> 00:22:56,549
compensatory justice making up for bad

00:22:54,419 --> 00:23:00,840
things and the third kind that's

00:22:56,549 --> 00:23:09,869
escaping me so ethics what's that do you

00:23:00,840 --> 00:23:12,480
know the fair so so I mean all kinds so

00:23:09,869 --> 00:23:14,399
so there are some you know in virtue

00:23:12,480 --> 00:23:16,649
ethics for example Aristotle explicitly

00:23:14,399 --> 00:23:19,039
called out justice as one of the virtues

00:23:16,649 --> 00:23:24,690
and he said yeah that's what I meant

00:23:19,039 --> 00:23:27,600
because justice is virtuous yes but then

00:23:24,690 --> 00:23:30,749
you're like without talking about it you

00:23:27,600 --> 00:23:34,139
are talking about ethical justice and

00:23:30,749 --> 00:23:36,740
legal justice for sure yeah i smush them

00:23:34,139 --> 00:23:40,730
together for the lightning talk because

00:23:36,740 --> 00:23:45,169
you say that whistleblowers are punished

00:23:40,730 --> 00:23:47,879
often yeah by the legal system yes and

00:23:45,169 --> 00:23:52,470
those standing by they are not always

00:23:47,879 --> 00:23:55,769
saying that okay the punishment is by

00:23:52,470 --> 00:23:59,070
legal system but intrinsically what the

00:23:55,769 --> 00:24:01,679
person did is okay from the ethical

00:23:59,070 --> 00:24:03,450
point of view yes absolutely yeah that's

00:24:01,679 --> 00:24:05,279
what I wanted to clarify yeah no I

00:24:03,450 --> 00:24:06,749
appreciate that and and I was swishing

00:24:05,279 --> 00:24:08,970
all the kinds of justice into one word

00:24:06,749 --> 00:24:11,909
and there's absolutely intrinsic justice

00:24:08,970 --> 00:24:14,240
and ethical justice that exists thank

00:24:11,909 --> 00:24:14,240
you for that

00:24:14,690 --> 00:24:21,110
anyone else cool stay for a the stock is

00:24:18,779 --> 00:24:21,110
up next

00:24:21,950 --> 00:24:28,920
[Applause]

00:24:38,700 --> 00:24:41,700

YouTube URL: https://www.youtube.com/watch?v=QKNVAHrzJ5c


