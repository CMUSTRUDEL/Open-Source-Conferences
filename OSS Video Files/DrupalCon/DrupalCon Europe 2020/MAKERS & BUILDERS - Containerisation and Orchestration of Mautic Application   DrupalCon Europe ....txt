Title: MAKERS & BUILDERS - Containerisation and Orchestration of Mautic Application   DrupalCon Europe ...
Publication date: 2021-01-21
Playlist: DrupalCon Europe 2020
Description: 
	Containerisation and Orchestration of Mautic Application Problem Statement
Captions: 
	00:00:04,059 --> 00:00:09,889
[Music]

00:00:13,120 --> 00:00:16,960
so

00:00:13,519 --> 00:00:19,439
hello everyone thank you for joining my

00:00:16,960 --> 00:00:21,039
session on containerization and

00:00:19,439 --> 00:00:24,240
orchestration

00:00:21,039 --> 00:00:25,840
of the martinique application i hope you

00:00:24,240 --> 00:00:28,080
will have a great time and this is my

00:00:25,840 --> 00:00:30,960
first time in a drupal con

00:00:28,080 --> 00:00:31,519
and i'm hoping that i'll be making use

00:00:30,960 --> 00:00:34,880
of your

00:00:31,519 --> 00:00:37,760
uh productive time so uh

00:00:34,880 --> 00:00:39,360
this is a bit about me i work as a site

00:00:37,760 --> 00:00:42,160
reliability engineer at

00:00:39,360 --> 00:00:42,879
uh excellent technologies and over the

00:00:42,160 --> 00:00:45,440
nine

00:00:42,879 --> 00:00:47,440
i have about over nine plus years of

00:00:45,440 --> 00:00:48,719
experience in it i worked as a system

00:00:47,440 --> 00:00:51,600
administrator in the

00:00:48,719 --> 00:00:52,960
uh in the beginning and then i move out

00:00:51,600 --> 00:00:55,760
more out

00:00:52,960 --> 00:00:58,320
to a better road and currently i'm now

00:00:55,760 --> 00:01:00,800
working as a site reliability engineer

00:00:58,320 --> 00:01:03,039
so these are some of my hobbies and i'm

00:01:00,800 --> 00:01:05,920
currently working on openshift gitlab ci

00:01:03,039 --> 00:01:07,600
ansible and kubernetes and i basically

00:01:05,920 --> 00:01:08,640
love contributing to the kubernetes

00:01:07,600 --> 00:01:13,040
project

00:01:08,640 --> 00:01:15,200
so i'm actually making my efforts

00:01:13,040 --> 00:01:17,119
for giving my best for the kubernetes

00:01:15,200 --> 00:01:20,240
community in the coming days

00:01:17,119 --> 00:01:23,360
and i'm also aws community builder so

00:01:20,240 --> 00:01:26,880
there is a program in aws which is aws

00:01:23,360 --> 00:01:30,000
community builders program which is

00:01:26,880 --> 00:01:32,320
uh where like-minded professionals

00:01:30,000 --> 00:01:33,200
who are enthusiastic in contributing to

00:01:32,320 --> 00:01:36,320
aws

00:01:33,200 --> 00:01:39,119
come together and contribute to the

00:01:36,320 --> 00:01:40,240
community so um i've recently been

00:01:39,119 --> 00:01:42,320
absorbed into that

00:01:40,240 --> 00:01:44,479
program and i'm very glad that i'll be

00:01:42,320 --> 00:01:47,520
working in the next coming days

00:01:44,479 --> 00:01:48,720
and as just as how the drupal community

00:01:47,520 --> 00:01:51,439
is even the

00:01:48,720 --> 00:01:52,560
aws community also is growing bigger and

00:01:51,439 --> 00:01:55,360
bigger

00:01:52,560 --> 00:01:56,240
and i have i hold two certifications one

00:01:55,360 --> 00:01:58,799
in

00:01:56,240 --> 00:02:01,280
aws solution architect and another one

00:01:58,799 --> 00:02:04,320
is a certified kubernetes administrator

00:02:01,280 --> 00:02:06,079
and these are my social media handles if

00:02:04,320 --> 00:02:09,200
you would like to connect with me

00:02:06,079 --> 00:02:12,000
please do contact me or we'll and we

00:02:09,200 --> 00:02:12,000
will get in touch

00:02:12,879 --> 00:02:17,120
so these are some of the key takeaways

00:02:16,239 --> 00:02:19,920
from my session

00:02:17,120 --> 00:02:21,360
i would be explaining what a bit about

00:02:19,920 --> 00:02:24,160
architectural design

00:02:21,360 --> 00:02:26,160
of the application how we have decoupled

00:02:24,160 --> 00:02:27,920
different components and made use of

00:02:26,160 --> 00:02:29,920
native kubernetes services to address

00:02:27,920 --> 00:02:33,280
the problems

00:02:29,920 --> 00:02:36,080
and how we manage docker image updates

00:02:33,280 --> 00:02:37,519
and then how we consolidated all the

00:02:36,080 --> 00:02:39,599
configuration files

00:02:37,519 --> 00:02:41,360
available inside martic and brought them

00:02:39,599 --> 00:02:44,080
into one single file

00:02:41,360 --> 00:02:44,959
called config.php and some security

00:02:44,080 --> 00:02:48,000
concentrations

00:02:44,959 --> 00:02:52,080
and most of the part of our session

00:02:48,000 --> 00:02:52,959
would align uh with troll factor app

00:02:52,080 --> 00:02:55,280
methodology

00:02:52,959 --> 00:02:56,640
and i'm hoping that you guys are aware

00:02:55,280 --> 00:02:59,760
of 12 factor map

00:02:56,640 --> 00:03:02,959
methodology if not then you would be uh

00:02:59,760 --> 00:03:05,680
knowing in the coming time so

00:03:02,959 --> 00:03:07,120
what is martin well motik is an open

00:03:05,680 --> 00:03:09,599
source marketing platform

00:03:07,120 --> 00:03:10,319
it has been uh very famous in the recent

00:03:09,599 --> 00:03:13,920
times and

00:03:10,319 --> 00:03:16,080
it has been the greatest uh advantage

00:03:13,920 --> 00:03:16,959
of using martic is that it provides the

00:03:16,080 --> 00:03:19,120
greatest level of

00:03:16,959 --> 00:03:20,480
integration and deep audience

00:03:19,120 --> 00:03:24,159
intelligence

00:03:20,480 --> 00:03:25,519
and you could also do email marketing

00:03:24,159 --> 00:03:27,840
social media marketing marketing

00:03:25,519 --> 00:03:31,200
analysis lead generation

00:03:27,840 --> 00:03:32,640
and roi reporting many of the digital

00:03:31,200 --> 00:03:33,840
marketing strategies or digital

00:03:32,640 --> 00:03:37,920
marketing campaigns

00:03:33,840 --> 00:03:40,239
can easily be run using martin

00:03:37,920 --> 00:03:41,440
so uh coming to the customers problem

00:03:40,239 --> 00:03:44,159
statement so when a customer

00:03:41,440 --> 00:03:46,400
one of our customers has come to us that

00:03:44,159 --> 00:03:49,440
they wanted they were running

00:03:46,400 --> 00:03:52,560
uh martic on a aws

00:03:49,440 --> 00:03:53,200
infrastructure on a normal ec2 virtual

00:03:52,560 --> 00:03:56,480
machines

00:03:53,200 --> 00:03:58,799
and they wanted us to help them because

00:03:56,480 --> 00:04:00,000
uh they they some of the challenges they

00:03:58,799 --> 00:04:01,760
faced was that the

00:04:00,000 --> 00:04:03,280
scalability of their applications during

00:04:01,760 --> 00:04:06,640
their intense marketing

00:04:03,280 --> 00:04:08,959
camp marketing campaigns and also

00:04:06,640 --> 00:04:10,080
uh how they weren't they were having a

00:04:08,959 --> 00:04:14,239
tough time in

00:04:10,080 --> 00:04:16,160
of managing the application updates

00:04:14,239 --> 00:04:17,919
so these were major concerns of the

00:04:16,160 --> 00:04:21,120
customer when they came to us

00:04:17,919 --> 00:04:23,360
with this and and uh that's what

00:04:21,120 --> 00:04:25,360
that's when we started working on this

00:04:23,360 --> 00:04:29,199
particular martic application

00:04:25,360 --> 00:04:31,280
so on on initial analysis of the matic

00:04:29,199 --> 00:04:32,240
application at that point of time when

00:04:31,280 --> 00:04:36,160
we checked

00:04:32,240 --> 00:04:39,600
what we saw is that moti application is

00:04:36,160 --> 00:04:41,600
a totally not enterprise ready and not

00:04:39,600 --> 00:04:43,040
in line with the 12 factor app

00:04:41,600 --> 00:04:45,759
methodology

00:04:43,040 --> 00:04:47,520
well there are many multiple uh like

00:04:45,759 --> 00:04:47,840
there are multiple configuration files

00:04:47,520 --> 00:04:49,919
that

00:04:47,840 --> 00:04:51,680
have been uh like to inject

00:04:49,919 --> 00:04:54,880
environmental variables

00:04:51,680 --> 00:04:55,360
and that was one of the tough factors

00:04:54,880 --> 00:04:57,199
where

00:04:55,360 --> 00:04:58,720
there are many multiple files involved

00:04:57,199 --> 00:05:00,639
in this and uh

00:04:58,720 --> 00:05:01,759
managing all those files together was

00:05:00,639 --> 00:05:03,840
very hard

00:05:01,759 --> 00:05:06,160
and apart from that there was also an

00:05:03,840 --> 00:05:07,440
open source uh document image formatting

00:05:06,160 --> 00:05:10,000
which was

00:05:07,440 --> 00:05:10,800
uh not enterprise ready leave about

00:05:10,000 --> 00:05:12,560
leave alone

00:05:10,800 --> 00:05:14,720
running it on kubernetes or any of the

00:05:12,560 --> 00:05:16,560
container orchestration engines

00:05:14,720 --> 00:05:18,800
so apart from that there is also the

00:05:16,560 --> 00:05:20,560
affordable and persistent components

00:05:18,800 --> 00:05:24,320
like the logs and cash

00:05:20,560 --> 00:05:27,120
which are not decoupled so

00:05:24,320 --> 00:05:28,479
and including all this there's another

00:05:27,120 --> 00:05:31,360
factor also where

00:05:28,479 --> 00:05:33,440
uh there are multiple processes uh

00:05:31,360 --> 00:05:35,680
running in the same container

00:05:33,440 --> 00:05:37,199
so that is actually an anti-pattern to

00:05:35,680 --> 00:05:40,479
the containerization

00:05:37,199 --> 00:05:42,960
so taking all these facts into the

00:05:40,479 --> 00:05:44,560
consideration we have designed our

00:05:42,960 --> 00:05:46,479
architecture like this

00:05:44,560 --> 00:05:48,080
so the architecture has the following

00:05:46,479 --> 00:05:51,520
components it's

00:05:48,080 --> 00:05:55,600
the kubernetes and we have martic

00:05:51,520 --> 00:05:58,720
and nginx deployed as a service where

00:05:55,600 --> 00:06:01,919
engine x in nginx ingress controller

00:05:58,720 --> 00:06:06,639
has been used as the

00:06:01,919 --> 00:06:07,680
load balancer and then we use cron jobs

00:06:06,639 --> 00:06:10,000
to run

00:06:07,680 --> 00:06:11,280
uh intense marketing campaigns so

00:06:10,000 --> 00:06:13,199
previously the marketing

00:06:11,280 --> 00:06:15,919
whenever the marketing campaigns were

00:06:13,199 --> 00:06:18,240
run uh the customer had to have a

00:06:15,919 --> 00:06:19,199
lot of load on the application so we

00:06:18,240 --> 00:06:22,240
decoupled that

00:06:19,199 --> 00:06:23,520
and we made use of kubernetes cron jobs

00:06:22,240 --> 00:06:26,720
to run the

00:06:23,520 --> 00:06:29,759
scheduled or ad hoc marketing campaigns

00:06:26,720 --> 00:06:30,639
and we also used rabbitmq as a separate

00:06:29,759 --> 00:06:33,440
service

00:06:30,639 --> 00:06:34,800
and so that it it reduces the load out

00:06:33,440 --> 00:06:38,400
of the application

00:06:34,800 --> 00:06:41,759
and we also have my sequel instance

00:06:38,400 --> 00:06:44,080
uh running as an rds in aws this

00:06:41,759 --> 00:06:45,120
has not much of change because the

00:06:44,080 --> 00:06:48,560
earlier use

00:06:45,120 --> 00:06:50,800
also was on aws ideas itself so there is

00:06:48,560 --> 00:06:53,039
not much change on the uh database side

00:06:50,800 --> 00:06:54,000
it was running on ideas before and even

00:06:53,039 --> 00:06:57,520
after

00:06:54,000 --> 00:07:00,880
uh we worked on it and then we also have

00:06:57,520 --> 00:07:04,000
an application load balancer which acts

00:07:00,880 --> 00:07:04,720
as a front end to the nginx load

00:07:04,000 --> 00:07:08,080
balancer

00:07:04,720 --> 00:07:08,880
or engine x ingress controller and we

00:07:08,080 --> 00:07:12,400
also have

00:07:08,880 --> 00:07:13,520
route 53 which is a dns service from aws

00:07:12,400 --> 00:07:16,960
that we have used

00:07:13,520 --> 00:07:20,400
for all the dns dns related

00:07:16,960 --> 00:07:20,400
queries so

00:07:21,759 --> 00:07:25,599
if you look at the diagram or if you if

00:07:23,599 --> 00:07:29,199
you look at the architecture

00:07:25,599 --> 00:07:30,080
we have also made use of the efs volumes

00:07:29,199 --> 00:07:34,160
which are

00:07:30,080 --> 00:07:37,599
the uh kind of block storage

00:07:34,160 --> 00:07:40,720
or file storage for in in the aws

00:07:37,599 --> 00:07:45,120
and for cache we we actually

00:07:40,720 --> 00:07:48,800
disintegrated and we made the

00:07:45,120 --> 00:07:52,080
logs and cache uh upload to a pvc

00:07:48,800 --> 00:07:56,319
i mean which is again a volume that is

00:07:52,080 --> 00:07:58,800
being used by which is the efs volume

00:07:56,319 --> 00:08:00,400
and so how did we start the work so the

00:07:58,800 --> 00:08:01,919
first thing that we started to work was

00:08:00,400 --> 00:08:03,039
decoupling of different components from

00:08:01,919 --> 00:08:05,360
the application

00:08:03,039 --> 00:08:07,440
so what we did was we actually decoupled

00:08:05,360 --> 00:08:08,160
the application container from the web

00:08:07,440 --> 00:08:10,960
tier

00:08:08,160 --> 00:08:11,759
thus ensuring that we all the app and

00:08:10,960 --> 00:08:13,680
the web

00:08:11,759 --> 00:08:15,039
have their own resource limits so

00:08:13,680 --> 00:08:15,759
previously what happened was that they

00:08:15,039 --> 00:08:19,280
were

00:08:15,759 --> 00:08:20,400
using both application and the web tier

00:08:19,280 --> 00:08:23,680
in the same server

00:08:20,400 --> 00:08:25,360
so thereby the traffic to the server is

00:08:23,680 --> 00:08:27,440
very high and it's causing a lot of

00:08:25,360 --> 00:08:29,039
problems so as part of the 12-factor

00:08:27,440 --> 00:08:32,159
methodology we decoupled

00:08:29,039 --> 00:08:34,800
both the web tier and the app tier

00:08:32,159 --> 00:08:36,159
and so that each of them has their own

00:08:34,800 --> 00:08:37,440
resource limits and

00:08:36,159 --> 00:08:40,320
they they could actually work

00:08:37,440 --> 00:08:42,880
independently and

00:08:40,320 --> 00:08:43,440
for as i said before we also use cron

00:08:42,880 --> 00:08:46,240
jobs

00:08:43,440 --> 00:08:46,800
which run in parallel to the application

00:08:46,240 --> 00:08:49,680
and

00:08:46,800 --> 00:08:50,240
uh whenever there is a need for a bad

00:08:49,680 --> 00:08:52,399
job

00:08:50,240 --> 00:08:53,440
or a scheduled or ad hoc marketing

00:08:52,399 --> 00:08:56,480
campaigns

00:08:53,440 --> 00:08:57,680
and then we also uh decoupled the

00:08:56,480 --> 00:09:00,720
messaging queue

00:08:57,680 --> 00:09:01,040
and we ran it as a distinct service on

00:09:00,720 --> 00:09:04,880
the

00:09:01,040 --> 00:09:08,080
uh on the platform and

00:09:04,880 --> 00:09:10,320
apart from that uh the next the next the

00:09:08,080 --> 00:09:13,120
next thing that we we were trying to do

00:09:10,320 --> 00:09:15,680
is that the configuration file

00:09:13,120 --> 00:09:18,000
uh the there were many configuration

00:09:15,680 --> 00:09:20,080
files uh to talk about formatting

00:09:18,000 --> 00:09:21,440
and all we did was we wanted to

00:09:20,080 --> 00:09:24,560
consolidate all of them

00:09:21,440 --> 00:09:26,000
and we brought in we brought all of them

00:09:24,560 --> 00:09:28,320
into one

00:09:26,000 --> 00:09:30,880
file called config.php so this

00:09:28,320 --> 00:09:32,880
particular config.php is the file which

00:09:30,880 --> 00:09:34,399
has the database information and other

00:09:32,880 --> 00:09:36,640
connection information

00:09:34,399 --> 00:09:38,800
so that wherever if whenever there is a

00:09:36,640 --> 00:09:42,000
change or whenever there is

00:09:38,800 --> 00:09:46,800
a a change needed it would actually

00:09:42,000 --> 00:09:46,800
uh be only updated in the config.php

00:09:47,440 --> 00:09:53,600
next is the docker image updates so uh

00:09:50,560 --> 00:09:56,240
the reason why we uh separated this

00:09:53,600 --> 00:09:56,880
out is that in order to make the

00:09:56,240 --> 00:09:59,920
application

00:09:56,880 --> 00:10:02,160
extensible what it does is we like for

00:09:59,920 --> 00:10:05,440
example whenever a developer

00:10:02,160 --> 00:10:07,040
adds some plugins or extensions so

00:10:05,440 --> 00:10:09,200
they actually have to build the whole

00:10:07,040 --> 00:10:11,120
source code from the beginning in order

00:10:09,200 --> 00:10:13,839
to have the changes reflected on the

00:10:11,120 --> 00:10:14,640
website however in this case what we did

00:10:13,839 --> 00:10:17,200
was that we

00:10:14,640 --> 00:10:18,640
added the composer install command as

00:10:17,200 --> 00:10:21,680
part of the docker image

00:10:18,640 --> 00:10:24,399
docker file so that all that

00:10:21,680 --> 00:10:26,160
all the developer has to do is that just

00:10:24,399 --> 00:10:27,760
add the extensions and plugins that they

00:10:26,160 --> 00:10:29,760
want in the application

00:10:27,760 --> 00:10:31,760
and let docker take care of installing

00:10:29,760 --> 00:10:34,560
all of them so this is one of the

00:10:31,760 --> 00:10:38,160
important part because

00:10:34,560 --> 00:10:40,000
it saves a lot of time and it saves

00:10:38,160 --> 00:10:41,680
it it actually saves the time of the

00:10:40,000 --> 00:10:44,839
developer it actually

00:10:41,680 --> 00:10:47,200
builds new image whenever there are

00:10:44,839 --> 00:10:50,720
changes

00:10:47,200 --> 00:10:53,920
and apart from these we also went on to

00:10:50,720 --> 00:10:55,839
do further improvements where the

00:10:53,920 --> 00:10:57,200
the most important aspect in the

00:10:55,839 --> 00:11:00,399
containerized world is

00:10:57,200 --> 00:11:03,200
also security so what we do is that

00:11:00,399 --> 00:11:05,040
since security is paramount uh when

00:11:03,200 --> 00:11:07,440
containerizing the applications

00:11:05,040 --> 00:11:08,720
what we did is that we made sure that

00:11:07,440 --> 00:11:11,440
all these containers

00:11:08,720 --> 00:11:11,760
are running as a non-root user so that

00:11:11,440 --> 00:11:13,839
we

00:11:11,760 --> 00:11:16,079
reduce the attack surface on these

00:11:13,839 --> 00:11:20,079
containers and

00:11:16,079 --> 00:11:22,959
make sure they are tightly secured

00:11:20,079 --> 00:11:23,839
so when we come to the deployment

00:11:22,959 --> 00:11:26,640
section

00:11:23,839 --> 00:11:28,160
we have used these tools gitlab cicd

00:11:26,640 --> 00:11:31,440
terraform and amazon

00:11:28,160 --> 00:11:31,839
eks the reason why we chose gitlab ci cd

00:11:31,440 --> 00:11:33,839
is that

00:11:31,839 --> 00:11:35,920
because of its declarative way to create

00:11:33,839 --> 00:11:38,640
a ci cd pipeline

00:11:35,920 --> 00:11:40,959
and along with that it also has an easy

00:11:38,640 --> 00:11:42,800
integration with the amazon eks

00:11:40,959 --> 00:11:44,160
which is elastic kubernetes service from

00:11:42,800 --> 00:11:46,320
the amazon

00:11:44,160 --> 00:11:48,399
so the declaration the declarative

00:11:46,320 --> 00:11:51,600
configuration of the pipeline makes it

00:11:48,399 --> 00:11:54,079
easily configurable and customizable

00:11:51,600 --> 00:11:55,040
so the build when it comes to the build

00:11:54,079 --> 00:11:57,920
process

00:11:55,040 --> 00:11:59,040
what happens is that whenever there is a

00:11:57,920 --> 00:12:02,560
building of

00:11:59,040 --> 00:12:05,200
images involved for martic and the matic

00:12:02,560 --> 00:12:06,399
engineering and what happens is that

00:12:05,200 --> 00:12:08,560
once those once these

00:12:06,399 --> 00:12:10,720
images are built they are pushed to the

00:12:08,560 --> 00:12:12,399
ecr registry ecr is nothing but

00:12:10,720 --> 00:12:15,360
elastic container registry which is a

00:12:12,399 --> 00:12:17,920
container registry software from the aws

00:12:15,360 --> 00:12:18,639
so in the deploy step that is a build

00:12:17,920 --> 00:12:20,639
step

00:12:18,639 --> 00:12:22,320
and after the build is done the deploy

00:12:20,639 --> 00:12:25,279
step what it does is it

00:12:22,320 --> 00:12:27,760
pulls the image from the ecr repository

00:12:25,279 --> 00:12:29,680
and deploys them onto the kubernetes

00:12:27,760 --> 00:12:31,279
so by doing that what happens is that

00:12:29,680 --> 00:12:32,880
every time a code is checked into the

00:12:31,279 --> 00:12:35,440
non-master branch

00:12:32,880 --> 00:12:37,360
a new namespace is created in the

00:12:35,440 --> 00:12:40,639
kubernetes with the branch name

00:12:37,360 --> 00:12:43,200
and application is deployed into the

00:12:40,639 --> 00:12:44,240
uh into the particular name space

00:12:43,200 --> 00:12:46,399
thereby you don't

00:12:44,240 --> 00:12:47,680
actually have to create multiple

00:12:46,399 --> 00:12:50,639
kubernetes clusters

00:12:47,680 --> 00:12:51,760
you can just create multiple namespaces

00:12:50,639 --> 00:12:53,440
in a single cluster

00:12:51,760 --> 00:12:54,800
and manage all your applications this is

00:12:53,440 --> 00:12:57,120
the advantages

00:12:54,800 --> 00:12:58,000
advantage that we have with eks or for

00:12:57,120 --> 00:13:01,440
the matter of fact

00:12:58,000 --> 00:13:04,320
for with the kubernetes and

00:13:01,440 --> 00:13:05,440
apart from that the reason why we have

00:13:04,320 --> 00:13:08,160
chosen gitlab

00:13:05,440 --> 00:13:09,040
gitlab over other ci cd tools is that it

00:13:08,160 --> 00:13:12,399
only has

00:13:09,040 --> 00:13:13,920
a yaml based ci file which is gitlab

00:13:12,399 --> 00:13:16,079
hyphen c a data

00:13:13,920 --> 00:13:18,240
so what happens is that whenever we

00:13:16,079 --> 00:13:21,519
define something inside this yaml file

00:13:18,240 --> 00:13:24,399
these definitions could be reused and

00:13:21,519 --> 00:13:26,240
only changes to the uh all you have to

00:13:24,399 --> 00:13:27,040
do is that you just have to change the

00:13:26,240 --> 00:13:28,880
variable names

00:13:27,040 --> 00:13:31,519
so that's the that's one of the reasons

00:13:28,880 --> 00:13:34,720
why we have used gitlab csd for our

00:13:31,519 --> 00:13:38,560
csd and then coming to terraform

00:13:34,720 --> 00:13:40,480
since uh in the previous instances what

00:13:38,560 --> 00:13:42,160
happened was that the customer was using

00:13:40,480 --> 00:13:43,920
it on ec2 instances

00:13:42,160 --> 00:13:46,079
they were deployed they deployed the

00:13:43,920 --> 00:13:47,360
martic application on ec2 instances but

00:13:46,079 --> 00:13:50,240
here in this case

00:13:47,360 --> 00:13:51,440
what happens is the terraform was used

00:13:50,240 --> 00:13:54,480
to create

00:13:51,440 --> 00:13:57,839
a eks cluster along with the

00:13:54,480 --> 00:14:01,279
persistent volumes for efs and

00:13:57,839 --> 00:14:03,440
rds so what happens by this is that

00:14:01,279 --> 00:14:04,480
all the infrastructure is set up within

00:14:03,440 --> 00:14:06,560
few minutes

00:14:04,480 --> 00:14:08,959
and that that is the reason why we've

00:14:06,560 --> 00:14:10,720
used terraform and within few minutes we

00:14:08,959 --> 00:14:11,279
have an eks cluster up and running and

00:14:10,720 --> 00:14:12,720
then

00:14:11,279 --> 00:14:15,040
we can proceed with the application

00:14:12,720 --> 00:14:16,160
deployment so terraform in that way

00:14:15,040 --> 00:14:18,800
helped us a lot and

00:14:16,160 --> 00:14:20,480
and we personally recommend terraform

00:14:18,800 --> 00:14:23,839
over any other

00:14:20,480 --> 00:14:27,839
uh infrastructure provision too and

00:14:23,839 --> 00:14:30,079
for once the terraform dipped oil

00:14:27,839 --> 00:14:30,959
once terraform deploys the

00:14:30,079 --> 00:14:34,079
infrastructure

00:14:30,959 --> 00:14:36,160
on the ek on the aws cloud

00:14:34,079 --> 00:14:37,440
that's when we use helm charts for the

00:14:36,160 --> 00:14:39,920
matic application

00:14:37,440 --> 00:14:40,480
so we also developed a helm chart

00:14:39,920 --> 00:14:42,240
in-house

00:14:40,480 --> 00:14:43,839
for deploying the martic application

00:14:42,240 --> 00:14:45,120
which has the actual application

00:14:43,839 --> 00:14:49,360
configuration

00:14:45,120 --> 00:14:51,600
so we've um this particular helm chart

00:14:49,360 --> 00:14:54,240
has been made in such a way that it is

00:14:51,600 --> 00:14:57,519
completely parameterized and portable

00:14:54,240 --> 00:14:58,160
so whenever you want to deploy an

00:14:57,519 --> 00:14:59,680
application

00:14:58,160 --> 00:15:01,519
or for the matter of fact whenever you

00:14:59,680 --> 00:15:05,040
want to deploy multi application

00:15:01,519 --> 00:15:08,639
you just have to go and uh change

00:15:05,040 --> 00:15:10,320
the values.yaml file so this values.ml

00:15:08,639 --> 00:15:11,199
file is something like a configurable

00:15:10,320 --> 00:15:13,600
file which

00:15:11,199 --> 00:15:14,560
actually has the values of all the

00:15:13,600 --> 00:15:17,120
variables

00:15:14,560 --> 00:15:19,920
so once you deploy the uh once you

00:15:17,120 --> 00:15:22,959
change the values in the values.yaml

00:15:19,920 --> 00:15:23,600
you can go ahead and deploy the uh helm

00:15:22,959 --> 00:15:26,480
chart

00:15:23,600 --> 00:15:27,440
so that's what we used for deploying a

00:15:26,480 --> 00:15:31,519
martic application

00:15:27,440 --> 00:15:34,959
on our eks cluster and then

00:15:31,519 --> 00:15:38,160
uh the reason

00:15:34,959 --> 00:15:41,839
uh the reason why we use

00:15:38,160 --> 00:15:43,759
aws eks is that because

00:15:41,839 --> 00:15:45,920
many there are many reasons why we chose

00:15:43,759 --> 00:15:49,120
eks but some of the reasons that

00:15:45,920 --> 00:15:50,639
we chose over others are we have when it

00:15:49,120 --> 00:15:53,360
comes to the costing as you know

00:15:50,639 --> 00:15:54,800
hosting involves costing and every

00:15:53,360 --> 00:15:58,000
customer wants

00:15:54,800 --> 00:16:00,000
to save money on hosting so

00:15:58,000 --> 00:16:02,320
when it comes to eks what happens is

00:16:00,000 --> 00:16:03,360
that point ten dollars per hour is

00:16:02,320 --> 00:16:07,120
charged for

00:16:03,360 --> 00:16:09,199
the clusters master known so

00:16:07,120 --> 00:16:10,880
i mean if you ask me it's very cheap and

00:16:09,199 --> 00:16:12,560
it's like affordable for

00:16:10,880 --> 00:16:14,880
running huge workloads or if you're

00:16:12,560 --> 00:16:17,440
running something on a temporary basis

00:16:14,880 --> 00:16:18,079
and as i said before you can actually

00:16:17,440 --> 00:16:20,560
use

00:16:18,079 --> 00:16:21,519
only a single eks cluster and deploy

00:16:20,560 --> 00:16:23,759
applications

00:16:21,519 --> 00:16:25,680
in the form of indifferent by using

00:16:23,759 --> 00:16:27,920
different name spaces

00:16:25,680 --> 00:16:29,199
or you can even deploy multiple eks

00:16:27,920 --> 00:16:32,399
clusters

00:16:29,199 --> 00:16:34,720
so that is one of the way where

00:16:32,399 --> 00:16:35,759
we actually took advantage of the awc

00:16:34,720 --> 00:16:39,360
environment

00:16:35,759 --> 00:16:42,399
apart from the and also we also have eks

00:16:39,360 --> 00:16:43,680
uh i mean along with eks we also have

00:16:42,399 --> 00:16:46,240
aws fargate

00:16:43,680 --> 00:16:46,880
and also on chromis uh which is called

00:16:46,240 --> 00:16:49,279
aws

00:16:46,880 --> 00:16:50,880
outputs which has which has been a very

00:16:49,279 --> 00:16:54,160
good solution the recent times from

00:16:50,880 --> 00:16:56,320
aws and another cost

00:16:54,160 --> 00:16:57,680
uh effective solution is that if you are

00:16:56,320 --> 00:17:01,040
using ec2

00:16:57,680 --> 00:17:02,399
so we have master and worker nodes in

00:17:01,040 --> 00:17:05,439
the kubernetes cluster

00:17:02,399 --> 00:17:06,959
so if you you're using ec2 you only pay

00:17:05,439 --> 00:17:09,600
for what you use

00:17:06,959 --> 00:17:10,319
so which is very cost effective because

00:17:09,600 --> 00:17:12,720
you're not

00:17:10,319 --> 00:17:14,319
being charged for uh on the whole of a

00:17:12,720 --> 00:17:16,079
server but just the utilization of

00:17:14,319 --> 00:17:18,640
whatever you're using

00:17:16,079 --> 00:17:20,400
and another important factor for any

00:17:18,640 --> 00:17:21,360
modern cloud native application is that

00:17:20,400 --> 00:17:23,919
auto scaling

00:17:21,360 --> 00:17:25,360
auto scaling is very important and in

00:17:23,919 --> 00:17:26,799
this particular case

00:17:25,360 --> 00:17:29,360
it is very much needed because the

00:17:26,799 --> 00:17:32,080
customer was having uh

00:17:29,360 --> 00:17:33,760
heavy workloads running and they wanted

00:17:32,080 --> 00:17:36,640
to make sure that

00:17:33,760 --> 00:17:38,559
the application is stable and working

00:17:36,640 --> 00:17:41,679
fine even if the workload is high

00:17:38,559 --> 00:17:44,080
so auto scaling comes in handy whenever

00:17:41,679 --> 00:17:44,960
such a requirement comes and it takes a

00:17:44,080 --> 00:17:47,919
load

00:17:44,960 --> 00:17:49,840
during the uh peak marketing campaigns

00:17:47,919 --> 00:17:50,240
so these are some of the reasons why we

00:17:49,840 --> 00:17:53,760
chose

00:17:50,240 --> 00:17:57,679
uh amazon aws eks solution for

00:17:53,760 --> 00:18:00,720
hosting the kubernetes workloads

00:17:57,679 --> 00:18:03,120
so i think uh

00:18:00,720 --> 00:18:03,919
i'm i'm hoping that the session was

00:18:03,120 --> 00:18:07,520
helpful

00:18:03,919 --> 00:18:13,360
and i think we can now start

00:18:07,520 --> 00:18:13,360

YouTube URL: https://www.youtube.com/watch?v=5NKpJD5uyRc


