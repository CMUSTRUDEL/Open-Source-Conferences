Title: DrupalCon New Orleans 2016: How Puppet Labs runs Drupal on AWS
Publication date: 2016-05-10
Playlist: DrupalCon New Orleans - May 2016
Description: 
	At Puppet Labs, we're running our own Drupal web infrastructure on AWS. This talk will describe the architectural decisions we've made, our operational experience, and specific implementation details. This will be an advanced talk for developers with modest operational experience, or a beginner-to-intermediate-level talk for people with strong web operations experience.

Specifically, we'll walk through the puppetlabs.com infrastructure and talk about:

why we're hosting our own, rather than using one of the many excellent Drupal hosting services
what AWS services we're using (RDS MySQL, EFS for NFS) and why we decided to run our own haproxy failover cluster rather than using Elastic Load Balancers
why we got rid of varnish and memcached
how we use haproxy to mitigate high-volume account registration spam attempts
how our puppet-based workflow has allowed web developers to drive changes to production, without blocking (much) on operations
how puppet-based provisioning of AWS resources allows us to treat infrastructure as disposable, ephemeral nodes
how we trace problems thorughout the stack by injecting UUID headers at the load balancer
where our solution falls short compared to hosted options we've evaluated
The objective of this talk is to share our experiences and thought processes. The particular decisions we made may not be appropriate for everybody, but the questions that drove those decisions will apply to nearly everybody who's deciding whether to host their own infrastructure on AWS or outsource it to a hosted service.
Captions: 
	00:00:00,000 --> 00:00:08,550
alright so I'm Daniel I'm Daniel dryer I

00:00:05,040 --> 00:00:10,860
work at puppet i'm a senior sis ops

00:00:08,550 --> 00:00:12,860
engineer there I'm the primary service

00:00:10,860 --> 00:00:19,890
owner for our Drupal web infrastructure

00:00:12,860 --> 00:00:22,260
can't quite hear all right doesn't quite

00:00:19,890 --> 00:00:25,699
fit my laptop is kind of up on the mic

00:00:22,260 --> 00:00:30,750
here so I'll try and lean in how's that

00:00:25,699 --> 00:00:33,329
cool thank you alright so I'm a senior

00:00:30,750 --> 00:00:35,070
c/s ops engineer at puppet I work on our

00:00:33,329 --> 00:00:40,100
web infrastructure which pretty much

00:00:35,070 --> 00:00:42,989
means the Drupal website so puppet com

00:00:40,100 --> 00:00:46,140
the puppet cough sites including the

00:00:42,989 --> 00:00:49,020
older ones and I also run infrastructure

00:00:46,140 --> 00:00:51,239
for puppet Forge which is a ruby thing

00:00:49,020 --> 00:00:53,660
but is a pretty similar architecture in

00:00:51,239 --> 00:00:58,379
terms of the infrastructure backing it I

00:00:53,660 --> 00:01:01,859
moved all of that stuff from mostly from

00:00:58,379 --> 00:01:03,629
line owed over to AWS and I'm currently

00:01:01,859 --> 00:01:08,310
in the process of going through that

00:01:03,629 --> 00:01:11,189
again for Forge so that's pretty much

00:01:08,310 --> 00:01:13,799
the basis for this talk before i was at

00:01:11,189 --> 00:01:16,259
puppet I was doing two things I ran a

00:01:13,799 --> 00:01:20,400
small web hosting business that was

00:01:16,259 --> 00:01:24,900
focused on Drupal and I did consulting

00:01:20,400 --> 00:01:27,180
mostly for small medium-sized drupal web

00:01:24,900 --> 00:01:29,520
dev outfits that had done some kind of

00:01:27,180 --> 00:01:36,960
their own hosting and had outgrown it or

00:01:29,520 --> 00:01:39,900
it became too painful and so I'm what

00:01:36,960 --> 00:01:43,799
I'm hoping to share here is about a

00:01:39,900 --> 00:01:45,479
dozen ish key decisions that when you're

00:01:43,799 --> 00:01:48,960
building this kind of a web

00:01:45,479 --> 00:01:51,270
infrastructure in AWS that kind of

00:01:48,960 --> 00:01:55,290
dictate the rest of the architecture

00:01:51,270 --> 00:01:57,600
that you end up with because what my

00:01:55,290 --> 00:02:00,780
experience has been seeing other people

00:01:57,600 --> 00:02:02,930
go through this is that people make some

00:02:00,780 --> 00:02:06,149
really important architectural decisions

00:02:02,930 --> 00:02:10,140
early on without realizing that they're

00:02:06,149 --> 00:02:12,540
important and so they'll make decisions

00:02:10,140 --> 00:02:13,770
about their dns their load balancing

00:02:12,540 --> 00:02:15,990
their web servers

00:02:13,770 --> 00:02:18,330
based on what they've worked with before

00:02:15,990 --> 00:02:20,100
what their colleagues or friends have

00:02:18,330 --> 00:02:23,940
worked with what they read about in a

00:02:20,100 --> 00:02:27,180
blog post and the symptom down the road

00:02:23,940 --> 00:02:29,250
is that two months in there spending all

00:02:27,180 --> 00:02:31,110
day working on varnish or they're

00:02:29,250 --> 00:02:34,320
spending a ton of money on giant easy

00:02:31,110 --> 00:02:36,810
two instances and they don't necessarily

00:02:34,320 --> 00:02:40,440
realize that that is actually

00:02:36,810 --> 00:02:43,590
subsidizing architectural decisions that

00:02:40,440 --> 00:02:45,510
aren't necessarily objectively wrong but

00:02:43,590 --> 00:02:51,540
are the wrong set of trade-offs for

00:02:45,510 --> 00:02:55,200
their organization and so coming into

00:02:51,540 --> 00:02:57,590
the whole AWS thing my experience was it

00:02:55,200 --> 00:03:00,300
was this sort of amorphous blob of

00:02:57,590 --> 00:03:03,450
technologies and questions and phrases

00:03:00,300 --> 00:03:05,280
and it wasn't clear to me how to put

00:03:03,450 --> 00:03:07,740
that all together and how to make those

00:03:05,280 --> 00:03:11,820
decisions and so what I'm hoping you all

00:03:07,740 --> 00:03:14,130
come away with is a more linear way of

00:03:11,820 --> 00:03:16,620
thinking about what the key decisions

00:03:14,130 --> 00:03:21,240
are at least in my view so that you can

00:03:16,620 --> 00:03:27,450
go through that process in a more

00:03:21,240 --> 00:03:29,850
deliberate way and put the the downsides

00:03:27,450 --> 00:03:32,970
in places that are acceptable for your

00:03:29,850 --> 00:03:36,180
situation and more consciously pick the

00:03:32,970 --> 00:03:39,750
the upsides that you can get out of AWS

00:03:36,180 --> 00:03:42,420
but you don't automatically get so

00:03:39,750 --> 00:03:45,840
before I get too far into that just to

00:03:42,420 --> 00:03:48,660
get a sense of what experience levels

00:03:45,840 --> 00:03:51,750
there are how many of you actually run

00:03:48,660 --> 00:03:54,390
or are involved in running some kind of

00:03:51,750 --> 00:04:00,600
server infrastructure web infrastructure

00:03:54,390 --> 00:04:03,930
at all ok cool just about everybody how

00:04:00,600 --> 00:04:08,070
many of those run Drupal that's what I

00:04:03,930 --> 00:04:12,110
expected pretty much everyone and how

00:04:08,070 --> 00:04:16,919
many of you are doing that in AWS oh

00:04:12,110 --> 00:04:19,530
cool lots of you like half how about for

00:04:16,919 --> 00:04:23,790
configuration management is that a thing

00:04:19,530 --> 00:04:26,820
that we're all doing or what much much

00:04:23,790 --> 00:04:27,600
less so ok cool so that that hopefully

00:04:26,820 --> 00:04:29,970
means that this

00:04:27,600 --> 00:04:36,120
isn't something that you just already

00:04:29,970 --> 00:04:39,600
know so the first question that i always

00:04:36,120 --> 00:04:41,970
get talking with people is why aren't

00:04:39,600 --> 00:04:45,120
you just hosting your your site it

00:04:41,970 --> 00:04:47,010
doesn't get that much traffic and the

00:04:45,120 --> 00:04:53,240
fact is that most of the time it's

00:04:47,010 --> 00:04:53,240
better to use some hosted service so

00:04:53,600 --> 00:04:59,820
sort of the factor the key fat one of

00:04:57,450 --> 00:05:02,940
the key factors for us at puppet that

00:04:59,820 --> 00:05:05,090
probably won't apply to any of you

00:05:02,940 --> 00:05:08,820
except for people from maybe chef is

00:05:05,090 --> 00:05:11,430
that in operations at puppet we have a

00:05:08,820 --> 00:05:14,070
bias toward running as much of our

00:05:11,430 --> 00:05:17,100
infrastructure as we can in house in

00:05:14,070 --> 00:05:18,750
order to have a larger infrastructure so

00:05:17,100 --> 00:05:22,290
that we can dog food puppet better

00:05:18,750 --> 00:05:24,360
because if we went and outsource

00:05:22,290 --> 00:05:27,030
everything we possibly could we'd have

00:05:24,360 --> 00:05:31,140
like 20 or 30 servers and that would not

00:05:27,030 --> 00:05:34,950
be at all a representative user of

00:05:31,140 --> 00:05:38,910
puppet enterprise where the the really

00:05:34,950 --> 00:05:40,910
big PE clients are large corporations

00:05:38,910 --> 00:05:43,140
that try to run everything in house and

00:05:40,910 --> 00:05:45,270
they'll also tend to run their web

00:05:43,140 --> 00:05:48,120
infrastructure in house for at least

00:05:45,270 --> 00:05:51,000
some of it and so that's that's one

00:05:48,120 --> 00:05:53,160
factor for us the other things that are

00:05:51,000 --> 00:05:55,980
probably more in common with your

00:05:53,160 --> 00:05:59,040
concerns are more that we want to

00:05:55,980 --> 00:06:01,470
integrate with our other infrastructure

00:05:59,040 --> 00:06:03,900
and we want to use our own workflows

00:06:01,470 --> 00:06:06,150
rather than using the vendor workflows

00:06:03,900 --> 00:06:08,340
that you get from a lot of even the

00:06:06,150 --> 00:06:10,200
really great hosting providers so

00:06:08,340 --> 00:06:13,110
specifically we want to be sending

00:06:10,200 --> 00:06:17,100
graphite metrics in web server logs into

00:06:13,110 --> 00:06:19,740
our log stash and we want to be

00:06:17,100 --> 00:06:23,220
provisioning access either using puppet

00:06:19,740 --> 00:06:25,620
or by integrating with ldap and it's

00:06:23,220 --> 00:06:27,630
been surprisingly difficult I've made

00:06:25,620 --> 00:06:29,550
quite an effort to find that and it's

00:06:27,630 --> 00:06:32,250
it's harder than you would think because

00:06:29,550 --> 00:06:35,010
once you start talking ldap you end up

00:06:32,250 --> 00:06:38,700
in somebody's 5k a month Enterprise here

00:06:35,010 --> 00:06:41,009
and that you know I can't justify that

00:06:38,700 --> 00:06:43,219
for like the 2015

00:06:41,009 --> 00:06:47,430
conf asite that no one visits anymore

00:06:43,219 --> 00:06:49,259
that's it doesn't doesn't make sense and

00:06:47,430 --> 00:06:51,180
the last thing and I hope that there's

00:06:49,259 --> 00:06:57,300
some people from these hosting companies

00:06:51,180 --> 00:07:00,539
here is the question of SL leis so you

00:06:57,300 --> 00:07:02,939
look around you see SLA s that are you

00:07:00,539 --> 00:07:05,759
know three nines four nines a hundred

00:07:02,939 --> 00:07:09,349
percent but the commitments backing that

00:07:05,759 --> 00:07:11,699
are typically relatively weak and

00:07:09,349 --> 00:07:15,749
they're typically aren't published

00:07:11,699 --> 00:07:18,120
actual uptime numbers and so if somebody

00:07:15,749 --> 00:07:20,699
from marketing is asking me if I can

00:07:18,120 --> 00:07:23,729
stand behind a thing and I have no real

00:07:20,699 --> 00:07:25,620
hard evidence that they're actually up

00:07:23,729 --> 00:07:28,020
that much even though these companies

00:07:25,620 --> 00:07:30,539
have a great reputation and I have had

00:07:28,020 --> 00:07:33,059
personally really great experiences with

00:07:30,539 --> 00:07:35,550
them if I can't quantitatively support

00:07:33,059 --> 00:07:38,099
it like I can my own servers that I can

00:07:35,550 --> 00:07:39,779
monitor the heck out of it's harder for

00:07:38,099 --> 00:07:45,389
me to tell other people that they should

00:07:39,779 --> 00:07:47,789
trust it so architecture wise I'm going

00:07:45,389 --> 00:07:50,969
to be running through basically each of

00:07:47,789 --> 00:07:53,149
the pieces of infrastructure that you

00:07:50,969 --> 00:07:55,860
would see when you make an HTTP request

00:07:53,149 --> 00:07:57,930
so it's going to be almost like a series

00:07:55,860 --> 00:07:59,939
of lightning talks about DNS load

00:07:57,930 --> 00:08:02,930
balancer etc and I'm going to have to

00:07:59,939 --> 00:08:05,639
move through it relatively quickly

00:08:02,930 --> 00:08:07,919
because we only have about 50 minutes

00:08:05,639 --> 00:08:10,110
left before questions and there's

00:08:07,919 --> 00:08:11,999
there's just more infrastructure there

00:08:10,110 --> 00:08:14,999
then you can reasonably talk about in an

00:08:11,999 --> 00:08:17,099
hour so there's detail in some of these

00:08:14,999 --> 00:08:20,219
slides that I'm just going to skip if

00:08:17,099 --> 00:08:23,459
you're interested in it jump back to it

00:08:20,219 --> 00:08:25,499
later pull me aside afterward the idea

00:08:23,459 --> 00:08:29,069
is to present a relatively high level

00:08:25,499 --> 00:08:31,559
overview you all are competent enough to

00:08:29,069 --> 00:08:33,839
do the googling of the specific things

00:08:31,559 --> 00:08:36,599
and there's no shortage of tutorials

00:08:33,839 --> 00:08:38,719
about you know how to puppet eyes or

00:08:36,599 --> 00:08:43,370
chef eyes or whatever you call that

00:08:38,719 --> 00:08:43,370
engine X and H a proxy and so on

00:08:44,099 --> 00:08:51,480
so the first point of contact that your

00:08:48,319 --> 00:08:55,170
user has with you the way they actually

00:08:51,480 --> 00:08:59,339
find your server at all is DNS and a lot

00:08:55,170 --> 00:09:03,529
of people go into DNS when they register

00:08:59,339 --> 00:09:06,750
their domain they just use something and

00:09:03,529 --> 00:09:09,060
they don't realize that that is limiting

00:09:06,750 --> 00:09:12,420
them in certain ways so specifically

00:09:09,060 --> 00:09:14,940
with AWS the key decision here is am I

00:09:12,420 --> 00:09:19,139
going to use route 53 which is Amazon's

00:09:14,940 --> 00:09:22,250
hosted DNS it's really great or are I'm

00:09:19,139 --> 00:09:25,500
am I going to use some third-party DNS

00:09:22,250 --> 00:09:27,720
we're using a third-party we're using

00:09:25,500 --> 00:09:30,269
dine because we were standardized on

00:09:27,720 --> 00:09:34,860
them before we started using AWS and

00:09:30,269 --> 00:09:40,470
they've been totally solid if that

00:09:34,860 --> 00:09:44,040
wasn't the case route 53 would be a

00:09:40,470 --> 00:09:47,910
really compelling option the

00:09:44,040 --> 00:09:54,449
complication around route 53 is ipv6 and

00:09:47,910 --> 00:09:57,689
is elastic load balancers and naked

00:09:54,449 --> 00:10:01,819
domains so if you want your domain to

00:09:57,689 --> 00:10:06,180
the example.com instead of dub dub dub

00:10:01,819 --> 00:10:08,910
example.com the dns spec says that that

00:10:06,180 --> 00:10:12,389
has to be an a record but an elastic

00:10:08,910 --> 00:10:15,810
load balancer only gives you a cname so

00:10:12,389 --> 00:10:18,209
if you're not using route 53 amazon's

00:10:15,810 --> 00:10:20,399
route 53 because they just cheat and let

00:10:18,209 --> 00:10:23,339
you have a cname anyway they just have

00:10:20,399 --> 00:10:25,589
an integration you have to figure that

00:10:23,339 --> 00:10:28,769
out and you it probably means if you're

00:10:25,589 --> 00:10:31,350
using not route 53 that you can't use an

00:10:28,769 --> 00:10:33,319
elastic load balancer for a bear domain

00:10:31,350 --> 00:10:40,740
that catches a lot of people off guard

00:10:33,319 --> 00:10:42,990
the other question is ipv6 so I hope

00:10:40,740 --> 00:10:44,790
someone from amazon maybe listens to

00:10:42,990 --> 00:10:47,689
this sometime and improves the ipv6

00:10:44,790 --> 00:10:47,689
situation

00:10:48,540 --> 00:10:57,160
so it's it's you can't win basically

00:10:52,750 --> 00:11:01,600
because route 53 will serve DNA ipv6

00:10:57,160 --> 00:11:06,880
quad a records but route 53 s DNS

00:11:01,600 --> 00:11:12,390
servers are not available over ipv6 so

00:11:06,880 --> 00:11:12,390
you have to make the request over ipv4

00:11:12,450 --> 00:11:18,310
so that's kind of a non-starter if

00:11:16,540 --> 00:11:21,700
you're trying to help people who have an

00:11:18,310 --> 00:11:24,430
ipv6 only stack which I didn't think

00:11:21,700 --> 00:11:27,610
there really were any but then we had an

00:11:24,430 --> 00:11:30,520
ipv6 only outage of young puppet labs

00:11:27,610 --> 00:11:32,740
com and apt puppet labs com and we heard

00:11:30,520 --> 00:11:38,230
from people on the same day so it turns

00:11:32,740 --> 00:11:44,260
out those people do exist I know right

00:11:38,230 --> 00:11:47,470
and so unfortunately elastic load

00:11:44,260 --> 00:11:53,020
balancers are also the only way to get a

00:11:47,470 --> 00:11:56,230
public ipv6 address in Amazon so if you

00:11:53,020 --> 00:11:58,150
want to be using ipv6 you have to be on

00:11:56,230 --> 00:12:04,510
rel 53 which means that you're not

00:11:58,150 --> 00:12:07,540
available on ipv6 only so we're using

00:12:04,510 --> 00:12:11,380
dine they are a little bit more

00:12:07,540 --> 00:12:13,600
expensive than route 53 they're really

00:12:11,380 --> 00:12:15,100
really well established in the almost

00:12:13,600 --> 00:12:17,500
two years I've been at puppet we have

00:12:15,100 --> 00:12:19,710
not had any problems whatsoever with

00:12:17,500 --> 00:12:22,450
them so we've just stayed with them

00:12:19,710 --> 00:12:24,520
obviously we can't use elastic load

00:12:22,450 --> 00:12:26,260
balancers but there's other reasons we

00:12:24,520 --> 00:12:28,210
couldn't use those anyway that i'll get

00:12:26,260 --> 00:12:32,920
to momentarily so that hasn't been an

00:12:28,210 --> 00:12:35,050
issue the next hop there that your

00:12:32,920 --> 00:12:37,600
packet will make is to whatever load

00:12:35,050 --> 00:12:40,960
balancer you're running so I've

00:12:37,600 --> 00:12:45,040
highlighted for options here the sneak

00:12:40,960 --> 00:12:48,100
preview is we're on h a proxy and i'll

00:12:45,040 --> 00:12:50,440
go over each one fairly quickly so if

00:12:48,100 --> 00:12:52,750
you can use an elastic load balancer

00:12:50,440 --> 00:12:55,209
because it's by far the lowest

00:12:52,750 --> 00:12:57,100
management overhead and if you're

00:12:55,209 --> 00:13:00,120
thinking it really wouldn't be that much

00:12:57,100 --> 00:13:02,480
work to just stand up in h a proxy

00:13:00,120 --> 00:13:06,029
it isn't but it's keeping it running and

00:13:02,480 --> 00:13:08,910
being on call for it and figuring out

00:13:06,029 --> 00:13:10,920
the edge cases of keeping it highly

00:13:08,910 --> 00:13:14,190
available across multiple availability

00:13:10,920 --> 00:13:17,640
zones that turns out to be a lot more

00:13:14,190 --> 00:13:20,640
work than I had expected the

00:13:17,640 --> 00:13:23,610
disadvantage there beyond the ipv6 stuff

00:13:20,640 --> 00:13:25,440
I described is it's not quite as

00:13:23,610 --> 00:13:26,760
full-featured as some of the other load

00:13:25,440 --> 00:13:28,130
balancers if you want to do more

00:13:26,760 --> 00:13:30,420
advanced stuff at the load balancer

00:13:28,130 --> 00:13:32,040
that's not a huge disadvantage because

00:13:30,420 --> 00:13:34,830
you can typically do that once you get

00:13:32,040 --> 00:13:37,140
to the web server anyway the big killer

00:13:34,830 --> 00:13:39,750
for us was the lack of a static IP

00:13:37,140 --> 00:13:41,880
address so we have enterprise customers

00:13:39,750 --> 00:13:45,060
whose firewall you know IT security

00:13:41,880 --> 00:13:47,610
policies require them to whitelist every

00:13:45,060 --> 00:13:50,130
IP for every website there people can

00:13:47,610 --> 00:13:52,560
visit and we want those people to be

00:13:50,130 --> 00:13:55,770
able their users to be able to visit our

00:13:52,560 --> 00:13:58,710
websites an elastic load balancer you

00:13:55,770 --> 00:14:01,800
just get a cname the IP changes so given

00:13:58,710 --> 00:14:05,570
that hard requirement of a static IP we

00:14:01,800 --> 00:14:08,490
just ruled that option out entirely

00:14:05,570 --> 00:14:09,900
engine X is another option when i

00:14:08,490 --> 00:14:13,140
started at puppet that's what we were

00:14:09,900 --> 00:14:14,880
using it's already familiar to a lot of

00:14:13,140 --> 00:14:16,980
you it's really nice to be able to have

00:14:14,880 --> 00:14:18,360
the same thing for your load balancer as

00:14:16,980 --> 00:14:21,660
your web server because you don't have

00:14:18,360 --> 00:14:24,420
to learn as many things the the thing

00:14:21,660 --> 00:14:26,279
that is a real killer for me and the

00:14:24,420 --> 00:14:29,940
reason that we switched away from it is

00:14:26,279 --> 00:14:33,150
that doing health checks of your back

00:14:29,940 --> 00:14:35,910
end web servers in engine X requires the

00:14:33,150 --> 00:14:38,520
commercial engine X plus which the

00:14:35,910 --> 00:14:42,570
sticker price on that is like 1,900 a

00:14:38,520 --> 00:14:46,170
year per node and so given that H a

00:14:42,570 --> 00:14:48,420
proxy will do that for free it's

00:14:46,170 --> 00:14:51,390
difficult for me to justify even though

00:14:48,420 --> 00:14:53,610
engine X is a really cool company if if

00:14:51,390 --> 00:14:56,339
you had a need for commercial support

00:14:53,610 --> 00:14:58,350
and you're already running engine X as a

00:14:56,339 --> 00:15:00,209
web server and you need commercial

00:14:58,350 --> 00:15:03,209
support for it this would be a really

00:15:00,209 --> 00:15:06,720
good option just for us we don't need

00:15:03,209 --> 00:15:08,130
that so it wasn't a good option the last

00:15:06,720 --> 00:15:11,280
thing I want to plug here is if you're

00:15:08,130 --> 00:15:13,790
coming from a big enterprise shop you

00:15:11,280 --> 00:15:16,100
may already be running f5 there

00:15:13,790 --> 00:15:18,620
sort of the go to big enterprise load

00:15:16,100 --> 00:15:20,960
balancer they have virtual appliances

00:15:18,620 --> 00:15:22,760
available on AWS so you can have the

00:15:20,960 --> 00:15:25,190
same load balance or workflow that you

00:15:22,760 --> 00:15:27,380
would with your hardware stuff they're

00:15:25,190 --> 00:15:31,190
really expensive so if you don't have

00:15:27,380 --> 00:15:33,650
that specific need you it's probably not

00:15:31,190 --> 00:15:36,080
a great fit so we ended up with a che

00:15:33,650 --> 00:15:37,910
proxy it's open source it's free it's

00:15:36,080 --> 00:15:43,940
super lightweight we can run it on an

00:15:37,910 --> 00:15:47,710
Amazon on T to medium instance I really

00:15:43,940 --> 00:15:50,390
love how they do SSL termination so with

00:15:47,710 --> 00:15:53,270
every time i've set up ssl anywhere i

00:15:50,390 --> 00:15:55,880
end up googling because you've got your

00:15:53,270 --> 00:15:58,010
how do you point it to the what's the

00:15:55,880 --> 00:16:01,390
cert and the intermediate cert and the

00:15:58,010 --> 00:16:04,190
key it always seemed to flub something

00:16:01,390 --> 00:16:06,380
with h a proxy you point it to a folder

00:16:04,190 --> 00:16:08,900
and you dump all your certs in that

00:16:06,380 --> 00:16:11,660
folder and then it loads all of them

00:16:08,900 --> 00:16:13,760
when it starts and it just checks to see

00:16:11,660 --> 00:16:16,250
what domains they can handle and when a

00:16:13,760 --> 00:16:17,900
request comes into that domain it just

00:16:16,250 --> 00:16:21,110
maps them and it just handles it and

00:16:17,900 --> 00:16:22,820
there's no configuration so I really

00:16:21,110 --> 00:16:24,200
like that because there's a whole class

00:16:22,820 --> 00:16:26,090
of things that you can't screw up

00:16:24,200 --> 00:16:31,370
anymore and I don't have to Google when

00:16:26,090 --> 00:16:33,410
i add a new ssl cert so the big downside

00:16:31,370 --> 00:16:35,960
and this would apply it to or the big

00:16:33,410 --> 00:16:38,030
operational overhead to running your own

00:16:35,960 --> 00:16:42,260
load balancers this would apply to h a

00:16:38,030 --> 00:16:45,020
proxy and engine X is that if you're an

00:16:42,260 --> 00:16:48,560
Amazon any services you're running

00:16:45,020 --> 00:16:53,480
really need to be in three availability

00:16:48,560 --> 00:16:57,260
zones or at least two or else you don't

00:16:53,480 --> 00:16:59,660
so in Amazon there s la's are on a per

00:16:57,260 --> 00:17:02,360
availability zone basis there isn't an

00:16:59,660 --> 00:17:05,560
SLA for just one ec2 instance what

00:17:02,360 --> 00:17:08,060
they're committing to is that the whole

00:17:05,560 --> 00:17:10,940
availability zone will basically be up

00:17:08,060 --> 00:17:13,220
so much of the time so you need to have

00:17:10,940 --> 00:17:16,520
your stuff especially key things like

00:17:13,220 --> 00:17:20,480
load balancers high availability across

00:17:16,520 --> 00:17:22,820
them an elastic load balancer just does

00:17:20,480 --> 00:17:24,620
that for you with h a proxy or engine X

00:17:22,820 --> 00:17:27,080
you're doing that yourself and the way

00:17:24,620 --> 00:17:27,800
we do that ourselves is using keep alive

00:17:27,080 --> 00:17:31,250
d

00:17:27,800 --> 00:17:37,960
which uses vrr p which is a protocol

00:17:31,250 --> 00:17:42,020
like TC tcp or UDP and you get three

00:17:37,960 --> 00:17:45,770
servers H a proxy servers that all have

00:17:42,020 --> 00:17:48,670
a keepalive d process on them they form

00:17:45,770 --> 00:17:53,000
a cluster they do a master election and

00:17:48,670 --> 00:17:55,550
the master is only qualified to become a

00:17:53,000 --> 00:17:58,160
master if the H a proxy process is

00:17:55,550 --> 00:17:59,630
running you can write other custom

00:17:58,160 --> 00:18:04,100
health checks but we haven't really

00:17:59,630 --> 00:18:06,760
found it necessary and when it I'll show

00:18:04,100 --> 00:18:09,530
you the script I use for this shortly

00:18:06,760 --> 00:18:12,350
when it when one of those nodes wins

00:18:09,530 --> 00:18:14,660
that election it runs a really trivial

00:18:12,350 --> 00:18:16,970
shell script that uses the Amazon

00:18:14,660 --> 00:18:20,420
command-line tool to take over the

00:18:16,970 --> 00:18:24,230
elastic IP and when one of these nodes

00:18:20,420 --> 00:18:26,540
disappears then the it fails the health

00:18:24,230 --> 00:18:29,690
checks with the other keep alive d nodes

00:18:26,540 --> 00:18:32,780
they do another master election and one

00:18:29,690 --> 00:18:35,810
of them wins and then it takes over the

00:18:32,780 --> 00:18:37,850
elastic IP and that process takes like a

00:18:35,810 --> 00:18:44,210
few seconds so you don't really notice

00:18:37,850 --> 00:18:48,380
an outage so I'm not going to go through

00:18:44,210 --> 00:18:51,040
the details of this can I mouse over

00:18:48,380 --> 00:18:53,780
that no it doesn't look like i can so

00:18:51,040 --> 00:18:56,930
the nutshell of this at the very top

00:18:53,780 --> 00:18:58,760
there it's just the health check script

00:18:56,930 --> 00:19:01,700
is just checking that H a proxy is

00:18:58,760 --> 00:19:03,860
running its listing some unicast peers

00:19:01,700 --> 00:19:06,820
by default keep alive d wants to

00:19:03,860 --> 00:19:09,620
broadcast but you can't multicast in

00:19:06,820 --> 00:19:12,110
easy to sew a lot of people think you

00:19:09,620 --> 00:19:16,520
can't use keep alive d there but you can

00:19:12,110 --> 00:19:18,260
use keep alive d in unicast mode you

00:19:16,520 --> 00:19:20,990
just have to tell it what its peers are

00:19:18,260 --> 00:19:23,330
we just use puppet to template this out

00:19:20,990 --> 00:19:26,180
because puppet DB already knows who

00:19:23,330 --> 00:19:29,660
those peers are so that's easy enough to

00:19:26,180 --> 00:19:31,790
set up the other thing to keep in mind

00:19:29,660 --> 00:19:35,090
there I forgot to put this in this slide

00:19:31,790 --> 00:19:38,060
is that you will need to enable the via

00:19:35,090 --> 00:19:41,130
or P protocol in your security groups or

00:19:38,060 --> 00:19:47,010
else it will silently drop them

00:19:41,130 --> 00:19:49,890
and you'll waste a day like I did here's

00:19:47,010 --> 00:19:53,550
the the shell script that it runs when

00:19:49,890 --> 00:19:56,190
it takes over so the line 10 is the one

00:19:53,550 --> 00:20:00,000
you care about and literally it's just

00:19:56,190 --> 00:20:01,980
running the same like shell command that

00:20:00,000 --> 00:20:04,320
you would if you were taking this over

00:20:01,980 --> 00:20:06,510
by hand it's it's really dumb we

00:20:04,320 --> 00:20:08,940
template this out as well using puppets

00:20:06,510 --> 00:20:11,730
so that we won't have to like create one

00:20:08,940 --> 00:20:15,810
of these for every elastic IP that we

00:20:11,730 --> 00:20:17,880
use there's another cool thing that we

00:20:15,810 --> 00:20:20,040
do with H a proxy that I wanted to share

00:20:17,880 --> 00:20:21,840
because i haven't seen other people

00:20:20,040 --> 00:20:25,020
doing this and i think it's a pattern

00:20:21,840 --> 00:20:26,490
that would be useful to copy so if you

00:20:25,020 --> 00:20:31,560
look at the very if you squint a little

00:20:26,490 --> 00:20:39,870
bit and the very oh geez oh man i'm

00:20:31,560 --> 00:20:43,200
sorry so if you imagine that that the

00:20:39,870 --> 00:20:46,500
very last line of this set had an X this

00:20:43,200 --> 00:20:48,360
is a this is an HTTP request and this we

00:20:46,500 --> 00:20:51,960
have the headers here that we're getting

00:20:48,360 --> 00:20:54,990
back and the very last line is a header

00:20:51,960 --> 00:20:59,670
called X unique ID and it has a long

00:20:54,990 --> 00:21:03,120
nonsense uuid we have h a proxy every

00:20:59,670 --> 00:21:06,690
time an HTTP request comes in it in it

00:21:03,120 --> 00:21:10,380
generates a uuid it injects the uuid

00:21:06,690 --> 00:21:12,210
into that custom header and then when

00:21:10,380 --> 00:21:16,740
the response to that comes back from the

00:21:12,210 --> 00:21:19,530
back end it injects the same uuid header

00:21:16,740 --> 00:21:23,240
into the response and so if you do a

00:21:19,530 --> 00:21:27,630
curl that shows headers you'll see this

00:21:23,240 --> 00:21:30,150
uuid so we have h a proxy and engine X

00:21:27,630 --> 00:21:33,630
and Drupal all configured with custom

00:21:30,150 --> 00:21:36,240
logging options so that they'll log that

00:21:33,630 --> 00:21:38,670
header and what's cool about that is

00:21:36,240 --> 00:21:41,250
your troubleshooting workflow then if

00:21:38,670 --> 00:21:43,400
you have a centralized logging tool that

00:21:41,250 --> 00:21:47,100
you can search all those logs at once

00:21:43,400 --> 00:21:48,780
your web devs without talking to ops if

00:21:47,100 --> 00:21:51,900
they have access to the logging tools

00:21:48,780 --> 00:21:54,470
they can say they're having trouble with

00:21:51,900 --> 00:21:56,210
some page they're not

00:21:54,470 --> 00:21:59,600
sure if it's happening at the load

00:21:56,210 --> 00:22:01,640
balancer the web server php-fpm in

00:21:59,600 --> 00:22:05,299
Drupal and they want to see all those

00:22:01,640 --> 00:22:08,150
logs instead of asking ops for help and

00:22:05,299 --> 00:22:11,780
gripping through logs and stuff they can

00:22:08,150 --> 00:22:15,919
search all of those all the logs for all

00:22:11,780 --> 00:22:18,980
those things for that uuid and in one

00:22:15,919 --> 00:22:22,640
place they get all the relevant logs for

00:22:18,980 --> 00:22:24,770
that specific request and they can the

00:22:22,640 --> 00:22:27,140
other cool thing you can do with that is

00:22:24,770 --> 00:22:31,760
if you want to be able to make your

00:22:27,140 --> 00:22:34,429
users able to file better error tickets

00:22:31,760 --> 00:22:39,140
if you have your front-end discreetly

00:22:34,429 --> 00:22:41,179
display that uuid somewhere you can tell

00:22:39,140 --> 00:22:44,570
them you could add that say to your

00:22:41,179 --> 00:22:46,400
custom like 500 page tell them to copy

00:22:44,570 --> 00:22:49,220
paste that or take a screenshot when

00:22:46,400 --> 00:22:51,500
they file an error and then you can go

00:22:49,220 --> 00:22:53,600
and look up the logs for that exact

00:22:51,500 --> 00:22:55,850
request and you don't have to go back

00:22:53,600 --> 00:23:01,400
and forth with them to figure out which

00:22:55,850 --> 00:23:04,970
request that was the last thing that we

00:23:01,400 --> 00:23:07,220
do in h a proxy that I'm proud of is it

00:23:04,970 --> 00:23:09,350
and it's a dumb hack how many of you

00:23:07,220 --> 00:23:13,669
have had that problem where a botnet

00:23:09,350 --> 00:23:18,919
shows up and registers like 10,000 users

00:23:13,669 --> 00:23:20,390
with your site right and the bet half of

00:23:18,919 --> 00:23:22,280
the rest of you who didn't raise your

00:23:20,390 --> 00:23:24,919
hand the reason you're not is because

00:23:22,280 --> 00:23:28,340
you have some kind of anti spam service

00:23:24,919 --> 00:23:31,880
that you're signed up for right so we do

00:23:28,340 --> 00:23:33,669
too we get these and it blocks a hundred

00:23:31,880 --> 00:23:36,860
percent of spam registrations

00:23:33,669 --> 00:23:39,260
consistently it's really great but you

00:23:36,860 --> 00:23:42,559
can't cash the register or the login

00:23:39,260 --> 00:23:45,860
endpoint and so if you get a real high

00:23:42,559 --> 00:23:48,770
volume registration effort by a large

00:23:45,860 --> 00:23:52,309
botnet it functions as like as a DDoS

00:23:48,770 --> 00:23:54,730
and I've seen at puppet and a couple of

00:23:52,309 --> 00:23:57,320
consulting clients I've worked with that

00:23:54,730 --> 00:23:59,780
their site will go down for like 20

00:23:57,320 --> 00:24:02,120
minutes every few months and they don't

00:23:59,780 --> 00:24:04,909
know why and the web servers get really

00:24:02,120 --> 00:24:08,330
really busy for a while and they don't

00:24:04,909 --> 00:24:11,360
get it the and typically the reason is

00:24:08,330 --> 00:24:13,070
is that there's some spam bot trying to

00:24:11,360 --> 00:24:15,260
register all these accounts and it's

00:24:13,070 --> 00:24:18,440
just burning so much CPU on all their

00:24:15,260 --> 00:24:22,429
web servers that it blocks the whole

00:24:18,440 --> 00:24:25,820
site so what we do in h a proxy is on

00:24:22,429 --> 00:24:29,539
the the front end we have this ACL up

00:24:25,820 --> 00:24:32,090
here that tags all the requests that

00:24:29,539 --> 00:24:35,149
start with user login or user register

00:24:32,090 --> 00:24:37,519
with this Drupal SEC ACL and then it

00:24:35,149 --> 00:24:40,870
sends everything with Drupal sec to a

00:24:37,519 --> 00:24:43,669
back-end called dub-dub-dub throttle and

00:24:40,870 --> 00:24:46,159
what dub dub dub throttle does is it's

00:24:43,669 --> 00:24:49,429
basically a separate q it hits the same

00:24:46,159 --> 00:24:52,909
web servers as you normally would but it

00:24:49,429 --> 00:24:56,539
has a real limited number of connections

00:24:52,909 --> 00:24:59,919
so each back end web server is only

00:24:56,539 --> 00:25:04,970
allowed to handle one of those at a time

00:24:59,919 --> 00:25:08,179
so if you get a billion of these spam

00:25:04,970 --> 00:25:10,789
registration attempts those queue at the

00:25:08,179 --> 00:25:14,450
load balancer in a separate cue from the

00:25:10,789 --> 00:25:18,080
rest of your website traffic and so if

00:25:14,450 --> 00:25:20,149
if that gets slow as heck the only

00:25:18,080 --> 00:25:22,340
people who are impacted are legitimate

00:25:20,149 --> 00:25:25,549
users trying to register or login and

00:25:22,340 --> 00:25:27,590
since 99 percent of our traffic is

00:25:25,549 --> 00:25:30,350
anonymous and the people who are logged

00:25:27,590 --> 00:25:33,769
in are typically marketing people or

00:25:30,350 --> 00:25:35,960
really dedicated users who are logged in

00:25:33,769 --> 00:25:39,200
all the time they don't notice because

00:25:35,960 --> 00:25:41,690
they're not hitting those endpoints so

00:25:39,200 --> 00:25:45,470
that has completely solved the problem

00:25:41,690 --> 00:25:47,840
for us and it's it's kind of a dumb hack

00:25:45,470 --> 00:25:50,630
but because it doesn't actually get rid

00:25:47,840 --> 00:25:53,600
of the traffic but it's sidestepped the

00:25:50,630 --> 00:25:55,190
question of figuring out which traffic

00:25:53,600 --> 00:26:00,350
was malicious because that's

00:25:55,190 --> 00:26:03,529
surprisingly hard to do so we've gotten

00:26:00,350 --> 00:26:06,980
through the load balancer somehow the

00:26:03,529 --> 00:26:09,409
request has gotten to something running

00:26:06,980 --> 00:26:12,679
engine X in PHP or Apache or whatever

00:26:09,409 --> 00:26:14,299
and this is typically the part in Drupal

00:26:12,679 --> 00:26:17,450
or other web infrastructure where you

00:26:14,299 --> 00:26:19,700
need the most compute and you have to

00:26:17,450 --> 00:26:22,070
run that in something when you're going

00:26:19,700 --> 00:26:27,140
into AWS pretty much the two

00:26:22,070 --> 00:26:30,950
these options are you're either spinning

00:26:27,140 --> 00:26:34,640
up ec2 instances which if you haven't

00:26:30,950 --> 00:26:36,110
used AWS are virtual machines very much

00:26:34,640 --> 00:26:38,990
like what you would get out of a line

00:26:36,110 --> 00:26:42,110
note or a digital ocean or you're using

00:26:38,990 --> 00:26:44,540
ecs elastic container service which is

00:26:42,110 --> 00:26:48,380
there doc or hosting as a service that

00:26:44,540 --> 00:26:50,270
they more manage for you and then the

00:26:48,380 --> 00:26:52,670
other key question when you're coming in

00:26:50,270 --> 00:26:57,760
is what kind of instance type you're

00:26:52,670 --> 00:27:00,290
looking for I'll get to that momentarily

00:26:57,760 --> 00:27:03,410
the the good thing about this is that

00:27:00,290 --> 00:27:08,120
the decision is really easy how many of

00:27:03,410 --> 00:27:09,740
you are using docker okay the people who

00:27:08,120 --> 00:27:13,400
raise their hand should probably look

00:27:09,740 --> 00:27:18,110
into ecs the rest of you should probably

00:27:13,400 --> 00:27:21,140
use ec2 because ecs is a perfectly good

00:27:18,110 --> 00:27:23,960
service but in my view it's not

00:27:21,140 --> 00:27:27,020
compelling enough to warrant a switch to

00:27:23,960 --> 00:27:28,970
docker just to have it especially if

00:27:27,020 --> 00:27:30,800
you're using decent configuration

00:27:28,970 --> 00:27:32,660
management and you have good automation

00:27:30,800 --> 00:27:37,070
there if you have a an in-house

00:27:32,660 --> 00:27:40,160
capability to use easy to or to use to

00:27:37,070 --> 00:27:43,640
run virtual machines the normal way just

00:27:40,160 --> 00:27:46,070
spin up ec2 instances and you can run

00:27:43,640 --> 00:27:49,540
those as ephemeral E as you would docker

00:27:46,070 --> 00:27:52,850
containers if that's important to you

00:27:49,540 --> 00:27:55,520
the next question so we're going to

00:27:52,850 --> 00:27:58,970
assume that you know since we're me and

00:27:55,520 --> 00:28:01,910
we decided to use ec2 the next questions

00:27:58,970 --> 00:28:03,950
at hand are how do you provision it and

00:28:01,910 --> 00:28:06,560
how big of instances what type of

00:28:03,950 --> 00:28:09,470
instances do you use and the general

00:28:06,560 --> 00:28:12,860
contours of those decisions are more

00:28:09,470 --> 00:28:16,010
smaller instances fewer bigger instances

00:28:12,860 --> 00:28:19,040
if you're running things more ad hoc by

00:28:16,010 --> 00:28:22,130
hand fewer bigger instances is obviously

00:28:19,040 --> 00:28:23,600
less work if you have good automation it

00:28:22,130 --> 00:28:25,670
doesn't really matter how many

00:28:23,600 --> 00:28:27,950
especially if you have goods centralized

00:28:25,670 --> 00:28:30,710
logging and monitoring and so on because

00:28:27,950 --> 00:28:33,410
hopefully then you're not SSH into we

00:28:30,710 --> 00:28:35,440
have like 12 tiny back end web servers I

00:28:33,410 --> 00:28:37,899
don't want to have to SS a

00:28:35,440 --> 00:28:43,419
to all of those to grep for a grep logs

00:28:37,899 --> 00:28:46,990
or whatever that's terrible so we use t

00:28:43,419 --> 00:28:48,879
to medium instances and what's cool

00:28:46,990 --> 00:28:53,529
about the t2 how many of you know about

00:28:48,879 --> 00:28:58,179
the t2 stuff okay so this is worth

00:28:53,529 --> 00:29:02,230
explaining so T two instances basically

00:28:58,179 --> 00:29:06,549
have a pool of CPU credits and when the

00:29:02,230 --> 00:29:11,110
machine is not very busy that pool is

00:29:06,549 --> 00:29:14,049
growing to a ceiling and when you use

00:29:11,110 --> 00:29:16,149
when you need to burst capacity it can

00:29:14,049 --> 00:29:19,990
go to about two and a half times it's

00:29:16,149 --> 00:29:23,830
sort of base line load but it draws down

00:29:19,990 --> 00:29:26,230
those CPU credits what's cool about that

00:29:23,830 --> 00:29:30,100
compared to using Auto scale groups to

00:29:26,230 --> 00:29:31,600
grow dynamically is that it's instant so

00:29:30,100 --> 00:29:33,340
with an auto scale group even if you

00:29:31,600 --> 00:29:35,350
have baked am I as you're waiting at

00:29:33,340 --> 00:29:40,120
least one two three minutes for a new

00:29:35,350 --> 00:29:43,509
machine to come up a t2 instance can go

00:29:40,120 --> 00:29:46,269
from totally idle to burst it all the

00:29:43,509 --> 00:29:50,470
way out as soon as fast as you can send

00:29:46,269 --> 00:29:53,049
requests to it the so I really like that

00:29:50,470 --> 00:29:56,950
because you get the burst ability

00:29:53,049 --> 00:29:58,720
without worrying about Auto scale policy

00:29:56,950 --> 00:30:01,259
or trying to predict what your load is

00:29:58,720 --> 00:30:06,100
going to be the only downside there is

00:30:01,259 --> 00:30:08,220
that you can run out of t2 CPU credits

00:30:06,100 --> 00:30:11,110
and then the machines slowed to a crawl

00:30:08,220 --> 00:30:14,710
so the really dumb way that we work

00:30:11,110 --> 00:30:17,710
around that is knowing that these t two

00:30:14,710 --> 00:30:22,419
instances a t2 medium has about twenty

00:30:17,710 --> 00:30:25,120
percent of one cpu available when it's

00:30:22,419 --> 00:30:27,309
out of credits and so we just did some

00:30:25,120 --> 00:30:30,700
benchmarking and figure it out I just

00:30:27,309 --> 00:30:33,309
drew all those CPU credits down a

00:30:30,700 --> 00:30:35,529
benchmarked it at about the rate that we

00:30:33,309 --> 00:30:38,350
normally get and we provisioned enough

00:30:35,529 --> 00:30:40,029
t-to instances that we can basically run

00:30:38,350 --> 00:30:43,029
the site it's a little pokey but we can

00:30:40,029 --> 00:30:46,269
basically run the site when they're out

00:30:43,029 --> 00:30:48,730
of credits and we can also burst to the

00:30:46,269 --> 00:30:49,330
kinds of increased traffic that we get

00:30:48,730 --> 00:30:53,410
around

00:30:49,330 --> 00:30:56,770
conf we also monitor we use i Singa to

00:30:53,410 --> 00:30:59,290
for monitoring and we monitor those

00:30:56,770 --> 00:31:01,000
credits so we also have that but even if

00:30:59,290 --> 00:31:04,570
we drop the ball and don't respond to a

00:31:01,000 --> 00:31:11,380
page the site will basically stay up and

00:31:04,570 --> 00:31:16,030
we can burst let's see all right moving

00:31:11,380 --> 00:31:20,440
along here so the next interesting

00:31:16,030 --> 00:31:22,600
question so we'll we'll get back to the

00:31:20,440 --> 00:31:24,340
details of how these how we're

00:31:22,600 --> 00:31:25,780
provisioning these things a little bit

00:31:24,340 --> 00:31:28,660
later right now i'm just working through

00:31:25,780 --> 00:31:32,380
that packet flow the next interesting

00:31:28,660 --> 00:31:35,500
question in my view is the question of

00:31:32,380 --> 00:31:37,630
shared storage because you can find

00:31:35,500 --> 00:31:42,010
result in google all day about how to

00:31:37,630 --> 00:31:43,900
run a decent ec2 engine X Apache

00:31:42,010 --> 00:31:50,170
whatever you don't need my help for that

00:31:43,900 --> 00:31:52,950
shared storage is is painful so when I

00:31:50,170 --> 00:31:55,120
say shared storage how many of you like

00:31:52,950 --> 00:32:00,880
does is this a concept that needs

00:31:55,120 --> 00:32:05,140
explaining or no ok so there's pretty

00:32:00,880 --> 00:32:08,140
much there's pretty much four options

00:32:05,140 --> 00:32:10,570
here and they consist of whether you're

00:32:08,140 --> 00:32:13,890
going to use some variant of s3 or

00:32:10,570 --> 00:32:16,270
you're somehow going to use NFS and

00:32:13,890 --> 00:32:17,890
please come and talk to me if there's

00:32:16,270 --> 00:32:20,500
another good option because these are

00:32:17,890 --> 00:32:22,180
all kind of painful I know you can use

00:32:20,500 --> 00:32:26,230
Gloucester or whatever but that's even

00:32:22,180 --> 00:32:30,460
worse so the best option if you can cut

00:32:26,230 --> 00:32:33,730
it is s 3 with a drupal module that

00:32:30,460 --> 00:32:35,650
doesn't native integration I have been

00:32:33,730 --> 00:32:38,050
part of a couple of projects that tried

00:32:35,650 --> 00:32:39,700
to do this and we all got about ninety

00:32:38,050 --> 00:32:41,410
five percent of the way and where it

00:32:39,700 --> 00:32:45,090
basically worked but there was some

00:32:41,410 --> 00:32:47,890
module that we couldn't get rid of that

00:32:45,090 --> 00:32:50,140
expected POSIX semantics because it

00:32:47,890 --> 00:32:53,830
bypassed the Drupal file system

00:32:50,140 --> 00:32:56,020
abstraction it's really frustrating but

00:32:53,830 --> 00:32:58,180
I've been in this has been my experience

00:32:56,020 --> 00:33:00,580
every time I've tried to do this both

00:32:58,180 --> 00:33:02,630
with puppet in Drupal 7 and Drupal 8

00:33:00,580 --> 00:33:06,140
with consulting clients before

00:33:02,630 --> 00:33:07,790
that it it should just work and my

00:33:06,140 --> 00:33:10,010
experience has been it doesn't quite

00:33:07,790 --> 00:33:11,810
especially for a brownfield project

00:33:10,010 --> 00:33:16,550
where somebody has already locked in the

00:33:11,810 --> 00:33:19,910
module decisions the the next way to use

00:33:16,550 --> 00:33:24,590
s3 is a fuse file system mount fuses a

00:33:19,910 --> 00:33:26,630
user space file system you can mount s3

00:33:24,590 --> 00:33:33,290
so it looks like a local file system

00:33:26,630 --> 00:33:37,400
there's two problems here it it's a

00:33:33,290 --> 00:33:40,210
leaky abstraction s3 is not a POSIX file

00:33:37,400 --> 00:33:43,670
system it's it's a lot different and

00:33:40,210 --> 00:33:46,520
fuse presents a POSIX abstraction to it

00:33:43,670 --> 00:33:51,710
so you have kind of a messy interface

00:33:46,520 --> 00:33:54,740
there it's also slow so I know that I

00:33:51,710 --> 00:33:57,590
wanted to list this because I know a lot

00:33:54,740 --> 00:33:59,930
of people use it successfully and it's a

00:33:57,590 --> 00:34:02,960
lot lower operational intensity

00:33:59,930 --> 00:34:06,760
operational overhead than running NFS

00:34:02,960 --> 00:34:09,230
and it's a lot easier on developers than

00:34:06,760 --> 00:34:11,540
having to redo everything so that you

00:34:09,230 --> 00:34:15,260
can use s3 with a bunch of poorly

00:34:11,540 --> 00:34:16,970
behaved modules so it is an option but

00:34:15,260 --> 00:34:20,899
before you go into production with this

00:34:16,970 --> 00:34:23,389
benchmark the heck out of it the most

00:34:20,899 --> 00:34:26,570
traditional option you all know is to

00:34:23,389 --> 00:34:30,830
just stand up an NFS server the failure

00:34:26,570 --> 00:34:34,879
mode of NFS is really gross it will hold

00:34:30,830 --> 00:34:37,460
a lock on whatever it's trying to read

00:34:34,879 --> 00:34:40,220
it'll it'll hold it'll try to read

00:34:37,460 --> 00:34:42,620
forever your NFS server can disappear

00:34:40,220 --> 00:34:45,020
and it will just keep trying to read and

00:34:42,620 --> 00:34:47,770
the failure mode I've seen there if

00:34:45,020 --> 00:34:51,320
you're running php-fpm is it's basically

00:34:47,770 --> 00:34:53,240
poisoning each worker process as it

00:34:51,320 --> 00:34:56,000
tries to make requests and they start

00:34:53,240 --> 00:34:57,740
blocking and so you get this weird

00:34:56,000 --> 00:35:00,790
failure mode where when it makes a

00:34:57,740 --> 00:35:05,140
request for a shared file that worker

00:35:00,790 --> 00:35:08,630
disappears until it times out and dies

00:35:05,140 --> 00:35:11,870
if you google a lot of people will tell

00:35:08,630 --> 00:35:14,960
you that it's easy to tell NFS to break

00:35:11,870 --> 00:35:17,930
these requests etc

00:35:14,960 --> 00:35:20,780
if if that's true please pull me aside

00:35:17,930 --> 00:35:23,000
afterward and demo it to me because I

00:35:20,780 --> 00:35:24,890
spent weeks trying because all these

00:35:23,000 --> 00:35:28,550
blog entries told me how easy it was and

00:35:24,890 --> 00:35:31,400
I just felt dumb I just couldn't do it I

00:35:28,550 --> 00:35:34,400
just end up the only way I figured out

00:35:31,400 --> 00:35:40,820
to reliably get it to break in NFS mount

00:35:34,400 --> 00:35:45,109
is to reboot the box so we've got a real

00:35:40,820 --> 00:35:49,130
dumb hack so of course this is the one

00:35:45,109 --> 00:35:50,900
we do but we've got a real dumb hack

00:35:49,130 --> 00:35:53,270
that's kept it from being a problem for

00:35:50,900 --> 00:35:56,119
us tell you in just a sec the last

00:35:53,270 --> 00:35:58,820
option came out like three weeks after

00:35:56,119 --> 00:36:01,250
we committed to using NFS and went into

00:35:58,820 --> 00:36:07,099
production which is Amazon's elastic

00:36:01,250 --> 00:36:10,460
file system EFS is is a hosted NFS

00:36:07,099 --> 00:36:12,080
service it it looks really cool I

00:36:10,460 --> 00:36:14,000
haven't run it in production so I don't

00:36:12,080 --> 00:36:19,220
want to tell you that it's the bee's

00:36:14,000 --> 00:36:22,339
knees but it looks like sort of the

00:36:19,220 --> 00:36:24,859
ultimate answer to this question if it

00:36:22,339 --> 00:36:27,920
does as promised the only real caveat is

00:36:24,859 --> 00:36:31,190
it's in beta and you also have to use

00:36:27,920 --> 00:36:34,160
their dns for internal dns resolution so

00:36:31,190 --> 00:36:35,839
if your leap if you have a VPN link to

00:36:34,160 --> 00:36:38,900
your v pc and you have to use your

00:36:35,839 --> 00:36:43,040
corporate dns for policy reasons this

00:36:38,900 --> 00:36:44,780
may not be an option so i don't have a

00:36:43,040 --> 00:36:48,859
slide for this but what we do to make

00:36:44,780 --> 00:36:50,540
NFS less of a problem is this is almost

00:36:48,859 --> 00:36:52,339
an embarrassing hack to admit but it's

00:36:50,540 --> 00:36:54,320
worked so well that i want to share it

00:36:52,339 --> 00:36:56,690
because i know a lot of people run NFS

00:36:54,320 --> 00:37:00,830
we have a cron job that every two hours

00:36:56,690 --> 00:37:03,230
our sinks the the NFS mount to a local

00:37:00,830 --> 00:37:06,500
sort of cache directory and we have an

00:37:03,230 --> 00:37:10,070
engine extra files directive that tries

00:37:06,500 --> 00:37:14,359
that path first and if it can't find the

00:37:10,070 --> 00:37:16,130
file there it hits the NFS mount we

00:37:14,359 --> 00:37:19,490
don't have people uploading stuff very

00:37:16,130 --> 00:37:21,980
often so well upwards of ninety nine

00:37:19,490 --> 00:37:24,230
point nine percent of read requests get

00:37:21,980 --> 00:37:26,359
served out of the file system but if

00:37:24,230 --> 00:37:28,310
it's not there it immediately gets it

00:37:26,359 --> 00:37:32,120
out of NFS

00:37:28,310 --> 00:37:35,690
it's faster and it effectively solves

00:37:32,120 --> 00:37:38,960
that worker poisoning problem because so

00:37:35,690 --> 00:37:41,330
few workers are hitting if you're dia if

00:37:38,960 --> 00:37:46,700
you're NFS server goes down there's so

00:37:41,330 --> 00:37:50,990
few workers most of the time hitting NFS

00:37:46,700 --> 00:37:52,820
for things that the handful of workers

00:37:50,990 --> 00:37:56,510
that might get sort of poisoned will

00:37:52,820 --> 00:37:58,910
time out die and respawn before it

00:37:56,510 --> 00:38:01,400
becomes a problem your respawn rate can

00:37:58,910 --> 00:38:05,120
keep up with your loss rate and so the

00:38:01,400 --> 00:38:06,560
site stays up so even though we have an

00:38:05,120 --> 00:38:10,760
architecture there that I wouldn't

00:38:06,560 --> 00:38:12,830
recommend which is one NFS server 43

00:38:10,760 --> 00:38:15,140
availability zones that's a clear single

00:38:12,830 --> 00:38:17,270
point of failure we've been running like

00:38:15,140 --> 00:38:21,020
that for more than a year and have not

00:38:17,270 --> 00:38:25,850
had any problems as a result and we've

00:38:21,020 --> 00:38:42,110
had a performance boost as well next

00:38:25,850 --> 00:38:44,360
part yes yeah yeah yeah so there are

00:38:42,110 --> 00:38:47,120
some really cool distributed file

00:38:44,360 --> 00:38:49,580
systems out there I think gluster and

00:38:47,120 --> 00:38:52,040
SEF are the obvious ones if you're

00:38:49,580 --> 00:38:55,250
willing to go commercial there's there's

00:38:52,040 --> 00:39:01,310
there are other ones they're super

00:38:55,250 --> 00:39:03,550
compelling I have when I have

00:39:01,310 --> 00:39:06,140
experimented with gluster the

00:39:03,550 --> 00:39:09,290
operational overhead and the amount of

00:39:06,140 --> 00:39:12,380
knowledge that it required to run was

00:39:09,290 --> 00:39:16,730
just beyond what I was willing to impose

00:39:12,380 --> 00:39:21,520
on my colleagues so all of us are on

00:39:16,730 --> 00:39:24,020
call for everyone else's stuff and so

00:39:21,520 --> 00:39:26,630
you know i might get woken up for

00:39:24,020 --> 00:39:28,760
network gear that i've never used the

00:39:26,630 --> 00:39:31,340
network guys might get woken up for web

00:39:28,760 --> 00:39:34,600
server stuff so it all has to be kind of

00:39:31,340 --> 00:39:38,990
you know every man a rifleman kind of

00:39:34,600 --> 00:39:41,930
stuff and Gluster is is really really

00:39:38,990 --> 00:39:43,670
cool it's not the kind of thing that

00:39:41,930 --> 00:39:46,400
everybody should have to learn how to

00:39:43,670 --> 00:39:48,170
run so if if you're starting your own

00:39:46,400 --> 00:39:51,619
drupal hosting service you should

00:39:48,170 --> 00:39:54,559
probably look at gluster if you're doing

00:39:51,619 --> 00:39:56,599
your own hosting you probably should use

00:39:54,559 --> 00:40:05,349
one of these does that kind of answer

00:39:56,599 --> 00:40:05,349
your cool no I I don't know that time oh

00:40:11,079 --> 00:40:18,859
oh cool no that sounds like a good good

00:40:15,680 --> 00:40:25,630
thing to look into I know I see one in

00:40:18,859 --> 00:40:25,630
the back to ya

00:40:44,750 --> 00:40:47,750
right

00:40:53,110 --> 00:40:55,770
Brett

00:41:00,720 --> 00:41:06,480
I don't think that the Drupal 7 Drupal 8

00:41:05,100 --> 00:41:08,130
and please correct me if I'm wrong here

00:41:06,480 --> 00:41:09,900
I don't think that that will make any

00:41:08,130 --> 00:41:12,750
difference as far as your shared file

00:41:09,900 --> 00:41:14,270
back end your your decision-making tree

00:41:12,750 --> 00:41:21,660
is pretty much going to be the same

00:41:14,270 --> 00:41:23,100
they're cool so I'm going to I'm going

00:41:21,660 --> 00:41:27,480
to skip forward because I'm running

00:41:23,100 --> 00:41:29,640
slower than I should be I'm real

00:41:27,480 --> 00:41:33,690
strongly opposed to running your own

00:41:29,640 --> 00:41:36,720
database stuff in AWS with postgres

00:41:33,690 --> 00:41:39,650
there there are a handful of reasons if

00:41:36,720 --> 00:41:42,690
you need custom postgres extensions in

00:41:39,650 --> 00:41:46,800
the Drupal my sequel world there's I

00:41:42,690 --> 00:41:48,930
just have never met a good excuse so the

00:41:46,800 --> 00:41:51,540
options here when you're making in that

00:41:48,930 --> 00:41:54,510
part of the decision tree of how do i

00:41:51,540 --> 00:41:58,050
run Drupal on AWS you have three options

00:41:54,510 --> 00:42:00,870
you run your own my sequel you use my

00:41:58,050 --> 00:42:03,390
sequel on RDS or you use Amazon's Aurora

00:42:00,870 --> 00:42:05,550
which is a my sequel compatible multi

00:42:03,390 --> 00:42:11,520
master database that they run as part of

00:42:05,550 --> 00:42:13,530
RDS don't run your own RDS is what we

00:42:11,520 --> 00:42:16,470
use because we got set up before Aurora

00:42:13,530 --> 00:42:21,840
existed the base of the trade-off there

00:42:16,470 --> 00:42:23,940
is real simple Aurora that the only real

00:42:21,840 --> 00:42:27,480
downside that I'm aware of to Aurora is

00:42:23,940 --> 00:42:30,510
that the smallest instance size you can

00:42:27,480 --> 00:42:34,410
buy is really big so it's a little bit

00:42:30,510 --> 00:42:36,590
expensive it is that consistent with

00:42:34,410 --> 00:42:36,590
your

00:42:36,829 --> 00:42:47,150
the smallest instance eyes on already

00:42:42,619 --> 00:42:51,769
else if you run it do a multi-zone is

00:42:47,150 --> 00:42:55,400
about is about twenty-five percent less

00:42:51,769 --> 00:42:58,009
than aurora but you know Aurora has a

00:42:55,400 --> 00:43:00,289
lot of other benefits it doesn't because

00:42:58,009 --> 00:43:08,059
it's a beer instance it runs faster and

00:43:00,289 --> 00:43:13,069
correct so frakking really important you

00:43:08,059 --> 00:43:16,700
right I totally agree aurora is should

00:43:13,069 --> 00:43:18,349
be the default choice if you're super

00:43:16,700 --> 00:43:20,029
price sensitive which I know some people

00:43:18,349 --> 00:43:23,479
will be because they're running like

00:43:20,029 --> 00:43:25,999
custom hosting for some tiny outfit that

00:43:23,479 --> 00:43:28,039
counts every dollar you might care about

00:43:25,999 --> 00:43:30,259
like if you're tempted to run your own

00:43:28,039 --> 00:43:32,989
my sequel because it'll save you a

00:43:30,259 --> 00:43:35,829
couple bucks a month run then you might

00:43:32,989 --> 00:43:38,420
be interested in the cost savings of RDS

00:43:35,829 --> 00:43:41,719
if you're running development instances

00:43:38,420 --> 00:43:44,059
that need like a t2 micro-sized database

00:43:41,719 --> 00:43:46,699
you might want to run that on RDS if

00:43:44,059 --> 00:43:48,289
you're running production and you don't

00:43:46,699 --> 00:43:50,719
have a compelling reason to stick with

00:43:48,289 --> 00:43:54,349
RDS use Aurora because it's multi master

00:43:50,719 --> 00:43:56,239
and so the failure mode there when one

00:43:54,349 --> 00:43:59,690
goes down is going to be you won't

00:43:56,239 --> 00:44:05,029
notice whereas with RDS you have a

00:43:59,690 --> 00:44:08,150
traditional primary slave failover model

00:44:05,029 --> 00:44:10,039
and my experience has been it fails over

00:44:08,150 --> 00:44:12,319
basically instantaneously but it's not

00:44:10,039 --> 00:44:14,299
guaranteed it can take a few minutes and

00:44:12,319 --> 00:44:22,400
you'll be throwing database errors on

00:44:14,299 --> 00:44:24,559
your site while that's happening yeah

00:44:22,400 --> 00:44:26,329
unless it's I mean if its production I

00:44:24,559 --> 00:44:29,089
don't think so you would want a the

00:44:26,329 --> 00:44:32,359
comment was that single AZ RDS doesn't

00:44:29,089 --> 00:44:38,449
make sense what we do is in puppet we

00:44:32,359 --> 00:44:42,079
have in huayra we have based on this

00:44:38,449 --> 00:44:45,799
stage we set the the replication options

00:44:42,079 --> 00:44:48,319
so for the dev version of the site of

00:44:45,799 --> 00:44:50,299
the the infrastructure it's single

00:44:48,319 --> 00:44:52,699
availability zone the staging

00:44:50,299 --> 00:44:54,920
version is multi availability zone and

00:44:52,699 --> 00:44:56,689
the production is multi availability

00:44:54,920 --> 00:44:58,400
zone so we just save a couple bucks

00:44:56,689 --> 00:45:00,049
because in the dev version that's just

00:44:58,400 --> 00:45:06,189
the putzing around environment I don't

00:45:00,049 --> 00:45:06,189
need a che for that one quick thing here

00:45:06,699 --> 00:45:10,939
one thing that surprises people is we

00:45:09,079 --> 00:45:14,239
got rid of varnish and we got rid of

00:45:10,939 --> 00:45:17,449
memcache this is probably not the right

00:45:14,239 --> 00:45:20,059
answer for everybody but it it

00:45:17,449 --> 00:45:22,939
simplified our architecture because we

00:45:20,059 --> 00:45:24,589
could just do some my sequel tuning it

00:45:22,939 --> 00:45:26,479
turned out when I came on board the

00:45:24,589 --> 00:45:29,809
reason that we needed varnish to make

00:45:26,479 --> 00:45:32,630
the site perform acceptable was my

00:45:29,809 --> 00:45:34,989
sequel was was not tuned basically at

00:45:32,630 --> 00:45:38,449
all it was at its default settings and

00:45:34,989 --> 00:45:40,699
so and that's a scenario I've seen with

00:45:38,449 --> 00:45:42,739
a lot of consulting clients it's the

00:45:40,699 --> 00:45:45,859
first thing I look for when I go into an

00:45:42,739 --> 00:45:47,719
engagement because the default my sequel

00:45:45,859 --> 00:45:49,309
settings if nothing else that default my

00:45:47,719 --> 00:45:51,979
sequel settings give you 10 megs of

00:45:49,309 --> 00:45:54,619
memory use and it does amazingly well

00:45:51,979 --> 00:45:57,529
with that but you can do a lot better

00:45:54,619 --> 00:45:59,359
right so once we tuned my sequel

00:45:57,529 --> 00:46:01,459
correctly we didn't actually need we

00:45:59,359 --> 00:46:04,519
could get to a like a one second page

00:46:01,459 --> 00:46:07,039
load without using or page generation

00:46:04,519 --> 00:46:09,079
without garnish and that just took a

00:46:07,039 --> 00:46:10,579
piece out of the stack and removed the

00:46:09,079 --> 00:46:12,859
question of how to clear those caches

00:46:10,579 --> 00:46:16,059
because the drupal caching works fine I

00:46:12,859 --> 00:46:16,059
see a question in the back

00:46:22,540 --> 00:46:30,830
yep so the question is whether to run

00:46:26,630 --> 00:46:32,630
memcache locally on the ec2 instance and

00:46:30,830 --> 00:46:37,180
versus using the hosted elastic cash

00:46:32,630 --> 00:46:40,970
excuse me hosted elastic cash service I

00:46:37,180 --> 00:46:43,760
think it depends on your use case so if

00:46:40,970 --> 00:46:48,370
you're just running what what we used to

00:46:43,760 --> 00:46:51,680
do when we had memcache was we

00:46:48,370 --> 00:46:55,010
configured all of them we had every web

00:46:51,680 --> 00:46:57,350
server running a memcache and because we

00:46:55,010 --> 00:47:00,710
wanted those caches to be consistent we

00:46:57,350 --> 00:47:03,230
used puppet to configure the same order

00:47:00,710 --> 00:47:06,530
of failover so that every web server

00:47:03,230 --> 00:47:09,020
would hit every other web servers

00:47:06,530 --> 00:47:10,820
memcache in the same order so that if

00:47:09,020 --> 00:47:12,950
one of them went offline all of them

00:47:10,820 --> 00:47:16,220
would hit the next memcache so those

00:47:12,950 --> 00:47:17,690
caches would be consistent I was really

00:47:16,220 --> 00:47:20,360
happy when we could just get rid of that

00:47:17,690 --> 00:47:23,390
and not think about that so if you don't

00:47:20,360 --> 00:47:25,460
really care about consistency and you

00:47:23,390 --> 00:47:27,350
don't need to learn a ton of memory you

00:47:25,460 --> 00:47:30,680
kind of might as well run that locally

00:47:27,350 --> 00:47:33,200
if you want a shared instance and you

00:47:30,680 --> 00:47:37,880
you do care about cash consistency my

00:47:33,200 --> 00:47:40,340
impulse would be to use elastic cash and

00:47:37,880 --> 00:47:43,220
let their let them handle the high

00:47:40,340 --> 00:47:45,110
availability question and just point

00:47:43,220 --> 00:47:50,630
everything there and make it their

00:47:45,110 --> 00:48:00,140
problem and not yours oh do they not in

00:47:50,630 --> 00:48:04,160
memcache oh I say I say haha oh that

00:48:00,140 --> 00:48:05,600
doesn't sound like any fun at all all

00:48:04,160 --> 00:48:11,510
right well scratch that recommendation

00:48:05,600 --> 00:48:14,420
then you're on your own when the last

00:48:11,510 --> 00:48:16,660
thing that I want to say here this isn't

00:48:14,420 --> 00:48:19,460
really varnish memcache but the reason

00:48:16,660 --> 00:48:20,930
the ultimate reason that we were able to

00:48:19,460 --> 00:48:23,990
get rid of that is because we were

00:48:20,930 --> 00:48:26,720
benchmarking in a reasonable way most of

00:48:23,990 --> 00:48:29,240
the people that I've worked with who run

00:48:26,720 --> 00:48:33,500
Drupal infrastructure benchmark in a

00:48:29,240 --> 00:48:34,220
very ad hoc way the process that I use

00:48:33,500 --> 00:48:37,700
that has

00:48:34,220 --> 00:48:40,640
worked fairly well for me is all for a

00:48:37,700 --> 00:48:42,800
brownfield type environment is I'll take

00:48:40,640 --> 00:48:46,430
existing web server load balancer logs

00:48:42,800 --> 00:48:49,790
parse the URLs out of that generate a

00:48:46,430 --> 00:48:53,420
list that I can feed into siege and then

00:48:49,790 --> 00:48:55,490
just generate traffic with seek I don't

00:48:53,420 --> 00:48:58,520
look at the numbers siege gives me at

00:48:55,490 --> 00:49:00,980
all I just wanted to generate load it's

00:48:58,520 --> 00:49:03,770
just a way to have users in my

00:49:00,980 --> 00:49:07,430
benchmarking environment then I use new

00:49:03,770 --> 00:49:11,840
relic or some other tool like that and I

00:49:07,430 --> 00:49:14,869
adjust how aggressive siege is until the

00:49:11,840 --> 00:49:17,859
load numbers look about the same in my

00:49:14,869 --> 00:49:20,510
dev environment and my prod environment

00:49:17,859 --> 00:49:23,330
after that I have a reasonable degree of

00:49:20,510 --> 00:49:26,359
confidence that I have a similar load

00:49:23,330 --> 00:49:28,550
profile going into it and then I start

00:49:26,359 --> 00:49:30,200
doing performance tuning but the only

00:49:28,550 --> 00:49:33,590
numbers on looking at are what I'm

00:49:30,200 --> 00:49:35,390
getting out of my metrics tool not what

00:49:33,590 --> 00:49:38,240
siege is telling me siege is a great

00:49:35,390 --> 00:49:40,310
tool for really ad-hoc stuff i might

00:49:38,240 --> 00:49:43,190
start off using their numbers but you

00:49:40,310 --> 00:49:46,369
get so much more in-depth data if you're

00:49:43,190 --> 00:49:48,109
using a more powerful tool your whole

00:49:46,369 --> 00:49:49,369
benchmarking process will be better and

00:49:48,109 --> 00:49:56,480
you'll end up with better infrastructure

00:49:49,369 --> 00:49:57,950
for a ton less work for it so obviously

00:49:56,480 --> 00:50:05,109
i'm going to be talking about config

00:49:57,950 --> 00:50:05,109
management we use puppet obviously

00:50:07,380 --> 00:50:13,450
we use it for just about everything and

00:50:10,920 --> 00:50:16,569
but I'm not going to talk about a whole

00:50:13,450 --> 00:50:18,609
lot of puppet specific stuff here for

00:50:16,569 --> 00:50:23,079
two reasons I don't want to just be a

00:50:18,609 --> 00:50:24,700
product pushing person I also don't

00:50:23,079 --> 00:50:27,279
think that I have anything to offer

00:50:24,700 --> 00:50:28,839
that's particularly unique except for a

00:50:27,279 --> 00:50:31,329
couple of slides that I'll share because

00:50:28,839 --> 00:50:32,799
you can google about how do I puppet

00:50:31,329 --> 00:50:36,250
eyes or whatever and you'll get an

00:50:32,799 --> 00:50:40,630
answer what I'm going to focus on here

00:50:36,250 --> 00:50:43,720
is basically the provisioning side

00:50:40,630 --> 00:50:45,970
because that's the one thing that I have

00:50:43,720 --> 00:50:48,430
had a real hard time that question of

00:50:45,970 --> 00:50:50,019
how do I get a note from not existing to

00:50:48,430 --> 00:50:51,819
being a puppet eyes part of my

00:50:50,019 --> 00:50:56,250
infrastructure that's the part that

00:50:51,819 --> 00:50:56,250
there's like a shortage of tutorials on

00:50:58,140 --> 00:51:03,430
side note sometimes they'll hear people

00:51:01,299 --> 00:51:06,039
say that they they don't want to use

00:51:03,430 --> 00:51:08,230
config management that they they have

00:51:06,039 --> 00:51:09,579
people who you know won't learn it who

00:51:08,230 --> 00:51:12,279
aren't sort of technically sophisticated

00:51:09,579 --> 00:51:14,440
enough that's a clue that you should be

00:51:12,279 --> 00:51:17,589
using a hosted service I don't mean that

00:51:14,440 --> 00:51:19,119
in a derogatory way but one way or

00:51:17,589 --> 00:51:21,430
another you're going to pay the cost

00:51:19,119 --> 00:51:23,410
either it's going to be by the initial

00:51:21,430 --> 00:51:25,240
investment in config management where

00:51:23,410 --> 00:51:28,390
you're going to pay a higher cost long

00:51:25,240 --> 00:51:31,720
term through the pain of operating

00:51:28,390 --> 00:51:33,910
without good tooling you're going to pay

00:51:31,720 --> 00:51:35,890
that cost so either pay somebody else to

00:51:33,910 --> 00:51:39,099
do the config management and use their

00:51:35,890 --> 00:51:48,400
service or you know bite the bullet and

00:51:39,099 --> 00:51:50,710
and just go into that at a high level

00:51:48,400 --> 00:51:53,769
the hard parts I see of config

00:51:50,710 --> 00:51:56,079
management are managing secrets there

00:51:53,769 --> 00:51:58,839
aren't great stories for that in most

00:51:56,079 --> 00:52:02,170
tooling the question of bootstrapping

00:51:58,839 --> 00:52:04,299
which is what I'm going to focus on the

00:52:02,170 --> 00:52:08,049
question of code complexity is an

00:52:04,299 --> 00:52:10,569
important one most of us who run

00:52:08,049 --> 00:52:13,329
infrastructure don't come from a

00:52:10,569 --> 00:52:16,869
software development background and so

00:52:13,329 --> 00:52:18,040
writing code to manage a lot of complex

00:52:16,869 --> 00:52:20,710
moving things

00:52:18,040 --> 00:52:25,900
ends up with complex code with a lot of

00:52:20,710 --> 00:52:28,810
code pads and if you don't use some kind

00:52:25,900 --> 00:52:31,960
of reasonable methodologies in terms of

00:52:28,810 --> 00:52:35,920
refactoring testing structuring your

00:52:31,960 --> 00:52:41,080
code in a reasonable way you will end up

00:52:35,920 --> 00:52:43,120
with real messy code and so that turns

00:52:41,080 --> 00:52:45,910
into a you you have to be aware of

00:52:43,120 --> 00:52:48,160
managing code complexity when you go

00:52:45,910 --> 00:52:49,990
into this because otherwise you'll

00:52:48,160 --> 00:52:52,600
you'll end up frustrated you'll switch

00:52:49,990 --> 00:52:54,160
configuration management tools and then

00:52:52,600 --> 00:52:55,900
three years later you'll be in the same

00:52:54,160 --> 00:52:59,620
spot because it wasn't the tool it was

00:52:55,900 --> 00:53:02,410
the code complexity I'm going to skip

00:52:59,620 --> 00:53:04,630
drift because I'm short on time the way

00:53:02,410 --> 00:53:10,300
we provision is we use the puppet labs

00:53:04,630 --> 00:53:12,490
AWS module we have the puppet master it

00:53:10,300 --> 00:53:16,300
has just enough I am permissions to

00:53:12,490 --> 00:53:19,600
create a tiny little like a t2 small

00:53:16,300 --> 00:53:22,090
provision or instance which has I am

00:53:19,600 --> 00:53:25,660
permissions to do to provision more

00:53:22,090 --> 00:53:28,570
services the main reason we do that is

00:53:25,660 --> 00:53:30,850
just because you it's a way to contain

00:53:28,570 --> 00:53:32,830
complexity like it's easier to reason

00:53:30,850 --> 00:53:37,600
about a puppet master that isn't also

00:53:32,830 --> 00:53:39,970
provisioning all my other stuff it the

00:53:37,600 --> 00:53:42,850
provisioner then we have one for each of

00:53:39,970 --> 00:53:45,190
the dev stage and production stages and

00:53:42,850 --> 00:53:47,560
so the dev one will provision all the

00:53:45,190 --> 00:53:50,020
dev versions of services the stage one

00:53:47,560 --> 00:53:52,450
the staging versions of services that

00:53:50,020 --> 00:53:56,400
allows us to test changes to how we

00:53:52,450 --> 00:53:56,400
provision without blowing up production

00:53:56,820 --> 00:54:04,570
the actual sort of high-level path for

00:54:01,300 --> 00:54:06,910
how we get a node from not existing to

00:54:04,570 --> 00:54:09,490
being part of our infrastructure took a

00:54:06,910 --> 00:54:12,160
lot of work to for me to I tried a bunch

00:54:09,490 --> 00:54:17,380
of different permutations and what we

00:54:12,160 --> 00:54:21,460
ended up with was the bit of background

00:54:17,380 --> 00:54:23,590
here is the way puppet works is when you

00:54:21,460 --> 00:54:26,860
connect an ode to a puppet an agent to a

00:54:23,590 --> 00:54:29,950
puppet master the first time puppet runs

00:54:26,860 --> 00:54:31,840
it requests on SSL certificate to be

00:54:29,950 --> 00:54:32,170
signed by the master if you don't have

00:54:31,840 --> 00:54:33,819
any

00:54:32,170 --> 00:54:36,520
Automation that means you either have to

00:54:33,819 --> 00:54:38,650
go into a PE console and click a thing

00:54:36,520 --> 00:54:41,980
or use the command line to sign it

00:54:38,650 --> 00:54:43,660
yourself there's a tool called policy

00:54:41,980 --> 00:54:45,670
based auto signing that allows you to

00:54:43,660 --> 00:54:51,339
automate this but it's more of an API

00:54:45,670 --> 00:54:53,650
then a end-user tool so I wrote a tool

00:54:51,339 --> 00:54:55,839
called puppet auto sign it's public I'll

00:54:53,650 --> 00:54:58,869
include a link at the end that generates

00:54:55,839 --> 00:55:03,309
a cryptographically signed token it's H

00:54:58,869 --> 00:55:05,530
Mac signed the provisioner when the

00:55:03,309 --> 00:55:08,140
puppet compile happens generates one of

00:55:05,530 --> 00:55:12,819
these tokens it puts it in a cloud init

00:55:08,140 --> 00:55:15,160
script that it gives to the easy to

00:55:12,819 --> 00:55:19,390
instance when it provisions it using the

00:55:15,160 --> 00:55:23,170
puppet labs AWS module the cloud init

00:55:19,390 --> 00:55:26,859
script runs the first time that the node

00:55:23,170 --> 00:55:28,809
when it boots and it sends that token

00:55:26,859 --> 00:55:31,420
back to the master along with the

00:55:28,809 --> 00:55:33,460
request to sign the cert and then the

00:55:31,420 --> 00:55:36,460
master can cryptographically validate

00:55:33,460 --> 00:55:39,609
that that is authentic and it also has a

00:55:36,460 --> 00:55:42,369
local registry that verifies that those

00:55:39,609 --> 00:55:44,319
tokens can only be used once there's

00:55:42,369 --> 00:55:45,790
also a time stamp signed into them so

00:55:44,319 --> 00:55:50,980
they're not valid after more than an

00:55:45,790 --> 00:55:52,720
hour maybe it's to anyone so the cloud

00:55:50,980 --> 00:55:55,089
in its grip then installs puppet gets

00:55:52,720 --> 00:55:57,609
the cert runs puppet again a couple of

00:55:55,089 --> 00:55:59,049
times and that way by the end of that

00:55:57,609 --> 00:56:00,760
the note is part of your puppet

00:55:59,049 --> 00:56:04,770
infrastructure and you can do things the

00:56:00,760 --> 00:56:09,250
normal way and we're not managing am is

00:56:04,770 --> 00:56:10,990
particularly we've sort of bypassed that

00:56:09,250 --> 00:56:13,809
problem we haven't had a need to bring

00:56:10,990 --> 00:56:17,980
up nodes that quickly it also provisions

00:56:13,809 --> 00:56:20,380
the RDS security groups elastic it

00:56:17,980 --> 00:56:26,140
assigns the elastic IP unfortunately

00:56:20,380 --> 00:56:28,140
can't provision them I'm running a

00:56:26,140 --> 00:56:31,930
little short on time so I'm going to

00:56:28,140 --> 00:56:34,109
skip this it's kind of cool come back to

00:56:31,930 --> 00:56:34,109
it

00:56:35,430 --> 00:56:41,730
I want to make a point about using

00:56:39,210 --> 00:56:44,280
configuration management end-to-end I

00:56:41,730 --> 00:56:47,579
know a lot of us use configuration

00:56:44,280 --> 00:56:50,819
management a lot of people don't have a

00:56:47,579 --> 00:56:52,650
process where it's used we're like a

00:56:50,819 --> 00:56:54,540
server can go through its whole life

00:56:52,650 --> 00:56:58,440
cycle without a person changing anything

00:56:54,540 --> 00:57:00,650
manually there's a huge psychological

00:56:58,440 --> 00:57:02,819
difference especially when it comes to

00:57:00,650 --> 00:57:04,859
operations and developers working

00:57:02,819 --> 00:57:07,890
together because you get a culture

00:57:04,859 --> 00:57:10,440
change when you take the high stakes

00:57:07,890 --> 00:57:12,569
traditional sysadmin got to get it right

00:57:10,440 --> 00:57:15,030
the first time thing out of the equation

00:57:12,569 --> 00:57:18,109
when you can just delete a busted node

00:57:15,030 --> 00:57:20,970
and rerun puppet and it comes back up

00:57:18,109 --> 00:57:23,309
because then a developer who doesn't

00:57:20,970 --> 00:57:25,410
really know much puppet or anything they

00:57:23,309 --> 00:57:27,960
can experiment in a dev environment they

00:57:25,410 --> 00:57:30,839
can make a pull request bypass the whole

00:57:27,960 --> 00:57:34,500
process of filing a jira ticket for

00:57:30,839 --> 00:57:37,230
operations they document the change they

00:57:34,500 --> 00:57:39,540
want just by making it and making a PR

00:57:37,230 --> 00:57:41,819
it's great it saves me so much time

00:57:39,540 --> 00:57:44,190
because they do most of the ongoing

00:57:41,819 --> 00:57:51,210
maintenance now and they don't have to

00:57:44,190 --> 00:57:54,030
block on ops another thing that I wanted

00:57:51,210 --> 00:57:56,520
to point out that i haven't seen other

00:57:54,030 --> 00:57:59,309
people talking about it's a real simple

00:57:56,520 --> 00:58:02,790
thing i see a lot of people templating

00:57:59,309 --> 00:58:06,059
out config.php with puppet or chef or

00:58:02,790 --> 00:58:09,150
ansible or whatever it's really gross to

00:58:06,059 --> 00:58:11,970
template out executable code it's hard

00:58:09,150 --> 00:58:14,250
to reliably generate syntactically valid

00:58:11,970 --> 00:58:17,490
PHP from templates that's like a

00:58:14,250 --> 00:58:20,069
conceptually hard problem generating

00:58:17,490 --> 00:58:22,980
valid JSON from a data structure is a

00:58:20,069 --> 00:58:25,410
conceptually easy problem so we generate

00:58:22,980 --> 00:58:28,619
a JSON and then have a trivial little

00:58:25,410 --> 00:58:32,730
bit of PHP in the config.php that just

00:58:28,619 --> 00:58:35,190
loads that JSON and sets values from

00:58:32,730 --> 00:58:36,690
that so stop templating out PHP because

00:58:35,190 --> 00:58:41,730
there's there's a better way to do it

00:58:36,690 --> 00:58:43,650
that's actually less work and the last

00:58:41,730 --> 00:58:45,720
thing I want to get to is sort of the

00:58:43,650 --> 00:58:49,080
the operational experience of putting

00:58:45,720 --> 00:58:52,590
this all together and the bottom line is

00:58:49,080 --> 00:58:54,540
we ended up with pretty good up time we

00:58:52,590 --> 00:58:56,280
had some periods in there when we were

00:58:54,540 --> 00:58:58,380
getting this figured out where we were

00:58:56,280 --> 00:59:01,110
hovering around like ninety nine point

00:58:58,380 --> 00:59:03,120
four percent up time and I wouldn't have

00:59:01,110 --> 00:59:05,670
included this slide if I had been doing

00:59:03,120 --> 00:59:07,620
the demo back then because it feels

00:59:05,670 --> 00:59:09,660
pretty bad it made me wonder why am I

00:59:07,620 --> 00:59:14,730
hosting this not having someone else do

00:59:09,660 --> 00:59:17,220
it I'm hoping that this presentation

00:59:14,730 --> 00:59:20,550
will convey some of the the lessons we

00:59:17,220 --> 00:59:25,080
learned that and you can sort of skip to

00:59:20,550 --> 00:59:28,140
the better part of this curve and we are

00:59:25,080 --> 00:59:33,000
at the I think at just about that time

00:59:28,140 --> 00:59:36,500
to take questions are we done alright we

00:59:33,000 --> 00:59:36,500

YouTube URL: https://www.youtube.com/watch?v=vTlDgnUSHUg


