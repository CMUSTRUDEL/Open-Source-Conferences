Title: stackconf online 2020 | Artificial Intelligence? - more like Artificial Stupidity!
Publication date: 2020-06-27
Playlist: stackconf online 2020
Description: 
	...by Aiko Klostermann 

Nowadays “Artificial Intelligence” is everywhere! And rightly so, it does enable us to do really cool things, things we couldn’t even imagine doing just a decade ago. In fact, it sometimes just feels like magic. This ‘magic’ behind it is often powered by “Machine Learning”. But even “AI” has its limitations.
I’ll show examples where “AI” and ML have failed (sometimes with horrible consequences) and will explain why failures are unavoidable in ML but also mention what we can do to reduce them in the future.
Furthermore, I’ll showcase how current AI implementations discriminate against minorities and how that in some cases even leads to a higher risk of death for those groups.
I’ll cover the bias that humans introduce and I’ll explain how poor choice of data makes our world even more unjust than it already is.


NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de/
Blog: http://blog.netways.de/
NWS: https://nws.netways.de 

Webinare
Archiv Link: https://www.netways.de/webinare/archi...
Aktuell: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh/

Music: Walking on Air - https://www.frametraxx.de/
Captions: 
	00:00:00,450 --> 00:00:11,189
[Music]

00:00:11,210 --> 00:00:17,880
my name is Aiko and welcome to my talk

00:00:14,190 --> 00:00:21,600
on artificial intelligence or more like

00:00:17,880 --> 00:00:23,189
artificial stupidity and what I'll be

00:00:21,600 --> 00:00:25,470
doing today is I'll be telling you a

00:00:23,189 --> 00:00:29,010
couple of stories I'll be telling you

00:00:25,470 --> 00:00:30,900
stories about artificial intelligence

00:00:29,010 --> 00:00:32,970
I'll be telling stories about machine

00:00:30,900 --> 00:00:35,850
learning and I'll be telling you stories

00:00:32,970 --> 00:00:38,790
about data and some of those stories are

00:00:35,850 --> 00:00:41,220
success stories but some of those are

00:00:38,790 --> 00:00:44,010
not and we will in particular look at

00:00:41,220 --> 00:00:46,170
those who are not and we will look at

00:00:44,010 --> 00:00:48,030
why they failed and we will look at

00:00:46,170 --> 00:00:50,660
things how we can in the future prevent

00:00:48,030 --> 00:00:54,059
the same things from failing again and

00:00:50,660 --> 00:00:56,100
when I say I'll be telling your stories

00:00:54,059 --> 00:00:58,140
let's start with one of the greatest

00:00:56,100 --> 00:01:00,000
storytellers out there which is status

00:00:58,140 --> 00:01:02,280
Adams and then I'm sure some of you have

00:01:00,000 --> 00:01:05,430
read this book The Hitchhiker's Guide to

00:01:02,280 --> 00:01:09,030
the galaxy which has brought us a couple

00:01:05,430 --> 00:01:11,280
of particularly famous memes like the

00:01:09,030 --> 00:01:13,590
one that the answer to the question of

00:01:11,280 --> 00:01:17,159
life the universe and everything is 42

00:01:13,590 --> 00:01:20,010
and they'll be sure not panic and has

00:01:17,159 --> 00:01:23,490
been particularly popular in the tech

00:01:20,010 --> 00:01:25,320
community and one thing that is not as

00:01:23,490 --> 00:01:28,470
popular but that douglas adams also

00:01:25,320 --> 00:01:31,740
introduced is this this babel fish it's

00:01:28,470 --> 00:01:34,770
this fictitious fish at least somewhere

00:01:31,740 --> 00:01:37,650
in the universe that is tiny and if you

00:01:34,770 --> 00:01:40,229
you can put that into your ear and what

00:01:37,650 --> 00:01:42,710
this fish does is it picks up all the

00:01:40,229 --> 00:01:46,229
spoken languages around you and

00:01:42,710 --> 00:01:48,900
translates that into a language that you

00:01:46,229 --> 00:01:49,590
understand so any language in the

00:01:48,900 --> 00:01:51,390
universe

00:01:49,590 --> 00:01:52,680
it gets directly translated into a

00:01:51,390 --> 00:01:56,340
language that you understand if you have

00:01:52,680 --> 00:01:58,770
this fish in your in your ear and

00:01:56,340 --> 00:02:00,720
obviously there is fictitious and it's a

00:01:58,770 --> 00:02:02,610
crazy idea it's a fantastic but a crazy

00:02:00,720 --> 00:02:09,019
idea right that would never work at real

00:02:02,610 --> 00:02:11,850
life let's let me show you a little demo

00:02:09,019 --> 00:02:15,709
hello on heads if you come to mine for

00:02:11,850 --> 00:02:19,579
talk we bar conciliate elegans

00:02:15,709 --> 00:02:22,170
now if we look at this right this is

00:02:19,579 --> 00:02:25,260
almost as cool as putting a

00:02:22,170 --> 00:02:27,690
in your ear I mean admittedly you still

00:02:25,260 --> 00:02:29,370
I still have to click that button and we

00:02:27,690 --> 00:02:31,739
have to go to that website to translate

00:02:29,370 --> 00:02:33,450
it and it doesn't I could I could click

00:02:31,739 --> 00:02:35,069
this this listen button and would

00:02:33,450 --> 00:02:36,599
actually read it out but I would still

00:02:35,069 --> 00:02:40,980
need to click it but it's not quite

00:02:36,599 --> 00:02:44,930
there yet it's not quite this this baby

00:02:40,980 --> 00:02:48,930
fish that you put into your ear however

00:02:44,930 --> 00:02:51,510
Google also sells these in-ear wireless

00:02:48,930 --> 00:02:53,550
headphones and they use the same

00:02:51,510 --> 00:02:56,610
technology as as the Google Translate

00:02:53,550 --> 00:02:59,910
page that we just saw and they use the

00:02:56,610 --> 00:03:02,040
voice recognition and what they do is it

00:02:59,910 --> 00:03:04,260
is more or less the the exact same thing

00:03:02,040 --> 00:03:05,730
as this papers fish someone can speak

00:03:04,260 --> 00:03:08,069
something in the language but you don't

00:03:05,730 --> 00:03:11,150
understand you'll still need a phone a

00:03:08,069 --> 00:03:13,769
little bit but then the earphones do

00:03:11,150 --> 00:03:16,920
translate that and directly into your

00:03:13,769 --> 00:03:21,510
ear say the translation in for example

00:03:16,920 --> 00:03:24,090
English so this this fictitious fish

00:03:21,510 --> 00:03:28,829
which sounded so absolutely amazing

00:03:24,090 --> 00:03:32,209
right is due to the use of of AI and

00:03:28,829 --> 00:03:35,280
machine learning is almost already there

00:03:32,209 --> 00:03:38,040
and that is one of the fantastic

00:03:35,280 --> 00:03:41,630
examples where where a I am machine

00:03:38,040 --> 00:03:44,280
learning has definitely succeeded and

00:03:41,630 --> 00:03:46,980
there are more examples and this one in

00:03:44,280 --> 00:03:48,600
particular I mean the translation one is

00:03:46,980 --> 00:03:52,700
pretty good but this one is

00:03:48,600 --> 00:03:56,100
life-changing basically over the last

00:03:52,700 --> 00:03:56,880
couple of years people have been

00:03:56,100 --> 00:04:01,019
developing

00:03:56,880 --> 00:04:04,430
image recognition systems for cancer and

00:04:01,019 --> 00:04:09,180
in some cases machine learning nowadays

00:04:04,430 --> 00:04:12,959
can predict cancer with a higher rate

00:04:09,180 --> 00:04:15,630
than human doctors can so a couple of

00:04:12,959 --> 00:04:18,269
years ago I think it started with breast

00:04:15,630 --> 00:04:20,639
cancer and last year Google released a

00:04:18,269 --> 00:04:24,450
machine learning model that can based on

00:04:20,639 --> 00:04:26,700
pictures they that we can see here for

00:04:24,450 --> 00:04:30,150
example can predict that there are

00:04:26,700 --> 00:04:32,099
cancer cells more accurately than human

00:04:30,150 --> 00:04:33,659
doctors can and I think that is

00:04:32,099 --> 00:04:35,820
something that is really impressive and

00:04:33,659 --> 00:04:37,140
that is a great success story for

00:04:35,820 --> 00:04:39,600
for machine learning and for artificial

00:04:37,140 --> 00:04:41,310
intelligence but we all know and

00:04:39,600 --> 00:04:43,770
especially if you think of the title of

00:04:41,310 --> 00:04:47,070
this talk it doesn't always work that

00:04:43,770 --> 00:04:49,650
well right for example this person on

00:04:47,070 --> 00:04:51,660
the on the very right is sending a tweet

00:04:49,650 --> 00:04:55,770
to indigo which is an Indian airline and

00:04:51,660 --> 00:04:57,510
he's complaining about that his beggars

00:04:55,770 --> 00:05:00,900
who went to Hyderabad while he was

00:04:57,510 --> 00:05:04,200
flying to Calcutta and the response of

00:05:00,900 --> 00:05:07,410
indigo and I assumed this is in AI their

00:05:04,200 --> 00:05:09,180
responses we were glad to hear that so

00:05:07,410 --> 00:05:12,570
obviously they didn't they didn't pick

00:05:09,180 --> 00:05:15,720
up on the on the sarcastic tone that the

00:05:12,570 --> 00:05:17,750
user was using here and in the middle

00:05:15,720 --> 00:05:20,820
you can see the screens of someone

00:05:17,750 --> 00:05:23,520
contacting PayPal and and mentioning

00:05:20,820 --> 00:05:28,200
that he got scammed and papers responses

00:05:23,520 --> 00:05:30,870
great so clearly there are a couple of

00:05:28,200 --> 00:05:33,660
cases where AI where I simply fails and

00:05:30,870 --> 00:05:36,320
not even difficult cases right and this

00:05:33,660 --> 00:05:38,790
one is one of my favorite fails of AI

00:05:36,320 --> 00:05:40,500
and if you speak Mandarin you could

00:05:38,790 --> 00:05:43,380
already figure out what it what it says

00:05:40,500 --> 00:05:44,460
there and figure out what's going on for

00:05:43,380 --> 00:05:47,130
everyone else I give you a little bit of

00:05:44,460 --> 00:05:51,380
a backstory and in a couple of cities in

00:05:47,130 --> 00:05:54,380
China they have cameras installed at

00:05:51,380 --> 00:05:58,770
zebra crossing as pedestrian crossings

00:05:54,380 --> 00:05:59,850
cross streets and when there's a and

00:05:58,770 --> 00:06:02,850
there's a red light

00:05:59,850 --> 00:06:04,890
these cameras record the person crossing

00:06:02,850 --> 00:06:07,680
the street at red light the they record

00:06:04,890 --> 00:06:11,340
the person jaywalking and what they then

00:06:07,680 --> 00:06:14,730
do is they in this case what you can see

00:06:11,340 --> 00:06:16,560
here is a big screen and what what

00:06:14,730 --> 00:06:19,560
happens is if you get caught jaywalking

00:06:16,560 --> 00:06:21,620
with the camera they show your face and

00:06:19,560 --> 00:06:25,590
they zoom in on you and they basically

00:06:21,620 --> 00:06:28,590
shame you and make everyone see that you

00:06:25,590 --> 00:06:32,550
jaywalked and some other cities even

00:06:28,590 --> 00:06:36,600
have have the system connected to which

00:06:32,550 --> 00:06:38,850
set pay where they they record your face

00:06:36,600 --> 00:06:41,100
they use image recognition facial

00:06:38,850 --> 00:06:43,710
recognition they figure out who you are

00:06:41,100 --> 00:06:45,750
and when they know who you are they know

00:06:43,710 --> 00:06:48,720
your bank account they figure out your

00:06:45,750 --> 00:06:51,980
which at pay wallet and they immediately

00:06:48,720 --> 00:06:55,290
deduct the fine for jaywalking from your

00:06:51,980 --> 00:06:58,670
WeChat pack up your WeChat wallet

00:06:55,290 --> 00:07:01,860
immediately gets deducted that fine

00:06:58,670 --> 00:07:03,860
right and now with that background if we

00:07:01,860 --> 00:07:06,180
look at if we look at this picture here

00:07:03,860 --> 00:07:08,910
we can see this is not really a person

00:07:06,180 --> 00:07:12,600
crossing the street what happened here

00:07:08,910 --> 00:07:14,580
is there's a bus which has the face of a

00:07:12,600 --> 00:07:16,740
person printed on it in fact this is a

00:07:14,580 --> 00:07:19,320
the CEO of a company and and they're

00:07:16,740 --> 00:07:21,870
advertising with her on the bus and the

00:07:19,320 --> 00:07:24,990
bus like the the pedestrian had a red

00:07:21,870 --> 00:07:27,600
light and the bus drove by so the camera

00:07:24,990 --> 00:07:29,810
was pointing at the bus seeing the side

00:07:27,600 --> 00:07:32,940
of the bus seeing the face and

00:07:29,810 --> 00:07:35,570
recognizing oh there's a person crossing

00:07:32,940 --> 00:07:38,070
the street there's a red light and then

00:07:35,570 --> 00:07:39,720
showed that face oomp in on that face

00:07:38,070 --> 00:07:42,030
and say and so everyone hey this person

00:07:39,720 --> 00:07:44,490
just crossed the red light and in fact

00:07:42,030 --> 00:07:46,530
they even send a message to this woman

00:07:44,490 --> 00:07:49,020
which was a couple like in in a

00:07:46,530 --> 00:07:50,450
different City at that time and when she

00:07:49,020 --> 00:07:53,550
got the message hey we just saw you

00:07:50,450 --> 00:07:56,490
jaywalking now in that other City and

00:07:53,550 --> 00:07:58,650
this is where I mean from a technical

00:07:56,490 --> 00:08:00,660
perspective is a very interesting right

00:07:58,650 --> 00:08:03,540
like having this patient recognition

00:08:00,660 --> 00:08:05,310
having it connected to to your bank

00:08:03,540 --> 00:08:07,169
account more or less very interesting

00:08:05,310 --> 00:08:10,560
from a different perspective you could

00:08:07,169 --> 00:08:13,979
also say that if you let machine

00:08:10,560 --> 00:08:16,229
learning or AI decide whether you have

00:08:13,979 --> 00:08:18,090
committed a crime or not it's

00:08:16,229 --> 00:08:20,610
questionable and this is a great example

00:08:18,090 --> 00:08:23,960
why it is questionable because you can

00:08:20,610 --> 00:08:26,520
see that it also fails we have seen

00:08:23,960 --> 00:08:29,280
other cases where machine learning fails

00:08:26,520 --> 00:08:31,530
and if you don't want to you don't want

00:08:29,280 --> 00:08:33,950
to give it that much power if you can't

00:08:31,530 --> 00:08:36,360
be sure that it is that it does not fail

00:08:33,950 --> 00:08:38,130
so we've been talking about or I've been

00:08:36,360 --> 00:08:40,770
talking about artificial intelligence

00:08:38,130 --> 00:08:42,240
and machine learning a lot all of these

00:08:40,770 --> 00:08:44,159
terms have been hyped in the last couple

00:08:42,240 --> 00:08:46,500
of years and there lots of people have

00:08:44,159 --> 00:08:49,260
been talk have are talking about them

00:08:46,500 --> 00:08:50,339
and I think in the context of this talk

00:08:49,260 --> 00:08:52,290
it would be good to have a common

00:08:50,339 --> 00:08:55,170
understanding of what when I say these

00:08:52,290 --> 00:08:56,940
terms what I what I mean by them and we

00:08:55,170 --> 00:08:58,830
can start with artificial intelligence

00:08:56,940 --> 00:09:01,560
and there's this this definition that I

00:08:58,830 --> 00:09:02,579
really like that comes from the oxford

00:09:01,560 --> 00:09:04,980
dictionaries which

00:09:02,579 --> 00:09:07,259
by the way also the definition if you

00:09:04,980 --> 00:09:08,339
type in Google define artificial

00:09:07,259 --> 00:09:10,410
intelligence you also get this

00:09:08,339 --> 00:09:13,110
particular definition and this

00:09:10,410 --> 00:09:14,670
definition reads out as artificial

00:09:13,110 --> 00:09:16,709
intelligence is the theory and

00:09:14,670 --> 00:09:19,619
development of computer systems able to

00:09:16,709 --> 00:09:22,739
perform tasks normally requiring human

00:09:19,619 --> 00:09:24,809
intelligence and the important thing to

00:09:22,739 --> 00:09:28,619
note here is that it does not talk about

00:09:24,809 --> 00:09:30,059
any technical approach they are when we

00:09:28,619 --> 00:09:31,379
get to machine learning later we'll talk

00:09:30,059 --> 00:09:33,179
about technical pros artificial

00:09:31,379 --> 00:09:35,819
intelligence is a very broad term

00:09:33,179 --> 00:09:38,040
describing anything that a computer does

00:09:35,819 --> 00:09:41,249
that you would think takes human

00:09:38,040 --> 00:09:44,549
intelligence to do and a chess program

00:09:41,249 --> 00:09:46,350
is a good example where if you play

00:09:44,549 --> 00:09:48,239
chess against a human you would usually

00:09:46,350 --> 00:09:50,129
assume that person or either person can

00:09:48,239 --> 00:09:51,600
plate as you would assume the person is

00:09:50,129 --> 00:09:54,569
some kind of some kind of level of

00:09:51,600 --> 00:09:56,850
intelligence now if you think about how

00:09:54,569 --> 00:09:58,769
we would implement a chess program like

00:09:56,850 --> 00:10:01,319
a computer to play chess

00:09:58,769 --> 00:10:03,239
it could be relatively simple approach

00:10:01,319 --> 00:10:05,149
right you could just write a program

00:10:03,239 --> 00:10:09,059
that looked at the current state and

00:10:05,149 --> 00:10:13,139
then brute forces all the possible moves

00:10:09,059 --> 00:10:15,629
and subsequent moves to see which set of

00:10:13,139 --> 00:10:17,869
moves arrives as a desired state which

00:10:15,629 --> 00:10:21,509
is winning that that chess match and

00:10:17,869 --> 00:10:24,299
brute forcing and a couple of if else's

00:10:21,509 --> 00:10:27,449
is not a very sophisticated technical

00:10:24,299 --> 00:10:29,129
approach it would however fit into into

00:10:27,449 --> 00:10:31,860
this this definition of artificial

00:10:29,129 --> 00:10:33,929
intelligence so from my understanding

00:10:31,860 --> 00:10:35,489
and following this definition artificial

00:10:33,929 --> 00:10:38,549
intelligence is very broad term that

00:10:35,489 --> 00:10:40,919
just describes a use case where you

00:10:38,549 --> 00:10:43,410
would assume human intelligence is

00:10:40,919 --> 00:10:45,059
necessary done by a machine machine

00:10:43,410 --> 00:10:47,759
learning on the other hand is where it

00:10:45,059 --> 00:10:49,769
becomes interesting machine learning

00:10:47,759 --> 00:10:51,360
describes the technical approach to

00:10:49,769 --> 00:10:54,749
solving a problem and in machine

00:10:51,360 --> 00:10:57,360
learning the algorithm to solve a

00:10:54,749 --> 00:10:58,949
problem is not actually implemented by a

00:10:57,360 --> 00:11:02,249
programmer it's not implemented by a

00:10:58,949 --> 00:11:05,069
software engineer the data you feed into

00:11:02,249 --> 00:11:08,489
a machine learning model defines the

00:11:05,069 --> 00:11:10,319
algorithm to solve that problem you feed

00:11:08,489 --> 00:11:12,119
a different data and it solves the

00:11:10,319 --> 00:11:14,129
problem differently you feed in more

00:11:12,119 --> 00:11:15,959
data and it solves it more accurately in

00:11:14,129 --> 00:11:16,430
certain cases right you like you give it

00:11:15,959 --> 00:11:18,860
some day

00:11:16,430 --> 00:11:20,510
I this is the input data and I expect

00:11:18,860 --> 00:11:24,230
that output and do that a couple of

00:11:20,510 --> 00:11:28,250
times and it learns from that input data

00:11:24,230 --> 00:11:31,630
what output you are expecting neural

00:11:28,250 --> 00:11:34,820
networks is as you can see a subset of

00:11:31,630 --> 00:11:36,860
machine learning and neural networks is

00:11:34,820 --> 00:11:40,339
just a way of building a machine

00:11:36,860 --> 00:11:43,640
learning model that is inspired by by

00:11:40,339 --> 00:11:45,980
the human brain or a brain a biological

00:11:43,640 --> 00:11:49,029
brain in general with neurons and they

00:11:45,980 --> 00:11:51,200
are connected and deep learning which

00:11:49,029 --> 00:11:54,380
surely you have heard which is one of

00:11:51,200 --> 00:11:56,959
the most hype terms in the soul have

00:11:54,380 --> 00:11:58,760
lotta topic machine learning is a subset

00:11:56,959 --> 00:12:01,100
of neural networks of artificial neural

00:11:58,760 --> 00:12:04,220
networks which just means that it has

00:12:01,100 --> 00:12:08,870
particularly many layers of neurons that

00:12:04,220 --> 00:12:11,810
are connected to each other and so we

00:12:08,870 --> 00:12:14,990
have been we've been seeing how these

00:12:11,810 --> 00:12:18,170
machine learning models fail and one of

00:12:14,990 --> 00:12:21,170
the reasons why it fails is that they

00:12:18,170 --> 00:12:23,570
are just very complex as you can see

00:12:21,170 --> 00:12:26,870
here this is a is actually a very simple

00:12:23,570 --> 00:12:30,860
deep neural network in real life these

00:12:26,870 --> 00:12:33,320
deep neural networks can have thousands

00:12:30,860 --> 00:12:37,490
of nodes and and fifty or hundred or

00:12:33,320 --> 00:12:39,529
hundreds of layers and basically all

00:12:37,490 --> 00:12:41,390
these all these nodes do like some small

00:12:39,529 --> 00:12:43,550
computation side and they are connected

00:12:41,390 --> 00:12:45,290
and the result goes into some other node

00:12:43,550 --> 00:12:47,510
and that result goes to some other node

00:12:45,290 --> 00:12:49,360
and there are a lot of them are

00:12:47,510 --> 00:12:52,670
interconnected in this case all of them

00:12:49,360 --> 00:12:55,760
they are fully connected and I think the

00:12:52,670 --> 00:12:57,080
point here to understand is that these

00:12:55,760 --> 00:13:00,620
machine learning models that we use

00:12:57,080 --> 00:13:02,870
nowadays are so complex that it is it is

00:13:00,620 --> 00:13:06,020
virtually impossible for a human to

00:13:02,870 --> 00:13:08,630
understand why things happened in a

00:13:06,020 --> 00:13:10,730
certain way why certain input resulted

00:13:08,630 --> 00:13:13,820
in a certain output why this model

00:13:10,730 --> 00:13:16,490
decided this or and not the other way

00:13:13,820 --> 00:13:18,920
there's it's so complicated that we can

00:13:16,490 --> 00:13:21,950
simply not not see that so that is one

00:13:18,920 --> 00:13:24,200
of the reasons why machine learning

00:13:21,950 --> 00:13:26,810
fails like you test a couple you test a

00:13:24,200 --> 00:13:29,089
lot of cases but you can never cover all

00:13:26,810 --> 00:13:30,410
of the cases right and

00:13:29,089 --> 00:13:34,189
they're clearly cases that will fail

00:13:30,410 --> 00:13:36,350
because you simply couldn't anticipate

00:13:34,189 --> 00:13:40,040
that the model would react in that way

00:13:36,350 --> 00:13:41,689
because it's simply too complicated and

00:13:40,040 --> 00:13:45,249
let me tell you about this other story

00:13:41,689 --> 00:13:48,319
which is a which is a great story from

00:13:45,249 --> 00:13:50,809
2011 in 2011 there was the city of

00:13:48,319 --> 00:13:53,629
Boston the City of Boston had a problem

00:13:50,809 --> 00:13:56,149
and it was potholes they had too many

00:13:53,629 --> 00:13:58,819
potholes in the city and they wanted to

00:13:56,149 --> 00:14:00,949
find a way to solve that problem and it

00:13:58,819 --> 00:14:04,670
was 2011 they did what everyone did in

00:14:00,949 --> 00:14:06,709
2011 they built an app so they paid an

00:14:04,670 --> 00:14:09,290
app for that and it was actually a

00:14:06,709 --> 00:14:11,329
pretty smart app it worked in a way that

00:14:09,290 --> 00:14:13,519
people would install the app on their

00:14:11,329 --> 00:14:15,769
phone and then put the phone on the

00:14:13,519 --> 00:14:18,709
passenger seat of a car and then they

00:14:15,769 --> 00:14:20,420
would drive around and when there was

00:14:18,709 --> 00:14:23,059
some vibration and the phone noticed

00:14:20,420 --> 00:14:25,730
there was a bump it saved the GPS

00:14:23,059 --> 00:14:28,100
coordinates of where the phone was at

00:14:25,730 --> 00:14:31,550
that time and send it to the server of

00:14:28,100 --> 00:14:33,740
the city of Boston so then basically the

00:14:31,550 --> 00:14:35,420
city of Boston could map where all the

00:14:33,740 --> 00:14:37,699
phones detected potholes and could go

00:14:35,420 --> 00:14:39,379
and fix them now there was one very

00:14:37,699 --> 00:14:43,490
interesting thing that the people found

00:14:39,379 --> 00:14:46,399
out and it looked like only in the in

00:14:43,490 --> 00:14:49,999
the areas with the rich people only in

00:14:46,399 --> 00:14:53,600
the high cost of living areas where

00:14:49,999 --> 00:14:55,850
potholes found and clearly there is a

00:14:53,600 --> 00:14:58,639
there's a very interesting result right

00:14:55,850 --> 00:15:01,879
why why would only in rich areas be

00:14:58,639 --> 00:15:03,050
potholes and so the thing that that

00:15:01,879 --> 00:15:07,550
people didn't think of in the first

00:15:03,050 --> 00:15:09,110
place was it was 2011 a smartphone that

00:15:07,550 --> 00:15:12,529
could install an end or and that could

00:15:09,110 --> 00:15:14,120
install an app was very expensive at

00:15:12,529 --> 00:15:16,550
that time not everyone had a smartphone

00:15:14,120 --> 00:15:18,800
it wasn't as ubiquitous as it is now

00:15:16,550 --> 00:15:20,660
where every five-year-old child is

00:15:18,800 --> 00:15:23,899
watching tick tock videos on their

00:15:20,660 --> 00:15:25,999
smartphone 2011 was it was a different

00:15:23,899 --> 00:15:28,160
time and and what that meant was only

00:15:25,999 --> 00:15:31,639
the rich people had the smartphone and

00:15:28,160 --> 00:15:34,579
installed the app so the interesting

00:15:31,639 --> 00:15:36,170
thing here to notice there was no

00:15:34,579 --> 00:15:40,730
machine learning involves there was no

00:15:36,170 --> 00:15:42,649
AI whatsoever the the thing that went

00:15:40,730 --> 00:15:47,180
wrong here was that the

00:15:42,649 --> 00:15:52,069
data was wrong the data only came from a

00:15:47,180 --> 00:15:54,019
certain subset from a subset of the of

00:15:52,069 --> 00:15:56,240
the citizens of the City of Boston and

00:15:54,019 --> 00:15:58,999
that's what made the data wrong there

00:15:56,240 --> 00:16:00,740
were obviously potholes in in poor

00:15:58,999 --> 00:16:02,779
living areas and they found that later

00:16:00,740 --> 00:16:04,999
out because they they fix it which I

00:16:02,779 --> 00:16:09,559
also have to admit is particularly smart

00:16:04,999 --> 00:16:11,509
they put the phone on the app in public

00:16:09,559 --> 00:16:13,490
buses and on garbage trucks that would

00:16:11,509 --> 00:16:15,649
basically cover the whole city and

00:16:13,490 --> 00:16:21,439
that's when they figured out that

00:16:15,649 --> 00:16:24,139
they're also obviously potholes in areas

00:16:21,439 --> 00:16:25,910
with the lower cost of living but the

00:16:24,139 --> 00:16:27,949
the interesting thing here is to note

00:16:25,910 --> 00:16:30,230
that there was no machine learning and

00:16:27,949 --> 00:16:35,529
yet it failed because the data was long

00:16:30,230 --> 00:16:35,529
and now let me show you another example

00:16:40,899 --> 00:16:46,399
let's let's see what happens if i

00:16:44,929 --> 00:16:52,569
strands late the following sentences

00:16:46,399 --> 00:16:59,029
from English to Malay let's say she is

00:16:52,569 --> 00:17:00,470
doctor and he is the nurse and there

00:16:59,029 --> 00:17:03,019
also something similar happened a couple

00:17:00,470 --> 00:17:07,339
of years ago and got popular in the

00:17:03,019 --> 00:17:09,740
turkish translation so why I selected

00:17:07,339 --> 00:17:15,909
Malay here is particularly interesting

00:17:09,740 --> 00:17:19,909
because Malay has the male does have

00:17:15,909 --> 00:17:22,250
gender neutral preposition so these this

00:17:19,909 --> 00:17:24,770
XI and this he that I'm using here in

00:17:22,250 --> 00:17:27,980
English results in the same word in

00:17:24,770 --> 00:17:29,659
Malay right so that it kind of loses the

00:17:27,980 --> 00:17:32,450
gender and now we see how it translates

00:17:29,659 --> 00:17:35,179
into into male and if I now translate

00:17:32,450 --> 00:17:38,059
this Malay back into English see what

00:17:35,179 --> 00:17:43,309
happens remember I wrote she is a doctor

00:17:38,059 --> 00:17:45,470
and he is a nurse if I translate that

00:17:43,309 --> 00:17:47,270
back if I translate the exact same that

00:17:45,470 --> 00:17:50,179
we just received one by translating it

00:17:47,270 --> 00:17:54,559
back into English we get he's a doctor

00:17:50,179 --> 00:17:55,570
and she is a nurse now how did that

00:17:54,559 --> 00:17:57,720
happen right

00:17:55,570 --> 00:18:01,989
clearly we saw translating it into Malay

00:17:57,720 --> 00:18:04,779
the the gender information got lost but

00:18:01,989 --> 00:18:07,389
yet somehow now this introduced again

00:18:04,779 --> 00:18:09,279
but wrongly like we had she is the

00:18:07,389 --> 00:18:11,200
doctor and all of a sudden it's him all

00:18:09,279 --> 00:18:13,659
of a sudden as he is a doctor and the

00:18:11,200 --> 00:18:16,629
reason for that again is it's the

00:18:13,659 --> 00:18:18,519
historical data that is being used this

00:18:16,629 --> 00:18:22,029
Google Translate and other translation

00:18:18,519 --> 00:18:25,479
service basically read a lot of text

00:18:22,029 --> 00:18:27,489
they have text in in that in multiple

00:18:25,479 --> 00:18:28,929
languages and they read Board of

00:18:27,489 --> 00:18:31,239
Alderman compare okay this sentence

00:18:28,929 --> 00:18:34,119
translates to they listen and what they

00:18:31,239 --> 00:18:37,979
found in the historical text in there

00:18:34,119 --> 00:18:42,729
like the historical data that doctors

00:18:37,979 --> 00:18:46,080
were mostly male and nurses are mostly

00:18:42,729 --> 00:18:49,090
female and based on that historical data

00:18:46,080 --> 00:18:52,119
this is the translation that best fits

00:18:49,090 --> 00:18:53,169
this male sentence and there is

00:18:52,119 --> 00:18:55,299
something that we can see there's

00:18:53,169 --> 00:18:57,700
clearly a sexist approach right nowadays

00:18:55,299 --> 00:19:00,399
we know that there's no reason why a

00:18:57,700 --> 00:19:04,149
woman can't be a doctor or a man can

00:19:00,399 --> 00:19:06,580
can't be a nurse but I think that that

00:19:04,149 --> 00:19:08,080
again shows how even though there is

00:19:06,580 --> 00:19:12,759
machine learning this particular issue

00:19:08,080 --> 00:19:16,779
is based on the on the 40 data to begin

00:19:12,759 --> 00:19:19,029
with and it was discussed some time ago

00:19:16,779 --> 00:19:21,039
a couple of years it came came public

00:19:19,029 --> 00:19:22,570
when that the same issue was found in

00:19:21,039 --> 00:19:26,259
Turkish because there's same genderless

00:19:22,570 --> 00:19:28,960
pronouns and what Google introduced then

00:19:26,259 --> 00:19:32,320
was they knew about that issue and they

00:19:28,960 --> 00:19:37,239
they introduced both options both gender

00:19:32,320 --> 00:19:39,279
options to show right let's talk about

00:19:37,239 --> 00:19:43,269
some other issues of data right there is

00:19:39,279 --> 00:19:47,710
a the compares system you have probably

00:19:43,269 --> 00:19:51,129
not heard of it unless you are in the US

00:19:47,710 --> 00:19:55,539
and might have been to court this

00:19:51,129 --> 00:20:00,669
compassed system is used in the u.s.

00:19:55,539 --> 00:20:01,840
American jurisdiction and it is as it

00:20:00,669 --> 00:20:03,759
says a correctional offender management

00:20:01,840 --> 00:20:07,090
profiling for Toronto sanctions system

00:20:03,759 --> 00:20:08,559
it basically what it does is people

00:20:07,090 --> 00:20:11,230
who've committed a crime fill

00:20:08,559 --> 00:20:13,539
to form like 200 something questions and

00:20:11,230 --> 00:20:17,200
based on how they fill out that form

00:20:13,539 --> 00:20:20,740
this computer system predicts whether

00:20:17,200 --> 00:20:24,279
they will commit a crime again or not

00:20:20,740 --> 00:20:26,470
and based on that outcome the the bail

00:20:24,279 --> 00:20:29,379
that they have to pay and the time they

00:20:26,470 --> 00:20:32,320
are going to spend in jail is adjusted

00:20:29,379 --> 00:20:33,909
to that so if a judge sees oh it's very

00:20:32,320 --> 00:20:38,200
likely that that person commits the

00:20:33,909 --> 00:20:43,090
crime again they and the sentence that

00:20:38,200 --> 00:20:45,369
they speak will be higher than if the

00:20:43,090 --> 00:20:48,309
the system would have said it was a very

00:20:45,369 --> 00:20:54,369
clean guy he won't commit a crime again

00:20:48,309 --> 00:20:59,850
and obviously that fails right if we

00:20:54,369 --> 00:21:02,769
look here at the number of people that

00:20:59,850 --> 00:21:09,480
were labeled a higher risk but did not

00:21:02,769 --> 00:21:12,850
reoffend so that is the false positive a

00:21:09,480 --> 00:21:15,460
false negative well however you want

00:21:12,850 --> 00:21:19,240
right you can see that that only

00:21:15,460 --> 00:21:21,399
affected 23 percent 23 percent of the

00:21:19,240 --> 00:21:25,240
white people got labeled higher risk and

00:21:21,399 --> 00:21:28,090
did not do anything but almost 45

00:21:25,240 --> 00:21:30,009
percent of african-americans that got

00:21:28,090 --> 00:21:30,610
labeled a higher risk and did not

00:21:30,009 --> 00:21:33,369
reoffend

00:21:30,610 --> 00:21:35,110
and if you turn it around people who got

00:21:33,369 --> 00:21:38,409
estimated to have lower risk of

00:21:35,110 --> 00:21:41,559
reoffending but then did actually commit

00:21:38,409 --> 00:21:43,779
a crime again that is particularly true

00:21:41,559 --> 00:21:46,690
less for almost 50 percent for white

00:21:43,779 --> 00:21:50,470
people and for african-americans only 28

00:21:46,690 --> 00:21:53,080
percent so what this means is that the

00:21:50,470 --> 00:21:55,779
data that is being put into that system

00:21:53,080 --> 00:21:57,759
and there are very generic questions

00:21:55,779 --> 00:22:00,429
there are questions that are about the

00:21:57,759 --> 00:22:03,039
neighborhood we live in the questions

00:22:00,429 --> 00:22:04,960
about your parents and so on based on

00:22:03,039 --> 00:22:07,779
that and there's no information about

00:22:04,960 --> 00:22:10,210
whether people are Caucasian or African

00:22:07,779 --> 00:22:16,950
American right but based on on the data

00:22:10,210 --> 00:22:19,480
that is put in the system has a clearly

00:22:16,950 --> 00:22:22,919
racist position against black people

00:22:19,480 --> 00:22:26,080
where it is significantly

00:22:22,919 --> 00:22:29,559
higher percentage of label them

00:22:26,080 --> 00:22:32,200
labelling them as high reoffending

00:22:29,559 --> 00:22:35,230
chance and the same thing the opposite

00:22:32,200 --> 00:22:37,179
is the case if you are white and the and

00:22:35,230 --> 00:22:42,240
this system is still being used and this

00:22:37,179 --> 00:22:47,770
analysis was done by Pro Publica

00:22:42,240 --> 00:22:51,549
and they that is very interesting since

00:22:47,770 --> 00:22:53,260
the the there was no in this in this

00:22:51,549 --> 00:22:57,179
questionnaire there was no mentioning of

00:22:53,260 --> 00:23:01,630
your after skin color or your ethnicity

00:22:57,179 --> 00:23:04,570
whatsoever but based on other aspects

00:23:01,630 --> 00:23:07,960
the pseudo identifiers in a way in a way

00:23:04,570 --> 00:23:09,760
you go the system figured out ok there's

00:23:07,960 --> 00:23:13,210
a black person and then discriminated

00:23:09,760 --> 00:23:15,640
against them and that is still being in

00:23:13,210 --> 00:23:17,679
jiwoo's nowadays and there are a couple

00:23:15,640 --> 00:23:21,070
of other situations there's there's a

00:23:17,679 --> 00:23:25,510
study as you can read here that found

00:23:21,070 --> 00:23:29,020
out that self-driving cars have a higher

00:23:25,510 --> 00:23:31,090
risk of failing to detect dark-skinned

00:23:29,020 --> 00:23:33,460
pedestrians and what that means is

00:23:31,090 --> 00:23:36,820
there's a car driving safe driving right

00:23:33,460 --> 00:23:39,340
no driver and it what it's doing is had

00:23:36,820 --> 00:23:41,530
a couple of cameras right and and lidar

00:23:39,340 --> 00:23:43,900
radars around to identify the

00:23:41,530 --> 00:23:49,210
environment to not do not run into

00:23:43,900 --> 00:23:52,179
people and the systems have lower chance

00:23:49,210 --> 00:23:56,020
of registering dark-skinned people as

00:23:52,179 --> 00:23:58,720
pedestrians that means there if there is

00:23:56,020 --> 00:24:02,230
a white person it is more likely to be

00:23:58,720 --> 00:24:04,390
identified and and not being driven over

00:24:02,230 --> 00:24:06,040
where if it's a dark-skinned person

00:24:04,390 --> 00:24:07,510
there is a higher chance so there the

00:24:06,040 --> 00:24:10,200
person gets not identified as the

00:24:07,510 --> 00:24:15,490
pedestrian and gets potentially killed

00:24:10,200 --> 00:24:18,490
and I think that is again the the reason

00:24:15,490 --> 00:24:21,520
for that is most likely that the data

00:24:18,490 --> 00:24:24,370
that was used to train those models to

00:24:21,520 --> 00:24:26,350
train the image recognition was mostly

00:24:24,370 --> 00:24:29,440
white skinned people and therefore it

00:24:26,350 --> 00:24:31,840
was not trained to identify dark-skinned

00:24:29,440 --> 00:24:36,040
pedestrians and that's just that that

00:24:31,840 --> 00:24:37,080
will kill people if that gets widely

00:24:36,040 --> 00:24:43,240
deployed

00:24:37,080 --> 00:24:45,190
so basically if you feed bad data into a

00:24:43,240 --> 00:24:49,539
machine learning model it will react in

00:24:45,190 --> 00:24:52,240
a certain way right now you're looking

00:24:49,539 --> 00:24:52,799
at me saying I call what you're telling

00:24:52,240 --> 00:24:54,850
us

00:24:52,799 --> 00:24:56,200
machine learning starts machine learning

00:24:54,850 --> 00:24:59,140
is everywhere the world is on fire I'll

00:24:56,200 --> 00:25:02,019
be all doomed and I can tell you we are

00:24:59,140 --> 00:25:04,960
not but they are a couple of things that

00:25:02,019 --> 00:25:07,059
we need to do and when we decide on what

00:25:04,960 --> 00:25:08,769
machine learning model we want to use we

00:25:07,059 --> 00:25:11,620
want to follow a couple of criteria and

00:25:08,769 --> 00:25:13,480
explain ability as the first class model

00:25:11,620 --> 00:25:15,220
selection criteria is one of them that

00:25:13,480 --> 00:25:18,100
means we need to understand why a model

00:25:15,220 --> 00:25:20,500
aside in a certain way for this

00:25:18,100 --> 00:25:22,809
particular case of convolutional neural

00:25:20,500 --> 00:25:25,659
networks which is mostly used for image

00:25:22,809 --> 00:25:29,799
recognition and you can see here on the

00:25:25,659 --> 00:25:32,529
bottom there is each layer there is a

00:25:29,799 --> 00:25:36,909
form of visualization that lets you see

00:25:32,529 --> 00:25:39,190
what each layer is deciding on what each

00:25:36,909 --> 00:25:41,409
layer is recognizing and then making a

00:25:39,190 --> 00:25:44,470
decision based on that

00:25:41,409 --> 00:25:45,970
in a other way if stress what we need to

00:25:44,470 --> 00:25:48,399
change we need to change from our

00:25:45,970 --> 00:25:52,210
standard ml approach to a to an approach

00:25:48,399 --> 00:25:55,029
where we use interpretability

00:25:52,210 --> 00:25:59,500
and feed that into the model and then I

00:25:55,029 --> 00:26:01,840
train the model again there's there's a

00:25:59,500 --> 00:26:03,639
technique that I that I really like

00:26:01,840 --> 00:26:06,820
which is called lime which stands for

00:26:03,639 --> 00:26:11,250
local interpreter will model agnostic

00:26:06,820 --> 00:26:14,380
explanations and what this does is it

00:26:11,250 --> 00:26:16,960
basically looks at your model and it is

00:26:14,380 --> 00:26:20,649
model agnostic it goes for for random

00:26:16,960 --> 00:26:23,799
fork for various machine learning models

00:26:20,649 --> 00:26:26,620
it goes for your networks and what it

00:26:23,799 --> 00:26:28,480
does is it it tries to explain why

00:26:26,620 --> 00:26:30,789
things happen and if we take for example

00:26:28,480 --> 00:26:32,860
this there's image recognition where

00:26:30,789 --> 00:26:35,440
this the image that we can see on the

00:26:32,860 --> 00:26:43,059
left was taken and it was identified as

00:26:35,440 --> 00:26:47,620
a as a as a tree frog with the 54% score

00:26:43,059 --> 00:26:49,480
and what this line then does is and this

00:26:47,620 --> 00:26:53,500
is a nice visualization

00:26:49,480 --> 00:26:55,900
it takes away certain data from the

00:26:53,500 --> 00:26:59,049
trainings data set from the from the

00:26:55,900 --> 00:27:02,799
test data and then see how the model

00:26:59,049 --> 00:27:05,140
what the model does predict and you can

00:27:02,799 --> 00:27:07,630
and what that means is you just throw

00:27:05,140 --> 00:27:10,059
out some data and if it still predicts

00:27:07,630 --> 00:27:14,200
the same thing you know the data that

00:27:10,059 --> 00:27:15,910
you threw out is irrelevant right the

00:27:14,200 --> 00:27:18,610
data that you kept is relevant and you

00:27:15,910 --> 00:27:20,770
can see here on the on the Frog you keep

00:27:18,610 --> 00:27:22,690
the eyes and you keep the green face and

00:27:20,770 --> 00:27:24,429
that is the relevant part and if you see

00:27:22,690 --> 00:27:26,890
in the middle page you keep the

00:27:24,429 --> 00:27:32,020
background that is irrelevant for the

00:27:26,890 --> 00:27:35,710
prediction of this of this image another

00:27:32,020 --> 00:27:37,690
thing is we need more diverse teams we

00:27:35,710 --> 00:27:40,679
need to have a different perspective on

00:27:37,690 --> 00:27:44,950
things right if the you remember the the

00:27:40,679 --> 00:27:48,360
automatic autonomous driving cars if

00:27:44,950 --> 00:27:51,340
there were more team if there were more

00:27:48,360 --> 00:27:54,130
higher diversity in the team maybe they

00:27:51,340 --> 00:27:55,630
would have tested for dark screen

00:27:54,130 --> 00:27:59,380
pedestrians on the pedestrian

00:27:55,630 --> 00:28:01,600
recognition system another thing that I

00:27:59,380 --> 00:28:07,090
that I really like is there's this data

00:28:01,600 --> 00:28:10,090
ethics canvas which gets published by

00:28:07,090 --> 00:28:12,700
the open data Institute and it basically

00:28:10,090 --> 00:28:15,880
lets you ask a couple of questions like

00:28:12,700 --> 00:28:18,340
it has a list of questions and before

00:28:15,880 --> 00:28:19,990
you start defining what mall you want to

00:28:18,340 --> 00:28:22,120
use you have to ask yourself these

00:28:19,990 --> 00:28:23,710
questions like who you share the data

00:28:22,120 --> 00:28:25,240
with what are the limitations of your

00:28:23,710 --> 00:28:28,030
data sources and so on basically lists

00:28:25,240 --> 00:28:30,130
that you can go through and see what you

00:28:28,030 --> 00:28:34,660
need to do sometimes you need to remove

00:28:30,130 --> 00:28:36,460
data do you really need like if you have

00:28:34,660 --> 00:28:39,850
a racial data and your data set and you

00:28:36,460 --> 00:28:41,500
try to predict something where you don't

00:28:39,850 --> 00:28:43,210
need that information do you should you

00:28:41,500 --> 00:28:47,440
just keep it in maybe just throw it out

00:28:43,210 --> 00:28:48,880
right it could if it if you if you don't

00:28:47,440 --> 00:28:51,750
need that data you can throw it out

00:28:48,880 --> 00:28:54,250
before it starts affecting your result

00:28:51,750 --> 00:29:01,210
there so this other thing that I really

00:28:54,250 --> 00:29:03,160
like which is open source visualization

00:29:01,210 --> 00:29:04,900
tool from Google

00:29:03,160 --> 00:29:07,540
which is festive switch you can just

00:29:04,900 --> 00:29:10,180
feed in data and you can just select

00:29:07,540 --> 00:29:14,550
certain subsets and you can see or in

00:29:10,180 --> 00:29:16,990
this case for example 85% of my

00:29:14,550 --> 00:29:19,540
participants in that data were male so

00:29:16,990 --> 00:29:22,930
clearly you are your data astute into

00:29:19,540 --> 00:29:25,240
that into the male category and you have

00:29:22,930 --> 00:29:30,730
underrepresented females for example or

00:29:25,240 --> 00:29:34,270
other genders and a similar thing is the

00:29:30,730 --> 00:29:37,090
what-if tool which lets you compare

00:29:34,270 --> 00:29:40,330
multiple models with the same workflow

00:29:37,090 --> 00:29:42,700
and integrates well with the facets

00:29:40,330 --> 00:29:47,050
system that I just showed you and it

00:29:42,700 --> 00:29:53,170
lets you inspect a little bit how your

00:29:47,050 --> 00:29:56,350
model would react in certain ways yes so

00:29:53,170 --> 00:29:58,990
I hope I could show you that there are a

00:29:56,350 --> 00:30:00,430
couple of issues with using machine

00:29:58,990 --> 00:30:03,510
learning Weisberg machine learning

00:30:00,430 --> 00:30:06,550
algorithms basically based on two things

00:30:03,510 --> 00:30:08,770
number one is that if you don't pay

00:30:06,550 --> 00:30:10,290
attention you basically have a black box

00:30:08,770 --> 00:30:13,210
and you don't know how your machine

00:30:10,290 --> 00:30:15,910
learning model reacts and in addition to

00:30:13,210 --> 00:30:19,870
that that your data might be screwed and

00:30:15,910 --> 00:30:22,960
if you don't figure out the player data

00:30:19,870 --> 00:30:24,640
is lacking and what underrepresented

00:30:22,960 --> 00:30:27,280
groups you have you might end up with

00:30:24,640 --> 00:30:29,590
wrong data but I hope I also showed you

00:30:27,280 --> 00:30:31,990
that there are certain ways how to how

00:30:29,590 --> 00:30:34,950
to fix those problems and with that

00:30:31,990 --> 00:30:38,400
being said thanks for your attention and

00:30:34,950 --> 00:30:38,400
let me know your questions

00:30:39,950 --> 00:30:56,459

YouTube URL: https://www.youtube.com/watch?v=IMxOlAxh7VU


