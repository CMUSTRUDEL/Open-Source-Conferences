Title: stackconf online 2020 |Challenges in Building Multi-Cloud-Provider Platform with Managed Kubernetes
Publication date: 2020-06-27
Playlist: stackconf online 2020
Description: 
	...by JÃ¶rg Schad


Building a cloud-agnostic platform used to be a challenging task as one had to deal with a large number of different cloud APIs and service offerings. Today, as most Cloud providers are offering a managed Kubernetes solution (e.g., GKE, AKS, or EKS), it seems like developers could simply build a platform based on Kubernetes and be cloud-agnostic. While this assumption is mostly correct, there are still a number of differences and pitfalls when deploying across those managed Kubernetes solutions. This talk discusses the experiences made while building the ArangoDB Managed Service offering across and GKE, AKS, or EKS. While the (managed) Kubernetes API being a great abstraction from the actual cloud provider, a number of challenges remain including for example networking, autoscaler, cluster provisioning, or node sizing. This talk provides an overview of those challenges and also discusses how they were solved as part of the ArangoDB managed Service.


NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de/
Blog: http://blog.netways.de/
NWS: https://nws.netways.de 

Webinare
Archiv Link: https://www.netways.de/webinare/archi...
Aktuell: https://www.netways.de/wb

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh/

Music: Walking on Air - https://www.frametraxx.de/
Captions: 
	00:00:00,630 --> 00:00:12,539
you

00:00:00,780 --> 00:00:15,129
[Music]

00:00:12,539 --> 00:00:17,410
welcome to our talk it's tech hump

00:00:15,129 --> 00:00:19,210
online challenges in building a multi

00:00:17,410 --> 00:00:22,480
cloud provider platform with managed

00:00:19,210 --> 00:00:25,180
kubernetes if you wonder what this is

00:00:22,480 --> 00:00:26,890
all about it's basically about building

00:00:25,180 --> 00:00:30,279
a managed database service across

00:00:26,890 --> 00:00:32,439
multiple cloud vendors we have taken on

00:00:30,279 --> 00:00:35,289
this challenge over the past one and a

00:00:32,439 --> 00:00:37,930
half years and kind of tricked LDRs it's

00:00:35,289 --> 00:00:40,210
still challenging kubernetes is a great

00:00:37,930 --> 00:00:42,879
abstraction and especially managed

00:00:40,210 --> 00:00:45,579
kubernetes solutions such as Amazon eks

00:00:42,879 --> 00:00:47,859
or Google kubernetes engine they also

00:00:45,579 --> 00:00:51,219
remove a lot of the operational effort

00:00:47,859 --> 00:00:53,289
in for a managed database or other

00:00:51,219 --> 00:00:55,600
service you might want to build but

00:00:53,289 --> 00:00:58,030
there are also its challenges because

00:00:55,600 --> 00:00:59,649
when trying to support different cloud

00:00:58,030 --> 00:01:02,019
providers you have to take care of

00:00:59,649 --> 00:01:04,839
different implementations for security

00:01:02,019 --> 00:01:07,299
authentication authorization different

00:01:04,839 --> 00:01:09,190
networking solutions storage different

00:01:07,299 --> 00:01:11,979
kubernetes versions and to upgrade

00:01:09,190 --> 00:01:14,680
policies in particular container

00:01:11,979 --> 00:01:17,170
runtimes and blogging and this is what

00:01:14,680 --> 00:01:18,880
we'll discuss in this talk so basically

00:01:17,170 --> 00:01:21,250
this talk is for you if you are

00:01:18,880 --> 00:01:25,240
interested in implementing your service

00:01:21,250 --> 00:01:27,930
either on kubernetes on top of one of

00:01:25,240 --> 00:01:30,760
the different kubernetes managed

00:01:27,930 --> 00:01:33,760
services by any of your favorite cloud

00:01:30,760 --> 00:01:36,160
providers or even if you are trying to

00:01:33,760 --> 00:01:41,410
implement it across multiple of those

00:01:36,160 --> 00:01:43,990
cloud vendors so first of all the team

00:01:41,410 --> 00:01:46,360
behind Arango DB oasis so this is he

00:01:43,990 --> 00:01:48,730
managed a database service we've been

00:01:46,360 --> 00:01:50,770
talking about here I just want to give

00:01:48,730 --> 00:01:54,340
kudos to the team who has actually built

00:01:50,770 --> 00:01:56,230
this and so as a team leaders Evod and I

00:01:54,340 --> 00:01:57,910
think he and his team they have done a

00:01:56,230 --> 00:02:01,000
really great job in building this

00:01:57,910 --> 00:02:05,200
managed database service so feel free to

00:02:01,000 --> 00:02:06,820
also try that out I'm I'm York I'm the a

00:02:05,200 --> 00:02:09,789
head of engineering and machine learning

00:02:06,820 --> 00:02:11,440
over at Arango DB and prior to that I

00:02:09,789 --> 00:02:14,230
always been switching back and forth

00:02:11,440 --> 00:02:16,660
between building infrastructure so for

00:02:14,230 --> 00:02:19,620
example at Mesa Sphere working early on

00:02:16,660 --> 00:02:21,910
kubernetes as well and building database

00:02:19,620 --> 00:02:23,740
services so right now I'm pretty

00:02:21,910 --> 00:02:26,140
peow can actually combine both those

00:02:23,740 --> 00:02:28,030
passions and I even can combine both

00:02:26,140 --> 00:02:29,500
those passion than this talk because it

00:02:28,030 --> 00:02:32,200
can talk a little tiny bit about

00:02:29,500 --> 00:02:36,130
databases not too much don't worry but

00:02:32,200 --> 00:02:38,590
mostly about infrastructure so why do

00:02:36,130 --> 00:02:41,770
you care a wrangler DB is actually a

00:02:38,590 --> 00:02:44,380
database so you might wonder why do they

00:02:41,770 --> 00:02:46,540
care about talking about just setting up

00:02:44,380 --> 00:02:50,170
a service across different cloud

00:02:46,540 --> 00:02:53,110
providers so overall as mentioned

00:02:50,170 --> 00:02:54,280
Arango DB might be a database so if you

00:02:53,110 --> 00:02:57,600
haven't heard of it

00:02:54,280 --> 00:02:59,950
it is a native multi-model database

00:02:57,600 --> 00:03:01,810
imagine if a graph database and a

00:02:59,950 --> 00:03:05,080
document database had a kid together

00:03:01,810 --> 00:03:07,210
and so we can basically handle different

00:03:05,080 --> 00:03:09,310
data models natively

00:03:07,210 --> 00:03:11,260
and you don't have to switch between

00:03:09,310 --> 00:03:13,660
different database products to for

00:03:11,260 --> 00:03:16,410
example combine your graph problem

00:03:13,660 --> 00:03:20,230
together was like a document data story

00:03:16,410 --> 00:03:22,540
it's distributed which is very important

00:03:20,230 --> 00:03:25,090
if we're talking about different setup

00:03:22,540 --> 00:03:26,709
and deployments on kubernetes because we

00:03:25,090 --> 00:03:29,080
actually have to take care of connecting

00:03:26,709 --> 00:03:32,290
all the different pieces together it has

00:03:29,080 --> 00:03:35,590
a ql a sequel like korean language and

00:03:32,290 --> 00:03:39,070
we also support transactions to some

00:03:35,590 --> 00:03:41,140
degree so it's open source so feel free

00:03:39,070 --> 00:03:43,930
to go and try it out but I don't want to

00:03:41,140 --> 00:03:45,640
talk too much about Arango DB but why do

00:03:43,930 --> 00:03:48,250
we care about this container and

00:03:45,640 --> 00:03:51,130
kubernetes space if we are looking at

00:03:48,250 --> 00:03:54,940
the CN CF landscape so CN CF says the

00:03:51,130 --> 00:03:57,010
cloud native foundation the kind of

00:03:54,940 --> 00:03:59,350
foundation behind kubernetes and a lot

00:03:57,010 --> 00:04:03,040
of the other projects in this space you

00:03:59,350 --> 00:04:05,530
can see that the tile for databases is

00:04:03,040 --> 00:04:08,590
ever-growing and we see like most of our

00:04:05,530 --> 00:04:12,010
like best-known database solutions like

00:04:08,590 --> 00:04:14,140
my sequel Postgres so if you're in the

00:04:12,010 --> 00:04:16,570
database space you probably have heard

00:04:14,140 --> 00:04:18,940
of many of those solutions furthermore

00:04:16,570 --> 00:04:21,280
we see that for our communities are open

00:04:18,940 --> 00:04:24,820
source users as well as for our

00:04:21,280 --> 00:04:27,010
customers containers and kubernetes is

00:04:24,820 --> 00:04:28,690
becoming more and more important so

00:04:27,010 --> 00:04:31,900
we're seeing a lot of polls on our

00:04:28,690 --> 00:04:34,390
docker images for rango DB but then also

00:04:31,900 --> 00:04:35,529
even more importantly we see a lot of

00:04:34,390 --> 00:04:39,369
polls

00:04:35,529 --> 00:04:42,429
for our crew Panetta's operator and in

00:04:39,369 --> 00:04:45,399
my experience kubernetes operators a

00:04:42,429 --> 00:04:48,059
really upped sea game for deploying

00:04:45,399 --> 00:04:50,319
stateful services on top of kubernetes

00:04:48,059 --> 00:04:52,359
originally was kind of designed more for

00:04:50,319 --> 00:04:54,929
like stateless services but with

00:04:52,359 --> 00:04:57,909
kubernetes operator or the operator

00:04:54,929 --> 00:05:01,059
pattern we're actually able to run state

00:04:57,909 --> 00:05:04,209
full services and also support the

00:05:01,059 --> 00:05:06,389
complex patterns so if prior to that and

00:05:04,209 --> 00:05:09,549
relax those run books on how to update

00:05:06,389 --> 00:05:11,759
your database we have a lot of chapters

00:05:09,549 --> 00:05:15,159
in our documentation on how to maintain

00:05:11,759 --> 00:05:17,859
upgrade duty a recovery inside a regular

00:05:15,159 --> 00:05:21,159
DB but this is basically all encoded

00:05:17,859 --> 00:05:24,489
into the operator and imaginers just is

00:05:21,159 --> 00:05:27,009
kind of like this run broke s code and

00:05:24,489 --> 00:05:29,679
so it really makes spinning up a large

00:05:27,009 --> 00:05:32,109
distributed database faster and much

00:05:29,679 --> 00:05:35,889
much easier and then also maintaining it

00:05:32,109 --> 00:05:38,139
updating it upgrading it dealing with

00:05:35,889 --> 00:05:39,699
fail overs and basically all of the

00:05:38,139 --> 00:05:41,319
stuff you have to do otherwise from a

00:05:39,699 --> 00:05:43,589
day-to-day basis as a database

00:05:41,319 --> 00:05:48,069
administrator

00:05:43,589 --> 00:05:49,989
Arango DB oasis is our managed solution

00:05:48,069 --> 00:05:52,659
so if you don't want to set it up

00:05:49,989 --> 00:05:54,369
yourself you can basically just go there

00:05:52,659 --> 00:05:57,639
and was a few clicks have a running

00:05:54,369 --> 00:06:01,029
instance running cluster of a rango DB

00:05:57,639 --> 00:06:02,860
here we have a large number of

00:06:01,029 --> 00:06:05,889
deployments across different cloud

00:06:02,860 --> 00:06:07,989
providers so AWS Google and Azure as a

00:06:05,889 --> 00:06:10,299
currently supported ones and they are

00:06:07,989 --> 00:06:13,299
fully managed deployments so you don't

00:06:10,299 --> 00:06:16,479
have to worry about anything you can

00:06:13,299 --> 00:06:17,679
operate with a single click we do all

00:06:16,479 --> 00:06:21,999
the maintenance for you in the

00:06:17,679 --> 00:06:23,949
background has inbuilt security and us

00:06:21,999 --> 00:06:28,209
also just scalable with a single click

00:06:23,949 --> 00:06:31,209
of a button so if for example if you

00:06:28,209 --> 00:06:33,369
look at the UI you can choose here

00:06:31,209 --> 00:06:36,099
already a provider which basically this

00:06:33,369 --> 00:06:38,139
talk is about how do we support those

00:06:36,099 --> 00:06:40,059
different providers and then you can

00:06:38,139 --> 00:06:43,149
configure how large should you note be

00:06:40,059 --> 00:06:44,339
and how much disk you want and so on and

00:06:43,149 --> 00:06:48,560
so on

00:06:44,339 --> 00:06:50,360
so just a little bit on the oasis arc

00:06:48,560 --> 00:06:52,250
sure I don't want to dive into that too

00:06:50,360 --> 00:06:55,639
much but it's important to actually

00:06:52,250 --> 00:06:57,530
understand some of those details to

00:06:55,639 --> 00:07:01,460
understand how we actually set it up

00:06:57,530 --> 00:07:04,280
across different cloud providers so the

00:07:01,460 --> 00:07:07,160
kind of general architecture is we have

00:07:04,280 --> 00:07:09,110
a control plane which is basically the

00:07:07,160 --> 00:07:11,540
central control plane for all the

00:07:09,110 --> 00:07:15,400
different clusters which are running out

00:07:11,540 --> 00:07:18,980
there so there's monitoring there's the

00:07:15,400 --> 00:07:20,870
dashboard of there's alerting logging so

00:07:18,980 --> 00:07:23,270
basically anything we also need to

00:07:20,870 --> 00:07:25,490
control all the different instances of

00:07:23,270 --> 00:07:27,740
the Arango to be clusters running out

00:07:25,490 --> 00:07:30,620
there and then there are multiple data

00:07:27,740 --> 00:07:34,750
clusters so a David cluster can't be for

00:07:30,620 --> 00:07:39,020
example one kubernetes cluster running

00:07:34,750 --> 00:07:41,510
in AWS and one of those data clusters

00:07:39,020 --> 00:07:43,790
and in turn can hold multiple customer

00:07:41,510 --> 00:07:48,139
deployments which basically translates

00:07:43,790 --> 00:07:49,880
to Arango DB clusters so as a control

00:07:48,139 --> 00:07:53,300
plane as mentioned it's kind of the

00:07:49,880 --> 00:07:55,190
brain it's actually also running on gke

00:07:53,300 --> 00:07:57,590
so there's just one instance for

00:07:55,190 --> 00:07:59,740
controlling all the different data

00:07:57,590 --> 00:08:03,650
clusters and that's currently running in

00:07:59,740 --> 00:08:07,180
Google on the Google managed kubernetes

00:08:03,650 --> 00:08:10,190
solution and this is basically a

00:08:07,180 --> 00:08:13,669
similarity kubernetes model canvas its

00:08:10,190 --> 00:08:16,610
central component component and managing

00:08:13,669 --> 00:08:18,410
the creation removal and it's also

00:08:16,610 --> 00:08:22,039
CAHSEE communication half for like

00:08:18,410 --> 00:08:24,110
alerting logging and other strings the

00:08:22,039 --> 00:08:25,669
data clusters there's the actual ones

00:08:24,110 --> 00:08:28,370
running see different

00:08:25,669 --> 00:08:31,850
Arango DB clusters so we have a large

00:08:28,370 --> 00:08:34,010
number of data clusters imagine roughly

00:08:31,850 --> 00:08:37,159
speaking there's one for each cloud

00:08:34,010 --> 00:08:39,800
provider and then for each zone or each

00:08:37,159 --> 00:08:42,530
availability zone supported in that

00:08:39,800 --> 00:08:44,990
cloud provider there's one data cluster

00:08:42,530 --> 00:08:47,330
so as we support a large number of

00:08:44,990 --> 00:08:49,610
different zones across Street cloud

00:08:47,330 --> 00:08:51,200
providers you can already imagine this

00:08:49,610 --> 00:08:54,770
actually amounts to a large number of

00:08:51,200 --> 00:08:58,940
data clusters it's abstracting away the

00:08:54,770 --> 00:09:01,640
cluster details and it's also aiming to

00:08:58,940 --> 00:09:04,670
abstract away Z cloud provider details

00:09:01,640 --> 00:09:06,770
that basically for the control component

00:09:04,670 --> 00:09:12,110
it's pretty much the same whether it

00:09:06,770 --> 00:09:16,910
creates a customer deployment on on for

00:09:12,110 --> 00:09:21,410
example AWS or on Google but this is

00:09:16,910 --> 00:09:23,630
only true mostly so we'll dive into

00:09:21,410 --> 00:09:27,830
likes Network players and other details

00:09:23,630 --> 00:09:29,630
in just a few slides so but let's look

00:09:27,830 --> 00:09:32,810
at what it actually means for us to

00:09:29,630 --> 00:09:36,530
create a new data cluster it's actually

00:09:32,810 --> 00:09:39,050
a fully automatic but in the end we have

00:09:36,530 --> 00:09:41,620
different operators for different cloud

00:09:39,050 --> 00:09:44,870
providers and they are really different

00:09:41,620 --> 00:09:47,120
we choose to implement it using

00:09:44,870 --> 00:09:49,280
operators as there's actually been that

00:09:47,120 --> 00:09:52,820
question before why don't we go for

00:09:49,280 --> 00:09:54,950
something abstract abstracting ways a

00:09:52,820 --> 00:09:58,460
different cloud provider details because

00:09:54,950 --> 00:10:01,400
we really need that a different level of

00:09:58,460 --> 00:10:04,190
control on the different data clusters

00:10:01,400 --> 00:10:08,270
so in the end it actually means we have

00:10:04,190 --> 00:10:11,570
an kubernetes operator which allows us

00:10:08,270 --> 00:10:13,880
to create and manage all those different

00:10:11,570 --> 00:10:17,660
data clusters across different cloud

00:10:13,880 --> 00:10:21,290
providers it's designed it's like a pull

00:10:17,660 --> 00:10:23,330
or only setup and this is kind of for

00:10:21,290 --> 00:10:25,790
the future what we are currently working

00:10:23,330 --> 00:10:29,750
on for the next release to actually also

00:10:25,790 --> 00:10:35,150
support custom VP C's where you actually

00:10:29,750 --> 00:10:38,300
sealed off with your network so SF was

00:10:35,150 --> 00:10:40,670
kind of the overview of Arango DB and

00:10:38,300 --> 00:10:43,430
oasis let's actually get to the

00:10:40,670 --> 00:10:45,580
interesting part and this the first step

00:10:43,430 --> 00:10:48,200
is kind of kubernetes as an abstraction

00:10:45,580 --> 00:10:50,630
so if we think about containers

00:10:48,200 --> 00:10:53,090
containers were designed to be an

00:10:50,630 --> 00:10:54,830
abstraction and this is not only true

00:10:53,090 --> 00:10:57,110
for the containers we talk about in

00:10:54,830 --> 00:10:59,450
software but also in the earlier

00:10:57,110 --> 00:11:01,460
containers in kind of the real world

00:10:59,450 --> 00:11:03,740
where they've really helped to kind of

00:11:01,460 --> 00:11:05,960
standardize you don't have to worry

00:11:03,740 --> 00:11:08,090
about whether you what's inside the

00:11:05,960 --> 00:11:09,860
container you can put it on a train you

00:11:08,090 --> 00:11:12,650
can put it on a ship you can put it on a

00:11:09,860 --> 00:11:14,660
truck and similarly in software that's

00:11:12,650 --> 00:11:17,269
kind of CIDR you don't really have to

00:11:14,660 --> 00:11:19,879
about what's inside but you can just run

00:11:17,269 --> 00:11:22,850
your container anywhere or almost

00:11:19,879 --> 00:11:26,810
anywhere where you have a container run

00:11:22,850 --> 00:11:28,850
time but then the next challenge kind of

00:11:26,810 --> 00:11:30,740
came containers they are great but we

00:11:28,850 --> 00:11:34,370
also kind of have to orchestrate them

00:11:30,740 --> 00:11:36,709
first of all for example docker a docker

00:11:34,370 --> 00:11:38,990
container by itself if it crashes if

00:11:36,709 --> 00:11:42,410
there's some kind of failure if it runs

00:11:38,990 --> 00:11:44,480
out of memory it doesn't restart

00:11:42,410 --> 00:11:46,399
automatically so we need something to

00:11:44,480 --> 00:11:49,550
actually deal with that and schedule

00:11:46,399 --> 00:11:52,339
done and we're scheduling also restart

00:11:49,550 --> 00:11:54,079
exam if there's a failure furthermore we

00:11:52,339 --> 00:11:56,600
also have to deal with kind of resource

00:11:54,079 --> 00:11:59,779
management how do we ensure that

00:11:56,600 --> 00:12:02,269
actually our container lands on a box

00:11:59,779 --> 00:12:04,519
was large enough resources and in our

00:12:02,269 --> 00:12:06,800
cluster not all containers are started

00:12:04,519 --> 00:12:09,230
on like a single box and then this box

00:12:06,800 --> 00:12:11,629
is totally overloaded and then probably

00:12:09,230 --> 00:12:14,120
the biggest challenge is how can we

00:12:11,629 --> 00:12:18,130
actually connect multiple containers to

00:12:14,120 --> 00:12:21,620
meaningful micro service architecture

00:12:18,130 --> 00:12:24,560
usually inside one container we have

00:12:21,620 --> 00:12:28,670
like a limited business logic and to

00:12:24,560 --> 00:12:30,769
really get first of all a more complex

00:12:28,670 --> 00:12:33,230
set of services attached to each other

00:12:30,769 --> 00:12:35,750
and then also being able to scale

00:12:33,230 --> 00:12:37,639
individual components which probably is

00:12:35,750 --> 00:12:39,740
one of the big advantages of a micro

00:12:37,639 --> 00:12:41,120
service architecture we actually have to

00:12:39,740 --> 00:12:45,380
do some service management and

00:12:41,120 --> 00:12:50,300
connecting those pieces together to to a

00:12:45,380 --> 00:12:52,339
larger architecture so if we want to

00:12:50,300 --> 00:12:55,430
look at the different abstraction layers

00:12:52,339 --> 00:12:57,889
containers are at our first front then

00:12:55,430 --> 00:13:00,259
actually is this different container

00:12:57,889 --> 00:13:02,509
orchestration we talked about this is

00:13:00,259 --> 00:13:04,930
actually what kubernetes is doing for us

00:13:02,509 --> 00:13:08,209
so kubernetes this is really helpful

00:13:04,930 --> 00:13:10,759
tool which we can deploy across large

00:13:08,209 --> 00:13:13,550
clusters and then actually this helps us

00:13:10,759 --> 00:13:16,309
to manage our containers to deploy them

00:13:13,550 --> 00:13:19,189
to operate them it will automatically

00:13:16,309 --> 00:13:22,480
restart them if we tell it to do so and

00:13:19,189 --> 00:13:26,029
so this is basically our Orchestrator

00:13:22,480 --> 00:13:29,140
now this Orchestrator can still be

00:13:26,029 --> 00:13:32,270
deployed it basically makes it

00:13:29,140 --> 00:13:34,250
invisible to whatever is deployed

00:13:32,270 --> 00:13:36,560
underneath so we can deploy kubernetes

00:13:34,250 --> 00:13:38,779
on top of a bare-metal cluster we can

00:13:36,560 --> 00:13:42,640
deploy it on top of different cloud

00:13:38,779 --> 00:13:47,510
providers such as AWS Google or edger

00:13:42,640 --> 00:13:49,310
ourselves but if we dive a little bit

00:13:47,510 --> 00:13:51,980
into what it means to actually deploy

00:13:49,310 --> 00:13:54,589
our own kubernetes clusters if we just

00:13:51,980 --> 00:13:57,589
have a look at as the basic components

00:13:54,589 --> 00:14:00,440
are see they're usually the master nodes

00:13:57,589 --> 00:14:05,750
or the control plane again to use the

00:14:00,440 --> 00:14:08,779
same names as we have or the same yes

00:14:05,750 --> 00:14:11,180
same names as we have in the oasis base

00:14:08,779 --> 00:14:13,160
the control plane kind of beings of

00:14:11,180 --> 00:14:14,690
brain of the cluster this is here with

00:14:13,160 --> 00:14:16,940
some master nodes where we have C API

00:14:14,690 --> 00:14:19,790
server we get scheduler we get

00:14:16,940 --> 00:14:22,250
controller managers and then to persist

00:14:19,790 --> 00:14:24,470
all the important information a

00:14:22,250 --> 00:14:28,160
distributed key-value store called Etsy

00:14:24,470 --> 00:14:31,100
D and then we got some worker nodes or Z

00:14:28,160 --> 00:14:33,500
kind of data plane which is doing the

00:14:31,100 --> 00:14:36,529
actual works here we can see regards the

00:14:33,500 --> 00:14:39,529
couplet queue proxeny and then different

00:14:36,529 --> 00:14:42,950
pots running so pod is basically a

00:14:39,529 --> 00:14:44,720
combination of multiple containers so we

00:14:42,950 --> 00:14:47,899
see this can actually already get pretty

00:14:44,720 --> 00:14:50,329
complex especially if we consider a

00:14:47,899 --> 00:14:52,700
favorite code from a former colleague of

00:14:50,329 --> 00:14:54,950
mine that kubernetes clusters are

00:14:52,700 --> 00:14:55,459
actually like Pringles you can't just

00:14:54,950 --> 00:14:58,430
have one

00:14:55,459 --> 00:15:00,440
in typical setups you actually end up

00:14:58,430 --> 00:15:02,870
with like a large number of different

00:15:00,440 --> 00:15:05,630
kubernetes clusters at least you're

00:15:02,870 --> 00:15:08,390
going to probably have one staging one

00:15:05,630 --> 00:15:11,209
for production and then as an our case

00:15:08,390 --> 00:15:13,610
we actually have multiple clusters for

00:15:11,209 --> 00:15:15,800
different cloud providers so in our case

00:15:13,610 --> 00:15:18,320
we have like 10 plus different

00:15:15,800 --> 00:15:20,240
kubernetes clusters at least not even

00:15:18,320 --> 00:15:23,329
counting see at different staging

00:15:20,240 --> 00:15:25,430
environments so setting that up and

00:15:23,329 --> 00:15:28,459
controlling all of that becomes already

00:15:25,430 --> 00:15:30,529
a pretty big nightmare and this is where

00:15:28,459 --> 00:15:32,600
kind of see next abstraction layer comes

00:15:30,529 --> 00:15:35,209
in and this is actually the cloud

00:15:32,600 --> 00:15:38,420
providers giving us a managed community

00:15:35,209 --> 00:15:39,130
solution so Amazon is for example giving

00:15:38,420 --> 00:15:41,890
us

00:15:39,130 --> 00:15:44,050
the elastic kubernetes service Google is

00:15:41,890 --> 00:15:45,910
giving us a container engine and also in

00:15:44,050 --> 00:15:48,640
Asia we have the asia asia container

00:15:45,910 --> 00:15:50,770
service so all the big cloud providers

00:15:48,640 --> 00:15:52,960
and even if you're on different cloud

00:15:50,770 --> 00:15:55,480
providers they probably also help you a

00:15:52,960 --> 00:15:59,980
managed kubernetes solution by now

00:15:55,480 --> 00:16:02,860
and by that we can actually remove the

00:15:59,980 --> 00:16:06,210
hassle of having to set up and maintain

00:16:02,860 --> 00:16:08,560
all those different kubernetes clusters

00:16:06,210 --> 00:16:10,330
alright so this was kind of seen

00:16:08,560 --> 00:16:13,060
motivation why do we actually want to

00:16:10,330 --> 00:16:15,640
manage kubernetes solution next why do

00:16:13,060 --> 00:16:18,640
we might we want to have a multi cloud

00:16:15,640 --> 00:16:20,500
provider solution so multi cloud provide

00:16:18,640 --> 00:16:22,330
a solution refers to we're not just

00:16:20,500 --> 00:16:25,600
picking one of them and implementing our

00:16:22,330 --> 00:16:28,090
service there but we actually we choose

00:16:25,600 --> 00:16:32,880
to implement it across different cloud

00:16:28,090 --> 00:16:36,400
providers so from what we heard from our

00:16:32,880 --> 00:16:39,610
customers and users often it's actually

00:16:36,400 --> 00:16:42,070
a company policy so that can be in or

00:16:39,610 --> 00:16:45,520
exclusive so it's can for example mean

00:16:42,070 --> 00:16:48,640
we don't want to run on AWS we don't

00:16:45,520 --> 00:16:50,890
want to run on Azure for some reason

00:16:48,640 --> 00:16:57,250
often it might for example be because

00:16:50,890 --> 00:16:59,140
it's a competitor and also often used

00:16:57,250 --> 00:17:01,750
argument for a multi cloud provider

00:16:59,140 --> 00:17:04,270
solution is to avoid like dependency on

00:17:01,750 --> 00:17:06,819
a single vendor so it actually gives

00:17:04,270 --> 00:17:11,199
customers leverage to switch over from

00:17:06,819 --> 00:17:13,329
AWS to Google or to Microsoft more

00:17:11,199 --> 00:17:18,000
easily if they are not dependent on a

00:17:13,329 --> 00:17:21,939
single single manager Bonetti solution

00:17:18,000 --> 00:17:24,640
so overall flexibility and often I feel

00:17:21,939 --> 00:17:27,189
it's also in some cases at least it's

00:17:24,640 --> 00:17:28,720
actually a password where it just sounds

00:17:27,189 --> 00:17:31,450
cool to actually have a multi cloud

00:17:28,720 --> 00:17:33,850
provider solution so before actually

00:17:31,450 --> 00:17:35,860
going into implementing a multi cloud

00:17:33,850 --> 00:17:38,950
provider solution actually consider the

00:17:35,860 --> 00:17:40,630
benefits you get out of that and even

00:17:38,950 --> 00:17:43,630
though you decide to only implement it

00:17:40,630 --> 00:17:45,160
on a single server please remain on this

00:17:43,630 --> 00:17:48,610
talk because actually we're talking

00:17:45,160 --> 00:17:51,550
about a lot of the interesting parts for

00:17:48,610 --> 00:17:52,760
each and every cloud provider you should

00:17:51,550 --> 00:17:55,550
be

00:17:52,760 --> 00:17:59,810
you should carry about when implementing

00:17:55,550 --> 00:18:02,210
even a single cloud provider solution so

00:17:59,810 --> 00:18:05,420
what are the challenges we faced over

00:18:02,210 --> 00:18:07,310
the past of one and a half years and if

00:18:05,420 --> 00:18:09,290
we had to group that it's probably a

00:18:07,310 --> 00:18:11,990
resources and resource management

00:18:09,290 --> 00:18:14,210
resource creation different kubernetes

00:18:11,990 --> 00:18:16,370
versions different security

00:18:14,210 --> 00:18:18,290
implementations authentication

00:18:16,370 --> 00:18:21,500
authorization different logging

00:18:18,290 --> 00:18:25,160
solutions networking storage and then

00:18:21,500 --> 00:18:28,040
different container runtimes so if we

00:18:25,160 --> 00:18:30,200
kind of had to first blow of steam

00:18:28,040 --> 00:18:32,990
imagine we were had to write an email to

00:18:30,200 --> 00:18:34,790
each of those product managers running

00:18:32,990 --> 00:18:38,000
see different manage kubernetes

00:18:34,790 --> 00:18:41,690
solutions here's kind of the first level

00:18:38,000 --> 00:18:43,640
critique but keep in mind that overall

00:18:41,690 --> 00:18:48,550
we are pretty happy with each and every

00:18:43,640 --> 00:18:51,440
of these solutions so on eks it's the

00:18:48,550 --> 00:18:53,600
biggest challenge for us probably I said

00:18:51,440 --> 00:18:56,870
it creates many resources on the fly

00:18:53,600 --> 00:19:00,650
load balancers security groups and so on

00:18:56,870 --> 00:19:03,020
and so on so this basically each

00:19:00,650 --> 00:19:04,820
resource or most of the resources you

00:19:03,020 --> 00:19:07,070
creates they actually have dependency

00:19:04,820 --> 00:19:09,110
and that can make removal pretty

00:19:07,070 --> 00:19:13,130
challenging because not all resources

00:19:09,110 --> 00:19:16,760
are assigned text so we actually had to

00:19:13,130 --> 00:19:19,730
create like a pretty elaborate cleanup

00:19:16,760 --> 00:19:24,040
routine to remove all the components on

00:19:19,730 --> 00:19:27,080
the fly secondly you can actually feel

00:19:24,040 --> 00:19:28,670
eks is one of the older solutions which

00:19:27,080 --> 00:19:31,940
on the one hand is good for stability

00:19:28,670 --> 00:19:34,310
but on the other hand sometimes the API

00:19:31,940 --> 00:19:36,710
also feels a bit old and we especially

00:19:34,310 --> 00:19:39,290
fell to the ground error handling which

00:19:36,710 --> 00:19:44,060
is let's just say not so structured and

00:19:39,290 --> 00:19:47,780
involves a lot of string parsing on gke

00:19:44,060 --> 00:19:49,430
probably is the most annoying part on

00:19:47,780 --> 00:19:51,890
the other hand is probably also pretty

00:19:49,430 --> 00:19:55,370
good it's the aggressive update policy

00:19:51,890 --> 00:19:57,620
so it actually updates to a new crew

00:19:55,370 --> 00:20:00,140
Panetta's version pretty quickly and

00:19:57,620 --> 00:20:02,360
then they also drop of old kubernetes

00:20:00,140 --> 00:20:05,390
solutions and if you haven't upgraded

00:20:02,360 --> 00:20:06,980
yourself from one kubernetes solution to

00:20:05,390 --> 00:20:10,429
the next they will happen

00:20:06,980 --> 00:20:14,080
do that for you but if you actually run

00:20:10,429 --> 00:20:16,520
the manage database service or any

00:20:14,080 --> 00:20:19,190
service which has like a persistence

00:20:16,520 --> 00:20:23,360
layer you probably want to be in control

00:20:19,190 --> 00:20:28,970
of the update policy yourself then on

00:20:23,360 --> 00:20:32,840
Asscher on other container service we

00:20:28,970 --> 00:20:34,820
felt as a young age which on the one

00:20:32,840 --> 00:20:36,830
hand again was pretty refreshing because

00:20:34,820 --> 00:20:39,290
they have a lot of great ideas around

00:20:36,830 --> 00:20:40,730
how to structure the API but on the

00:20:39,290 --> 00:20:43,100
other hand currently it's still missing

00:20:40,730 --> 00:20:46,070
some of the basic features so for

00:20:43,100 --> 00:20:48,860
example being skill sets they are quite

00:20:46,070 --> 00:20:52,820
restrictive and we actually had to work

00:20:48,860 --> 00:20:54,679
around as uPVC resizing issues which

00:20:52,820 --> 00:21:00,200
comes up on one of the next slides in

00:20:54,679 --> 00:21:02,360
more detail so let's get started with

00:21:00,200 --> 00:21:05,540
resource creation because that's

00:21:02,360 --> 00:21:08,030
probably the most important part if you

00:21:05,540 --> 00:21:09,679
bring up a kubernetes cluster you care

00:21:08,030 --> 00:21:11,900
about see different resources in the

00:21:09,679 --> 00:21:14,270
background because that's usually what

00:21:11,900 --> 00:21:17,179
ends up juanzi bill in the end as well

00:21:14,270 --> 00:21:20,480
so this is what you're paying for so on

00:21:17,179 --> 00:21:22,669
eks s mentioned on the previous slide it

00:21:20,480 --> 00:21:25,580
actually involves a lot of different

00:21:22,669 --> 00:21:28,580
resources so VPC Internet gateway in

00:21:25,580 --> 00:21:32,059
that gateway Sapna's routing tables

00:21:28,580 --> 00:21:36,440
security groups auto scaling groups we

00:21:32,059 --> 00:21:38,840
have separate AMI and then the actual

00:21:36,440 --> 00:21:41,390
IKS cluster so you can already see

00:21:38,840 --> 00:21:44,090
there's a lot of stuff happening in the

00:21:41,390 --> 00:21:47,660
background and AWS will create many

00:21:44,090 --> 00:21:49,850
resources on the fly and then there are

00:21:47,660 --> 00:21:53,120
also certain dependencies in which you

00:21:49,850 --> 00:21:55,669
can remove the resources again so you

00:21:53,120 --> 00:21:59,620
first have to remove part a and then you

00:21:55,669 --> 00:22:03,200
can remove Part B which depends on part

00:21:59,620 --> 00:22:05,960
which part a depends on this way around

00:22:03,200 --> 00:22:08,630
and this is especially challenging is

00:22:05,960 --> 00:22:11,330
not necessarily all resources are tacked

00:22:08,630 --> 00:22:14,780
so you kind of have to figure out how

00:22:11,330 --> 00:22:18,169
the dependencies are on Google it's

00:22:14,780 --> 00:22:20,360
actually much less so we have a vp c zg

00:22:18,169 --> 00:22:20,850
ke clusters and then the actual node

00:22:20,360 --> 00:22:24,480
pool

00:22:20,850 --> 00:22:28,020
so there we have a lot less number of

00:22:24,480 --> 00:22:31,320
resources to care about and similarly on

00:22:28,020 --> 00:22:33,720
edger it's also the resource groups aks

00:22:31,320 --> 00:22:37,230
clusters the scale sets and the storage

00:22:33,720 --> 00:22:40,140
accounts so that's also pretty pretty

00:22:37,230 --> 00:22:42,030
manageable but here we just ran into

00:22:40,140 --> 00:22:45,300
some limitations so for example

00:22:42,030 --> 00:22:47,490
currently zpm skill sets as they cannot

00:22:45,300 --> 00:22:49,740
be scaled down to zero which in some

00:22:47,490 --> 00:22:51,570
cases might actually be nice and then

00:22:49,740 --> 00:22:55,800
also on the upper side they actually

00:22:51,570 --> 00:22:57,750
limited to 8 which given that we need to

00:22:55,800 --> 00:23:00,150
create a large number of them for

00:22:57,750 --> 00:23:04,950
example for different groups this is a

00:23:00,150 --> 00:23:07,770
pretty tough limit for us kubernetes

00:23:04,950 --> 00:23:12,600
version and here is actually mostly

00:23:07,770 --> 00:23:14,280
Google where we have to deal a little

00:23:12,600 --> 00:23:16,890
bit with that or manage that a little

00:23:14,280 --> 00:23:19,200
bit there moving really quickly which on

00:23:16,890 --> 00:23:22,230
the one hand is great cruciate sinews

00:23:19,200 --> 00:23:24,930
kubernetes version both major and minor

00:23:22,230 --> 00:23:27,990
versions but they also actually forced

00:23:24,930 --> 00:23:30,060
those upgrades up on you they will also

00:23:27,990 --> 00:23:32,760
update your cluster but as mentioned

00:23:30,060 --> 00:23:34,560
earlier you actually if you have any

00:23:32,760 --> 00:23:36,990
stateful service you want to be in

00:23:34,560 --> 00:23:40,020
control yourself so you actually have to

00:23:36,990 --> 00:23:43,410
follow those updates pretty quickly and

00:23:40,020 --> 00:23:48,300
then operate your underlying kubernetes

00:23:43,410 --> 00:23:49,980
cluster or tell-tell gke to update your

00:23:48,300 --> 00:23:52,740
underlying kubernetes cluster on your

00:23:49,980 --> 00:23:55,500
own schedule it's not much work but

00:23:52,740 --> 00:23:58,520
basically you should trigger that on

00:23:55,500 --> 00:24:03,000
your own schedule and not rely on the

00:23:58,520 --> 00:24:06,090
Google and for scheduled kubernetes

00:24:03,000 --> 00:24:09,420
cluster options so when setting up your

00:24:06,090 --> 00:24:12,540
own kubernetes cluster manually maybe

00:24:09,420 --> 00:24:15,870
using kubernetes the hard way or any of

00:24:12,540 --> 00:24:18,870
the other solutions out there it's a lot

00:24:15,870 --> 00:24:20,340
of work so this is why we actually

00:24:18,870 --> 00:24:23,600
choose to have managed kubernetes

00:24:20,340 --> 00:24:26,580
clusters but managed kubernetes

00:24:23,600 --> 00:24:29,400
solutions or clusters they also limit

00:24:26,580 --> 00:24:32,140
the choices you have so for example

00:24:29,400 --> 00:24:34,419
access to the

00:24:32,140 --> 00:24:36,159
API server options is something which

00:24:34,419 --> 00:24:38,860
sometimes is really great

00:24:36,159 --> 00:24:41,380
so when bringing it up yourself you can

00:24:38,860 --> 00:24:43,950
configure a lot of cool things using the

00:24:41,380 --> 00:24:47,529
command line for example web hooks for

00:24:43,950 --> 00:24:50,620
Aussie really easy but if you have a

00:24:47,529 --> 00:24:52,960
managed kubernetes solution on each on

00:24:50,620 --> 00:24:54,850
on any of the cloud providers we

00:24:52,960 --> 00:24:57,250
actually you don't have this option

00:24:54,850 --> 00:25:01,330
exposed and this is something we had to

00:24:57,250 --> 00:25:04,240
work around authentication and

00:25:01,330 --> 00:25:06,820
authorization each of those providers

00:25:04,240 --> 00:25:08,889
actually has its own solution and

00:25:06,820 --> 00:25:12,490
typically it's a proprietary solution

00:25:08,889 --> 00:25:15,340
there exist a number of different open

00:25:12,490 --> 00:25:17,350
source and proprietary solution but we

00:25:15,340 --> 00:25:20,980
actually looked at some of them they

00:25:17,350 --> 00:25:23,080
seemed pretty insecure and none of those

00:25:20,980 --> 00:25:25,659
are really met our requirements and

00:25:23,080 --> 00:25:31,649
again one of the challenges was that we

00:25:25,659 --> 00:25:35,559
couldn't configure the these servers the

00:25:31,649 --> 00:25:37,870
authentication here as mentioned for the

00:25:35,559 --> 00:25:40,000
API server simply because we were

00:25:37,870 --> 00:25:42,730
running on a managed kubernetes solution

00:25:40,000 --> 00:25:46,000
so our current solution is actually to

00:25:42,730 --> 00:25:49,600
use kubernetes service accounts and then

00:25:46,000 --> 00:25:52,090
a pretty light layer on top as the Oasis

00:25:49,600 --> 00:25:54,340
authentication relying on those service

00:25:52,090 --> 00:25:56,590
accounts so this turned out to be a

00:25:54,340 --> 00:26:01,269
pretty scalable approach leveraging

00:25:56,590 --> 00:26:04,510
basically kubernetes native tools ok

00:26:01,269 --> 00:26:07,299
service accounts to deploy

00:26:04,510 --> 00:26:11,350
authentication and authorization across

00:26:07,299 --> 00:26:14,130
the different cloud providers similar

00:26:11,350 --> 00:26:17,019
problem was logging and audit so

00:26:14,130 --> 00:26:20,500
especially audit log are really

00:26:17,019 --> 00:26:23,950
important if you want to follow if you

00:26:20,500 --> 00:26:25,809
are dealing with stateful and valuable

00:26:23,950 --> 00:26:29,440
data you want to know who had access to

00:26:25,809 --> 00:26:31,769
s2 and who accessed it so again each

00:26:29,440 --> 00:26:35,289
cloud provider has its own proprietary

00:26:31,769 --> 00:26:37,809
solution and we basically ended up being

00:26:35,289 --> 00:26:42,760
pretty happy with grafanello key coming

00:26:37,809 --> 00:26:45,260
to the rescue so we just deployed it and

00:26:42,760 --> 00:26:48,290
that's been a pretty common

00:26:45,260 --> 00:26:52,240
solution across pretty scalable solution

00:26:48,290 --> 00:26:55,760
across all the different providers again

00:26:52,240 --> 00:26:57,560
for audit logging we'll discuss some of

00:26:55,760 --> 00:27:01,820
the challenges and when we actually talk

00:26:57,560 --> 00:27:03,950
about storage and again some of the

00:27:01,820 --> 00:27:09,410
challenges were that we can't control

00:27:03,950 --> 00:27:12,920
all the cube API server parameters for

00:27:09,410 --> 00:27:14,840
logging so again a slight drawback of

00:27:12,920 --> 00:27:21,620
having a managed to a community

00:27:14,840 --> 00:27:24,020
solution storage storage is actually has

00:27:21,620 --> 00:27:26,660
kind of two challenges first of all it's

00:27:24,020 --> 00:27:29,470
all slightly implemented differently

00:27:26,660 --> 00:27:31,790
across different cloud providers and

00:27:29,470 --> 00:27:35,030
secondly probably for kind of like

00:27:31,790 --> 00:27:36,460
sizing the biggest challenges that they

00:27:35,030 --> 00:27:40,190
all kind of have different performance

00:27:36,460 --> 00:27:44,030
some cloud providers offer options such

00:27:40,190 --> 00:27:46,490
as configurable I ops but overall we had

00:27:44,030 --> 00:27:48,950
to do it but a lot of benchmarking to

00:27:46,490 --> 00:27:51,860
actually achieve different performance

00:27:48,950 --> 00:27:56,030
classes which were which are comparable

00:27:51,860 --> 00:28:00,880
across different cloud providers and so

00:27:56,030 --> 00:28:03,980
this is both true for volumes so the

00:28:00,880 --> 00:28:06,320
actual volumes where a wranger to be is

00:28:03,980 --> 00:28:08,570
running on and then also the kind of

00:28:06,320 --> 00:28:10,610
cluster external storage so storage

00:28:08,570 --> 00:28:14,600
outside of secured minetti's clusters

00:28:10,610 --> 00:28:17,330
for example used for backup and restore

00:28:14,600 --> 00:28:18,320
so if you want to create a backup you

00:28:17,330 --> 00:28:21,260
don't want to store that in your

00:28:18,320 --> 00:28:25,340
kinetics cluster but for example on your

00:28:21,260 --> 00:28:27,470
s3 and that's actually where we

00:28:25,340 --> 00:28:29,840
currently need different implementations

00:28:27,470 --> 00:28:33,590
for the different cloud providers Chris

00:28:29,840 --> 00:28:35,330
that kind of cluster external storage is

00:28:33,590 --> 00:28:38,300
very different across different

00:28:35,330 --> 00:28:40,670
providers whereas if we are actually

00:28:38,300 --> 00:28:43,010
dealing with kubernetes volume the

00:28:40,670 --> 00:28:46,130
creation of them is pretty neatly hidden

00:28:43,010 --> 00:28:48,830
behind the communities API but as

00:28:46,130 --> 00:28:52,270
mentioned we still adds a challenge of

00:28:48,830 --> 00:28:55,280
different performance characteristics

00:28:52,270 --> 00:28:58,190
one of the issues we ran into on Azure

00:28:55,280 --> 00:28:59,390
is actually that the PVC resizing was

00:28:58,190 --> 00:29:02,140
broke

00:28:59,390 --> 00:29:05,600
so when you kind of resized a a

00:29:02,140 --> 00:29:07,940
persistent volume claim it changed the

00:29:05,600 --> 00:29:11,179
metadata and prising but we didn't see

00:29:07,940 --> 00:29:13,580
any changes to the filesystem the azure

00:29:11,179 --> 00:29:16,460
team it's they are really great people

00:29:13,580 --> 00:29:21,110
their buildings that and they helped us

00:29:16,460 --> 00:29:23,710
with a manual workaround so we now can

00:29:21,110 --> 00:29:26,269
deal with that but it was just a bit

00:29:23,710 --> 00:29:28,429
confusing at first because it looked

00:29:26,269 --> 00:29:30,799
changed and then it didn't change in the

00:29:28,429 --> 00:29:32,389
end but again kudos to Z as your team

00:29:30,799 --> 00:29:34,490
who actually helped us out there and

00:29:32,389 --> 00:29:37,490
they're really working on improving the

00:29:34,490 --> 00:29:42,110
overall structure as well so that

00:29:37,490 --> 00:29:45,260
hopefully is fixed soon so the next

00:29:42,110 --> 00:29:47,840
biggest challenge and I would say this

00:29:45,260 --> 00:29:50,269
is also probably a big challenge whether

00:29:47,840 --> 00:29:52,940
you go on a single cloud provider always

00:29:50,269 --> 00:29:54,850
we're actually aiming for multiple multi

00:29:52,940 --> 00:30:00,470
cloud provide multiple cloud providers

00:29:54,850 --> 00:30:03,409
so our oasis dated clusters so Vitas s

00:30:00,470 --> 00:30:06,769
like is in one Karima this cluster it's

00:30:03,409 --> 00:30:08,960
actually multiple multi tenants and so

00:30:06,769 --> 00:30:12,590
we only get like a single V PC for an

00:30:08,960 --> 00:30:14,779
entire data cluster so we still want

00:30:12,590 --> 00:30:18,919
strong network separations between

00:30:14,779 --> 00:30:23,090
different tenants for for networking and

00:30:18,919 --> 00:30:25,700
obvious security reasons so we actually

00:30:23,090 --> 00:30:29,149
choose here's psyllium as chemically

00:30:25,700 --> 00:30:31,309
based platform of course this gave us

00:30:29,149 --> 00:30:34,669
the best guarantees and also the best

00:30:31,309 --> 00:30:36,830
performance which is pretty crucial if

00:30:34,669 --> 00:30:39,950
you're trying to shuffle a lot of data

00:30:36,830 --> 00:30:43,460
around between different nodes or

00:30:39,950 --> 00:30:46,760
between different attendants between

00:30:43,460 --> 00:30:49,630
different virtual containers in your

00:30:46,760 --> 00:30:53,059
cluster that's the right term so

00:30:49,630 --> 00:30:56,149
psyllium what does it promised us it

00:30:53,059 --> 00:30:58,730
enables us to specify very simple

00:30:56,149 --> 00:31:00,919
network policy rules so for example

00:30:58,730 --> 00:31:04,750
parts a namespace a can talk to pass a

00:31:00,919 --> 00:31:07,639
namespace be but not pots in namespace C

00:31:04,750 --> 00:31:09,409
rules can be directional so for example

00:31:07,639 --> 00:31:11,210
even though paths in namespace a can

00:31:09,409 --> 00:31:12,710
talk to parts and namespace B this is

00:31:11,210 --> 00:31:15,440
not necessarily

00:31:12,710 --> 00:31:18,200
vice versa those rules are actually

00:31:15,440 --> 00:31:21,760
low-level implemented using Berkeley

00:31:18,200 --> 00:31:24,500
package filters which is a great indie a

00:31:21,760 --> 00:31:28,370
great class on the performance side

00:31:24,500 --> 00:31:29,779
so in short and you can skip that if you

00:31:28,370 --> 00:31:31,700
don't care too much about the

00:31:29,779 --> 00:31:33,169
implementation details this actually

00:31:31,700 --> 00:31:36,770
means that all the packages that can

00:31:33,169 --> 00:31:39,860
remain in the kernel and so as they

00:31:36,770 --> 00:31:42,100
don't have to be brought out and have a

00:31:39,860 --> 00:31:44,840
lot of context switching in between

00:31:42,100 --> 00:31:47,750
it's supposed to be cloud provider

00:31:44,840 --> 00:31:50,149
independent accepts the installation and

00:31:47,750 --> 00:31:53,659
setup which for us is okay because this

00:31:50,149 --> 00:31:56,570
has to be done once per data cluster and

00:31:53,659 --> 00:32:00,380
okay we can deal with that for each

00:31:56,570 --> 00:32:03,110
cloud provider so unfortunately the

00:32:00,380 --> 00:32:05,870
reality it's still great it's a pretty

00:32:03,110 --> 00:32:08,600
great tool and really kudos to the

00:32:05,870 --> 00:32:11,779
development team behind that so when it

00:32:08,600 --> 00:32:14,990
works it's great but the cloud provider

00:32:11,779 --> 00:32:18,409
independence it's not always the case in

00:32:14,990 --> 00:32:23,000
reality so for example is the probably

00:32:18,409 --> 00:32:28,370
biggest issue we hit was that pot ciders

00:32:23,000 --> 00:32:31,730
are differently set on AWS versus GCP so

00:32:28,370 --> 00:32:34,549
for example on AWS

00:32:31,730 --> 00:32:37,669
we had the issues that often teapot

00:32:34,549 --> 00:32:39,610
ciders were clashing for different nodes

00:32:37,669 --> 00:32:41,659
of your kubernetes cluster and that

00:32:39,610 --> 00:32:44,419
actually means that you could run a DS

00:32:41,659 --> 00:32:47,799
cluster that doesn't work because you

00:32:44,419 --> 00:32:52,010
need to have those different networks

00:32:47,799 --> 00:32:53,840
Sider spaces because otherwise two pots

00:32:52,010 --> 00:32:56,929
might end up with the same IP address

00:32:53,840 --> 00:32:59,510
and so we actually our workaround for

00:32:56,929 --> 00:33:03,080
that was pretty simple if we figure out

00:32:59,510 --> 00:33:05,110
that we have flashing pots eiders we're

00:33:03,080 --> 00:33:10,210
actually just manually draining the note

00:33:05,110 --> 00:33:15,710
so just as a result almost no problem on

00:33:10,210 --> 00:33:17,750
GC P so on Google overall I would assume

00:33:15,710 --> 00:33:21,200
that that's probably the default testing

00:33:17,750 --> 00:33:24,830
target for sylia and in the beginning we

00:33:21,200 --> 00:33:26,690
had relative frequent issues on AWS this

00:33:24,830 --> 00:33:30,200
actually improved quite a bit with

00:33:26,690 --> 00:33:33,890
the latest psyllium releases so from

00:33:30,200 --> 00:33:36,920
silly or 1.7 onwards this was just got a

00:33:33,890 --> 00:33:38,600
lot better and similarly for a sure it

00:33:36,920 --> 00:33:42,380
felt in the beginning for all the

00:33:38,600 --> 00:33:44,030
releases it was relatively new but by

00:33:42,380 --> 00:33:50,480
now we actually also feel much better

00:33:44,030 --> 00:33:54,020
about that so as a last item was kind of

00:33:50,480 --> 00:33:56,000
container runtimes so container runtime

00:33:54,020 --> 00:33:59,750
is kind of Z piece of software actually

00:33:56,000 --> 00:34:03,890
running your container making sure it's

00:33:59,750 --> 00:34:06,440
and forces its memory or CPU limits and

00:34:03,890 --> 00:34:09,500
there's actually like a large number out

00:34:06,440 --> 00:34:12,110
there so I feel like every year someone

00:34:09,500 --> 00:34:16,400
is proposing something new and just kind

00:34:12,110 --> 00:34:18,350
of as the first very simple structure is

00:34:16,400 --> 00:34:21,260
actually to structure that in container

00:34:18,350 --> 00:34:23,960
runtimes and kind of container isolation

00:34:21,260 --> 00:34:29,210
modules we can probably have our own

00:34:23,960 --> 00:34:30,860
talk just about that but currently we're

00:34:29,210 --> 00:34:35,630
just relying on kind of the default

00:34:30,860 --> 00:34:39,220
runtime which in most cases is going to

00:34:35,630 --> 00:34:43,190
be container deep underneath at least

00:34:39,220 --> 00:34:46,130
but I think in the future we also going

00:34:43,190 --> 00:34:48,050
to look into especially secure container

00:34:46,130 --> 00:34:50,420
runtimes so this is an ongoing project

00:34:48,050 --> 00:34:52,640
right now which actually helped us to

00:34:50,420 --> 00:34:56,060
isolate better between different

00:34:52,640 --> 00:34:59,150
containers so just recall a container

00:34:56,060 --> 00:35:02,930
underneath it's actually just C groups

00:34:59,150 --> 00:35:04,910
and namespaces so there's no real

00:35:02,930 --> 00:35:07,610
performance isolation it's all still

00:35:04,910 --> 00:35:09,830
running on the same Linux kernel and

00:35:07,610 --> 00:35:11,870
this is where some of those secure

00:35:09,830 --> 00:35:16,640
container initiatives are coming in like

00:35:11,870 --> 00:35:18,710
cutter containers having an own or

00:35:16,640 --> 00:35:21,170
lightweight kernel fire voice for

00:35:18,710 --> 00:35:22,730
example firecracker and not flood

00:35:21,170 --> 00:35:26,600
containers orgy visor

00:35:22,730 --> 00:35:29,630
where we actually implemented a little

00:35:26,600 --> 00:35:33,860
part of the Linux kernel in user space

00:35:29,630 --> 00:35:35,450
which is a pretty cool project and this

00:35:33,860 --> 00:35:37,130
is something where we actually also see

00:35:35,450 --> 00:35:39,410
that different cloud providers are

00:35:37,130 --> 00:35:39,930
focusing on different solutions so for

00:35:39,410 --> 00:35:42,030
example

00:35:39,930 --> 00:35:44,550
firecrackers heavily backed by AWS

00:35:42,030 --> 00:35:48,960
whereas G visor is mostly a Google

00:35:44,550 --> 00:35:52,619
product so what is the conclusion of all

00:35:48,960 --> 00:35:55,410
of this so in our experience if we had

00:35:52,619 --> 00:35:58,770
to summarize it AWS is clearly older

00:35:55,410 --> 00:36:01,920
than for example GCP and rosettes and

00:35:58,770 --> 00:36:04,589
also Azure and this is good on the one

00:36:01,920 --> 00:36:07,440
hand because it's actually the most

00:36:04,589 --> 00:36:11,400
stable of Sam so we had the least number

00:36:07,440 --> 00:36:13,650
of network outages it was a bit slower

00:36:11,400 --> 00:36:16,369
to bootstrap and see api's are showing

00:36:13,650 --> 00:36:21,900
their age but overall we must say like C

00:36:16,369 --> 00:36:24,839
managed the kind of the experience from

00:36:21,900 --> 00:36:28,619
the stability point was really the best

00:36:24,839 --> 00:36:31,859
on AWS the kind of experience for

00:36:28,619 --> 00:36:35,210
managed community solutions every in our

00:36:31,859 --> 00:36:39,750
experience we actually felt that GK is a

00:36:35,210 --> 00:36:43,500
Google Cloud was had a pretty pretty

00:36:39,750 --> 00:36:46,109
solid and pretty also stable X we had a

00:36:43,500 --> 00:36:50,940
pretty stable experience and also the

00:36:46,109 --> 00:36:56,819
API is really well thought out again

00:36:50,940 --> 00:36:59,640
just from kind of C age also the AWS API

00:36:56,819 --> 00:37:04,740
it has a lot of features for example

00:36:59,640 --> 00:37:06,660
provision I ops and so also from a

00:37:04,740 --> 00:37:09,329
performance point we had a lot of knobs

00:37:06,660 --> 00:37:11,069
we could use which was cool on the one

00:37:09,329 --> 00:37:12,869
hand buttons then on the other side you

00:37:11,069 --> 00:37:15,470
also have to figure out what to do and

00:37:12,869 --> 00:37:24,119
what's probably most important for you

00:37:15,470 --> 00:37:26,130
so we can say that there from an API

00:37:24,119 --> 00:37:27,270
perspective it was probably harder to

00:37:26,130 --> 00:37:32,490
deal with the other ones

00:37:27,270 --> 00:37:35,880
but then from from a stability point AWS

00:37:32,490 --> 00:37:36,750
kind of helped us out aks so the azure

00:37:35,880 --> 00:37:40,500
solutions

00:37:36,750 --> 00:37:41,309
it's I would call it an MC young cool

00:37:40,500 --> 00:37:44,790
kid on the block

00:37:41,309 --> 00:37:47,309
it's less mature if we look at for

00:37:44,790 --> 00:37:49,380
example stability or also is a different

00:37:47,309 --> 00:37:52,230
number of features available but they

00:37:49,380 --> 00:37:53,820
have a really dedicated team and coming

00:37:52,230 --> 00:37:56,280
up with cool features underneath

00:37:53,820 --> 00:37:57,150
we're really curious to see where this

00:37:56,280 --> 00:38:01,620
is going

00:37:57,150 --> 00:38:03,420
in the next year front so thanks for

00:38:01,620 --> 00:38:05,940
listening I hope that kind of helped you

00:38:03,420 --> 00:38:07,920
to give an overview of the challenges if

00:38:05,940 --> 00:38:10,290
you're actually trying to implement your

00:38:07,920 --> 00:38:12,540
service on top of a managed kubernetes

00:38:10,290 --> 00:38:14,880
solution if you don't want to deal with

00:38:12,540 --> 00:38:18,270
that if you just want to try out a

00:38:14,880 --> 00:38:22,440
database you can actually get a 14-day

00:38:18,270 --> 00:38:24,420
free trial on for Oasis also just try

00:38:22,440 --> 00:38:27,630
out our docker containers so just docker

00:38:24,420 --> 00:38:29,700
pull Arango DB will also get you started

00:38:27,630 --> 00:38:33,680
on your local box or your favorite cloud

00:38:29,700 --> 00:38:33,680
provider thanks very much for listening

00:38:35,300 --> 00:38:51,810

YouTube URL: https://www.youtube.com/watch?v=91c1G9a3k3s


