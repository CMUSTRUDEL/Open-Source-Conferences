Title: stackconf 2021 | Stretching the Service Mesh Beyond the Clouds
Publication date: 2021-06-24
Playlist: stackconf online 2021
Description: 
	by Rosemary Wang

We hear a lot about using service mesh with Kubernetes and public clouds, but what about outside the clouds? In this talk, you’ll learn creative ways to apply a service mesh across different platforms and environments to automate canary deployments, facilitate cloud migrations, and more. By combining HashiCorp Consul’s service mesh and Terraform’s infrastructure as code, you can build a more seamless operational experience across multiple environments.


NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de/
Blog: http://blog.netways.de/
NWS: https://nws.netways.de 

Webinare
Archiv Link: https://www.netways.de/netways/webinare/

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh/


Musik: https://www.frametraxx.de/
Captions: 
	00:00:05,660 --> 00:00:12,880
[Applause]

00:00:06,120 --> 00:00:14,559
[Music]

00:00:12,880 --> 00:00:15,360
welcome to the session on stretching the

00:00:14,559 --> 00:00:18,160
surface mesh

00:00:15,360 --> 00:00:18,160
beyond the clouds

00:00:18,960 --> 00:00:23,279
i worked for an organization where we

00:00:21,359 --> 00:00:24,000
moved from data center to amazon web

00:00:23,279 --> 00:00:26,880
services

00:00:24,000 --> 00:00:28,560
it's a pretty typical story where we had

00:00:26,880 --> 00:00:30,080
some applications in the data center and

00:00:28,560 --> 00:00:30,800
we said maybe some of them should be

00:00:30,080 --> 00:00:33,600
better in

00:00:30,800 --> 00:00:35,840
amazon web services and we did a a full

00:00:33,600 --> 00:00:37,920
assessment on which ones went to cloud

00:00:35,840 --> 00:00:39,680
and which ones didn't and we were in the

00:00:37,920 --> 00:00:41,040
process of moving some of them to the

00:00:39,680 --> 00:00:43,520
cloud when

00:00:41,040 --> 00:00:46,640
well one team came by and said they

00:00:43,520 --> 00:00:48,559
wanted to use microsoft azure instead

00:00:46,640 --> 00:00:50,399
this is a valid request right there were

00:00:48,559 --> 00:00:53,520
some important

00:00:50,399 --> 00:00:55,680
security requirements that azure

00:00:53,520 --> 00:00:58,000
fulfilled then amazon wasn't really

00:00:55,680 --> 00:01:01,520
sufficiently fulfilling for this team

00:00:58,000 --> 00:01:02,399
so we decided to implement a multiple

00:01:01,520 --> 00:01:06,320
cloud

00:01:02,399 --> 00:01:08,080
topology problematically we also found

00:01:06,320 --> 00:01:11,840
out that some of the applications needed

00:01:08,080 --> 00:01:13,520
to be refactored to use kubernetes

00:01:11,840 --> 00:01:15,200
original applications that we had moved

00:01:13,520 --> 00:01:17,280
were using virtual machines

00:01:15,200 --> 00:01:19,040
and we found out that some of them might

00:01:17,280 --> 00:01:21,360
actually be better using

00:01:19,040 --> 00:01:23,520
uh kubernetes specifically so then we

00:01:21,360 --> 00:01:26,720
had to build a kubernetes platform

00:01:23,520 --> 00:01:28,320
across azure and amazon and then

00:01:26,720 --> 00:01:29,759
eventually migrate each of these

00:01:28,320 --> 00:01:32,720
applications to both

00:01:29,759 --> 00:01:34,720
kubernetes services in there on top of

00:01:32,720 --> 00:01:36,079
that we found out that some applications

00:01:34,720 --> 00:01:37,680
would never be able to run in a

00:01:36,079 --> 00:01:39,920
container and could never be

00:01:37,680 --> 00:01:41,680
put in kubernetes and likely would never

00:01:39,920 --> 00:01:44,079
move out of the data center either

00:01:41,680 --> 00:01:45,920
so the result was that we ended up

00:01:44,079 --> 00:01:46,799
having about five different kinds of

00:01:45,920 --> 00:01:50,079
environments

00:01:46,799 --> 00:01:51,040
and workloads and topologies including

00:01:50,079 --> 00:01:53,840
data center

00:01:51,040 --> 00:01:55,439
amazon azure kubernetes and virtual

00:01:53,840 --> 00:01:58,560
machines

00:01:55,439 --> 00:02:00,560
this was rather difficult to manage and

00:01:58,560 --> 00:02:03,200
i tried to think of a better way that we

00:02:00,560 --> 00:02:06,000
could manage it

00:02:03,200 --> 00:02:08,080
my name is rosemary wong and i am a

00:02:06,000 --> 00:02:10,160
developer advocate at hashicorp my main

00:02:08,080 --> 00:02:11,520
goal is to try to find the solutions to

00:02:10,160 --> 00:02:13,520
some of these problems

00:02:11,520 --> 00:02:14,640
like how do you operate a multiple cloud

00:02:13,520 --> 00:02:17,599
environment

00:02:14,640 --> 00:02:18,720
and provide one point of control for all

00:02:17,599 --> 00:02:22,239
of them right that's

00:02:18,720 --> 00:02:25,360
the magic behind technology is can you

00:02:22,239 --> 00:02:27,520
offer one point of control across all of

00:02:25,360 --> 00:02:29,200
these environments that we have

00:02:27,520 --> 00:02:30,640
if you're curious to learn more about me

00:02:29,200 --> 00:02:33,680
or some of the work that i have

00:02:30,640 --> 00:02:35,599
feel free to check out the url i have

00:02:33,680 --> 00:02:38,360
in this slide as well as my handle on

00:02:35,599 --> 00:02:41,040
social media

00:02:38,360 --> 00:02:42,640
j-o-a-t-m-o-n-08

00:02:41,040 --> 00:02:44,319
all right so let's think about our main

00:02:42,640 --> 00:02:45,760
problem we're trying to solve our main

00:02:44,319 --> 00:02:47,599
problem we're trying to solve

00:02:45,760 --> 00:02:49,519
is that there are multiple platforms and

00:02:47,599 --> 00:02:50,959
environments in an organization that's

00:02:49,519 --> 00:02:54,560
taking advantage

00:02:50,959 --> 00:02:56,560
of different cloud offerings out there

00:02:54,560 --> 00:02:58,560
it's very common to have a couple

00:02:56,560 --> 00:03:00,239
different clouds as well as

00:02:58,560 --> 00:03:01,200
infrastructure providers and the data

00:03:00,239 --> 00:03:03,040
center

00:03:01,200 --> 00:03:05,120
it's you know it's important from an

00:03:03,040 --> 00:03:07,280
availability standpoint but also from a

00:03:05,120 --> 00:03:10,560
vendoring standpoint you want to try

00:03:07,280 --> 00:03:12,800
to you know get a little bit more um

00:03:10,560 --> 00:03:14,959
variety in there but more importantly

00:03:12,800 --> 00:03:16,480
even if you're using one vendor you also

00:03:14,959 --> 00:03:17,760
have multiple run times between

00:03:16,480 --> 00:03:19,920
kubernetes

00:03:17,760 --> 00:03:23,040
serverless virtual machines and all of

00:03:19,920 --> 00:03:25,120
the new offerings that are coming out

00:03:23,040 --> 00:03:26,080
so typically when you have multiple

00:03:25,120 --> 00:03:28,319
environments

00:03:26,080 --> 00:03:31,040
you usually control a lot of the

00:03:28,319 --> 00:03:34,159
communication between the environments

00:03:31,040 --> 00:03:35,959
with dns for example i might have a dns

00:03:34,159 --> 00:03:37,840
like my

00:03:35,959 --> 00:03:41,120
application.mycompany.net and

00:03:37,840 --> 00:03:43,680
it spans data center and cloud

00:03:41,120 --> 00:03:44,159
deployments so my application and data

00:03:43,680 --> 00:03:46,000
center

00:03:44,159 --> 00:03:48,319
is in front of a load balancer specific

00:03:46,000 --> 00:03:51,599
to data center and myapplication.cloud

00:03:48,319 --> 00:03:55,439
is specific to a load balancer in cloud

00:03:51,599 --> 00:03:55,439
then i have one dns to span it all

00:03:55,680 --> 00:03:59,360
rather than say that i have one dns to

00:03:58,159 --> 00:04:01,519
span at all

00:03:59,360 --> 00:04:03,519
i might think about putting some kind of

00:04:01,519 --> 00:04:06,080
infrastructure layer in place

00:04:03,519 --> 00:04:07,680
this infrastructure layer in place might

00:04:06,080 --> 00:04:10,040
have some vanity dns

00:04:07,680 --> 00:04:11,680
so it's a dns that's

00:04:10,040 --> 00:04:13,760
myapplication.mycompany.net

00:04:11,680 --> 00:04:15,760
but what it's able to do is immediately

00:04:13,760 --> 00:04:16,639
resolve if it needs to go to data center

00:04:15,760 --> 00:04:18,239
or cloud

00:04:16,639 --> 00:04:19,840
so you can build this infrastructure

00:04:18,239 --> 00:04:22,800
layer yourself

00:04:19,840 --> 00:04:24,960
or you can use a tool that exists to

00:04:22,800 --> 00:04:28,160
offer this kind of capability and that

00:04:24,960 --> 00:04:31,919
provides some kind of initial control

00:04:28,160 --> 00:04:33,520
over communication between environments

00:04:31,919 --> 00:04:35,520
the second piece that you need is some

00:04:33,520 --> 00:04:36,880
kind of automation to control the

00:04:35,520 --> 00:04:38,960
infrastructure layer

00:04:36,880 --> 00:04:40,479
so in most environments you'll find that

00:04:38,960 --> 00:04:42,800
some kind of

00:04:40,479 --> 00:04:44,960
control plane will exist and this could

00:04:42,800 --> 00:04:47,280
be usually in the network right

00:04:44,960 --> 00:04:49,199
so in the case of some kind of software

00:04:47,280 --> 00:04:49,919
network where you have some control

00:04:49,199 --> 00:04:51,759
plane

00:04:49,919 --> 00:04:53,440
that manages and configures that

00:04:51,759 --> 00:04:56,880
infrastructure layer to funnel

00:04:53,440 --> 00:04:58,720
traffic between multiple environments

00:04:56,880 --> 00:05:00,080
now remember this is important there's

00:04:58,720 --> 00:05:01,360
some automation and there's some

00:05:00,080 --> 00:05:03,600
infrastructure layer

00:05:01,360 --> 00:05:05,360
the combination of that is a service

00:05:03,600 --> 00:05:07,280
mesh a service mesh

00:05:05,360 --> 00:05:08,880
is an infrastructure layer that manages

00:05:07,280 --> 00:05:10,000
and abstracts service to service

00:05:08,880 --> 00:05:11,840
communication

00:05:10,000 --> 00:05:14,400
you hear a lot about service mesh in the

00:05:11,840 --> 00:05:16,560
microservices architecture space

00:05:14,400 --> 00:05:18,000
microservices have a lot of a lot of

00:05:16,560 --> 00:05:19,759
other problems and a lot of other

00:05:18,000 --> 00:05:23,199
complexities that service mesh

00:05:19,759 --> 00:05:26,160
offers to solve that also includes

00:05:23,199 --> 00:05:28,639
retries error handling preventing

00:05:26,160 --> 00:05:30,800
cascading failures for upstream

00:05:28,639 --> 00:05:32,320
applications so there's a lot of ways

00:05:30,800 --> 00:05:33,360
that surface mesh solves problems for

00:05:32,320 --> 00:05:35,360
microservices

00:05:33,360 --> 00:05:37,440
but we can actually apply it to multiple

00:05:35,360 --> 00:05:39,520
environments and platforms as well

00:05:37,440 --> 00:05:41,039
so service mesh provides the automation

00:05:39,520 --> 00:05:42,880
and the infrastructure layer

00:05:41,039 --> 00:05:45,199
to handle some of the service to service

00:05:42,880 --> 00:05:48,240
communication between multiple clouds

00:05:45,199 --> 00:05:48,800
environments and platforms the way this

00:05:48,240 --> 00:05:50,800
works

00:05:48,800 --> 00:05:52,080
is that as i mentioned before there's

00:05:50,800 --> 00:05:53,680
two pieces there's the

00:05:52,080 --> 00:05:55,759
infrastructure layer in the form of

00:05:53,680 --> 00:05:56,479
proxies and there's the automation layer

00:05:55,759 --> 00:05:59,039
in the form

00:05:56,479 --> 00:06:00,160
of the control plane in this case i'm

00:05:59,039 --> 00:06:01,680
referring to console

00:06:00,160 --> 00:06:03,360
but most service meshes use a very

00:06:01,680 --> 00:06:05,360
similar approach

00:06:03,360 --> 00:06:07,039
so console uses a control plane they're

00:06:05,360 --> 00:06:08,800
the console servers

00:06:07,039 --> 00:06:10,319
in this case i have a console cluster

00:06:08,800 --> 00:06:10,960
deployed in the data center and a

00:06:10,319 --> 00:06:14,319
console

00:06:10,960 --> 00:06:16,960
cluster deployed in cloud the proxies

00:06:14,319 --> 00:06:18,800
funnel traffic between them so anything

00:06:16,960 --> 00:06:20,639
that has to be service to service

00:06:18,800 --> 00:06:23,039
goes through the proxies first for

00:06:20,639 --> 00:06:24,639
example the ui application

00:06:23,039 --> 00:06:26,400
goes through the proxy in order to

00:06:24,639 --> 00:06:28,160
communicate with my application in the

00:06:26,400 --> 00:06:31,280
data center

00:06:28,160 --> 00:06:32,400
console will configure the proxies and

00:06:31,280 --> 00:06:35,440
therefore control

00:06:32,400 --> 00:06:37,520
the rules and the placement

00:06:35,440 --> 00:06:39,199
of where the traffic is allowed to

00:06:37,520 --> 00:06:41,840
travel and not allowed to travel within

00:06:39,199 --> 00:06:43,600
the environment

00:06:41,840 --> 00:06:45,360
then we also have the ui to my

00:06:43,600 --> 00:06:48,240
application so the ui

00:06:45,360 --> 00:06:50,560
can also use the proxies to go to my

00:06:48,240 --> 00:06:52,160
application in the cloud as well

00:06:50,560 --> 00:06:53,840
this is important to recognize

00:06:52,160 --> 00:06:56,240
especially because this

00:06:53,840 --> 00:06:58,880
allows the cross-cloud access so as long

00:06:56,240 --> 00:07:00,720
as the proxies themselves are able to

00:06:58,880 --> 00:07:02,400
communicate or able to provide that

00:07:00,720 --> 00:07:04,800
level of control

00:07:02,400 --> 00:07:06,160
even in console for example in the data

00:07:04,800 --> 00:07:08,560
center and the cloud

00:07:06,160 --> 00:07:09,199
we're able to control communication

00:07:08,560 --> 00:07:11,360
across

00:07:09,199 --> 00:07:12,960
environments as well so this is why the

00:07:11,360 --> 00:07:15,599
service mesh provides

00:07:12,960 --> 00:07:16,400
the somewhat solution to this problem

00:07:15,599 --> 00:07:17,919
set

00:07:16,400 --> 00:07:20,560
but you might look at this and say hey

00:07:17,919 --> 00:07:21,280
rosemary if i add a service mesh doesn't

00:07:20,560 --> 00:07:23,840
that introduce

00:07:21,280 --> 00:07:25,360
more problems and you're not wrong there

00:07:23,840 --> 00:07:27,199
is some added complexity

00:07:25,360 --> 00:07:29,599
right you're adding yet another agent to

00:07:27,199 --> 00:07:30,960
a machine or yet another agent to your

00:07:29,599 --> 00:07:32,960
deployment

00:07:30,960 --> 00:07:34,240
and there are a lot of benchmarks for

00:07:32,960 --> 00:07:37,039
this now but

00:07:34,240 --> 00:07:38,000
it doesn't add that much more it doesn't

00:07:37,039 --> 00:07:41,360
add that much more

00:07:38,000 --> 00:07:43,520
in terms of a performance loss right

00:07:41,360 --> 00:07:45,440
in fact for the most part the the added

00:07:43,520 --> 00:07:47,280
layer of abstraction doesn't necessarily

00:07:45,440 --> 00:07:50,000
add too much of a performance

00:07:47,280 --> 00:07:50,960
loss or gain one way or the other

00:07:50,000 --> 00:07:53,520
however

00:07:50,960 --> 00:07:55,280
there is an operational complexity that

00:07:53,520 --> 00:07:55,759
you're adding it's more to debug and

00:07:55,280 --> 00:07:58,879
it's

00:07:55,759 --> 00:08:00,800
more to operate the point of failure is

00:07:58,879 --> 00:08:02,319
now in the service mesh it's not in the

00:08:00,800 --> 00:08:03,599
applications themselves

00:08:02,319 --> 00:08:05,440
so you have to think about whether or

00:08:03,599 --> 00:08:07,680
not you want to take on the operational

00:08:05,440 --> 00:08:09,360
complexity of that

00:08:07,680 --> 00:08:11,120
on the other hand if you don't have a

00:08:09,360 --> 00:08:12,240
service mesh and you have many

00:08:11,120 --> 00:08:13,680
environments

00:08:12,240 --> 00:08:15,360
and you're planning to run them for many

00:08:13,680 --> 00:08:17,360
years you often end up having

00:08:15,360 --> 00:08:19,120
more operational responsibility in the

00:08:17,360 --> 00:08:20,960
long term anyway

00:08:19,120 --> 00:08:22,400
you don't really have any automation so

00:08:20,960 --> 00:08:24,879
you have the automation

00:08:22,400 --> 00:08:25,599
localized to each environment so for

00:08:24,879 --> 00:08:27,440
example

00:08:25,599 --> 00:08:29,360
you have localized automation specific

00:08:27,440 --> 00:08:33,120
to kubernetes and amazon

00:08:29,360 --> 00:08:35,440
versus virtual machines on azure

00:08:33,120 --> 00:08:37,519
you also have multiple sources of truth

00:08:35,440 --> 00:08:39,200
you have multiple sources of control

00:08:37,519 --> 00:08:40,560
right you don't necessarily have one

00:08:39,200 --> 00:08:43,279
view of how

00:08:40,560 --> 00:08:45,519
an azure service that's running on

00:08:43,279 --> 00:08:46,800
windows is communicating to a kubernetes

00:08:45,519 --> 00:08:50,160
service

00:08:46,800 --> 00:08:51,680
in amazon running on linux there's not

00:08:50,160 --> 00:08:52,399
really one place that you can control

00:08:51,680 --> 00:08:54,560
that

00:08:52,399 --> 00:08:56,560
and you get that view on top of that

00:08:54,560 --> 00:08:58,240
there are multiple metrics approaches

00:08:56,560 --> 00:08:59,920
for every environment that you have you

00:08:58,240 --> 00:09:02,560
have a unique metric

00:08:59,920 --> 00:09:04,080
solution and sometimes it helps to work

00:09:02,560 --> 00:09:06,160
with the vendored solution

00:09:04,080 --> 00:09:08,399
and it's since it's the path of least

00:09:06,160 --> 00:09:09,760
resistance and it's easiest the problem

00:09:08,399 --> 00:09:11,360
is that when you have multiple clouds

00:09:09,760 --> 00:09:13,519
and you need to aggregate all of the

00:09:11,360 --> 00:09:15,839
transactions across the clouds

00:09:13,519 --> 00:09:17,279
well you don't really know what kind of

00:09:15,839 --> 00:09:20,000
standardized metrics you're going to be

00:09:17,279 --> 00:09:20,000
able to retrieve

00:09:20,080 --> 00:09:23,440
so what are some of the solutions to

00:09:21,680 --> 00:09:24,640
this we talked a little bit about the

00:09:23,440 --> 00:09:26,959
problem which is multiple

00:09:24,640 --> 00:09:28,480
platforms and multiple clouds but what

00:09:26,959 --> 00:09:30,240
are some solutions we can think about in

00:09:28,480 --> 00:09:32,240
the service mesh space

00:09:30,240 --> 00:09:34,000
well service mesh has a couple of

00:09:32,240 --> 00:09:36,480
deployment topologies

00:09:34,000 --> 00:09:37,760
so service mesh itself you don't have to

00:09:36,480 --> 00:09:39,360
do service mesh across

00:09:37,760 --> 00:09:41,279
every cloud right so the beginning of

00:09:39,360 --> 00:09:42,320
this talk i introduced it as stretching

00:09:41,279 --> 00:09:43,760
the service mesh

00:09:42,320 --> 00:09:45,200
we're not going to try to stretch one

00:09:43,760 --> 00:09:46,160
service mesh across multiple

00:09:45,200 --> 00:09:48,560
environments

00:09:46,160 --> 00:09:50,240
um that's a misnomer so i want to show

00:09:48,560 --> 00:09:51,519
the different types of topologies that

00:09:50,240 --> 00:09:53,360
you'll actually have

00:09:51,519 --> 00:09:55,519
so imagine this is the topology that you

00:09:53,360 --> 00:09:57,120
have you have a data center you have a

00:09:55,519 --> 00:09:59,440
cloud you need to communicate data

00:09:57,120 --> 00:10:00,959
center and cloud

00:09:59,440 --> 00:10:03,200
the first thing you'll do is deploy the

00:10:00,959 --> 00:10:05,440
service mesh in the cloud right so

00:10:03,200 --> 00:10:07,360
if you have a newer environment you

00:10:05,440 --> 00:10:09,839
haven't really done as much to it

00:10:07,360 --> 00:10:11,519
it's easier to port in the service mesh

00:10:09,839 --> 00:10:12,240
if you have a containerized environment

00:10:11,519 --> 00:10:13,760
in the cloud

00:10:12,240 --> 00:10:15,760
it's also much easier to implement the

00:10:13,760 --> 00:10:16,560
service mesh there so you start with the

00:10:15,760 --> 00:10:18,560
service mesh

00:10:16,560 --> 00:10:21,440
in the newer environment or the

00:10:18,560 --> 00:10:23,440
containerized environment first

00:10:21,440 --> 00:10:24,800
and then you add a network automation

00:10:23,440 --> 00:10:27,200
piece to synchronize

00:10:24,800 --> 00:10:27,839
the catalog of the service mesh in the

00:10:27,200 --> 00:10:31,839
cloud

00:10:27,839 --> 00:10:33,839
to anything in the older environments

00:10:31,839 --> 00:10:35,920
benefit to this is that in your service

00:10:33,839 --> 00:10:37,839
mesh you have the ability to control

00:10:35,920 --> 00:10:40,959
retries and error handlings

00:10:37,839 --> 00:10:44,480
to the non-service mesh services so

00:10:40,959 --> 00:10:46,000
for example if i had the ui in the cloud

00:10:44,480 --> 00:10:48,800
and i needed to communicate

00:10:46,000 --> 00:10:49,440
to my application in the data center the

00:10:48,800 --> 00:10:51,200
ui

00:10:49,440 --> 00:10:52,880
in the service mesh has the ability to

00:10:51,200 --> 00:10:55,760
control some of the retry and error

00:10:52,880 --> 00:10:57,120
handling on the service mesh side

00:10:55,760 --> 00:10:59,279
there's also the ability to use

00:10:57,120 --> 00:11:01,680
progressive delivery techniques within

00:10:59,279 --> 00:11:02,720
the service mesh itself so i could do

00:11:01,680 --> 00:11:06,160
something like canary

00:11:02,720 --> 00:11:08,640
a b testing or feature flagging however

00:11:06,160 --> 00:11:11,200
in the non-service mesh

00:11:08,640 --> 00:11:11,839
space the non-service mesh space still

00:11:11,200 --> 00:11:14,079
gets the

00:11:11,839 --> 00:11:16,160
the benefit of automated control so the

00:11:14,079 --> 00:11:17,360
network automation is still allowing my

00:11:16,160 --> 00:11:20,399
non-service mesh

00:11:17,360 --> 00:11:22,240
to be a part of the main service mesh it

00:11:20,399 --> 00:11:23,760
gives the illusion of stretching it even

00:11:22,240 --> 00:11:26,320
though there is no service mesh at all

00:11:23,760 --> 00:11:27,839
in the data center and the added benefit

00:11:26,320 --> 00:11:28,800
is that i don't have to change existing

00:11:27,839 --> 00:11:31,040
applications

00:11:28,800 --> 00:11:31,839
it's hard to retrofit a service mesh

00:11:31,040 --> 00:11:33,920
into

00:11:31,839 --> 00:11:34,959
an older environment and so i don't have

00:11:33,920 --> 00:11:38,079
to change anything

00:11:34,959 --> 00:11:41,200
in the non-service mesh

00:11:38,079 --> 00:11:42,480
so today's demo actually implements this

00:11:41,200 --> 00:11:44,880
kind of topology

00:11:42,480 --> 00:11:46,560
i'll use a console service mesh in the

00:11:44,880 --> 00:11:49,600
cloud so the cloud is

00:11:46,560 --> 00:11:51,120
using kubernetes and i also deploy a

00:11:49,600 --> 00:11:52,880
console ingress gateway the ingress

00:11:51,120 --> 00:11:53,600
gateway just helps control traffic from

00:11:52,880 --> 00:11:56,240
outbound

00:11:53,600 --> 00:11:58,000
into the kubernetes cluster it just

00:11:56,240 --> 00:11:59,200
makes it easier from a security

00:11:58,000 --> 00:12:01,040
standpoint

00:11:59,200 --> 00:12:03,680
so what happens is that console ingress

00:12:01,040 --> 00:12:06,399
gateway registers a subset of services

00:12:03,680 --> 00:12:08,240
it recognizes any updates as well as any

00:12:06,399 --> 00:12:09,519
host headers that need to be added et

00:12:08,240 --> 00:12:11,600
cetera

00:12:09,519 --> 00:12:13,279
and it will synchronize that information

00:12:11,600 --> 00:12:15,680
to console terraform sync

00:12:13,279 --> 00:12:17,040
console terraform sync it serves as the

00:12:15,680 --> 00:12:18,320
network automation piece

00:12:17,040 --> 00:12:20,800
i'll speak a little bit about that in

00:12:18,320 --> 00:12:22,800
the next slide console terraform sync

00:12:20,800 --> 00:12:24,000
will then configure the application load

00:12:22,800 --> 00:12:25,920
balancer so

00:12:24,000 --> 00:12:27,839
in terms of the topology we've got a

00:12:25,920 --> 00:12:30,240
service mesh in the form of console

00:12:27,839 --> 00:12:32,160
and the network automation and the form

00:12:30,240 --> 00:12:33,200
of console terraform sync you can use

00:12:32,160 --> 00:12:35,440
other tools

00:12:33,200 --> 00:12:38,800
and this is just one way to demonstrate

00:12:35,440 --> 00:12:41,040
how the topology and the pattern works

00:12:38,800 --> 00:12:42,000
the great part about this implementation

00:12:41,040 --> 00:12:44,880
is that it is

00:12:42,000 --> 00:12:46,720
private connectivity between the data

00:12:44,880 --> 00:12:49,200
center and cloud

00:12:46,720 --> 00:12:50,639
however the downside to this is that you

00:12:49,200 --> 00:12:53,040
have to have a separate piece of

00:12:50,639 --> 00:12:53,680
network automation so i did have to

00:12:53,040 --> 00:12:56,639
write

00:12:53,680 --> 00:12:58,160
the implementation to allow the console

00:12:56,639 --> 00:12:59,760
services to synchronize

00:12:58,160 --> 00:13:01,920
and configure the application load

00:12:59,760 --> 00:13:04,240
balancer it's a terraform module

00:13:01,920 --> 00:13:05,440
but you still have to write it and on

00:13:04,240 --> 00:13:06,079
top of this i don't have a way of

00:13:05,440 --> 00:13:07,760
consolidating

00:13:06,079 --> 00:13:09,279
metrics there are still multiple metrics

00:13:07,760 --> 00:13:10,720
in this environment

00:13:09,279 --> 00:13:13,360
so the way the console terraform sync

00:13:10,720 --> 00:13:14,720
works is that console terraform sync

00:13:13,360 --> 00:13:16,720
checks whether or not the service

00:13:14,720 --> 00:13:19,440
changed in console

00:13:16,720 --> 00:13:20,320
it gets the event and it uses a template

00:13:19,440 --> 00:13:22,800
to create the

00:13:20,320 --> 00:13:23,760
terraform configuration the terraform

00:13:22,800 --> 00:13:25,920
configuration

00:13:23,760 --> 00:13:26,959
references a module that module that

00:13:25,920 --> 00:13:28,959
i've pre-written

00:13:26,959 --> 00:13:30,560
to configure an application load

00:13:28,959 --> 00:13:32,560
balancer in amazon

00:13:30,560 --> 00:13:34,079
it runs terraform and then it will

00:13:32,560 --> 00:13:34,800
download the module and apply changes

00:13:34,079 --> 00:13:36,720
automatically

00:13:34,800 --> 00:13:38,079
and the cycle continues so anytime the

00:13:36,720 --> 00:13:41,279
service changes

00:13:38,079 --> 00:13:41,920
uh the console terraform sync agent gets

00:13:41,279 --> 00:13:44,480
the event

00:13:41,920 --> 00:13:46,320
and then runs through this template and

00:13:44,480 --> 00:13:49,839
then terraform apply

00:13:46,320 --> 00:13:49,839
cycle constantly

00:13:50,800 --> 00:13:54,639
so in this network automation demo

00:13:52,720 --> 00:13:55,440
you're welcome to recreate it yourself

00:13:54,639 --> 00:13:58,240
if you'd like

00:13:55,440 --> 00:13:59,440
um i did deploy it all in aws it is in

00:13:58,240 --> 00:14:01,199
multiple regions

00:13:59,440 --> 00:14:02,880
um but it is something that you can

00:14:01,199 --> 00:14:05,279
recreate i wanted to make sure it was in

00:14:02,880 --> 00:14:07,839
one cloud so you have the ability to do

00:14:05,279 --> 00:14:10,079
a recreation if you wanted it's deployed

00:14:07,839 --> 00:14:10,720
in aws the data center uses virtual

00:14:10,079 --> 00:14:14,320
machines

00:14:10,720 --> 00:14:15,519
in us east 2. cloud uses kubernetes in

00:14:14,320 --> 00:14:18,800
us west 2.

00:14:15,519 --> 00:14:20,800
so data center cluster cloud cluster

00:14:18,800 --> 00:14:22,560
there's also a network automation piece

00:14:20,800 --> 00:14:31,199
and that's console terraform sync and it

00:14:22,560 --> 00:14:33,040
configures the application load balancer

00:14:31,199 --> 00:14:34,959
so in this demo we're going to show the

00:14:33,040 --> 00:14:36,240
network automation topology using

00:14:34,959 --> 00:14:38,800
console terraform sync

00:14:36,240 --> 00:14:39,680
and console service mesh so the first

00:14:38,800 --> 00:14:42,800
thing we'll do

00:14:39,680 --> 00:14:44,959
is set up the defaults

00:14:42,800 --> 00:14:47,519
so by default what happens in this

00:14:44,959 --> 00:14:49,680
environment is that the data center gets

00:14:47,519 --> 00:14:51,440
a hundred percent of traffic

00:14:49,680 --> 00:14:53,920
so here we're trying to do i guess a

00:14:51,440 --> 00:14:55,279
canary but it's more manual in nature so

00:14:53,920 --> 00:14:57,199
we ideally want to send a certain

00:14:55,279 --> 00:15:00,079
percentage of traffic to the cloud right

00:14:57,199 --> 00:15:00,079
now it's at zero

00:15:01,440 --> 00:15:04,720
the only time we actually send traffic

00:15:03,360 --> 00:15:06,920
to the cloud is when

00:15:04,720 --> 00:15:08,480
the host header is

00:15:06,920 --> 00:15:10,839
myapplication.mycompany.net

00:15:08,480 --> 00:15:12,240
so if the host header is

00:15:10,839 --> 00:15:14,240
myapplication.mycompany.net

00:15:12,240 --> 00:15:17,839
then it will forward everything to data

00:15:14,240 --> 00:15:20,639
center or ingress gateway cloud

00:15:17,839 --> 00:15:22,160
this is actually a new rule on the load

00:15:20,639 --> 00:15:23,920
balancer that i've created

00:15:22,160 --> 00:15:25,839
as part of consultair from sync as sort

00:15:23,920 --> 00:15:28,160
of the initial reload

00:15:25,839 --> 00:15:29,199
so this configuration by default will

00:15:28,160 --> 00:15:32,000
actually go to

00:15:29,199 --> 00:15:33,920
http uh we'll go to data center this is

00:15:32,000 --> 00:15:36,320
where the default

00:15:33,920 --> 00:15:37,680
request will go so if anybody is

00:15:36,320 --> 00:15:38,480
actually doing this from a user

00:15:37,680 --> 00:15:40,079
standpoint

00:15:38,480 --> 00:15:42,399
what happens is that all the traffic

00:15:40,079 --> 00:15:44,800
will go to data center by default

00:15:42,399 --> 00:15:46,959
i'm using the host header my application

00:15:44,800 --> 00:15:49,279
mycompany.net as a testing

00:15:46,959 --> 00:15:50,320
mechanism so i can actually ensure

00:15:49,279 --> 00:15:52,560
separately

00:15:50,320 --> 00:15:56,480
that my cloud application is working

00:15:52,560 --> 00:15:56,480
before i try to deploy the data center

00:16:00,320 --> 00:16:04,240
you'll notice that the logs coming from

00:16:02,720 --> 00:16:06,240
one of the terminals

00:16:04,240 --> 00:16:07,279
has some errors you know connecting to

00:16:06,240 --> 00:16:09,040
console this is

00:16:07,279 --> 00:16:11,040
consulter from sync what it's doing is

00:16:09,040 --> 00:16:12,880
retrying and connecting to console

00:16:11,040 --> 00:16:14,079
um console terraform sync has already

00:16:12,880 --> 00:16:17,279
run once

00:16:14,079 --> 00:16:19,279
and applied the rule that is my

00:16:17,279 --> 00:16:20,880
application my company.net so

00:16:19,279 --> 00:16:22,399
that's why you'll see it try to connect

00:16:20,880 --> 00:16:24,320
just to make sure that it's

00:16:22,399 --> 00:16:26,079
updated and what it's doing is

00:16:24,320 --> 00:16:27,759
synchronizing the catalog for the

00:16:26,079 --> 00:16:30,800
ingress gateway in console

00:16:27,759 --> 00:16:32,800
to this particular application

00:16:30,800 --> 00:16:34,320
if i examine the target group four data

00:16:32,800 --> 00:16:36,000
center you'll notice that the target

00:16:34,320 --> 00:16:38,160
group for the data center has one

00:16:36,000 --> 00:16:40,000
instance it is a virtual machine that

00:16:38,160 --> 00:16:42,959
has my application on it

00:16:40,000 --> 00:16:44,639
so it is healthy it's available just to

00:16:42,959 --> 00:16:48,480
show that it is healthy and available

00:16:44,639 --> 00:16:51,519
i'll actually make a test to it

00:16:48,480 --> 00:16:54,639
and the test will use the host header

00:16:51,519 --> 00:16:55,199
to the load balancer endpoints by

00:16:54,639 --> 00:16:57,360
default

00:16:55,199 --> 00:16:58,560
it's sending 100 of traffic to the data

00:16:57,360 --> 00:17:01,519
center which is where

00:16:58,560 --> 00:17:03,920
you're seeing the api response going and

00:17:01,519 --> 00:17:04,319
this is a handy application that shows

00:17:03,920 --> 00:17:06,000
where

00:17:04,319 --> 00:17:07,360
the request is going and in this case

00:17:06,000 --> 00:17:09,280
i've labeled it

00:17:07,360 --> 00:17:19,199
to go to data center so it is going to

00:17:09,280 --> 00:17:20,640
my application in the data center

00:17:19,199 --> 00:17:23,039
all right so now what we're going to try

00:17:20,640 --> 00:17:25,679
to do is increase some traffic and

00:17:23,039 --> 00:17:27,199
test whether or not the cloud

00:17:25,679 --> 00:17:30,080
application is working

00:17:27,199 --> 00:17:32,080
so the cloud application is connected to

00:17:30,080 --> 00:17:33,679
the ingress gateway for console i

00:17:32,080 --> 00:17:35,440
connected it to an ingress gateway in

00:17:33,679 --> 00:17:37,120
console just because it controls the

00:17:35,440 --> 00:17:38,960
traffic going into the cluster and just

00:17:37,120 --> 00:17:41,440
secures it a little bit more

00:17:38,960 --> 00:17:43,039
but in this case we have three instances

00:17:41,440 --> 00:17:45,840
of my application

00:17:43,039 --> 00:17:46,320
the three instances of my application

00:17:45,840 --> 00:17:49,440
connect

00:17:46,320 --> 00:17:52,240
to the ingress gateway if i

00:17:49,440 --> 00:17:53,039
look at the tagging and metadata within

00:17:52,240 --> 00:17:54,960
the instances

00:17:53,039 --> 00:17:56,880
themselves the tagging and metadata show

00:17:54,960 --> 00:17:58,799
that the weight is zero so

00:17:56,880 --> 00:18:00,080
this is actually where i control the

00:17:58,799 --> 00:18:02,799
weight of how much traffic

00:18:00,080 --> 00:18:04,799
is going to the application load

00:18:02,799 --> 00:18:07,440
balancer listener rule so remember

00:18:04,799 --> 00:18:08,559
right now it's at zero when i change

00:18:07,440 --> 00:18:10,880
that weight

00:18:08,559 --> 00:18:12,559
in console that weight will then get

00:18:10,880 --> 00:18:14,880
reflected in

00:18:12,559 --> 00:18:16,400
the application load balancer thanks to

00:18:14,880 --> 00:18:17,280
the network automation from consultaire

00:18:16,400 --> 00:18:18,480
from sync so

00:18:17,280 --> 00:18:19,919
what i'm doing is i'm going into the

00:18:18,480 --> 00:18:20,960
kubernetes and changing the service

00:18:19,919 --> 00:18:22,799
metadata weight

00:18:20,960 --> 00:18:24,400
the service metadata weight right does

00:18:22,799 --> 00:18:26,799
correlate to

00:18:24,400 --> 00:18:29,120
and propagate to console so that's where

00:18:26,799 --> 00:18:30,720
that's actually going to be changed

00:18:29,120 --> 00:18:32,720
and what consulter from sync is doing

00:18:30,720 --> 00:18:33,280
the network automation piece is actually

00:18:32,720 --> 00:18:35,600
reading

00:18:33,280 --> 00:18:36,640
that weight from the service catalog and

00:18:35,600 --> 00:18:39,919
then

00:18:36,640 --> 00:18:42,559
inputting it to a terraform variable

00:18:39,919 --> 00:18:44,480
so you'll see that running i've added to

00:18:42,559 --> 00:18:45,760
the application and some

00:18:44,480 --> 00:18:47,760
you know it will take some time for the

00:18:45,760 --> 00:18:49,440
new one to roll out

00:18:47,760 --> 00:18:51,200
and there's a lot of pods you'll see a

00:18:49,440 --> 00:18:53,440
lot of console and

00:18:51,200 --> 00:18:55,200
clients and servers running as well but

00:18:53,440 --> 00:18:56,480
you'll notice at the bottom where the

00:18:55,200 --> 00:18:59,200
logs are scrolling

00:18:56,480 --> 00:19:00,880
for console terraform sync that you

00:18:59,200 --> 00:19:02,320
there's a terraform apply going on so

00:19:00,880 --> 00:19:03,919
you'll notice that they apply complete

00:19:02,320 --> 00:19:06,240
resource is added

00:19:03,919 --> 00:19:08,320
this is because it's being driven by

00:19:06,240 --> 00:19:10,559
some of the events from console

00:19:08,320 --> 00:19:12,880
the weight itself hasn't fully changed

00:19:10,559 --> 00:19:14,799
right so it's only going to change when

00:19:12,880 --> 00:19:16,559
the new application comes up with the

00:19:14,799 --> 00:19:18,240
new weights

00:19:16,559 --> 00:19:20,160
so just to check if whether or not the

00:19:18,240 --> 00:19:21,280
weights are applied to the new

00:19:20,160 --> 00:19:23,039
application you can check

00:19:21,280 --> 00:19:24,720
tags and metadata and notice that the

00:19:23,039 --> 00:19:26,720
weights that i changed in kubernetes

00:19:24,720 --> 00:19:29,360
have been propagated to console

00:19:26,720 --> 00:19:30,480
and will now be propagated to console

00:19:29,360 --> 00:19:32,160
terraform sync

00:19:30,480 --> 00:19:34,480
and that's where you see the one changed

00:19:32,160 --> 00:19:36,640
right i've changed the listener rule

00:19:34,480 --> 00:19:38,240
there were some modifications done to

00:19:36,640 --> 00:19:40,480
the load balancer

00:19:38,240 --> 00:19:42,559
and now i've done one change if i

00:19:40,480 --> 00:19:43,760
refresh the application load balancer

00:19:42,559 --> 00:19:46,320
listener rules

00:19:43,760 --> 00:19:47,120
50 will go to cloud 50 goes to data

00:19:46,320 --> 00:19:48,960
center

00:19:47,120 --> 00:19:50,720
what i can do is test it right so let's

00:19:48,960 --> 00:19:52,320
say i've gone through this process and i

00:19:50,720 --> 00:19:54,160
want to deliver this application that

00:19:52,320 --> 00:19:56,720
i've re-platformed to cloud

00:19:54,160 --> 00:19:58,000
or i've re-platformed to a new place and

00:19:56,720 --> 00:19:58,480
i'm testing it just to make sure it

00:19:58,000 --> 00:20:00,160
works

00:19:58,480 --> 00:20:02,240
unfortunately it does not seem to work

00:20:00,160 --> 00:20:04,000
at all um

00:20:02,240 --> 00:20:06,320
and i know that the you know i'm

00:20:04,000 --> 00:20:06,880
wondering is this 504 gateway timeout

00:20:06,320 --> 00:20:09,120
from

00:20:06,880 --> 00:20:10,720
the cloud or from the data center and

00:20:09,120 --> 00:20:13,520
when i examine aws

00:20:10,720 --> 00:20:14,880
notice that it is coming from the cloud

00:20:13,520 --> 00:20:16,400
so the cloud is unhealthy

00:20:14,880 --> 00:20:17,919
requests are not coming in there's

00:20:16,400 --> 00:20:19,280
something problem there's some problem

00:20:17,919 --> 00:20:20,640
with it

00:20:19,280 --> 00:20:22,880
and so that's why there's a gateway

00:20:20,640 --> 00:20:24,240
timeout so i'm able to debug fairly

00:20:22,880 --> 00:20:26,400
quickly and say okay

00:20:24,240 --> 00:20:27,840
you know i know that i turned on the

00:20:26,400 --> 00:20:29,760
service mesh component

00:20:27,840 --> 00:20:31,280
i was able to synchronize the network

00:20:29,760 --> 00:20:34,159
automation

00:20:31,280 --> 00:20:36,360
if i actually try to run the command

00:20:34,159 --> 00:20:38,240
again to the same host

00:20:36,360 --> 00:20:39,120
myapplicationmycompany.net it will load

00:20:38,240 --> 00:20:41,280
balance 50

00:20:39,120 --> 00:20:42,640
to data center so i've noticed that

00:20:41,280 --> 00:20:45,760
there is something indeed

00:20:42,640 --> 00:20:47,679
not right with my cloud deployment and

00:20:45,760 --> 00:20:48,000
in my service mesh i can debug it i can

00:20:47,679 --> 00:20:50,000
go

00:20:48,000 --> 00:20:51,120
check and see what's going on but

00:20:50,000 --> 00:20:53,679
because i've

00:20:51,120 --> 00:20:54,880
orchestrated this control right this

00:20:53,679 --> 00:20:57,600
mock control

00:20:54,880 --> 00:20:58,559
using network automation what i can do

00:20:57,600 --> 00:21:01,840
is now go back

00:20:58,559 --> 00:21:04,000
and reset everything from a user

00:21:01,840 --> 00:21:06,640
standpoint or a customer standpoint

00:21:04,000 --> 00:21:08,640
because i'm setting a host header um and

00:21:06,640 --> 00:21:10,320
isolating it to the host header i could

00:21:08,640 --> 00:21:12,000
still say you know okay

00:21:10,320 --> 00:21:14,080
you know the top level load balancing or

00:21:12,000 --> 00:21:15,760
the top level dns is unaffected

00:21:14,080 --> 00:21:17,679
everything is still going to go to the

00:21:15,760 --> 00:21:20,960
data center by default

00:21:17,679 --> 00:21:22,720
but in this case i'll reset it back and

00:21:20,960 --> 00:21:24,720
console terraform sync responds by

00:21:22,720 --> 00:21:29,840
changing the load balancer

00:21:24,720 --> 00:21:31,679
back to um zero percent traffic to cloud

00:21:29,840 --> 00:21:33,120
so notice this is not using

00:21:31,679 --> 00:21:35,520
uh fully a service mesh it's really

00:21:33,120 --> 00:21:38,480
heavily depending on the service mesh

00:21:35,520 --> 00:21:40,320
in cloud being able to synchronize to

00:21:38,480 --> 00:21:42,799
the network automation

00:21:40,320 --> 00:21:44,640
for something in data center so the the

00:21:42,799 --> 00:21:46,720
key you know the really nice part about

00:21:44,640 --> 00:21:48,240
this workflow is that i still get one

00:21:46,720 --> 00:21:50,240
point of control in some regards

00:21:48,240 --> 00:21:52,480
anytime new services are being

00:21:50,240 --> 00:21:54,960
registered or updated in my

00:21:52,480 --> 00:21:56,960
cloud or new cloud environment i'm able

00:21:54,960 --> 00:21:59,520
to synchronize them back to

00:21:56,960 --> 00:22:02,400
an older environment such as data center

00:21:59,520 --> 00:22:02,400
or an older cloud

00:22:03,280 --> 00:22:07,600
so now that 100 is going back to data

00:22:05,360 --> 00:22:10,840
center everything is back to normal

00:22:07,600 --> 00:22:12,000
i can resume testing cloud and fixing

00:22:10,840 --> 00:22:13,919
things

00:22:12,000 --> 00:22:16,080
so our next topology is a little bit

00:22:13,919 --> 00:22:17,919
different we're not going to use network

00:22:16,080 --> 00:22:18,559
automation what we are going to do is

00:22:17,919 --> 00:22:22,640
stretch

00:22:18,559 --> 00:22:25,280
the mesh a little bit more accurately

00:22:22,640 --> 00:22:26,720
so in this situation you would deploy a

00:22:25,280 --> 00:22:28,400
service mesh into cloud

00:22:26,720 --> 00:22:30,000
and you deploy a service mesh into the

00:22:28,400 --> 00:22:31,840
data center

00:22:30,000 --> 00:22:33,200
there are two separate service meshes

00:22:31,840 --> 00:22:35,039
right so they're not

00:22:33,200 --> 00:22:36,799
the same service mesh you're deploying

00:22:35,039 --> 00:22:38,480
to both they are two separate clusters

00:22:36,799 --> 00:22:39,600
of service mesh control

00:22:38,480 --> 00:22:42,080
and you set up something called

00:22:39,600 --> 00:22:44,080
federation between the service meshes

00:22:42,080 --> 00:22:45,280
so federation is the idea that there's

00:22:44,080 --> 00:22:48,080
two separate clusters

00:22:45,280 --> 00:22:48,960
and they have the ability to control

00:22:48,080 --> 00:22:51,760
between them

00:22:48,960 --> 00:22:53,520
so they have the awareness of what the

00:22:51,760 --> 00:22:54,400
other is registering from a services

00:22:53,520 --> 00:22:56,000
standpoint

00:22:54,400 --> 00:22:58,400
as well as what it is registering from a

00:22:56,000 --> 00:23:01,679
services standpoint

00:22:58,400 --> 00:23:02,960
the benefits of a federated service mesh

00:23:01,679 --> 00:23:04,720
is really actually neat because you'll

00:23:02,960 --> 00:23:05,600
get one place to control retries and

00:23:04,720 --> 00:23:08,000
error handling

00:23:05,600 --> 00:23:09,520
both meshes are able to control retries

00:23:08,000 --> 00:23:11,600
and handling

00:23:09,520 --> 00:23:13,200
you can also aggregate and standardize

00:23:11,600 --> 00:23:15,200
all of the metrics so recall

00:23:13,200 --> 00:23:16,720
in the network automation piece i have

00:23:15,200 --> 00:23:17,440
separate metrics going in different

00:23:16,720 --> 00:23:19,919
places

00:23:17,440 --> 00:23:21,679
in this case i have aggregate metrics

00:23:19,919 --> 00:23:24,240
from all of the proxies

00:23:21,679 --> 00:23:25,679
and i can standardize on them what's

00:23:24,240 --> 00:23:27,360
also really great about this is that you

00:23:25,679 --> 00:23:29,200
get the progressive delivery

00:23:27,360 --> 00:23:31,360
approach across all environments and

00:23:29,200 --> 00:23:33,039
frameworks you aren't limited to just

00:23:31,360 --> 00:23:34,960
the one in the service mesh

00:23:33,039 --> 00:23:36,559
so you get a you can do a fully

00:23:34,960 --> 00:23:37,760
automated canary deployment if you

00:23:36,559 --> 00:23:38,720
wanted to and that's what i'll show

00:23:37,760 --> 00:23:40,960
today

00:23:38,720 --> 00:23:42,960
and you can do a b testing so for

00:23:40,960 --> 00:23:45,600
example i could set a header

00:23:42,960 --> 00:23:47,679
to test specifically an application in

00:23:45,600 --> 00:23:49,919
cloud i don't have to test

00:23:47,679 --> 00:23:50,880
the one in data center and in that case

00:23:49,919 --> 00:23:54,240
the a b test

00:23:50,880 --> 00:23:55,200
can be done with one single control

00:23:54,240 --> 00:23:58,880
point

00:23:55,200 --> 00:23:58,880
in the form of both service meshes

00:23:59,120 --> 00:24:03,200
i know it's a little bit hard to picture

00:24:01,520 --> 00:24:04,640
from a federation standpoint so we'll

00:24:03,200 --> 00:24:06,320
actually should go into some of the

00:24:04,640 --> 00:24:07,679
technical details of what federation

00:24:06,320 --> 00:24:10,000
involves

00:24:07,679 --> 00:24:10,799
basically what we do is we deploy a

00:24:10,000 --> 00:24:13,120
service mesh

00:24:10,799 --> 00:24:15,840
a console service mesh in the cloud and

00:24:13,120 --> 00:24:17,679
we label it as the primary service mesh

00:24:15,840 --> 00:24:19,679
and we deploy something called a mesh

00:24:17,679 --> 00:24:22,159
gateway the mesh gateway allows actually

00:24:19,679 --> 00:24:24,000
uh connectivity to other mesh gateways

00:24:22,159 --> 00:24:25,840
um so the mesh gateways i think

00:24:24,000 --> 00:24:27,840
raise their hand and say hello other

00:24:25,840 --> 00:24:30,320
mesh gateway here's some of the services

00:24:27,840 --> 00:24:32,640
i have in console in the cloud

00:24:30,320 --> 00:24:34,240
can you join me and the data center or

00:24:32,640 --> 00:24:36,720
console service mesh has its own mesh

00:24:34,240 --> 00:24:38,240
gateway and says okay sure i'll join

00:24:36,720 --> 00:24:40,400
and the two of them are able to

00:24:38,240 --> 00:24:43,440
communicate with each other and register

00:24:40,400 --> 00:24:45,279
and identify services in each cluster so

00:24:43,440 --> 00:24:46,640
the data center has its own console

00:24:45,279 --> 00:24:47,600
service mesh and it behaves as a

00:24:46,640 --> 00:24:49,120
secondary

00:24:47,600 --> 00:24:50,559
the reason why i put the cloud as the

00:24:49,120 --> 00:24:51,520
primary and the data center as a

00:24:50,559 --> 00:24:53,840
secondary

00:24:51,520 --> 00:24:54,880
is that if you do this uh gradual

00:24:53,840 --> 00:24:57,200
topology

00:24:54,880 --> 00:24:58,320
where you go from a network automation

00:24:57,200 --> 00:25:01,039
and you grow

00:24:58,320 --> 00:25:01,760
into uh this federated service mesh

00:25:01,039 --> 00:25:04,000
approach

00:25:01,760 --> 00:25:05,840
you're likely to have deployed the

00:25:04,000 --> 00:25:08,480
primary service mesh in the cloud

00:25:05,840 --> 00:25:10,159
first and so you might as well go and

00:25:08,480 --> 00:25:11,279
add the older environments as

00:25:10,159 --> 00:25:13,679
secondaries while you

00:25:11,279 --> 00:25:15,600
keep the uh you know let's say the the

00:25:13,679 --> 00:25:18,080
first one as the primary

00:25:15,600 --> 00:25:19,120
up to you though the great part about

00:25:18,080 --> 00:25:20,960
this approach is that

00:25:19,120 --> 00:25:22,880
you can do it over public or private

00:25:20,960 --> 00:25:24,640
connectivity you don't have to have

00:25:22,880 --> 00:25:26,480
private connectivity to do this

00:25:24,640 --> 00:25:28,559
um the mesh gateways can communicate

00:25:26,480 --> 00:25:30,159
over a public network with tls

00:25:28,559 --> 00:25:32,080
encryption

00:25:30,159 --> 00:25:34,400
however the downside to this is that you

00:25:32,080 --> 00:25:36,960
do have to retrofit the service mesh

00:25:34,400 --> 00:25:39,200
into the data center or an older cloud

00:25:36,960 --> 00:25:40,799
environment

00:25:39,200 --> 00:25:42,320
and it's not always something that

00:25:40,799 --> 00:25:44,240
people want to do

00:25:42,320 --> 00:25:45,679
in this demonstration i actually am

00:25:44,240 --> 00:25:47,840
going to show a combination of

00:25:45,679 --> 00:25:48,880
federation as well as automated canary

00:25:47,840 --> 00:25:50,240
deployment

00:25:48,880 --> 00:25:51,279
which is a progressive delivery

00:25:50,240 --> 00:25:53,039
technique the idea behind the

00:25:51,279 --> 00:25:55,679
progressive delivery technique

00:25:53,039 --> 00:25:56,240
is that you're able to progressively

00:25:55,679 --> 00:25:59,279
deliver

00:25:56,240 --> 00:26:01,279
an application to production and

00:25:59,279 --> 00:26:03,279
mitigate the risk of let's say failure

00:26:01,279 --> 00:26:06,880
to your users or customers

00:26:03,279 --> 00:26:09,200
so in this demonstration what i'll show

00:26:06,880 --> 00:26:11,279
is that i have ui and my application i

00:26:09,200 --> 00:26:13,360
also have my application in cloud

00:26:11,279 --> 00:26:16,080
imagine that i am re-platforming and

00:26:13,360 --> 00:26:18,240
moving my application to the cloud

00:26:16,080 --> 00:26:20,080
so what i'm going to do is i'm going to

00:26:18,240 --> 00:26:23,039
canary deploy and make sure that my

00:26:20,080 --> 00:26:25,760
application in cloud is actually correct

00:26:23,039 --> 00:26:27,840
the way that this works is that each

00:26:25,760 --> 00:26:29,039
proxy in the service mesh for both data

00:26:27,840 --> 00:26:31,360
center and cloud

00:26:29,039 --> 00:26:33,120
have a set of metrics the proxies

00:26:31,360 --> 00:26:35,919
collect request metrics

00:26:33,120 --> 00:26:37,679
errors latency anything that you can

00:26:35,919 --> 00:26:39,200
really standardize from a proxy

00:26:37,679 --> 00:26:42,480
standpoint it will be able to

00:26:39,200 --> 00:26:44,559
expose so the proxies themselves surface

00:26:42,480 --> 00:26:46,559
these metrics and prometheus is able to

00:26:44,559 --> 00:26:48,880
scrape the proxy endpoints

00:26:46,559 --> 00:26:50,240
and collect those metrics prometheus

00:26:48,880 --> 00:26:52,799
aggregates the metrics

00:26:50,240 --> 00:26:53,360
and spinnaker can reference prometheus

00:26:52,799 --> 00:26:56,400
to

00:26:53,360 --> 00:26:59,760
do a canary analysis so spinnaker is

00:26:56,400 --> 00:27:01,520
a is a basically a progressive delivery

00:26:59,760 --> 00:27:03,840
platform that allows you to do an

00:27:01,520 --> 00:27:05,679
automated canary analysis

00:27:03,840 --> 00:27:08,480
it has a component called cayenneta

00:27:05,679 --> 00:27:10,960
which will do some statistical analysis

00:27:08,480 --> 00:27:12,559
on some data points and based on some

00:27:10,960 --> 00:27:14,240
metrics that you configure

00:27:12,559 --> 00:27:16,320
you can say whether or not the canary

00:27:14,240 --> 00:27:18,799
should fail or it should

00:27:16,320 --> 00:27:20,320
succeed so in this case spinnaker will

00:27:18,799 --> 00:27:21,919
look at one metric you could be more

00:27:20,320 --> 00:27:22,559
sophisticated but i've only configured

00:27:21,919 --> 00:27:24,640
one metric

00:27:22,559 --> 00:27:25,679
and that's errors so it will check for

00:27:24,640 --> 00:27:27,840
whether or not

00:27:25,679 --> 00:27:28,720
the errors in my application have

00:27:27,840 --> 00:27:32,799
increased

00:27:28,720 --> 00:27:32,799
in let's say the console server cloud

00:27:32,840 --> 00:27:36,960
deployment after it's done a canary

00:27:35,279 --> 00:27:40,320
analysis it will make the decision

00:27:36,960 --> 00:27:43,600
on how much traffic to split

00:27:40,320 --> 00:27:45,679
between data center and cloud the reason

00:27:43,600 --> 00:27:47,360
why this is important especially from a

00:27:45,679 --> 00:27:49,440
service mesh standpoint

00:27:47,360 --> 00:27:51,200
and from let's say a multiple cloud

00:27:49,440 --> 00:27:52,159
environment standpoint is that spinnaker

00:27:51,200 --> 00:27:54,240
doesn't really care

00:27:52,159 --> 00:27:55,520
if it's data center or cloud what it's

00:27:54,240 --> 00:27:57,520
able to do is configure

00:27:55,520 --> 00:27:59,200
console it's able to configure the

00:27:57,520 --> 00:27:59,760
control plane of the service mesh and

00:27:59,200 --> 00:28:01,600
say

00:27:59,760 --> 00:28:03,200
all right ninety percent should go to

00:28:01,600 --> 00:28:04,880
the data center and ten percent

00:28:03,200 --> 00:28:06,240
percent should go to the cloud for my

00:28:04,880 --> 00:28:08,480
application

00:28:06,240 --> 00:28:09,440
and the console configuration entry gets

00:28:08,480 --> 00:28:12,159
propagated

00:28:09,440 --> 00:28:14,320
to the proxies so you don't have to

00:28:12,159 --> 00:28:15,760
worry about going into a load balancer

00:28:14,320 --> 00:28:16,000
and figuring out how much traffic is

00:28:15,760 --> 00:28:17,840
going

00:28:16,000 --> 00:28:19,840
where you don't have to figure out which

00:28:17,840 --> 00:28:20,880
service in which load balancer needs to

00:28:19,840 --> 00:28:24,559
be configured

00:28:20,880 --> 00:28:27,520
instead what you do is you declaratively

00:28:24,559 --> 00:28:28,559
you declaratively configure the service

00:28:27,520 --> 00:28:30,559
mesh itself

00:28:28,559 --> 00:28:32,159
to control the traffic management you

00:28:30,559 --> 00:28:34,399
can also configure it

00:28:32,159 --> 00:28:35,200
with headers you can figure it with any

00:28:34,399 --> 00:28:37,360
layer 7

00:28:35,200 --> 00:28:39,279
routing that you need but for the most

00:28:37,360 --> 00:28:40,799
part in this case i'm going to show the

00:28:39,279 --> 00:28:42,960
canary deployment

00:28:40,799 --> 00:28:44,720
and how you're doing this with in an

00:28:42,960 --> 00:28:46,799
automated manner so

00:28:44,720 --> 00:28:47,840
as the canary succeeds we'll increase

00:28:46,799 --> 00:28:50,720
the traffic

00:28:47,840 --> 00:28:52,159
more traffic to cloud over time and

00:28:50,720 --> 00:28:54,559
eventually 100

00:28:52,159 --> 00:28:56,880
will go to cloud if you want to check

00:28:54,559 --> 00:28:59,760
out this demo and recreate it yourself

00:28:56,880 --> 00:29:01,200
this is a branch on the repository i

00:28:59,760 --> 00:29:03,039
showed earlier

00:29:01,200 --> 00:29:04,960
it is deployed in aws once again the

00:29:03,039 --> 00:29:07,600
data center uses virtual machines

00:29:04,960 --> 00:29:08,720
the cloud uses kubernetes and federation

00:29:07,600 --> 00:29:11,520
sets the cloud

00:29:08,720 --> 00:29:14,240
cluster as a primary so there are a lot

00:29:11,520 --> 00:29:16,320
of windows on the screen for this demo

00:29:14,240 --> 00:29:17,919
so i'll explain them first before i move

00:29:16,320 --> 00:29:21,120
forward with it

00:29:17,919 --> 00:29:23,919
the first window on the top left

00:29:21,120 --> 00:29:24,640
is grafana i'm using grafana just to

00:29:23,919 --> 00:29:28,159
show

00:29:24,640 --> 00:29:29,760
the requests and the errors for services

00:29:28,159 --> 00:29:32,880
specifically my application

00:29:29,760 --> 00:29:35,679
across both cloud and data center

00:29:32,880 --> 00:29:36,559
the one on the top right is spinnaker

00:29:35,679 --> 00:29:38,399
spinnaker

00:29:36,559 --> 00:29:40,559
is a progressive delivery tool that i'm

00:29:38,399 --> 00:29:43,360
using to

00:29:40,559 --> 00:29:43,840
analyze the canary and automatically

00:29:43,360 --> 00:29:47,760
increase

00:29:43,840 --> 00:29:49,679
traffic to the cloud application

00:29:47,760 --> 00:29:51,200
i also have two windows at the bottom

00:29:49,679 --> 00:29:54,320
showing two different

00:29:51,200 --> 00:29:57,520
console data centers the first

00:29:54,320 --> 00:30:00,880
is the console cluster for cloud on

00:29:57,520 --> 00:30:03,760
the left and then the right has

00:30:00,880 --> 00:30:05,919
the console data center cluster so there

00:30:03,760 --> 00:30:08,559
are two different clusters

00:30:05,919 --> 00:30:11,120
notice that it is one user interface

00:30:08,559 --> 00:30:12,960
i'll click a drop down and show but

00:30:11,120 --> 00:30:14,159
there's one user interface that you can

00:30:12,960 --> 00:30:16,720
see both

00:30:14,159 --> 00:30:18,320
console clusters so these are two

00:30:16,720 --> 00:30:20,240
separate ones but we are effectively

00:30:18,320 --> 00:30:22,720
stretching the mesh so that we can do

00:30:20,240 --> 00:30:24,080
an automated canary deployment now a lot

00:30:22,720 --> 00:30:27,200
of these techniques that you'll

00:30:24,080 --> 00:30:28,640
you'll be able to do for headers etc

00:30:27,200 --> 00:30:30,320
but the idea is today i just wanted to

00:30:28,640 --> 00:30:31,279
show more holistically how you would be

00:30:30,320 --> 00:30:33,760
able to

00:30:31,279 --> 00:30:34,799
aggregate and put together all of the

00:30:33,760 --> 00:30:37,760
metrics from

00:30:34,799 --> 00:30:38,799
all of the meshes that you're collecting

00:30:37,760 --> 00:30:41,039
and then be able to

00:30:38,799 --> 00:30:42,320
do some workflow for from it so in this

00:30:41,039 --> 00:30:46,000
case what we're trying to do

00:30:42,320 --> 00:30:46,559
is canary deploy cloud from the data

00:30:46,000 --> 00:30:49,039
center

00:30:46,559 --> 00:30:50,880
so the first thing we're going to look

00:30:49,039 --> 00:30:54,000
at is basically the

00:30:50,880 --> 00:30:54,399
two separate environments right now we

00:30:54,000 --> 00:30:56,240
have

00:30:54,399 --> 00:30:57,519
a lot of traffic going to data center

00:30:56,240 --> 00:30:58,960
you can imagine this as customer

00:30:57,519 --> 00:31:01,440
requests going to

00:30:58,960 --> 00:31:02,960
the data center my application and

00:31:01,440 --> 00:31:05,679
there's an upstream ui

00:31:02,960 --> 00:31:06,559
so the upstream ui is currently

00:31:05,679 --> 00:31:08,320
resolving to

00:31:06,559 --> 00:31:10,480
my application in the data center so you

00:31:08,320 --> 00:31:10,960
see me going into the console data

00:31:10,480 --> 00:31:13,120
center

00:31:10,960 --> 00:31:14,799
and tracing it and you'll notice that ui

00:31:13,120 --> 00:31:17,679
goes straight to data center

00:31:14,799 --> 00:31:18,080
and i can also access the cloud catalog

00:31:17,679 --> 00:31:20,640
as well

00:31:18,080 --> 00:31:21,600
from this view so i can actually use

00:31:20,640 --> 00:31:23,919
both of them

00:31:21,600 --> 00:31:25,120
and that works out pretty well if i want

00:31:23,919 --> 00:31:28,559
to go take a look at the

00:31:25,120 --> 00:31:30,960
actual service and what it's doing right

00:31:28,559 --> 00:31:34,000
now the service is going to

00:31:30,960 --> 00:31:35,440
from ui to my application and that my

00:31:34,000 --> 00:31:37,360
application's in the data center

00:31:35,440 --> 00:31:38,720
it's reflected in the topology view in

00:31:37,360 --> 00:31:40,240
console as well so

00:31:38,720 --> 00:31:42,240
you actually get kind of like an

00:31:40,240 --> 00:31:45,360
application dependency mapping

00:31:42,240 --> 00:31:46,080
view in console i'll start a new run of

00:31:45,360 --> 00:31:48,880
spinnaker

00:31:46,080 --> 00:31:50,240
this new run of spinnaker will deploy a

00:31:48,880 --> 00:31:52,240
canary deployment to

00:31:50,240 --> 00:31:53,279
the cloud instance you'll notice that

00:31:52,240 --> 00:31:54,640
console updates pretty much

00:31:53,279 --> 00:31:57,679
instantaneously

00:31:54,640 --> 00:32:00,399
um basically the resolver is 100

00:31:57,679 --> 00:32:02,240
well now it's going 10 to cloud and so

00:32:00,399 --> 00:32:04,720
it's starting the canary so it deploys

00:32:02,240 --> 00:32:07,279
about 10 percent of canary traffic to

00:32:04,720 --> 00:32:08,640
the cloud instance of my application

00:32:07,279 --> 00:32:10,480
and the way that this is done in

00:32:08,640 --> 00:32:12,960
spinnaker is that i'm using

00:32:10,480 --> 00:32:13,760
consoles kubernetes custom resource

00:32:12,960 --> 00:32:15,519
definitions

00:32:13,760 --> 00:32:17,440
you can configure these directly in

00:32:15,519 --> 00:32:18,480
console you don't need to use kubernetes

00:32:17,440 --> 00:32:20,159
specifically

00:32:18,480 --> 00:32:21,840
i used it in the case of spinnaker

00:32:20,159 --> 00:32:23,679
because spinnaker offers kubernetes

00:32:21,840 --> 00:32:25,360
integration out of the box

00:32:23,679 --> 00:32:27,279
so in this case i configure a service

00:32:25,360 --> 00:32:29,519
resolver and the service resolver

00:32:27,279 --> 00:32:30,960
just lets you know console know anything

00:32:29,519 --> 00:32:34,320
that goes to my application

00:32:30,960 --> 00:32:36,480
cloud goes to data center cloud

00:32:34,320 --> 00:32:37,600
which is the uh if you'll notice on the

00:32:36,480 --> 00:32:39,600
top bottom

00:32:37,600 --> 00:32:40,880
left there's the cloud instance with my

00:32:39,600 --> 00:32:42,799
application right

00:32:40,880 --> 00:32:44,559
so it goes to the data center cloud and

00:32:42,799 --> 00:32:46,960
the service my application

00:32:44,559 --> 00:32:48,240
console will recognize that it needs to

00:32:46,960 --> 00:32:51,360
redirect anything

00:32:48,240 --> 00:32:52,080
from uh for my application cloud to data

00:32:51,360 --> 00:32:55,360
center cloud

00:32:52,080 --> 00:32:58,000
service my application and there's also

00:32:55,360 --> 00:32:59,519
a service splitter as well the service

00:32:58,000 --> 00:33:01,760
splitter is available

00:32:59,519 --> 00:33:03,360
to control the weight so originally it

00:33:01,760 --> 00:33:05,120
starts with a hundred percent to the

00:33:03,360 --> 00:33:05,919
localized data center which is data

00:33:05,120 --> 00:33:08,880
center

00:33:05,919 --> 00:33:10,960
uh and then zero percent to cloud but in

00:33:08,880 --> 00:33:12,799
the case of canary i've got 90

00:33:10,960 --> 00:33:14,880
going to the localized data center

00:33:12,799 --> 00:33:17,600
instance of my application and 10

00:33:14,880 --> 00:33:18,640
going to cloud so these weights increase

00:33:17,600 --> 00:33:21,760
over time

00:33:18,640 --> 00:33:24,880
i had them in increments of uh 10

00:33:21,760 --> 00:33:26,880
to 30 to 50 to 70 to 100 so

00:33:24,880 --> 00:33:29,200
not perfect increments but wanting to

00:33:26,880 --> 00:33:31,120
show the the case of a canary deployment

00:33:29,200 --> 00:33:33,360
specifically

00:33:31,120 --> 00:33:34,640
in spinnaker you can configure a canary

00:33:33,360 --> 00:33:36,240
analysis stage

00:33:34,640 --> 00:33:38,080
i configure it for three minutes

00:33:36,240 --> 00:33:40,880
specifically in

00:33:38,080 --> 00:33:42,399
the the spinner core pipeline and you

00:33:40,880 --> 00:33:43,360
can do it for longer and you should do

00:33:42,399 --> 00:33:45,039
it for longer

00:33:43,360 --> 00:33:47,039
i was doing this for the sake of demon

00:33:45,039 --> 00:33:49,039
demo because this demo takes about 20 to

00:33:47,039 --> 00:33:51,679
30 minutes to run

00:33:49,039 --> 00:33:54,159
the other important thing to recognize

00:33:51,679 --> 00:33:57,600
is that the configuration also has

00:33:54,159 --> 00:34:00,000
a field for baseline and canary pair

00:33:57,600 --> 00:34:02,000
in spinnaker what you have to do is pass

00:34:00,000 --> 00:34:05,120
certain parameters like location

00:34:02,000 --> 00:34:05,679
as well as the name and scope to the

00:34:05,120 --> 00:34:07,200
metric

00:34:05,679 --> 00:34:09,280
and so what i've done is i've just

00:34:07,200 --> 00:34:09,919
specified a baseline location for data

00:34:09,280 --> 00:34:12,079
center

00:34:09,919 --> 00:34:14,079
and a canary location for cloud so my

00:34:12,079 --> 00:34:14,960
baseline is located in the data center

00:34:14,079 --> 00:34:18,839
my application

00:34:14,960 --> 00:34:20,879
and my canary is located in cloud my

00:34:18,839 --> 00:34:23,359
application

00:34:20,879 --> 00:34:24,800
i also set some pass fail expectations

00:34:23,359 --> 00:34:26,720
so if 90

00:34:24,800 --> 00:34:28,480
um you know fails then we know this

00:34:26,720 --> 00:34:30,000
canary fails

00:34:28,480 --> 00:34:31,599
we'll take a look at the configuration

00:34:30,000 --> 00:34:32,720
for the canary itself because this is

00:34:31,599 --> 00:34:36,000
where the service mesh

00:34:32,720 --> 00:34:38,240
aggregated metrics becomes really useful

00:34:36,000 --> 00:34:39,919
in the service mesh aggregated metrics

00:34:38,240 --> 00:34:42,639
i'm analyzing one group

00:34:39,919 --> 00:34:43,919
of metrics from envoy and that's errors

00:34:42,639 --> 00:34:46,800
so in this case the

00:34:43,919 --> 00:34:47,359
envoy error metrics are being collected

00:34:46,800 --> 00:34:49,359
by

00:34:47,359 --> 00:34:50,639
prometheus and then eventually surface

00:34:49,359 --> 00:34:52,879
to spinnaker

00:34:50,639 --> 00:34:53,679
the envoy metrics are coming from both

00:34:52,879 --> 00:34:55,520
data center

00:34:53,679 --> 00:34:57,359
and cloud so that's a really great part

00:34:55,520 --> 00:34:58,880
about this setup is that they're

00:34:57,359 --> 00:35:00,320
actually the same metrics

00:34:58,880 --> 00:35:02,160
there's the same envoy metrics they're

00:35:00,320 --> 00:35:03,359
just aggregated from two different

00:35:02,160 --> 00:35:05,440
environments

00:35:03,359 --> 00:35:08,640
and the main metric i'm examining today

00:35:05,440 --> 00:35:11,280
is the envoy cluster upstream requests

00:35:08,640 --> 00:35:11,920
this convoy cluster upstream requests

00:35:11,280 --> 00:35:14,240
basically

00:35:11,920 --> 00:35:15,520
tracks the number of requests and their

00:35:14,240 --> 00:35:18,079
response codes

00:35:15,520 --> 00:35:19,119
and they could be 200 they could be 500s

00:35:18,079 --> 00:35:21,200
the the idea is that

00:35:19,119 --> 00:35:22,800
the metric itself tracks it depends it's

00:35:21,200 --> 00:35:25,119
um a counter based on

00:35:22,800 --> 00:35:26,320
which one the response code is being

00:35:25,119 --> 00:35:27,599
returned

00:35:26,320 --> 00:35:29,200
and the other thing that i've done is

00:35:27,599 --> 00:35:30,640
i've configured a source service the

00:35:29,200 --> 00:35:32,079
source service is the name of the

00:35:30,640 --> 00:35:33,119
service and console which is my

00:35:32,079 --> 00:35:35,119
application

00:35:33,119 --> 00:35:37,440
i'm checking for the envoy response code

00:35:35,119 --> 00:35:39,359
which is well 500's

00:35:37,440 --> 00:35:40,960
the response code class is just looking

00:35:39,359 --> 00:35:43,440
for errors i could add

00:35:40,960 --> 00:35:44,960
more sophisticated metrics and groups

00:35:43,440 --> 00:35:46,480
but i've chosen just to do one for the

00:35:44,960 --> 00:35:48,720
sake of this example

00:35:46,480 --> 00:35:49,599
and recall i showed the baseline and

00:35:48,720 --> 00:35:52,000
canary pair

00:35:49,599 --> 00:35:53,760
in the spinnaker configuration that goes

00:35:52,000 --> 00:35:55,040
into the console source data center so

00:35:53,760 --> 00:35:58,240
console source data center

00:35:55,040 --> 00:35:59,839
is a prometheus tag like i said it's

00:35:58,240 --> 00:36:01,599
great to have the aggregated metrics

00:35:59,839 --> 00:36:03,520
from both service meshes

00:36:01,599 --> 00:36:05,359
because they're the same and each of

00:36:03,520 --> 00:36:06,720
them will have the consistent tag of

00:36:05,359 --> 00:36:07,599
console source data center so we'll

00:36:06,720 --> 00:36:10,000
actually identify

00:36:07,599 --> 00:36:12,400
which data center it's coming from so in

00:36:10,000 --> 00:36:15,200
this case i can pass location

00:36:12,400 --> 00:36:15,520
location being cloud or data center and

00:36:15,200 --> 00:36:19,680
so

00:36:15,520 --> 00:36:22,000
if the http 500s come from the location

00:36:19,680 --> 00:36:23,440
canary which is cloud spinnaker

00:36:22,000 --> 00:36:26,560
identifies that

00:36:23,440 --> 00:36:29,040
as a problem and is able to flag that

00:36:26,560 --> 00:36:29,760
but in this case we're you know i know

00:36:29,040 --> 00:36:32,560
that this

00:36:29,760 --> 00:36:33,680
application exists is successful and it

00:36:32,560 --> 00:36:36,400
does take about

00:36:33,680 --> 00:36:37,200
20 to 30 minutes to run this so we'll

00:36:36,400 --> 00:36:41,359
return

00:36:37,200 --> 00:36:42,880
when the canary pipeline completes

00:36:41,359 --> 00:36:45,440
and i'll show you what happens during a

00:36:42,880 --> 00:36:46,400
release but as this progresses you'll

00:36:45,440 --> 00:36:49,280
notice that the

00:36:46,400 --> 00:36:50,720
amount of traffic going to data center

00:36:49,280 --> 00:36:53,040
decreases in grafana

00:36:50,720 --> 00:36:54,960
and the amount of traffic going to cloud

00:36:53,040 --> 00:36:56,560
increases and the number of successful

00:36:54,960 --> 00:37:00,800
requests are coming back

00:36:56,560 --> 00:37:00,800
and increasing as well over time

00:37:01,920 --> 00:37:05,920
all right so we're back after about 20

00:37:04,560 --> 00:37:09,520
minutes or so

00:37:05,920 --> 00:37:11,599
and the canary is about wrapping up

00:37:09,520 --> 00:37:12,640
um it completed about a hundred percent

00:37:11,599 --> 00:37:15,200
you'll notice that

00:37:12,640 --> 00:37:16,320
in the grafana graph and the you you'll

00:37:15,200 --> 00:37:18,560
notice the

00:37:16,320 --> 00:37:20,400
number of requests to data center

00:37:18,560 --> 00:37:21,520
decreasing and the number of successful

00:37:20,400 --> 00:37:24,000
requests to

00:37:21,520 --> 00:37:25,440
cloud increasing they actually meet in

00:37:24,000 --> 00:37:28,160
the middle that's the 50

00:37:25,440 --> 00:37:29,440
canary um so 50 of traffic had been

00:37:28,160 --> 00:37:31,520
going to cloud 50

00:37:29,440 --> 00:37:33,599
i've been going to data center and

00:37:31,520 --> 00:37:35,119
eventually uh the cloud

00:37:33,599 --> 00:37:38,400
the amount the number of successful

00:37:35,119 --> 00:37:40,960
requests to cloud increase

00:37:38,400 --> 00:37:42,640
so as this wraps up um you'll notice

00:37:40,960 --> 00:37:43,839
that it's progressed through every

00:37:42,640 --> 00:37:45,599
spinnaker at least has progressed

00:37:43,839 --> 00:37:46,480
through every stage and it's completed

00:37:45,599 --> 00:37:48,880
the release stage

00:37:46,480 --> 00:37:50,000
the release stage is wrapping up and

00:37:48,880 --> 00:37:52,960
cleaning up some stuff

00:37:50,000 --> 00:37:53,920
in console so because the console mesh

00:37:52,960 --> 00:37:56,160
is the one that's

00:37:53,920 --> 00:37:59,119
offering the control we need to be able

00:37:56,160 --> 00:38:00,000
to clean up and set the default to cloud

00:37:59,119 --> 00:38:03,599
so what i've done

00:38:00,000 --> 00:38:05,280
is i've set the resolver by default

00:38:03,599 --> 00:38:07,599
anything that's my application

00:38:05,280 --> 00:38:08,880
goes to the data center cloud service my

00:38:07,599 --> 00:38:10,320
application

00:38:08,880 --> 00:38:12,400
this is something you'll actually notice

00:38:10,320 --> 00:38:15,599
in the console topology

00:38:12,400 --> 00:38:18,079
on the bottom uh on the bottom

00:38:15,599 --> 00:38:19,440
right you'll notice that the resolver

00:38:18,079 --> 00:38:22,000
now directly

00:38:19,440 --> 00:38:23,680
uh completes to goes to cloud nothing

00:38:22,000 --> 00:38:26,240
goes to the data center so

00:38:23,680 --> 00:38:26,960
even in the data center view i'm for

00:38:26,240 --> 00:38:28,720
console

00:38:26,960 --> 00:38:31,119
in the cluster itself i'm able to view

00:38:28,720 --> 00:38:33,200
the application dependency in a way that

00:38:31,119 --> 00:38:34,480
shows me very clearly that it's going to

00:38:33,200 --> 00:38:36,880
cloud if i

00:38:34,480 --> 00:38:37,520
refresh my service my service will now

00:38:36,880 --> 00:38:40,720
go

00:38:37,520 --> 00:38:41,599
to from ui data center to my application

00:38:40,720 --> 00:38:43,359
cloud

00:38:41,599 --> 00:38:44,800
and it's able to resolve that so

00:38:43,359 --> 00:38:46,800
regardless of

00:38:44,800 --> 00:38:49,440
the run time regardless of the

00:38:46,800 --> 00:38:51,599
environment the region et cetera

00:38:49,440 --> 00:38:53,359
i pretty much get one view and i'm able

00:38:51,599 --> 00:38:55,040
to do some

00:38:53,359 --> 00:38:57,599
techniques in progressive delivery

00:38:55,040 --> 00:39:00,320
basically using these aggregated metrics

00:38:57,599 --> 00:39:02,800
across the service meshes

00:39:00,320 --> 00:39:03,760
so i showed a successful canary

00:39:02,800 --> 00:39:06,240
deployment

00:39:03,760 --> 00:39:08,320
using the service mesh and the

00:39:06,240 --> 00:39:09,839
aggregated metrics on spinnaker

00:39:08,320 --> 00:39:11,760
but what happens in the case of a

00:39:09,839 --> 00:39:13,520
failing one remember in the network

00:39:11,760 --> 00:39:16,880
automation demo i showed

00:39:13,520 --> 00:39:19,040
i manually increased some traffic to

00:39:16,880 --> 00:39:21,119
the cloud instance and it actually

00:39:19,040 --> 00:39:22,320
didn't work so i reverted back to zero

00:39:21,119 --> 00:39:25,280
percent traffic to

00:39:22,320 --> 00:39:27,359
cloud and 100 data center what i've done

00:39:25,280 --> 00:39:30,880
here is i've started a pipeline

00:39:27,359 --> 00:39:32,960
that has the starts a canary deployment

00:39:30,880 --> 00:39:35,280
but the application itself has a small

00:39:32,960 --> 00:39:37,119
error rate and in this case we're

00:39:35,280 --> 00:39:39,040
proceeding with the same

00:39:37,119 --> 00:39:40,320
workflow right the same technique which

00:39:39,040 --> 00:39:42,079
is let's

00:39:40,320 --> 00:39:44,800
do a canary deployment send a certain

00:39:42,079 --> 00:39:46,640
percentage of traffic using the console

00:39:44,800 --> 00:39:47,839
service splitter send a certain

00:39:46,640 --> 00:39:50,160
percentage traffic

00:39:47,839 --> 00:39:52,880
use spinnaker to collect the metrics and

00:39:50,160 --> 00:39:54,880
evaluate if there are http 500 errors

00:39:52,880 --> 00:39:56,000
and then ultimately respond to the case

00:39:54,880 --> 00:39:58,400
so here what i'm showing

00:39:56,000 --> 00:40:00,240
is that this is fully automated and in

00:39:58,400 --> 00:40:01,760
the network automation case i was doing

00:40:00,240 --> 00:40:03,760
this manually i was

00:40:01,760 --> 00:40:05,200
looking at the metrics across two

00:40:03,760 --> 00:40:08,240
different places

00:40:05,200 --> 00:40:10,480
and sort of hand waving whether or not

00:40:08,240 --> 00:40:12,160
it was working in this case i'm actually

00:40:10,480 --> 00:40:15,040
allowing spinnaker and

00:40:12,160 --> 00:40:16,640
by extension cayenneta to analyze uh

00:40:15,040 --> 00:40:19,119
what the metrics are looking like

00:40:16,640 --> 00:40:21,359
so if you examine grafana the metrics

00:40:19,119 --> 00:40:22,560
are showing that there's some htv 500

00:40:21,359 --> 00:40:24,000
errors for cloud

00:40:22,560 --> 00:40:25,280
there's a small number of successful

00:40:24,000 --> 00:40:26,160
requests that are going through for

00:40:25,280 --> 00:40:28,079
cloud

00:40:26,160 --> 00:40:30,000
and the error rate is pretty small but

00:40:28,079 --> 00:40:31,520
it is introducing some errors and that

00:40:30,000 --> 00:40:34,240
metric is increasing

00:40:31,520 --> 00:40:35,040
and canary the canary analysis in

00:40:34,240 --> 00:40:38,240
spinnaker

00:40:35,040 --> 00:40:40,960
is going to recognize that and flag it

00:40:38,240 --> 00:40:43,599
as a problem right now there's only 10

00:40:40,960 --> 00:40:45,920
percent of traffic going to cloud

00:40:43,599 --> 00:40:47,760
in this scenario we again use the same

00:40:45,920 --> 00:40:50,000
workflow

00:40:47,760 --> 00:40:51,119
and it's just demonstrating a failing

00:40:50,000 --> 00:40:53,040
application

00:40:51,119 --> 00:40:54,400
the main difference is that when the

00:40:53,040 --> 00:40:56,480
canary fails

00:40:54,400 --> 00:40:57,760
what spinnaker will do is execute a

00:40:56,480 --> 00:40:59,119
rollback canary

00:40:57,760 --> 00:41:01,760
this is something i've created but the

00:40:59,119 --> 00:41:03,599
rollback canary just sets all of the

00:41:01,760 --> 00:41:04,640
traffic back to the data center

00:41:03,599 --> 00:41:06,480
application

00:41:04,640 --> 00:41:08,160
the reason why i set all of the traffic

00:41:06,480 --> 00:41:10,079
back to the data center application

00:41:08,160 --> 00:41:11,760
it's similar to the other scenario that

00:41:10,079 --> 00:41:13,680
i showed in network automation

00:41:11,760 --> 00:41:16,240
by sending a hundred percent of traffic

00:41:13,680 --> 00:41:18,000
back i'm able to

00:41:16,240 --> 00:41:20,560
ensure that none of the customers who

00:41:18,000 --> 00:41:23,280
are using my application are affected

00:41:20,560 --> 00:41:23,599
100 is still good they're able to use it

00:41:23,280 --> 00:41:26,960
but

00:41:23,599 --> 00:41:30,240
i can debug the application on the cloud

00:41:26,960 --> 00:41:35,839
separately and identify what is causing

00:41:30,240 --> 00:41:35,839
the ear

00:41:36,400 --> 00:41:40,400
all right so in summary there's two

00:41:38,880 --> 00:41:42,000
different kinds of topologies you can

00:41:40,400 --> 00:41:43,839
approach when you're stretching the mesh

00:41:42,000 --> 00:41:45,760
right the first is doing some kind of

00:41:43,839 --> 00:41:47,280
network infrastructure automation

00:41:45,760 --> 00:41:49,680
the benefit to it is that you use

00:41:47,280 --> 00:41:52,640
already exist what already exists

00:41:49,680 --> 00:41:53,440
and you add a layer of automation to it

00:41:52,640 --> 00:41:56,160
it's a nice

00:41:53,440 --> 00:41:57,920
middle ground or a stepping stone for

00:41:56,160 --> 00:41:59,839
full service mesh federation across

00:41:57,920 --> 00:42:01,680
all environments the idea is that you

00:41:59,839 --> 00:42:03,280
can abstract the environment application

00:42:01,680 --> 00:42:03,760
framework and runtime when you are able

00:42:03,280 --> 00:42:05,599
to

00:42:03,760 --> 00:42:07,200
federate across multiple service meshes

00:42:05,599 --> 00:42:09,200
in multiple environments

00:42:07,200 --> 00:42:10,560
it also adds an additional layer of

00:42:09,200 --> 00:42:14,000
control

00:42:10,560 --> 00:42:16,720
that aggregates and standardizes

00:42:14,000 --> 00:42:18,400
network policy metrics and service to

00:42:16,720 --> 00:42:21,440
service communication

00:42:18,400 --> 00:42:23,839
in one view so you don't have to go back

00:42:21,440 --> 00:42:24,960
and debug multiple systems and track

00:42:23,839 --> 00:42:26,480
through

00:42:24,960 --> 00:42:27,920
transactions across multiple

00:42:26,480 --> 00:42:28,640
environments the idea is that you have

00:42:27,920 --> 00:42:30,960
one

00:42:28,640 --> 00:42:32,800
place to control all of that now it's

00:42:30,960 --> 00:42:34,960
very difficult to do right it's not

00:42:32,800 --> 00:42:36,480
that this is the end solution for

00:42:34,960 --> 00:42:37,520
everything and at the end of the day you

00:42:36,480 --> 00:42:39,599
can choose

00:42:37,520 --> 00:42:41,200
to either stay with some kind of network

00:42:39,599 --> 00:42:43,119
automation topology

00:42:41,200 --> 00:42:44,400
and continue that for a couple of years

00:42:43,119 --> 00:42:47,359
or you can

00:42:44,400 --> 00:42:49,280
invest time into making the federation

00:42:47,359 --> 00:42:51,200
the federation topology work

00:42:49,280 --> 00:42:52,960
either way there are some options in

00:42:51,200 --> 00:42:54,400
which you can actually use

00:42:52,960 --> 00:42:56,480
to manage service to service

00:42:54,400 --> 00:42:57,359
communication across multiple platforms

00:42:56,480 --> 00:42:59,200
clouds and

00:42:57,359 --> 00:43:02,160
it's agnostic to the environment

00:42:59,200 --> 00:43:04,319
application framework or runtime

00:43:02,160 --> 00:43:06,400
if you're curious to learn more i've

00:43:04,319 --> 00:43:08,079
aggregated the list of references here

00:43:06,400 --> 00:43:10,480
there's additional information on how to

00:43:08,079 --> 00:43:12,640
configure spinnaker with console to do

00:43:10,480 --> 00:43:14,000
automated canary deployment there's also

00:43:12,640 --> 00:43:15,760
some tutorials as well

00:43:14,000 --> 00:43:18,400
for the network automation that i've

00:43:15,760 --> 00:43:20,480
shown today like console terraform sync

00:43:18,400 --> 00:43:21,920
as well as some documentation on service

00:43:20,480 --> 00:43:23,520
mesh itself

00:43:21,920 --> 00:43:28,000
if you would like these slides you can

00:43:23,520 --> 00:43:29,760
find them at j o a joatmon08.github.io

00:43:28,000 --> 00:43:41,839
thank you and i appreciate you tuning

00:43:29,760 --> 00:43:41,839
into this session today

00:43:45,839 --> 00:43:47,920

YouTube URL: https://www.youtube.com/watch?v=whkF5VRtzYI


