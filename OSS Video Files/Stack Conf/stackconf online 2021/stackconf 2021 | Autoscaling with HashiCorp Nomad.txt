Title: stackconf 2021 | Autoscaling with HashiCorp Nomad
Publication date: 2021-06-24
Playlist: stackconf online 2021
Description: 
	by Bram Vogelaar

Things like Infrastructure as Code, Service Discovery and Config Management can and have helped us to quickly build and rebuild infrastructure, but we have not nearly spend enough time to train our self to review, monitor and respond to outages. Does our platform degrade in a graceful way or what does a high cpu load really mean? What can we learn from level 1 outages to be able to run our platforms more reliably? We all love infrastructure as code, we automate everything â„¢. However, making sure all of our infrastructure assets are monitored effectively can be slow and resource intensive multi stage process. During this talk we will investigate how we can setup nomad cluster that can automatically scale our infrastructure both horizontally as vertically to be able to cope with increased demand by users/ This talk will focus on making sure we on configuring Nomad and its new autoscaler component to be able to make data driven decisions about scaling nomad jobs in or out to fit current customers usage.


NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de/
Blog: http://blog.netways.de/
NWS: https://nws.netways.de 

Webinare
Archiv Link: https://www.netways.de/netways/webinare/

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh/


Musik: https://www.frametraxx.de/
Captions: 
	00:00:05,660 --> 00:00:13,119
[Applause]

00:00:06,120 --> 00:00:13,119
[Music]

00:00:14,400 --> 00:00:17,039
good day good morning good afternoon

00:00:16,240 --> 00:00:19,199
wherever you are

00:00:17,039 --> 00:00:20,880
welcome to my talk about auto scaling

00:00:19,199 --> 00:00:22,400
your no man jobs

00:00:20,880 --> 00:00:24,240
first let me introduce myself hello i'm

00:00:22,400 --> 00:00:26,240
bravo i went to

00:00:24,240 --> 00:00:28,840
university to become like a hydrologist

00:00:26,240 --> 00:00:32,239
and i according to my

00:00:28,840 --> 00:00:33,920
uh collies i descended into the dark

00:00:32,239 --> 00:00:37,120
side in a couple of steps

00:00:33,920 --> 00:00:39,920
first becoming a bio information then

00:00:37,120 --> 00:00:41,920
in an academic world the programmer guy

00:00:39,920 --> 00:00:45,680
is also the server guys so i

00:00:41,920 --> 00:00:48,480
started to do operations a lot and since

00:00:45,680 --> 00:00:50,719
the last decade or so i've been in

00:00:48,480 --> 00:00:51,920
operations and i'm currently employed as

00:00:50,719 --> 00:00:53,840
the cto

00:00:51,920 --> 00:00:56,559
at a little hosting company called hot

00:00:53,840 --> 00:00:59,199
potatoes in the netherlands

00:00:56,559 --> 00:01:00,559
today i would like to discuss moving it

00:00:59,199 --> 00:01:03,680
all to the cloud because

00:01:00,559 --> 00:01:05,439
at some point in your life a cto ceo is

00:01:03,680 --> 00:01:05,760
gonna come into your office and gonna

00:01:05,439 --> 00:01:07,680
say

00:01:05,760 --> 00:01:09,119
let's move it all to the cloud it's

00:01:07,680 --> 00:01:12,960
gonna be better it's gonna be faster

00:01:09,119 --> 00:01:15,600
it's gonna be cheaper um

00:01:12,960 --> 00:01:16,960
and let's work out if it is how you can

00:01:15,600 --> 00:01:20,320
actually build it

00:01:16,960 --> 00:01:23,520
in a scalable way in that cheap way

00:01:20,320 --> 00:01:25,040
um and if you can build something that

00:01:23,520 --> 00:01:26,640
can actually survive the thundering

00:01:25,040 --> 00:01:28,400
hurts in order to

00:01:26,640 --> 00:01:30,799
move into look into the future we need

00:01:28,400 --> 00:01:33,840
to look into the past first

00:01:30,799 --> 00:01:35,360
so back in the way back when um when you

00:01:33,840 --> 00:01:37,280
wanted to buy

00:01:35,360 --> 00:01:39,200
more compute power you would actually

00:01:37,280 --> 00:01:41,119
have to get by and from

00:01:39,200 --> 00:01:42,720
probably investors you would have to

00:01:41,119 --> 00:01:44,880
talk to suppliers

00:01:42,720 --> 00:01:46,640
you would have to raise a purchase order

00:01:44,880 --> 00:01:49,920
uh you would have to wait

00:01:46,640 --> 00:01:51,040
months for a very big box to show up to

00:01:49,920 --> 00:01:53,280
your loading dock

00:01:51,040 --> 00:01:54,079
that you would have to probably

00:01:53,280 --> 00:01:57,040
installed into

00:01:54,079 --> 00:01:57,759
a purpose-built room or even a building

00:01:57,040 --> 00:01:59,119
and

00:01:57,759 --> 00:02:00,320
if you wanted to have more you would

00:01:59,119 --> 00:02:02,240
basically have to do the whole thing

00:02:00,320 --> 00:02:04,640
again so

00:02:02,240 --> 00:02:06,399
capacity planning was a thing of years

00:02:04,640 --> 00:02:09,360
you would have to plan ahead

00:02:06,399 --> 00:02:10,399
and have to have quite a bit of of

00:02:09,360 --> 00:02:15,760
buying

00:02:10,399 --> 00:02:15,760
in the 90s early 90s or a bit later

00:02:16,080 --> 00:02:21,520
we got into a point where

00:02:19,760 --> 00:02:23,840
compute was commoditized it was a lot

00:02:21,520 --> 00:02:23,840
cheaper

00:02:24,400 --> 00:02:29,840
things like the internet kicked off

00:02:26,879 --> 00:02:32,160
websites kicked off google kicked off

00:02:29,840 --> 00:02:34,400
they instead of having to build bigger

00:02:32,160 --> 00:02:38,640
and bigger rooms or even buildings

00:02:34,400 --> 00:02:38,640
to house your compute

00:02:39,120 --> 00:02:43,040
the industry worked out a way of

00:02:41,200 --> 00:02:45,280
basically

00:02:43,040 --> 00:02:47,120
horizontally scaling so adding more

00:02:45,280 --> 00:02:47,760
nodes of the same size and just working

00:02:47,120 --> 00:02:50,080
out

00:02:47,760 --> 00:02:51,760
how to split your problem across

00:02:50,080 --> 00:02:53,519
multiple nodes

00:02:51,760 --> 00:02:55,280
in the beginning that was fine people

00:02:53,519 --> 00:02:58,480
knew task x

00:02:55,280 --> 00:03:01,680
run on server x task y

00:02:58,480 --> 00:03:04,800
ran on server y but when the

00:03:01,680 --> 00:03:07,440
the internet kicked off things like um

00:03:04,800 --> 00:03:08,879
vanity url so google.com or my

00:03:07,440 --> 00:03:12,159
webshop.com

00:03:08,879 --> 00:03:13,680
needed something in front of those those

00:03:12,159 --> 00:03:15,280
nodes and that's where we started

00:03:13,680 --> 00:03:16,879
calling load balancer you would have one

00:03:15,280 --> 00:03:19,680
node banner to have the

00:03:16,879 --> 00:03:20,480
vanity url your webshop.com and then

00:03:19,680 --> 00:03:23,680
basically which

00:03:20,480 --> 00:03:25,360
would serve out or rotate the

00:03:23,680 --> 00:03:26,720
the work between the servers and that's

00:03:25,360 --> 00:03:31,040
how it would scale out

00:03:26,720 --> 00:03:33,840
this would still this would allow us to

00:03:31,040 --> 00:03:33,840
set up

00:03:34,560 --> 00:03:39,599
a platform that would be able to accept

00:03:36,879 --> 00:03:42,640
far more traffic than before

00:03:39,599 --> 00:03:44,400
but still problems like the thundering

00:03:42,640 --> 00:03:47,280
herd that we now had

00:03:44,400 --> 00:03:49,200
whereas black friday christmas shopping

00:03:47,280 --> 00:03:52,400
there would be a massive peak

00:03:49,200 --> 00:03:53,760
um that we would have to basically buy

00:03:52,400 --> 00:03:55,680
hardware for

00:03:53,760 --> 00:03:58,239
uh that would be running at zero

00:03:55,680 --> 00:04:00,640
capacity max zero capacity at zero

00:03:58,239 --> 00:04:02,400
usage for most of the year basically to

00:04:00,640 --> 00:04:05,120
be able to

00:04:02,400 --> 00:04:05,519
absorb the additional traffic for those

00:04:05,120 --> 00:04:07,360
you know

00:04:05,519 --> 00:04:09,040
very limited amount of time and that's

00:04:07,360 --> 00:04:11,439
basically what's because of the same

00:04:09,040 --> 00:04:14,400
problem we had before

00:04:11,439 --> 00:04:16,320
buying new servers was very very

00:04:14,400 --> 00:04:19,680
expensive it was a time consuming thing

00:04:16,320 --> 00:04:20,560
so you could not basically say i would

00:04:19,680 --> 00:04:23,840
want to rent new

00:04:20,560 --> 00:04:24,880
capacity for a week two weeks no you

00:04:23,840 --> 00:04:26,479
would have to buy them and

00:04:24,880 --> 00:04:28,000
instead install them behind your load

00:04:26,479 --> 00:04:31,600
balancer just in case

00:04:28,000 --> 00:04:32,400
it would happen then the actual cloud

00:04:31,600 --> 00:04:34,479
kicked off

00:04:32,400 --> 00:04:37,440
so you could run someone else's computer

00:04:34,479 --> 00:04:39,280
in a data center that was behind an api

00:04:37,440 --> 00:04:40,479
uh containers were came in things the

00:04:39,280 --> 00:04:44,000
docker docker docker

00:04:40,479 --> 00:04:44,400
rules the world that needed a scheduler

00:04:44,000 --> 00:04:47,680
so

00:04:44,400 --> 00:04:50,880
kubernetes was invented that

00:04:47,680 --> 00:04:52,479
made life increasingly

00:04:50,880 --> 00:04:54,560
difficult for people that didn't want to

00:04:52,479 --> 00:04:57,040
program in yamo

00:04:54,560 --> 00:04:58,560
than multi-cloud became a thing so i'm

00:04:57,040 --> 00:05:02,160
not only running a cube

00:04:58,560 --> 00:05:05,360
in in iews i'm running one in azure or

00:05:02,160 --> 00:05:08,479
a google for good matter measure and

00:05:05,360 --> 00:05:11,919
life became very difficult

00:05:08,479 --> 00:05:15,199
for people that wanted to run a stable

00:05:11,919 --> 00:05:19,039
simple keep it simple stupid

00:05:15,199 --> 00:05:23,759
platform so the lovely people at

00:05:19,039 --> 00:05:26,000
hashicorp created another

00:05:23,759 --> 00:05:27,759
container schedule well actually it does

00:05:26,000 --> 00:05:29,120
normal does far more than just container

00:05:27,759 --> 00:05:31,520
scheduling but most in

00:05:29,120 --> 00:05:32,479
during most of this talk i'm focusing on

00:05:31,520 --> 00:05:36,080
it being a

00:05:32,479 --> 00:05:36,080
container scheduler it can do

00:05:36,240 --> 00:05:39,280
it can do java directly you can do

00:05:38,000 --> 00:05:41,759
executes it

00:05:39,280 --> 00:05:43,440
is plugin based so i can or the

00:05:41,759 --> 00:05:46,639
community has written

00:05:43,440 --> 00:05:49,759
plugins for most things that can

00:05:46,639 --> 00:05:49,759
house vms

00:05:50,320 --> 00:05:54,320
which are available on their website but

00:05:52,880 --> 00:05:57,120
we're focusing on it

00:05:54,320 --> 00:05:58,479
being very good at doing containers so

00:05:57,120 --> 00:06:02,240
as an example let's

00:05:58,479 --> 00:06:03,919
deploy our blog well in case this is a

00:06:02,240 --> 00:06:07,280
overview of my blog and i'll run it

00:06:03,919 --> 00:06:11,759
somewhere in aws because i have a

00:06:07,280 --> 00:06:14,880
reserved instance and

00:06:11,759 --> 00:06:18,319
no my jobs look always

00:06:14,880 --> 00:06:19,120
like this there's a it's written in the

00:06:18,319 --> 00:06:21,919
hdl

00:06:19,120 --> 00:06:23,600
the hashicorp configuration language dsl

00:06:21,919 --> 00:06:25,520
they came up themselves for people that

00:06:23,600 --> 00:06:27,520
are used to puppet

00:06:25,520 --> 00:06:29,680
configuration management there this

00:06:27,520 --> 00:06:31,039
should look very familiar but there's a

00:06:29,680 --> 00:06:34,000
couple of

00:06:31,039 --> 00:06:35,360
peculiarities so you always need to have

00:06:34,000 --> 00:06:39,280
a job blog

00:06:35,360 --> 00:06:42,160
well yeah a job job stanza with the name

00:06:39,280 --> 00:06:42,560
uh it will have groups uh one or more

00:06:42,160 --> 00:06:44,400
and then

00:06:42,560 --> 00:06:46,240
a group can have one or more tasks so in

00:06:44,400 --> 00:06:48,000
my case i have a job that's called blog

00:06:46,240 --> 00:06:50,000
will run in the aws data

00:06:48,000 --> 00:06:51,120
center there is a tive service there's a

00:06:50,000 --> 00:06:54,800
couple other

00:06:51,120 --> 00:06:54,800
types that we won't go in today

00:06:54,960 --> 00:06:58,639
like batch for instance and then there's

00:06:57,440 --> 00:07:00,479
my group

00:06:58,639 --> 00:07:01,919
called my blogs written in a tool called

00:07:00,479 --> 00:07:04,560
hugo

00:07:01,919 --> 00:07:06,720
i'm going to expose the http port from

00:07:04,560 --> 00:07:06,720
the

00:07:07,039 --> 00:07:14,400
the container and my container runs in

00:07:10,960 --> 00:07:16,160
in a basically fairly standard nginx

00:07:14,400 --> 00:07:18,400
container that i pull from my private

00:07:16,160 --> 00:07:21,840
repository and then i expose

00:07:18,400 --> 00:07:25,919
the http port fairly simple we run this

00:07:21,840 --> 00:07:28,840
basically nomad run then file name

00:07:25,919 --> 00:07:30,080
or i can copy and paste it into ui which

00:07:28,840 --> 00:07:34,479
is

00:07:30,080 --> 00:07:36,319
fairly nice now we have

00:07:34,479 --> 00:07:37,680
basically the same situation as before

00:07:36,319 --> 00:07:39,550
we have

00:07:37,680 --> 00:07:41,199
one instance of something running on

00:07:39,550 --> 00:07:44,560
[Music]

00:07:41,199 --> 00:07:46,879
complicated infrastructure that

00:07:44,560 --> 00:07:48,800
was running on the vms or the bare

00:07:46,879 --> 00:07:49,840
metals or the cloud instances that i had

00:07:48,800 --> 00:07:53,039
before so that actually

00:07:49,840 --> 00:07:55,919
didn't yield us anything so um

00:07:53,039 --> 00:07:57,840
one is none so if that one craps out or

00:07:55,919 --> 00:07:59,840
gets

00:07:57,840 --> 00:08:02,000
taken out or the underlying hardware

00:07:59,840 --> 00:08:03,919
gets taken out then

00:08:02,000 --> 00:08:05,919
we're in trouble there's nothing nobody

00:08:03,919 --> 00:08:08,080
will try and restart the thing for you

00:08:05,919 --> 00:08:09,680
but we're still in trouble it will take

00:08:08,080 --> 00:08:13,280
a while for that to come back so

00:08:09,680 --> 00:08:14,479
nomad has the notion of counts so i can

00:08:13,280 --> 00:08:17,440
tell it counts

00:08:14,479 --> 00:08:18,319
equals two so nomad doesn't start just a

00:08:17,440 --> 00:08:22,000
one it will start

00:08:18,319 --> 00:08:24,800
two and by using a console which is

00:08:22,000 --> 00:08:26,240
hashicorp's service discovery tool and

00:08:24,800 --> 00:08:28,479
for instance a

00:08:26,240 --> 00:08:29,360
ingers controller named traffic that can

00:08:28,479 --> 00:08:31,680
read

00:08:29,360 --> 00:08:33,519
console services we can make sure that

00:08:31,680 --> 00:08:36,479
now we have two instances and both

00:08:33,519 --> 00:08:36,959
instances will be used to browse traffic

00:08:36,479 --> 00:08:38,640
too

00:08:36,959 --> 00:08:40,959
basically traffic will be our load

00:08:38,640 --> 00:08:44,080
balancer from the original

00:08:40,959 --> 00:08:46,959
image if you can remember

00:08:44,080 --> 00:08:49,120
now we have two instances we want we can

00:08:46,959 --> 00:08:52,800
lose one

00:08:49,120 --> 00:08:55,600
and the other one will take over so

00:08:52,800 --> 00:08:56,560
we're a bit we're a bit better prepared

00:08:55,600 --> 00:09:00,560
but

00:08:56,560 --> 00:09:03,920
um nomad has a default

00:09:00,560 --> 00:09:05,519
scheduling um algorithm that's called

00:09:03,920 --> 00:09:08,320
bin pack so it will actually try and

00:09:05,519 --> 00:09:10,800
stuff as much of your tasks onto

00:09:08,320 --> 00:09:10,800
the same

00:09:11,360 --> 00:09:15,600
node client node as possible that's

00:09:13,920 --> 00:09:17,760
basically because it wants

00:09:15,600 --> 00:09:19,920
you to do this in the cheapest way

00:09:17,760 --> 00:09:23,600
possible but since we're actually

00:09:19,920 --> 00:09:26,560
trying to extend

00:09:23,600 --> 00:09:28,800
our our risk factors well not our

00:09:26,560 --> 00:09:31,040
responses we want to cover all our bases

00:09:28,800 --> 00:09:32,880
covering our risks so when we do

00:09:31,040 --> 00:09:36,240
countless 2 it will actually

00:09:32,880 --> 00:09:38,240
originally it will

00:09:36,240 --> 00:09:39,920
start the two instances along the same

00:09:38,240 --> 00:09:40,640
client node which is not something we

00:09:39,920 --> 00:09:43,360
want so

00:09:40,640 --> 00:09:43,920
we want to make sure that we can survive

00:09:43,360 --> 00:09:47,120
that

00:09:43,920 --> 00:09:47,760
node dying or being taken out of the

00:09:47,120 --> 00:09:50,800
pool

00:09:47,760 --> 00:09:54,240
so for that

00:09:50,800 --> 00:09:58,320
the the hcl normal hdl has

00:09:54,240 --> 00:10:01,040
constraints so we can say

00:09:58,320 --> 00:10:02,079
force using constraints we can force the

00:10:01,040 --> 00:10:05,120
two jobs onto

00:10:02,079 --> 00:10:08,000
two different clients

00:10:05,120 --> 00:10:09,920
and then this case will the key to use

00:10:08,000 --> 00:10:12,800
is the operator's distinct host

00:10:09,920 --> 00:10:15,360
and force it and uv force it by using

00:10:12,800 --> 00:10:18,399
the value is true

00:10:15,360 --> 00:10:20,880
constraints is a very nice

00:10:18,399 --> 00:10:20,880
concept

00:10:21,839 --> 00:10:25,680
but if the constraint can't be met nomad

00:10:25,120 --> 00:10:28,160
will just

00:10:25,680 --> 00:10:29,600
not schedule it it will it will throw a

00:10:28,160 --> 00:10:32,240
warning or an alert

00:10:29,600 --> 00:10:33,680
but it won't actually schedule so you

00:10:32,240 --> 00:10:36,959
think you might be covered

00:10:33,680 --> 00:10:39,680
but you probably aren't because the

00:10:36,959 --> 00:10:41,920
the nomad will patiently wait before

00:10:39,680 --> 00:10:44,959
this constraint to come true

00:10:41,920 --> 00:10:46,000
different heart like forcing it onto

00:10:44,959 --> 00:10:48,399
different nodes is probably

00:10:46,000 --> 00:10:50,320
fine but you can have far more

00:10:48,399 --> 00:10:53,440
fine-grained stuff

00:10:50,320 --> 00:10:58,000
to use as a spread

00:10:53,440 --> 00:11:01,360
and so if you want to make sure that

00:10:58,000 --> 00:11:04,880
it is spread when

00:11:01,360 --> 00:11:06,560
um when possible then you don't use

00:11:04,880 --> 00:11:09,519
constraints you use spreads

00:11:06,560 --> 00:11:10,320
in this case i'm spreading across a meta

00:11:09,519 --> 00:11:12,560
tag

00:11:10,320 --> 00:11:13,760
called rack and i want to have 50

00:11:12,560 --> 00:11:16,480
percent

00:11:13,760 --> 00:11:19,279
deployed onto onto the rack called hiss

00:11:16,480 --> 00:11:22,959
and i want to have 50 percent onto rack

00:11:19,279 --> 00:11:25,519
hers so if any of those constraints

00:11:22,959 --> 00:11:29,839
isn't possible it will actually accept

00:11:25,519 --> 00:11:29,839
that it isn't possible and it will

00:11:30,560 --> 00:11:38,560
will provision the task onto like the

00:11:34,079 --> 00:11:40,560
least preferred condition

00:11:38,560 --> 00:11:42,000
but that in that way we can always make

00:11:40,560 --> 00:11:44,560
sure that there is at least

00:11:42,000 --> 00:11:46,640
two the count is at least two so even

00:11:44,560 --> 00:11:49,519
though it's it's not preferred we

00:11:46,640 --> 00:11:51,040
if we are worried about the thundering

00:11:49,519 --> 00:11:52,720
herd so that we actually need two

00:11:51,040 --> 00:11:55,760
instances of 10 or 20

00:11:52,720 --> 00:11:57,920
instances to accept the traffic then

00:11:55,760 --> 00:12:01,040
spread is probably the uh

00:11:57,920 --> 00:12:03,519
the way to go forward and in order to um

00:12:01,040 --> 00:12:05,200
push in while not pushing we need to

00:12:03,519 --> 00:12:06,880
push in that this metadata this

00:12:05,200 --> 00:12:09,600
metadata rack is something i came up

00:12:06,880 --> 00:12:12,880
with isn't her is not something nomad

00:12:09,600 --> 00:12:13,760
knows about so in the nomad

00:12:12,880 --> 00:12:17,360
configuration

00:12:13,760 --> 00:12:20,560
in the client section we need to add

00:12:17,360 --> 00:12:25,040
the meta tags that we want to apply to

00:12:20,560 --> 00:12:28,560
certain nodes

00:12:25,040 --> 00:12:30,720
this is still a very static way so i

00:12:28,560 --> 00:12:32,240
need to predict in advance

00:12:30,720 --> 00:12:34,480
how many instances i want to run and

00:12:32,240 --> 00:12:35,760
that's not something

00:12:34,480 --> 00:12:37,920
that's not something while we move to

00:12:35,760 --> 00:12:38,639
the cloud that's not why we set up nomad

00:12:37,920 --> 00:12:42,079
so

00:12:38,639 --> 00:12:44,160
um we want nomads to be able to make

00:12:42,079 --> 00:12:45,760
decisions on our behalf so it might be

00:12:44,160 --> 00:12:46,959
in the middle of the night when america

00:12:45,760 --> 00:12:50,639
comes online

00:12:46,959 --> 00:12:52,560
um yeah in the middle of the night

00:12:50,639 --> 00:12:54,480
that's not something you want a human to

00:12:52,560 --> 00:12:55,760
wake up for to basically scale out your

00:12:54,480 --> 00:12:59,200
infrastructure so

00:12:55,760 --> 00:13:01,920
um nomad came up the nomad team

00:12:59,200 --> 00:13:04,160
came up with a function called the nomad

00:13:01,920 --> 00:13:07,600
autoscaler it was introduced in nomad

00:13:04,160 --> 00:13:09,200
0.11 and it's

00:13:07,600 --> 00:13:10,800
it's in a currently an independent

00:13:09,200 --> 00:13:11,839
project so it has an independent release

00:13:10,800 --> 00:13:13,600
cycle so that

00:13:11,839 --> 00:13:15,200
they can because they're still

00:13:13,600 --> 00:13:16,160
developing the tool they're still

00:13:15,200 --> 00:13:18,240
developing

00:13:16,160 --> 00:13:20,000
or listening to the to the industry in

00:13:18,240 --> 00:13:23,680
the community about what use cases to

00:13:20,000 --> 00:13:25,120
uh to provide for so it's

00:13:23,680 --> 00:13:26,720
and that's why it's on an independent

00:13:25,120 --> 00:13:29,120
release cycle it's

00:13:26,720 --> 00:13:30,320
um and it is gaining new functionality

00:13:29,120 --> 00:13:32,720
every release

00:13:30,320 --> 00:13:34,880
uh it's built for both horizontal and

00:13:32,720 --> 00:13:37,760
first vertical scaling so i can

00:13:34,880 --> 00:13:38,480
scale the amount of instances i have but

00:13:37,760 --> 00:13:41,440
it also

00:13:38,480 --> 00:13:41,440
now is able to

00:13:42,160 --> 00:13:49,519
scale how many clients i have so

00:13:45,839 --> 00:13:50,399
based on how how much leeway your amazon

00:13:49,519 --> 00:13:53,680
account is you can

00:13:50,399 --> 00:13:55,440
basically scale infinitely until amazon

00:13:53,680 --> 00:13:57,120
runs out of hardware or your credit card

00:13:55,440 --> 00:14:01,279
runs out of

00:13:57,120 --> 00:14:04,160
money and same as nomad it's

00:14:01,279 --> 00:14:05,440
extendable by your own or community

00:14:04,160 --> 00:14:08,639
plugins

00:14:05,440 --> 00:14:10,399
which that now are becoming available

00:14:08,639 --> 00:14:13,839
several of those

00:14:10,399 --> 00:14:16,000
if hashicorp doesn't see the use case or

00:14:13,839 --> 00:14:19,360
is not comfortable

00:14:16,000 --> 00:14:22,639
maintaining code for cloud servers

00:14:19,360 --> 00:14:26,399
they're not particularly familiar with

00:14:22,639 --> 00:14:29,120
so how does the nomad autoscaler work

00:14:26,399 --> 00:14:30,000
it makes decisions based on checks that

00:14:29,120 --> 00:14:31,760
you've written yourself

00:14:30,000 --> 00:14:33,680
and a check is a combination of three

00:14:31,760 --> 00:14:35,920
things first you need to have

00:14:33,680 --> 00:14:36,720
data that is queried from some form of

00:14:35,920 --> 00:14:41,040
apm

00:14:36,720 --> 00:14:44,079
i think prometheus or or datadog

00:14:41,040 --> 00:14:47,839
then it's based on on a strategy

00:14:44,079 --> 00:14:47,839
basically you tell it you need to

00:14:48,000 --> 00:14:51,199
obtain a certain value and that's

00:14:50,160 --> 00:14:54,320
something you

00:14:51,199 --> 00:14:54,320
define in the target

00:14:54,399 --> 00:14:58,959
multiple checks can be combined and then

00:14:57,519 --> 00:15:01,199
um

00:14:58,959 --> 00:15:04,079
to in order to make sure that you will

00:15:01,199 --> 00:15:07,360
always have enough resources online

00:15:04,079 --> 00:15:07,600
they defined that the the the check with

00:15:07,360 --> 00:15:09,279
the

00:15:07,600 --> 00:15:11,360
that will have the outcome of the most

00:15:09,279 --> 00:15:12,880
resources will win so if you have

00:15:11,360 --> 00:15:15,120
two checks one we'll say we need to

00:15:12,880 --> 00:15:18,399
scale out one says we need to scale in

00:15:15,120 --> 00:15:21,040
scale up will win same for scale out

00:15:18,399 --> 00:15:21,680
versus and like we stay the same or

00:15:21,040 --> 00:15:24,399
let's say

00:15:21,680 --> 00:15:25,120
we need ten instances the answer for one

00:15:24,399 --> 00:15:27,279
is ten in

00:15:25,120 --> 00:15:28,959
10 new instances or we need to scale to

00:15:27,279 --> 00:15:31,759
10 and the other one says scale to five

00:15:28,959 --> 00:15:35,040
and scale to 10 will win that way

00:15:31,759 --> 00:15:38,800
you will always have the

00:15:35,040 --> 00:15:38,800
right amount of resources available

00:15:39,120 --> 00:15:43,920
but people new to auto scalers sometimes

00:15:41,839 --> 00:15:47,440
find this confusing so i'll

00:15:43,920 --> 00:15:49,279
i'll address it here um so

00:15:47,440 --> 00:15:51,120
it's it's a bit method the other scaler

00:15:49,279 --> 00:15:52,000
you know what other scalar can run in

00:15:51,120 --> 00:15:55,040
nomads so

00:15:52,000 --> 00:15:55,680
here i deploy the nomad auto scaler into

00:15:55,040 --> 00:15:57,440
nomad

00:15:55,680 --> 00:15:59,199
it's the same type as before as a

00:15:57,440 --> 00:16:02,000
service and

00:15:59,199 --> 00:16:03,120
we deploy off of a docker image that

00:16:02,000 --> 00:16:05,920
hashicorps

00:16:03,120 --> 00:16:07,120
pushes into docker hub and then we

00:16:05,920 --> 00:16:10,720
provided some

00:16:07,120 --> 00:16:10,720
some configuration in the file

00:16:11,120 --> 00:16:18,480
and the configuration basically

00:16:14,160 --> 00:16:21,040
is we pointed to our apm

00:16:18,480 --> 00:16:22,720
well now first we pointed to our nomad

00:16:21,040 --> 00:16:26,320
instance

00:16:22,720 --> 00:16:28,160
and by some nomad

00:16:26,320 --> 00:16:29,600
variable magic i basically pointed to

00:16:28,160 --> 00:16:32,800
its own rp address

00:16:29,600 --> 00:16:35,199
then we pointed to

00:16:32,800 --> 00:16:36,000
our prometheus instance which in our

00:16:35,199 --> 00:16:39,440
case is just

00:16:36,000 --> 00:16:42,240
a service we discovered using console

00:16:39,440 --> 00:16:44,079
and we defined that the strategy we're

00:16:42,240 --> 00:16:44,959
going to use for this project is target

00:16:44,079 --> 00:16:46,560
value

00:16:44,959 --> 00:16:49,040
so the project will need to tell us a

00:16:46,560 --> 00:16:52,240
certain value

00:16:49,040 --> 00:16:54,560
for whatever query we come up with so

00:16:52,240 --> 00:16:55,360
prometheus let's investigate a bit more

00:16:54,560 --> 00:16:58,320
what that is

00:16:55,360 --> 00:16:59,440
it's a metric system it's pretty much

00:16:58,320 --> 00:17:02,480
the default

00:16:59,440 --> 00:17:04,000
for cloud native um it

00:17:02,480 --> 00:17:05,919
came out of soundcloud a couple years

00:17:04,000 --> 00:17:08,480
ago and

00:17:05,919 --> 00:17:10,240
in contrary to the older generations of

00:17:08,480 --> 00:17:11,360
metric systems it's actually a pool

00:17:10,240 --> 00:17:13,039
based system

00:17:11,360 --> 00:17:15,039
versus what used to be a push-based

00:17:13,039 --> 00:17:15,439
system so we need prometheus needs to

00:17:15,039 --> 00:17:19,199
know

00:17:15,439 --> 00:17:20,400
where your your metrics are so it can

00:17:19,199 --> 00:17:23,919
scrape it

00:17:20,400 --> 00:17:24,880
um and but the pretty thing is you can

00:17:23,919 --> 00:17:29,360
now define

00:17:24,880 --> 00:17:30,960
your own exporters basically uh with

00:17:29,360 --> 00:17:32,640
that exposure the metrics that you're

00:17:30,960 --> 00:17:35,360
interested in

00:17:32,640 --> 00:17:36,000
so now let's integrate the autoscalers

00:17:35,360 --> 00:17:38,880
running also

00:17:36,000 --> 00:17:39,760
nomad my blocks running in nomad and

00:17:38,880 --> 00:17:43,520
let's add

00:17:39,760 --> 00:17:47,360
the the scaling standard

00:17:43,520 --> 00:17:50,559
scaling stance is going to groups and

00:17:47,360 --> 00:17:52,240
we basically we can add it in with

00:17:50,559 --> 00:17:53,440
enable this false then it's just in

00:17:52,240 --> 00:17:55,919
there and when we need it

00:17:53,440 --> 00:17:57,440
we can flick it on but i always started

00:17:55,919 --> 00:18:00,000
with

00:17:57,440 --> 00:18:00,559
with true there's a we can define a min

00:18:00,000 --> 00:18:04,559
and max

00:18:00,559 --> 00:18:07,120
so that if we have a thundering herd

00:18:04,559 --> 00:18:08,240
we don't actually blow up our platform

00:18:07,120 --> 00:18:10,720
but we

00:18:08,240 --> 00:18:13,039
i say there's a maximum 20 in system of

00:18:10,720 --> 00:18:16,559
my block and run

00:18:13,039 --> 00:18:16,559
and then we add the policy

00:18:17,200 --> 00:18:21,760
um we basically say go into prometheus

00:18:20,240 --> 00:18:25,039
do a certain query in

00:18:21,760 --> 00:18:25,679
my case is look at the metrics that

00:18:25,039 --> 00:18:28,799
traffic

00:18:25,679 --> 00:18:31,679
our index controller provides

00:18:28,799 --> 00:18:33,200
and look at how many people are actually

00:18:31,679 --> 00:18:35,840
concurrently connected to

00:18:33,200 --> 00:18:36,320
my blog and the target value should be

00:18:35,840 --> 00:18:38,799
five

00:18:36,320 --> 00:18:40,160
so if you go over five concurrent users

00:18:38,799 --> 00:18:43,039
you should be scaling

00:18:40,160 --> 00:18:44,320
or if you drop below five or an

00:18:43,039 --> 00:18:46,480
increment of five

00:18:44,320 --> 00:18:48,320
then you should start making decisions

00:18:46,480 --> 00:18:51,120
and

00:18:48,320 --> 00:18:52,400
we add a cool cool down option of 20

00:18:51,120 --> 00:18:55,440
seconds so

00:18:52,400 --> 00:18:56,320
um don't start flip-flopping when the

00:18:55,440 --> 00:18:59,280
traffic goes up

00:18:56,320 --> 00:19:00,080
graphic goes down no just make sure that

00:18:59,280 --> 00:19:03,520
you look at

00:19:00,080 --> 00:19:06,640
20 seconds increments so

00:19:03,520 --> 00:19:09,760
you don't start to

00:19:06,640 --> 00:19:10,320
go crazy while increasing or decreasing

00:19:09,760 --> 00:19:13,039
um

00:19:10,320 --> 00:19:15,039
we can visualize all of this stuff in in

00:19:13,039 --> 00:19:18,559
grafana grafana is

00:19:15,039 --> 00:19:19,120
for me the golan standard with regards

00:19:18,559 --> 00:19:23,520
to

00:19:19,120 --> 00:19:26,080
observability dashboards and we can add

00:19:23,520 --> 00:19:26,880
one for all the metrics we see coming

00:19:26,080 --> 00:19:28,480
for our platform

00:19:26,880 --> 00:19:30,080
not just our platform but also the

00:19:28,480 --> 00:19:34,080
autoscaler

00:19:30,080 --> 00:19:36,080
emits metrics which you can scrape so

00:19:34,080 --> 00:19:38,640
this is a sample dashboard it shows us

00:19:36,080 --> 00:19:40,720
some metrics about memory and how much

00:19:38,640 --> 00:19:42,799
cpu is used and but the block in the

00:19:40,720 --> 00:19:44,960
middle is what's interested

00:19:42,799 --> 00:19:46,160
here we're going to see two things one

00:19:44,960 --> 00:19:46,960
is the amount of connections that are

00:19:46,160 --> 00:19:50,480
coming in

00:19:46,960 --> 00:19:53,760
and on the left and the amount of

00:19:50,480 --> 00:19:57,120
nodes we have running on the right not

00:19:53,760 --> 00:19:59,760
just right it's the the right hand

00:19:57,120 --> 00:19:59,760
y-axis

00:20:00,320 --> 00:20:05,520
so if you

00:20:03,360 --> 00:20:06,559
remember correctly or i didn't really

00:20:05,520 --> 00:20:09,440
explain

00:20:06,559 --> 00:20:11,760
i started the project with a count of

00:20:09,440 --> 00:20:15,760
three

00:20:11,760 --> 00:20:18,880
and now we can see here on this

00:20:15,760 --> 00:20:20,880
this this graph that it actually

00:20:18,880 --> 00:20:23,120
already made a decision three is is too

00:20:20,880 --> 00:20:26,159
much we can go down to one

00:20:23,120 --> 00:20:27,840
um and

00:20:26,159 --> 00:20:29,760
if we look actually into the logging

00:20:27,840 --> 00:20:32,840
that the auto scaler emits

00:20:29,760 --> 00:20:34,000
um there's we can also see this so

00:20:32,840 --> 00:20:38,000
basically

00:20:34,000 --> 00:20:38,559
it um here there this is the logging and

00:20:38,000 --> 00:20:42,159
on the

00:20:38,559 --> 00:20:46,240
the third line you basically see it's um

00:20:42,159 --> 00:20:47,280
we want uh why it did the query it came

00:20:46,240 --> 00:20:49,440
up with the number

00:20:47,280 --> 00:20:51,760
and it says based on we have currently

00:20:49,440 --> 00:20:52,880
have three we should probably go down to

00:20:51,760 --> 00:20:54,799
zero

00:20:52,880 --> 00:20:56,320
because we have no traffic whatsoever

00:20:54,799 --> 00:20:59,679
but because

00:20:56,320 --> 00:21:03,200
the user says there's a minimum of one

00:20:59,679 --> 00:21:06,320
we'll gonna end up with just the one

00:21:03,200 --> 00:21:06,960
and then you see the actual the platform

00:21:06,320 --> 00:21:10,000
scaling

00:21:06,960 --> 00:21:12,240
which is pretty cool so if you do the

00:21:10,000 --> 00:21:15,200
reverse basically we apply some pressure

00:21:12,240 --> 00:21:16,240
uh using a tool in this case called hey

00:21:15,200 --> 00:21:20,559
or

00:21:16,240 --> 00:21:20,559
this gatling or whatever tooling you're

00:21:21,520 --> 00:21:25,039
you're comfortable with um you can

00:21:23,760 --> 00:21:28,480
actually see here

00:21:25,039 --> 00:21:31,200
now in this case from we apply

00:21:28,480 --> 00:21:33,840
30 concurrent uh connections over a

00:21:31,200 --> 00:21:37,360
minute and we push it basically into uh

00:21:33,840 --> 00:21:39,120
my local host and then we can see

00:21:37,360 --> 00:21:40,400
the number go up on the right that's the

00:21:39,120 --> 00:21:42,720
blue line

00:21:40,400 --> 00:21:44,240
that's the connections and then we also

00:21:42,720 --> 00:21:48,400
already see that it's

00:21:44,240 --> 00:21:51,280
um it starts ramping nomad

00:21:48,400 --> 00:21:51,679
uh the star is ramping up the amount of

00:21:51,280 --> 00:21:54,720
uh

00:21:51,679 --> 00:21:57,760
of hosts or instances

00:21:54,720 --> 00:21:58,159
that is online so and then the reverse

00:21:57,760 --> 00:22:00,080
if we

00:21:58,159 --> 00:22:01,679
after the minute the hay stops the

00:22:00,080 --> 00:22:03,919
connections we actually see

00:22:01,679 --> 00:22:05,280
the platform make this decision to go

00:22:03,919 --> 00:22:09,200
back down again

00:22:05,280 --> 00:22:10,880
so now we have a very nice way of

00:22:09,200 --> 00:22:13,840
automatically responding to the

00:22:10,880 --> 00:22:13,840
thundering herd

00:22:16,640 --> 00:22:23,280
this to me is very interesting

00:22:20,159 --> 00:22:24,640
but it's a bit scary basically having

00:22:23,280 --> 00:22:27,600
the platform

00:22:24,640 --> 00:22:28,960
make decisions on its own um that's what

00:22:27,600 --> 00:22:32,000
we paid for but

00:22:28,960 --> 00:22:34,400
not something we still want to track so

00:22:32,000 --> 00:22:35,280
um fun and the people are lovely people

00:22:34,400 --> 00:22:39,039
like gerfani

00:22:35,280 --> 00:22:42,480
piranha came up with another tool

00:22:39,039 --> 00:22:45,039
loki loki what they uh

00:22:42,480 --> 00:22:47,840
say is the prometheus for logs it's a

00:22:45,039 --> 00:22:51,039
very fairly simple way of

00:22:47,840 --> 00:22:53,280
of storing logs um

00:22:51,039 --> 00:22:55,039
historically people put them in splunk

00:22:53,280 --> 00:22:56,960
or in the elk stack but that's

00:22:55,039 --> 00:22:58,480
a platform in and of itself so i'm

00:22:56,960 --> 00:23:00,159
showing off loki

00:22:58,480 --> 00:23:01,600
which is a way to store the logs so

00:23:00,159 --> 00:23:02,240
that's something we can look at or

00:23:01,600 --> 00:23:06,080
actually

00:23:02,240 --> 00:23:07,280
correlate between the the graphs we're

00:23:06,080 --> 00:23:08,799
seeing the metrics in the graphs we're

00:23:07,280 --> 00:23:10,000
seeing and the look events that are

00:23:08,799 --> 00:23:12,000
happening

00:23:10,000 --> 00:23:14,320
there's two ways of getting stuff into

00:23:12,000 --> 00:23:16,240
loki and since we're using docker we

00:23:14,320 --> 00:23:19,919
could use the talker plugin

00:23:16,240 --> 00:23:22,080
and that the people like arfana provide

00:23:19,919 --> 00:23:23,679
then we do this using a command line

00:23:22,080 --> 00:23:26,880
docker plug-in install

00:23:23,679 --> 00:23:30,400
and then we actually update the

00:23:26,880 --> 00:23:32,240
our blog no man job

00:23:30,400 --> 00:23:33,760
by adding a little logging stance and

00:23:32,240 --> 00:23:36,880
saying we want

00:23:33,760 --> 00:23:39,120
the loc logging push to whatever

00:23:36,880 --> 00:23:41,679
plug-in loki provides and then we push

00:23:39,120 --> 00:23:44,480
it to a

00:23:41,679 --> 00:23:47,200
a certain url in that is exposed by our

00:23:44,480 --> 00:23:47,200
console cluster

00:23:48,240 --> 00:23:55,840
currently the loki driver is blocking

00:23:52,400 --> 00:23:57,679
so if your loki platform actually goes

00:23:55,840 --> 00:23:58,720
down you'll have trouble manipulating

00:23:57,679 --> 00:24:00,480
your container so

00:23:58,720 --> 00:24:03,200
probably what yet the moment you want to

00:24:00,480 --> 00:24:05,840
do is run a sidecar

00:24:03,200 --> 00:24:06,400
just like kubernetes nomad can deal can

00:24:05,840 --> 00:24:10,000
do

00:24:06,400 --> 00:24:13,919
sidecars in this case i'm starting the

00:24:10,000 --> 00:24:18,000
loki command line tool called promtale

00:24:13,919 --> 00:24:21,039
as a sidecar to our

00:24:18,000 --> 00:24:24,080
blog task so in our

00:24:21,039 --> 00:24:24,559
blog.nomad file we add another task into

00:24:24,080 --> 00:24:28,320
our

00:24:24,559 --> 00:24:31,440
um our group and which basically says i

00:24:28,320 --> 00:24:34,960
am a sidecar that needs to start before

00:24:31,440 --> 00:24:37,360
the actual container and

00:24:34,960 --> 00:24:38,400
we start want to start prom tail with a

00:24:37,360 --> 00:24:39,760
certain configuration and the

00:24:38,400 --> 00:24:41,440
configuration with people

00:24:39,760 --> 00:24:42,960
that are used to prometheus is fairly

00:24:41,440 --> 00:24:44,840
similar well it's actually similar

00:24:42,960 --> 00:24:47,200
because because it's code reuse from

00:24:44,840 --> 00:24:47,600
prometheus and it but basically what we

00:24:47,200 --> 00:24:50,240
do

00:24:47,600 --> 00:24:52,480
is we point it into a path that is

00:24:50,240 --> 00:24:54,480
available to our local tasks

00:24:52,480 --> 00:24:57,039
and since it's a sidecar it can actually

00:24:54,480 --> 00:24:57,039
read into

00:24:57,279 --> 00:25:01,039
the blog container space and we

00:25:00,320 --> 00:25:04,320
basically

00:25:01,039 --> 00:25:04,320
start scraping all the

00:25:05,120 --> 00:25:12,240
log files that are out of scalar related

00:25:08,640 --> 00:25:12,960
and we match on any event that is tagged

00:25:12,240 --> 00:25:16,159
with

00:25:12,960 --> 00:25:19,520
task auto scaler and then we convert the

00:25:16,159 --> 00:25:19,520
json intermitts into

00:25:19,840 --> 00:25:25,360
things we can query for in in loki

00:25:23,520 --> 00:25:27,200
this way we can now annotate our graph

00:25:25,360 --> 00:25:30,559
so basically

00:25:27,200 --> 00:25:32,400
we can correlate between metrics and

00:25:30,559 --> 00:25:36,000
events

00:25:32,400 --> 00:25:40,000
we correlate on on

00:25:36,000 --> 00:25:40,000
the key value per task is autoscaler

00:25:40,240 --> 00:25:43,440
but we filter out these scaling targets

00:25:42,880 --> 00:25:46,960
um

00:25:43,440 --> 00:25:48,880
that way we've seen the uh

00:25:46,960 --> 00:25:51,279
the vertical blue lines already on all

00:25:48,880 --> 00:25:54,720
the graphs before and that's

00:25:51,279 --> 00:25:54,720
now an annotation in

00:25:55,520 --> 00:25:58,640
and grafana which you can hover over so

00:25:57,760 --> 00:26:02,320
you can actually

00:25:58,640 --> 00:26:04,480
see the event that has caused

00:26:02,320 --> 00:26:05,840
well you can see the event that happened

00:26:04,480 --> 00:26:08,799
at the same time

00:26:05,840 --> 00:26:10,080
as the metric so now here if we hover

00:26:08,799 --> 00:26:12,799
over it we actually see

00:26:10,080 --> 00:26:13,520
that there was a decision made to scale

00:26:12,799 --> 00:26:16,720
from six

00:26:13,520 --> 00:26:20,000
back down to one uh

00:26:16,720 --> 00:26:21,840
running uh instances so if you wanna try

00:26:20,000 --> 00:26:23,039
this themselves the lovely people at

00:26:21,840 --> 00:26:26,480
hashicorp have

00:26:23,039 --> 00:26:29,520
packaged this into a demo

00:26:26,480 --> 00:26:29,520
you can use yourself

00:26:31,039 --> 00:26:34,720
but have we moved everything to the

00:26:33,120 --> 00:26:36,960
cloud so now we have

00:26:34,720 --> 00:26:38,960
a cloud cloud infrastructure with nomad

00:26:36,960 --> 00:26:40,480
running on top we have an auto scaler on

00:26:38,960 --> 00:26:44,159
top of that so we can

00:26:40,480 --> 00:26:47,520
survive some form of

00:26:44,159 --> 00:26:49,440
a peak in our traffic

00:26:47,520 --> 00:26:50,880
but still there's a limit there's a

00:26:49,440 --> 00:26:53,840
limit to

00:26:50,880 --> 00:26:55,840
how much instances i can run on the

00:26:53,840 --> 00:26:58,880
amount of nomad clients

00:26:55,840 --> 00:27:02,000
i've installed so far

00:26:58,880 --> 00:27:05,200
so capacity planning wise there's still

00:27:02,000 --> 00:27:07,440
i need to predict what's going to happen

00:27:05,200 --> 00:27:09,600
even i need to basically predict what

00:27:07,440 --> 00:27:11,520
the craziest peak is in the air so we're

00:27:09,600 --> 00:27:14,559
still basically in a situation

00:27:11,520 --> 00:27:18,799
where we need to come up plan

00:27:14,559 --> 00:27:21,840
ahead to what's going to happen

00:27:18,799 --> 00:27:21,840
for this

00:27:22,399 --> 00:27:28,960
we now have a plugin in the autoscaler

00:27:25,679 --> 00:27:31,760
that can manipulate

00:27:28,960 --> 00:27:32,080
in this case an aws auto scaling group

00:27:31,760 --> 00:27:35,120
so

00:27:32,080 --> 00:27:37,919
i can not only grow the amount of

00:27:35,120 --> 00:27:38,640
instances i'm running but actually i can

00:27:37,919 --> 00:27:41,760
scale

00:27:38,640 --> 00:27:42,399
the amount of hardware virtual hardware

00:27:41,760 --> 00:27:45,600
i'm running

00:27:42,399 --> 00:27:48,640
in this case i'm saying scale out my

00:27:45,600 --> 00:27:52,480
availability group within a certain

00:27:48,640 --> 00:27:56,000
uh allocation i'm giving you

00:27:52,480 --> 00:27:56,640
while within the auto allocation i've

00:27:56,000 --> 00:27:59,279
defined

00:27:56,640 --> 00:28:01,039
when we set up the auto scaling group so

00:27:59,279 --> 00:28:02,640
we can not only grow into our hardware

00:28:01,039 --> 00:28:03,279
we can grow our hardware where we can

00:28:02,640 --> 00:28:09,120
then again

00:28:03,279 --> 00:28:10,960
grow into so instead of the target

00:28:09,120 --> 00:28:12,880
that we have before we now have the aws

00:28:10,960 --> 00:28:15,600
asg auto scaling group

00:28:12,880 --> 00:28:16,000
target so how what does that look like

00:28:15,600 --> 00:28:20,320
in

00:28:16,000 --> 00:28:24,720
our our

00:28:20,320 --> 00:28:28,159
blog nomad file we now we change the

00:28:24,720 --> 00:28:28,720
the query in our policy but basically in

00:28:28,159 --> 00:28:31,039
this case

00:28:28,720 --> 00:28:31,919
it's quite long convoluted but what it

00:28:31,039 --> 00:28:35,520
boils down to

00:28:31,919 --> 00:28:36,159
is work out if my cpu load is above or

00:28:35,520 --> 00:28:39,679
below

00:28:36,159 --> 00:28:44,240
70 and if you go over

00:28:39,679 --> 00:28:45,039
start expanding my cluster which is the

00:28:44,240 --> 00:28:47,679
bit below

00:28:45,039 --> 00:28:49,440
my target and no longer is a hard number

00:28:47,679 --> 00:28:52,640
but this basically says

00:28:49,440 --> 00:28:55,600
uh based on the value that we have

00:28:52,640 --> 00:28:57,360
that coming from the the policy scaling

00:28:55,600 --> 00:28:58,000
scale out the amount of resources we

00:28:57,360 --> 00:28:59,600
have

00:28:58,000 --> 00:29:01,840
and if we look at the logging that

00:28:59,600 --> 00:29:05,039
submitted from that it's

00:29:01,840 --> 00:29:06,720
fairly similar so we have a

00:29:05,039 --> 00:29:09,679
prometheus query that comes up with an

00:29:06,720 --> 00:29:12,559
answer and in this case it says

00:29:09,679 --> 00:29:14,000
my average load is 95 so we need to add

00:29:12,559 --> 00:29:16,799
more nodes

00:29:14,000 --> 00:29:18,799
and and that's what we see in the bottom

00:29:16,799 --> 00:29:21,440
and now scalars from one to

00:29:18,799 --> 00:29:21,440
two nodes

00:29:22,320 --> 00:29:28,559
also this you can try yourself in the

00:29:25,440 --> 00:29:32,640
repo i mentioned before you have

00:29:28,559 --> 00:29:32,640
also the cloud demos

00:29:34,559 --> 00:29:40,880
and then you can do exactly what this is

00:29:37,600 --> 00:29:42,720
so now i believe we've proven that we

00:29:40,880 --> 00:29:46,960
can move to the cloud and

00:29:42,720 --> 00:29:49,360
it can't scale infinitely within

00:29:46,960 --> 00:29:50,960
within aws means or your credit cards

00:29:49,360 --> 00:29:54,080
mean

00:29:50,960 --> 00:29:56,799
thanks for listening

00:29:54,080 --> 00:29:57,600
if you want to contact me you can use

00:29:56,799 --> 00:30:01,200
email

00:29:57,600 --> 00:30:03,039
twitter and this

00:30:01,200 --> 00:30:04,640
presentation by the time the recording

00:30:03,039 --> 00:30:06,840
goes out will be uploaded

00:30:04,640 --> 00:30:23,840
into my slideshare if you want to

00:30:06,840 --> 00:30:23,840
re-visit this talk

00:30:25,120 --> 00:30:27,200

YouTube URL: https://www.youtube.com/watch?v=p9piluBK-_s


