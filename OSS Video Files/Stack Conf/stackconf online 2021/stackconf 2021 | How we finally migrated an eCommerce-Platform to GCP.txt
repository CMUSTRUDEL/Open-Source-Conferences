Title: stackconf 2021 | How we finally migrated an eCommerce-Platform to GCP
Publication date: 2021-06-24
Playlist: stackconf online 2021
Description: 
	by Paul Puschmann

As Squad Architect Platform I supported the platform-team to migrate a complete ecommerce-environment to Google Cloud Platform. By sketching out various migration-steps, technical concepts and tooling I will explain we did the migration exactly this way.


NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de/
Blog: http://blog.netways.de/
NWS: https://nws.netways.de 

Webinare
Archiv Link: https://www.netways.de/netways/webinare/

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh/


Musik: https://www.frametraxx.de/
Captions: 
	00:00:05,660 --> 00:00:13,119
[Applause]

00:00:06,120 --> 00:00:13,119
[Music]

00:00:14,920 --> 00:00:19,119
hey

00:00:16,080 --> 00:00:21,199
hello welcome to my talk with the title

00:00:19,119 --> 00:00:22,160
how we finally migrated an e-commerce

00:00:21,199 --> 00:00:25,359
platform

00:00:22,160 --> 00:00:26,240
to gcp i'm paul fushman i'm a squad

00:00:25,359 --> 00:00:29,439
architect at

00:00:26,240 --> 00:00:32,559
river digital and i'm there at

00:00:29,439 --> 00:00:33,760
digital since july 2014 and i'm taking

00:00:32,559 --> 00:00:36,480
care about the e-com

00:00:33,760 --> 00:00:36,480
infrastructure

00:00:37,040 --> 00:00:42,480
e-com so what is it e-com e-commerce

00:00:40,960 --> 00:00:44,879
environment is providing a

00:00:42,480 --> 00:00:48,320
customer-faced applications for

00:00:44,879 --> 00:00:52,079
a river river is a big food retail

00:00:48,320 --> 00:00:53,199
company in germany we are doing www.a

00:00:52,079 --> 00:00:56,480
shoprivda

00:00:53,199 --> 00:00:59,199
mobile apps and some apis all in the

00:00:56,480 --> 00:01:02,640
context of the food delivery service

00:00:59,199 --> 00:01:04,479
and pickup service as well as

00:01:02,640 --> 00:01:07,760
added services for the stationary

00:01:04,479 --> 00:01:07,760
markets we have

00:01:08,159 --> 00:01:12,720
in a previous talk in 2018 where i

00:01:11,520 --> 00:01:14,880
described the

00:01:12,720 --> 00:01:16,640
migration from a monolith to to

00:01:14,880 --> 00:01:20,000
microservice architecture

00:01:16,640 --> 00:01:23,439
i finished with a slide our future is

00:01:20,000 --> 00:01:28,880
gcp google cloud platform well let's

00:01:23,439 --> 00:01:28,880
see and find out what came out of this

00:01:29,280 --> 00:01:34,799
so back in the times we had vital

00:01:32,320 --> 00:01:37,920
machines running on some dedicated

00:01:34,799 --> 00:01:40,079
hardware and by metal machines driving

00:01:37,920 --> 00:01:42,320
this micro service infrastructure

00:01:40,079 --> 00:01:43,520
everything is automated and configured

00:01:42,320 --> 00:01:45,840
with with ansible

00:01:43,520 --> 00:01:47,280
and in general everything was under

00:01:45,840 --> 00:01:50,640
control but we

00:01:47,280 --> 00:01:53,119
needed more flexibility for scaling

00:01:50,640 --> 00:01:54,399
and you know for different configuration

00:01:53,119 --> 00:01:58,880
setups

00:01:54,399 --> 00:02:03,119
and back in 2018 or 2019 the plan was

00:01:58,880 --> 00:02:06,960
wow let's migrate to gcp which

00:02:03,119 --> 00:02:09,759
meant uh let's migrate to gke

00:02:06,960 --> 00:02:11,120
so migrate our docker-based environment

00:02:09,759 --> 00:02:13,200
to something

00:02:11,120 --> 00:02:15,280
based on kubernetes the google

00:02:13,200 --> 00:02:18,080
kubernetes environment

00:02:15,280 --> 00:02:18,879
with the idea yeah make it even more

00:02:18,080 --> 00:02:22,800
more stable

00:02:18,879 --> 00:02:26,400
scalable use some different approaches

00:02:22,800 --> 00:02:29,760
how to do things and yeah it looked

00:02:26,400 --> 00:02:30,640
very interesting and nice so we kicked

00:02:29,760 --> 00:02:34,080
off with

00:02:30,640 --> 00:02:36,400
some migration team infrastructure vpn

00:02:34,080 --> 00:02:38,959
everything was configured

00:02:36,400 --> 00:02:40,120
the first tests were made how to deploy

00:02:38,959 --> 00:02:41,760
how to

00:02:40,120 --> 00:02:43,680
intercommunicate with the old

00:02:41,760 --> 00:02:47,280
environment and

00:02:43,680 --> 00:02:49,920
then we very sadly ran into a wall

00:02:47,280 --> 00:02:51,280
and the wall was the very high

00:02:49,920 --> 00:02:53,519
communication overhead

00:02:51,280 --> 00:02:55,599
on coordination overhead with our

00:02:53,519 --> 00:02:58,720
development teams and especially

00:02:55,599 --> 00:03:02,000
the product owners who had

00:02:58,720 --> 00:03:04,879
yeah some some efforts in moving

00:03:02,000 --> 00:03:05,120
the current way of deploying services to

00:03:04,879 --> 00:03:07,520
an

00:03:05,120 --> 00:03:08,800
kubernetes style and

00:03:07,520 --> 00:03:11,120
[Music]

00:03:08,800 --> 00:03:13,120
yeah it was very difficult at this time

00:03:11,120 --> 00:03:15,200
because we couldn't do an exact

00:03:13,120 --> 00:03:16,800
estimation how long would it take to

00:03:15,200 --> 00:03:21,599
trans transform all

00:03:16,800 --> 00:03:25,200
services all of our teams to kubernetes

00:03:21,599 --> 00:03:28,159
and we wanted to keep the

00:03:25,200 --> 00:03:28,640
migration itself at this time as short

00:03:28,159 --> 00:03:32,080
as

00:03:28,640 --> 00:03:35,200
possible so not

00:03:32,080 --> 00:03:38,560
spending uh too much time on an

00:03:35,200 --> 00:03:42,159
yeah questionably be a reliable

00:03:38,560 --> 00:03:45,680
vpn tunnel for example and um

00:03:42,159 --> 00:03:49,519
it was very uncertain how this could

00:03:45,680 --> 00:03:51,680
gain traction again but we then found a

00:03:49,519 --> 00:03:53,760
different approach and the approach is

00:03:51,680 --> 00:03:54,879
what we called lift and shift although

00:03:53,760 --> 00:03:58,000
this is not

00:03:54,879 --> 00:04:01,120
the perfectly right term for this

00:03:58,000 --> 00:04:01,680
the idea was to use just the exact same

00:04:01,120 --> 00:04:04,400
tooling

00:04:01,680 --> 00:04:06,720
as with the current ecom setup so on

00:04:04,400 --> 00:04:09,920
this hosting provider

00:04:06,720 --> 00:04:10,879
with some nice additions so terraform

00:04:09,920 --> 00:04:14,319
everything at

00:04:10,879 --> 00:04:18,799
gcp so everything should be in code

00:04:14,319 --> 00:04:22,400
use just simple gce instances instead of

00:04:18,799 --> 00:04:26,320
kubernetes configure the gc instances

00:04:22,400 --> 00:04:28,240
with ansible yes all roles and playbooks

00:04:26,320 --> 00:04:32,800
were already in place

00:04:28,240 --> 00:04:32,800
and then just migrate data centers

00:04:33,840 --> 00:04:38,960
we had already had very interesting

00:04:37,199 --> 00:04:41,919
things in our stack already that

00:04:38,960 --> 00:04:43,040
supported this very nicely for example

00:04:41,919 --> 00:04:45,360
an external

00:04:43,040 --> 00:04:47,199
cortex and graffana set up so it's a

00:04:45,360 --> 00:04:49,520
scalable promising setup

00:04:47,199 --> 00:04:50,800
that is provided by us from from a

00:04:49,520 --> 00:04:53,840
different team

00:04:50,800 --> 00:04:54,720
and this is just outside of our

00:04:53,840 --> 00:04:58,400
infrastructure

00:04:54,720 --> 00:05:00,960
so we can just ship data to this

00:04:58,400 --> 00:05:01,919
cortex setup and just make use of it we

00:05:00,960 --> 00:05:05,440
don't have to take

00:05:01,919 --> 00:05:05,919
care of this on our own and this enabled

00:05:05,440 --> 00:05:08,840
us

00:05:05,919 --> 00:05:10,400
for example to just nicely compare

00:05:08,840 --> 00:05:12,639
metrics of

00:05:10,400 --> 00:05:15,039
the old environment and for example the

00:05:12,639 --> 00:05:18,160
new environment

00:05:15,039 --> 00:05:19,919
then we had an extensive setup of

00:05:18,160 --> 00:05:22,960
console so console

00:05:19,919 --> 00:05:24,479
as in service discovery tool was very

00:05:22,960 --> 00:05:26,800
deep into our infrastructure

00:05:24,479 --> 00:05:27,919
and i've shortly introduced uh how and

00:05:26,800 --> 00:05:30,960
why we do it

00:05:27,919 --> 00:05:31,440
this really helped us to to finally do

00:05:30,960 --> 00:05:35,360
this

00:05:31,440 --> 00:05:35,680
migration then also some postgres setup

00:05:35,360 --> 00:05:39,840
with

00:05:35,680 --> 00:05:39,840
pg bouncer as a connection cooler

00:05:39,919 --> 00:05:47,440
central connection pooling also is

00:05:44,080 --> 00:05:50,479
enabled us to just configure

00:05:47,440 --> 00:05:53,680
database routings from one single

00:05:50,479 --> 00:05:55,440
place very nice and last not but not

00:05:53,680 --> 00:05:58,319
least a nomad

00:05:55,440 --> 00:05:59,680
kafka and our centralized container

00:05:58,319 --> 00:06:02,960
deployment scripts

00:05:59,680 --> 00:06:05,280
where we could just easily

00:06:02,960 --> 00:06:06,960
for the deployment scripts make make

00:06:05,280 --> 00:06:09,520
changes at one time that had an

00:06:06,960 --> 00:06:13,039
effect on all of our development teams

00:06:09,520 --> 00:06:13,039
in this e-comm environment

00:06:13,600 --> 00:06:17,600
so console how does it work how do we

00:06:16,800 --> 00:06:20,880
use it

00:06:17,600 --> 00:06:25,039
so console is installed and

00:06:20,880 --> 00:06:28,160
as a server component on five nodes

00:06:25,039 --> 00:06:29,680
and all other console agents that are

00:06:28,160 --> 00:06:31,360
distributed on the other virtual

00:06:29,680 --> 00:06:34,639
machines or instances

00:06:31,360 --> 00:06:38,000
act just as clients and console is

00:06:34,639 --> 00:06:41,520
listening on the part 8500

00:06:38,000 --> 00:06:44,240
and 8600 by default

00:06:41,520 --> 00:06:45,039
the first one is doing http requests for

00:06:44,240 --> 00:06:49,280
web ui

00:06:45,039 --> 00:06:52,479
and api requests and the latter one

00:06:49,280 --> 00:06:56,560
is used for dns queries

00:06:52,479 --> 00:07:00,080
and we heavily rely on these

00:06:56,560 --> 00:07:03,840
dns features of console because

00:07:00,080 --> 00:07:03,840
we use it just a lot

00:07:03,919 --> 00:07:10,720
the subdomain dot console is

00:07:07,360 --> 00:07:13,520
delegated to this console servers so

00:07:10,720 --> 00:07:16,080
underneath this sub domain is just the

00:07:13,520 --> 00:07:21,039
console namespace

00:07:16,080 --> 00:07:21,039
to separate it from everything else

00:07:21,199 --> 00:07:26,319
as you can see in this graph or this

00:07:23,919 --> 00:07:26,319
picture

00:07:26,479 --> 00:07:33,919
every instance where you install console

00:07:30,720 --> 00:07:37,039
and given this this console is

00:07:33,919 --> 00:07:40,960
defining it itself as healthy

00:07:37,039 --> 00:07:44,720
is discoverable by

00:07:40,960 --> 00:07:44,720
its instance name plus

00:07:47,240 --> 00:07:51,919
node.console.myinternaldomain.net

00:07:48,720 --> 00:07:55,759
so you could also just address

00:07:51,919 --> 00:07:59,840
virtual machines or instances with just

00:07:55,759 --> 00:07:59,840
console usage

00:08:00,560 --> 00:08:03,599
as an additional feature and this is

00:08:03,039 --> 00:08:08,639
what we

00:08:03,599 --> 00:08:12,240
really um do do most of the times

00:08:08,639 --> 00:08:13,039
we register services inside of console

00:08:12,240 --> 00:08:17,199
for example

00:08:13,039 --> 00:08:19,599
in this case pg bouncer and for

00:08:17,199 --> 00:08:21,520
yeah pg bouncer just announces itself

00:08:19,599 --> 00:08:24,960
into console and

00:08:21,520 --> 00:08:27,039
every client that

00:08:24,960 --> 00:08:31,039
just wants to get to know okay where can

00:08:27,039 --> 00:08:34,159
i find uh pgbouncer.service.console

00:08:31,039 --> 00:08:34,959
um we'll just get as in return the ip

00:08:34,159 --> 00:08:38,159
address

00:08:34,959 --> 00:08:40,399
of this machine and uh this is the very

00:08:38,159 --> 00:08:42,320
first thing

00:08:40,399 --> 00:08:45,680
yeah to to create a connection to this

00:08:42,320 --> 00:08:50,000
pg monster how to find it

00:08:45,680 --> 00:08:53,200
and you also can do health checks and

00:08:50,000 --> 00:08:56,160
health action on the components that you

00:08:53,200 --> 00:08:56,959
advertise as in service and for example

00:08:56,160 --> 00:09:00,000
in this case

00:08:56,959 --> 00:09:03,279
we just do a simple tcp

00:09:00,000 --> 00:09:04,160
health check is this dc port a tcp port

00:09:03,279 --> 00:09:07,200
available or

00:09:04,160 --> 00:09:10,399
open check it every 10 seconds

00:09:07,200 --> 00:09:12,880
within timeout of one second and if this

00:09:10,399 --> 00:09:14,480
health check for example for example

00:09:12,880 --> 00:09:18,240
just

00:09:14,480 --> 00:09:21,200
fails then immediately

00:09:18,240 --> 00:09:22,959
this instance or this service provided

00:09:21,200 --> 00:09:26,320
on this instance a

00:09:22,959 --> 00:09:29,680
will just drop from console and is not

00:09:26,320 --> 00:09:29,680
any longer discoverable

00:09:31,600 --> 00:09:37,760
on our docker hosts we use

00:09:34,800 --> 00:09:38,480
nomad as an orchestrator so nomad

00:09:37,760 --> 00:09:40,640
ensures

00:09:38,480 --> 00:09:41,760
our docker containers are configured and

00:09:40,640 --> 00:09:44,959
started

00:09:41,760 --> 00:09:45,760
and novad also registers healthy

00:09:44,959 --> 00:09:49,200
containers

00:09:45,760 --> 00:09:50,080
in console and a container with the app

00:09:49,200 --> 00:09:52,720
name

00:09:50,080 --> 00:09:53,600
app c for example with the service name

00:09:52,720 --> 00:09:55,680
fc

00:09:53,600 --> 00:09:57,040
would get created as in service fc and

00:09:55,680 --> 00:10:00,720
console so if you

00:09:57,040 --> 00:10:03,680
doing a lookup on appc.service.console

00:10:00,720 --> 00:10:07,040
you would just get the ip address of the

00:10:03,680 --> 00:10:10,640
machine where this container is running

00:10:07,040 --> 00:10:12,560
and we have an interesting setup here

00:10:10,640 --> 00:10:14,399
together with console template and

00:10:12,560 --> 00:10:18,000
haproxy

00:10:14,399 --> 00:10:21,279
so console template is a small tool that

00:10:18,000 --> 00:10:24,640
just listens on console events

00:10:21,279 --> 00:10:28,320
and if anything changes it

00:10:24,640 --> 00:10:31,920
generates in our case a new haproxy

00:10:28,320 --> 00:10:32,399
config so if a container any container

00:10:31,920 --> 00:10:34,880
is

00:10:32,399 --> 00:10:35,760
started or stopped or getting healthy or

00:10:34,880 --> 00:10:38,480
unhealthy

00:10:35,760 --> 00:10:39,839
this is just an event that consumer

00:10:38,480 --> 00:10:43,680
template will then

00:10:39,839 --> 00:10:47,600
acknowledge every time

00:10:43,680 --> 00:10:49,920
a new hr proxy configuration is written

00:10:47,600 --> 00:10:51,680
and hr proxy of course is then also

00:10:49,920 --> 00:10:55,760
reloaded

00:10:51,680 --> 00:10:59,440
and by this we ensure all http requests

00:10:55,760 --> 00:11:02,160
fired against aj proxy are routed

00:10:59,440 --> 00:11:04,079
to an up and healthy container instance

00:11:02,160 --> 00:11:07,519
in this case for example for this

00:11:04,079 --> 00:11:09,600
app c and

00:11:07,519 --> 00:11:10,880
as you might guess we're using uh host

00:11:09,600 --> 00:11:14,320
names to to route

00:11:10,880 --> 00:11:17,440
these traffic these requests to the

00:11:14,320 --> 00:11:21,120
containers and we had a similar setup

00:11:17,440 --> 00:11:24,160
before with nginx and uh with traffic

00:11:21,120 --> 00:11:26,079
but now uh we we choose to to go to hp

00:11:24,160 --> 00:11:30,399
proxy

00:11:26,079 --> 00:11:30,399
because of some nice features

00:11:32,079 --> 00:11:35,600
here's some example with our docker

00:11:34,240 --> 00:11:37,920
hosts

00:11:35,600 --> 00:11:39,680
just to give you a short overview how

00:11:37,920 --> 00:11:43,360
this could like

00:11:39,680 --> 00:11:46,240
let's just assume the application app a

00:11:43,360 --> 00:11:46,959
in the lower left corner this is running

00:11:46,240 --> 00:11:50,079
on

00:11:46,959 --> 00:11:53,360
some docker host the docker host a

00:11:50,079 --> 00:11:55,600
with some ip address this wants to do an

00:11:53,360 --> 00:11:58,959
http call to

00:11:55,600 --> 00:12:01,360
an application called app b

00:11:58,959 --> 00:12:02,399
so of course what does the application

00:12:01,360 --> 00:12:05,519
do

00:12:02,399 --> 00:12:09,279
first an dns request is

00:12:05,519 --> 00:12:11,519
fired in this case against console

00:12:09,279 --> 00:12:12,639
and they are in the result are two

00:12:11,519 --> 00:12:16,639
records

00:12:12,639 --> 00:12:20,720
because the app b is living on

00:12:16,639 --> 00:12:23,200
uh on the left server um

00:12:20,720 --> 00:12:24,079
as well as on the right server so there

00:12:23,200 --> 00:12:27,360
are two instances

00:12:24,079 --> 00:12:28,079
running and that's the reason why we

00:12:27,360 --> 00:12:32,160
have two

00:12:28,079 --> 00:12:36,800
results in this ns lookup

00:12:32,160 --> 00:12:41,680
so the app a will then just create a

00:12:36,800 --> 00:12:44,320
request an http request and

00:12:41,680 --> 00:12:46,880
it could happen that this rpa request

00:12:44,320 --> 00:12:49,519
this http request will just

00:12:46,880 --> 00:12:51,200
get routed to the same instance like on

00:12:49,519 --> 00:12:54,000
docker host a

00:12:51,200 --> 00:12:56,399
so everything will go to through hi

00:12:54,000 --> 00:12:59,760
proxy definitely

00:12:56,399 --> 00:13:02,079
because we use a standard part 80 there

00:12:59,760 --> 00:13:03,120
or it also could happen that this

00:13:02,079 --> 00:13:06,880
request is

00:13:03,120 --> 00:13:09,760
or the next request maybe to this app b

00:13:06,880 --> 00:13:11,120
is just routed via the second docker

00:13:09,760 --> 00:13:14,720
host

00:13:11,120 --> 00:13:17,519
and here also if a

00:13:14,720 --> 00:13:17,920
container is getting unhealthy it will

00:13:17,519 --> 00:13:21,360
just

00:13:17,920 --> 00:13:24,399
drop out of the service discovery

00:13:21,360 --> 00:13:26,800
so it will just nearly immediately

00:13:24,399 --> 00:13:27,839
i guess it is immediately uh drop out of

00:13:26,800 --> 00:13:30,399
the aja proxy

00:13:27,839 --> 00:13:33,120
config as well so any configuration

00:13:30,399 --> 00:13:36,720
change we have in our environment

00:13:33,120 --> 00:13:39,519
is just very dynamic and very

00:13:36,720 --> 00:13:42,000
transparent and we like this feature

00:13:39,519 --> 00:13:42,000
very much

00:13:43,360 --> 00:13:46,720
so as an example um

00:13:47,040 --> 00:13:55,920
fc as in container

00:13:50,639 --> 00:13:55,920
is just uh request or queried as

00:13:56,519 --> 00:14:02,079
fc.service.console.mydomain

00:13:58,399 --> 00:14:04,800
as well as for example pgbounces

00:14:02,079 --> 00:14:05,360
in the same way so the pg bouncer

00:14:04,800 --> 00:14:08,560
service

00:14:05,360 --> 00:14:08,560
is also queried via

00:14:09,480 --> 00:14:14,240
pgbouncer.service.console.com

00:14:11,440 --> 00:14:15,199
and we tended in the past to use short

00:14:14,240 --> 00:14:17,120
names

00:14:15,199 --> 00:14:18,240
inside of the environment so you could

00:14:17,120 --> 00:14:21,440
just use

00:14:18,240 --> 00:14:25,120
fc to address the app c

00:14:21,440 --> 00:14:31,279
or pg bouncer to address the pg bouncer

00:14:25,120 --> 00:14:34,639
and this also worked very fine for us

00:14:31,279 --> 00:14:36,560
in terms of monitoring and observability

00:14:34,639 --> 00:14:39,680
metrics

00:14:36,560 --> 00:14:41,360
we use prometheus as i already uh told

00:14:39,680 --> 00:14:43,199
so prometheus is running somewhere in

00:14:41,360 --> 00:14:46,560
the data center

00:14:43,199 --> 00:14:50,720
and writing its metrics to this remote

00:14:46,560 --> 00:14:51,760
cortex setup and how does prometheus

00:14:50,720 --> 00:14:54,639
know

00:14:51,760 --> 00:14:56,240
where to get metrics from or where to

00:14:54,639 --> 00:14:59,680
fetch metrics

00:14:56,240 --> 00:15:03,519
well the um the containers or at least

00:14:59,680 --> 00:15:05,199
nomad nomad is regis registering

00:15:03,519 --> 00:15:07,440
the the container information into

00:15:05,199 --> 00:15:09,199
console so

00:15:07,440 --> 00:15:10,639
every time a container is brought up via

00:15:09,199 --> 00:15:14,000
nomad job

00:15:10,639 --> 00:15:18,079
it also registers information in

00:15:14,000 --> 00:15:21,440
in console and by using tags

00:15:18,079 --> 00:15:24,839
we can also

00:15:21,440 --> 00:15:26,240
direct prometheus to fetch data from the

00:15:24,839 --> 00:15:29,440
containers

00:15:26,240 --> 00:15:32,959
so the nomad

00:15:29,440 --> 00:15:36,800
will just add to its service designation

00:15:32,959 --> 00:15:40,560
attack okay prometheus

00:15:36,800 --> 00:15:44,320
colon admin metrics

00:15:40,560 --> 00:15:47,680
prometheus will um

00:15:44,320 --> 00:15:48,800
query a console for information so where

00:15:47,680 --> 00:15:50,800
can i

00:15:48,800 --> 00:15:52,639
discover some some services where i can

00:15:50,800 --> 00:15:55,519
fetch a metrics

00:15:52,639 --> 00:15:57,360
read this tags and then knows oh there's

00:15:55,519 --> 00:16:00,160
an application app c

00:15:57,360 --> 00:16:01,440
running somewhere on a docker host and i

00:16:00,160 --> 00:16:05,040
can fetch metrics

00:16:01,440 --> 00:16:08,639
uh on the endpoint admin metrics

00:16:05,040 --> 00:16:12,160
for example send this

00:16:08,639 --> 00:16:15,680
also here that applies if a container

00:16:12,160 --> 00:16:18,800
is killed or unhealthy or

00:16:15,680 --> 00:16:20,560
just disappears of other reason author

00:16:18,800 --> 00:16:23,360
promises get this information

00:16:20,560 --> 00:16:25,120
very quickly from console and just stop

00:16:23,360 --> 00:16:25,440
scraping for this endpoint because this

00:16:25,120 --> 00:16:27,759
is

00:16:25,440 --> 00:16:28,720
for example unhealthy in the same way

00:16:27,759 --> 00:16:31,040
goes around if you

00:16:28,720 --> 00:16:32,959
if just a new container is spawned up

00:16:31,040 --> 00:16:35,720
somewhere

00:16:32,959 --> 00:16:37,600
prometheus will just pick up this

00:16:35,720 --> 00:16:40,160
configuration and

00:16:37,600 --> 00:16:42,160
once this container is healthy

00:16:40,160 --> 00:16:42,800
immediately start fetching metrics

00:16:42,160 --> 00:16:45,360
without

00:16:42,800 --> 00:16:46,240
any other static configuration in a

00:16:45,360 --> 00:16:49,519
central place

00:16:46,240 --> 00:16:52,079
this information about which

00:16:49,519 --> 00:16:52,720
path to scrape is just inside of the

00:16:52,079 --> 00:16:56,880
nomad

00:16:52,720 --> 00:16:56,880
job definition this is very cool

00:16:57,680 --> 00:17:01,680
for example this 5-bit exporter living

00:16:59,920 --> 00:17:05,120
here on some

00:17:01,680 --> 00:17:07,520
instance somewhere in our infrastructure

00:17:05,120 --> 00:17:08,959
this 5-bit exporter is just running

00:17:07,520 --> 00:17:11,760
there to export

00:17:08,959 --> 00:17:13,120
metrics from firebeat to be scraped by

00:17:11,760 --> 00:17:15,679
prometheus

00:17:13,120 --> 00:17:17,039
and how do we announce that there is

00:17:15,679 --> 00:17:20,000
some fiber export on

00:17:17,039 --> 00:17:21,039
a machine we just register it as a

00:17:20,000 --> 00:17:23,839
service

00:17:21,039 --> 00:17:25,120
so on this a local machine is console

00:17:23,839 --> 00:17:28,319
running and console

00:17:25,120 --> 00:17:30,320
just gets some additional yaml object

00:17:28,319 --> 00:17:33,600
with a service designation of

00:17:30,320 --> 00:17:36,880
fiber exporter and this 5-bit exporter

00:17:33,600 --> 00:17:38,080
setting or service also has some health

00:17:36,880 --> 00:17:41,679
check inside

00:17:38,080 --> 00:17:44,559
and also contains tax tags

00:17:41,679 --> 00:17:45,679
with the information for prometheus hey

00:17:44,559 --> 00:17:49,120
please

00:17:45,679 --> 00:17:51,120
fetch my metrics i'm here please go to

00:17:49,120 --> 00:17:53,280
my endpoint slash metrics i have

00:17:51,120 --> 00:17:56,400
wonderful metrics for you

00:17:53,280 --> 00:18:00,400
and this is also very

00:17:56,400 --> 00:18:01,520
flexible and dynamic so we also in this

00:18:00,400 --> 00:18:04,000
case we don't have to

00:18:01,520 --> 00:18:05,280
keep an essential registry where to

00:18:04,000 --> 00:18:08,559
fetch which metric

00:18:05,280 --> 00:18:11,200
is just dynamically fetched via this

00:18:08,559 --> 00:18:12,840
service discovery and console together

00:18:11,200 --> 00:18:14,960
with

00:18:12,840 --> 00:18:17,280
prometheus

00:18:14,960 --> 00:18:19,679
so coming now a bit further into our

00:18:17,280 --> 00:18:22,559
main topic well it's a migration

00:18:19,679 --> 00:18:23,600
and for the migration we had set up a

00:18:22,559 --> 00:18:26,960
second

00:18:23,600 --> 00:18:29,840
logical data center in in console

00:18:26,960 --> 00:18:29,840
and

00:18:30,240 --> 00:18:35,919
i mentioned before that inside of a data

00:18:34,640 --> 00:18:40,480
center we can do

00:18:35,919 --> 00:18:40,480
lookups with something like

00:18:41,280 --> 00:18:44,320
dot servicename.service.cons and this is

00:18:43,840 --> 00:18:47,360
very

00:18:44,320 --> 00:18:50,960
very fine very cool but unfortunately

00:18:47,360 --> 00:18:53,039
this is only limited to one source data

00:18:50,960 --> 00:18:56,000
so to one data center

00:18:53,039 --> 00:18:57,120
so you can use this service a subdomain

00:18:56,000 --> 00:18:59,120
to query everything

00:18:57,120 --> 00:19:01,440
inside of one data center like for

00:18:59,120 --> 00:19:03,039
example everything that is just in the

00:19:01,440 --> 00:19:05,440
blue data center

00:19:03,039 --> 00:19:06,480
this is separated if you do the same

00:19:05,440 --> 00:19:08,720
query in the

00:19:06,480 --> 00:19:10,400
green data center so green data center

00:19:08,720 --> 00:19:10,880
will just give you only the results of

00:19:10,400 --> 00:19:12,400
the

00:19:10,880 --> 00:19:15,120
what is running inside of the screen

00:19:12,400 --> 00:19:15,120
data center

00:19:16,080 --> 00:19:22,640
but we found a solution to this and

00:19:19,120 --> 00:19:25,360
the solution to this is console's

00:19:22,640 --> 00:19:28,400
feature of prepared queries

00:19:25,360 --> 00:19:29,360
so these prepared queries help us to do

00:19:28,400 --> 00:19:32,000
lookups

00:19:29,360 --> 00:19:32,640
across data center boundaries without

00:19:32,000 --> 00:19:36,840
some

00:19:32,640 --> 00:19:39,520
reconfiguration of services or other

00:19:36,840 --> 00:19:42,880
infrastructure

00:19:39,520 --> 00:19:46,320
so the this

00:19:42,880 --> 00:19:49,679
prepared query will search do i

00:19:46,320 --> 00:19:51,360
find a service inside my data center in

00:19:49,679 --> 00:19:54,160
my local data center

00:19:51,360 --> 00:19:55,520
if not it will just extend the query to

00:19:54,160 --> 00:19:58,559
the other data centers

00:19:55,520 --> 00:19:59,520
that are registered in your console and

00:19:58,559 --> 00:20:03,360
then return

00:19:59,520 --> 00:20:03,360
the matching results and

00:20:03,600 --> 00:20:08,159
this is very cool because there's also a

00:20:06,720 --> 00:20:08,480
different feature in console where we

00:20:08,159 --> 00:20:13,280
could

00:20:08,480 --> 00:20:18,640
just query um distinct data centers

00:20:13,280 --> 00:20:18,640
so like in the above mentioned urls like

00:20:18,840 --> 00:20:24,320
servicename.service.datacenter2

00:20:21,120 --> 00:20:25,360
yes you can do this it also works very

00:20:24,320 --> 00:20:28,159
fine

00:20:25,360 --> 00:20:28,799
but in our case we didn't want to do it

00:20:28,159 --> 00:20:30,720
because

00:20:28,799 --> 00:20:32,799
we didn't want to touch the

00:20:30,720 --> 00:20:35,840
configuration of our services now

00:20:32,799 --> 00:20:35,840
of our infrastructure

00:20:36,320 --> 00:20:41,360
in this behalf so this would also mean

00:20:40,000 --> 00:20:44,159
that we

00:20:41,360 --> 00:20:47,520
yeah point the configuration directly to

00:20:44,159 --> 00:20:50,640
the different data center and this

00:20:47,520 --> 00:20:53,840
was just not a good idea for our

00:20:50,640 --> 00:20:55,919
um for our migration

00:20:53,840 --> 00:20:59,520
so how to move how to move from this

00:20:55,919 --> 00:21:02,320
service dot console to query dot console

00:20:59,520 --> 00:21:03,039
well i explained already using short

00:21:02,320 --> 00:21:06,559
names

00:21:03,039 --> 00:21:09,520
was very cool idea because we then could

00:21:06,559 --> 00:21:11,919
only just do some changes to the resolve

00:21:09,520 --> 00:21:15,120
conf on our machines

00:21:11,919 --> 00:21:16,240
so on the search domain and this was

00:21:15,120 --> 00:21:19,360
settled

00:21:16,240 --> 00:21:20,799
so every request doing a short name

00:21:19,360 --> 00:21:24,480
would then just use

00:21:20,799 --> 00:21:26,880
this wonderful prepared query

00:21:24,480 --> 00:21:28,960
and yet there were some some cases in

00:21:26,880 --> 00:21:32,080
our infrastructure where we had

00:21:28,960 --> 00:21:35,120
the use of fqdns so this

00:21:32,080 --> 00:21:36,159
whole long domain name already explained

00:21:35,120 --> 00:21:39,919
to you

00:21:36,159 --> 00:21:43,120
um yes we had to change these and

00:21:39,919 --> 00:21:46,159
this this was not very problematic

00:21:43,120 --> 00:21:47,120
we could not really do it with easy

00:21:46,159 --> 00:21:49,840
search and replace

00:21:47,120 --> 00:21:50,400
there was some more effort but it was

00:21:49,840 --> 00:21:53,120
doable

00:21:50,400 --> 00:21:54,640
and it was just completely unblocking us

00:21:53,120 --> 00:21:57,360
from the upcoming

00:21:54,640 --> 00:21:58,320
migration so and with the prepared

00:21:57,360 --> 00:22:02,000
queries

00:21:58,320 --> 00:22:03,360
in place we could just move around

00:22:02,000 --> 00:22:06,640
applications and

00:22:03,360 --> 00:22:09,840
other services between data centers

00:22:06,640 --> 00:22:12,720
and still have a very dynamic

00:22:09,840 --> 00:22:14,559
and intact communication between them

00:22:12,720 --> 00:22:18,640
look ups were just working

00:22:14,559 --> 00:22:20,880
fine and it didn't matter

00:22:18,640 --> 00:22:22,799
where the application was located if it

00:22:20,880 --> 00:22:25,600
was in the data center

00:22:22,799 --> 00:22:28,720
on the old hosting provider or it was on

00:22:25,600 --> 00:22:28,720
the google provider

00:22:28,799 --> 00:22:37,440
and by this we or with this tool we also

00:22:32,559 --> 00:22:40,640
managed just easily to migrate around

00:22:37,440 --> 00:22:42,240
our databases for example so on on the

00:22:40,640 --> 00:22:43,679
source data center on the left side you

00:22:42,240 --> 00:22:46,720
can see two applications

00:22:43,679 --> 00:22:50,559
they are running fa and a b and

00:22:46,720 --> 00:22:53,600
they are configured to use databases

00:22:50,559 --> 00:22:54,080
on the url pg bouncer so what do they do

00:22:53,600 --> 00:22:56,400
yeah they

00:22:54,080 --> 00:22:59,520
ask for pg bouncer get as in return

00:22:56,400 --> 00:23:02,720
value the local pg bouncer

00:22:59,520 --> 00:23:06,400
instance which then forwards the request

00:23:02,720 --> 00:23:09,440
to the postgresql primary server

00:23:06,400 --> 00:23:12,640
and if for example fc was

00:23:09,440 --> 00:23:13,919
just already migrated to the google data

00:23:12,640 --> 00:23:17,360
center or to the

00:23:13,919 --> 00:23:20,080
different data center um there was also

00:23:17,360 --> 00:23:22,720
already some pg bounce and since running

00:23:20,080 --> 00:23:23,200
the fc would also just connect against

00:23:22,720 --> 00:23:25,919
the

00:23:23,200 --> 00:23:26,960
pg bouncer as a shortening would get

00:23:25,919 --> 00:23:29,840
resolved well

00:23:26,960 --> 00:23:30,960
it's a local pg bouncer so in the google

00:23:29,840 --> 00:23:34,080
data center

00:23:30,960 --> 00:23:37,280
and only the pg bouncer knows ah

00:23:34,080 --> 00:23:39,600
my primary postgresql instance

00:23:37,280 --> 00:23:40,480
this is running on the old data center

00:23:39,600 --> 00:23:44,080
and routes the

00:23:40,480 --> 00:23:48,080
requests to the correct primary server

00:23:44,080 --> 00:23:49,360
and the replication of the movement of

00:23:48,080 --> 00:23:52,559
databases

00:23:49,360 --> 00:23:55,600
was just um somewhere didn't

00:23:52,559 --> 00:23:58,640
somewhere behind the scenes so it was

00:23:55,600 --> 00:24:01,919
just somewhere hidden between pg bouncer

00:23:58,640 --> 00:24:04,000
the apps still connected just to their

00:24:01,919 --> 00:24:05,279
local preview bouncer instance and

00:24:04,000 --> 00:24:08,799
everything else was just

00:24:05,279 --> 00:24:12,400
abstracted behind and the applications

00:24:08,799 --> 00:24:12,400
didn't have to take care about it

00:24:13,039 --> 00:24:19,840
and in general also this complete

00:24:16,480 --> 00:24:21,039
migration of our nomad jobs so our

00:24:19,840 --> 00:24:23,039
docker containers

00:24:21,039 --> 00:24:24,720
from the old data center to the new data

00:24:23,039 --> 00:24:28,080
center was

00:24:24,720 --> 00:24:31,120
yeah just very quickly done with some

00:24:28,080 --> 00:24:35,120
scripting on the nomad you api

00:24:31,120 --> 00:24:36,960
so we could just move our nomad jobs

00:24:35,120 --> 00:24:37,919
from one data center to the other data

00:24:36,960 --> 00:24:40,400
center

00:24:37,919 --> 00:24:41,200
with the exact configuration with the

00:24:40,400 --> 00:24:44,000
exact

00:24:41,200 --> 00:24:45,679
urls in the configuration because we

00:24:44,000 --> 00:24:48,320
ensured with console

00:24:45,679 --> 00:24:49,360
that everything was just discoverable

00:24:48,320 --> 00:24:52,080
and just

00:24:49,360 --> 00:24:52,080
working fine

00:24:55,440 --> 00:25:01,039
for the topic of kafka

00:24:58,559 --> 00:25:03,360
what we use for our event sourcing

00:25:01,039 --> 00:25:06,640
inside of our

00:25:03,360 --> 00:25:08,960
e-commerce environment this is not

00:25:06,640 --> 00:25:12,400
directly uh console related

00:25:08,960 --> 00:25:16,320
um but the migration is

00:25:12,400 --> 00:25:19,440
maybe a bit interesting at all

00:25:16,320 --> 00:25:20,400
so for for the kafka cluster we didn't

00:25:19,440 --> 00:25:23,440
set up

00:25:20,400 --> 00:25:26,640
a completely new and fresh kafka cluster

00:25:23,440 --> 00:25:27,679
at uh at the google data center but here

00:25:26,640 --> 00:25:29,919
we choose

00:25:27,679 --> 00:25:32,080
just to extend the kafka cluster with

00:25:29,919 --> 00:25:34,880
new brokers

00:25:32,080 --> 00:25:37,440
so that we had one big cluster spanning

00:25:34,880 --> 00:25:40,960
over both data centers

00:25:37,440 --> 00:25:42,559
and when you just add new brokers to

00:25:40,960 --> 00:25:45,679
your kafka cluster

00:25:42,559 --> 00:25:46,720
what does happen well nothing if you

00:25:45,679 --> 00:25:48,960
don't do any

00:25:46,720 --> 00:25:50,240
action on your topics like creating

00:25:48,960 --> 00:25:53,440
topics

00:25:50,240 --> 00:25:56,880
anything else will just continue working

00:25:53,440 --> 00:26:00,320
but you won't have any data on your

00:25:56,880 --> 00:26:03,919
freshly introduced kafka brokers in

00:26:00,320 --> 00:26:07,360
in the new data center because kafka

00:26:03,919 --> 00:26:08,880
wants you to move the data on your own

00:26:07,360 --> 00:26:11,679
so you have to do it with

00:26:08,880 --> 00:26:12,640
some script or some some command it is

00:26:11,679 --> 00:26:14,640
not behaving like

00:26:12,640 --> 00:26:16,080
some elastic search where you just add

00:26:14,640 --> 00:26:19,520
an instance and

00:26:16,080 --> 00:26:21,279
chants are moved around to optimize

00:26:19,520 --> 00:26:24,080
the distribution of shots it's

00:26:21,279 --> 00:26:27,200
completely different

00:26:24,080 --> 00:26:29,840
but but this was also a very fine for us

00:26:27,200 --> 00:26:32,080
so we could just provision new kafka

00:26:29,840 --> 00:26:35,360
brokers at google

00:26:32,080 --> 00:26:38,799
and then decide on our own when to

00:26:35,360 --> 00:26:39,440
move over or reallocate the partitions

00:26:38,799 --> 00:26:42,400
and all the

00:26:39,440 --> 00:26:44,080
the topics from the old data center to

00:26:42,400 --> 00:26:47,679
the new data center

00:26:44,080 --> 00:26:51,039
and since we knew what

00:26:47,679 --> 00:26:53,760
topics were on the one hand important so

00:26:51,039 --> 00:26:54,480
under very heavy huge usage by clients

00:26:53,760 --> 00:26:57,360
and you

00:26:54,480 --> 00:26:58,159
knew of course the size of our kafka

00:26:57,360 --> 00:27:00,799
topics

00:26:58,159 --> 00:27:02,159
we could make up make up a plan which

00:27:00,799 --> 00:27:05,120
topics to move first

00:27:02,159 --> 00:27:07,200
and which for example the very big ones

00:27:05,120 --> 00:27:10,320
move them later

00:27:07,200 --> 00:27:13,120
and after the finished rear location

00:27:10,320 --> 00:27:14,159
that looks then like this we could just

00:27:13,120 --> 00:27:17,360
safely

00:27:14,159 --> 00:27:19,679
shut down the kafka brokers and

00:27:17,360 --> 00:27:20,480
in the old environment and just have

00:27:19,679 --> 00:27:23,200
everything

00:27:20,480 --> 00:27:25,600
migrated to the google data center for

00:27:23,200 --> 00:27:25,600
example

00:27:26,559 --> 00:27:32,480
coming now to the migrations so how did

00:27:29,520 --> 00:27:35,120
our migrations actually work

00:27:32,480 --> 00:27:36,240
well exactly as i already explained for

00:27:35,120 --> 00:27:38,640
the various

00:27:36,240 --> 00:27:40,320
infrastructure components so nomad

00:27:38,640 --> 00:27:43,520
containers

00:27:40,320 --> 00:27:46,159
just move them back and forth databases

00:27:43,520 --> 00:27:47,919
we did the same so we migrated stuff and

00:27:46,159 --> 00:27:48,799
didn't tell anybody for our test

00:27:47,919 --> 00:27:52,159
environment

00:27:48,799 --> 00:27:54,000
because there was no high risk on the

00:27:52,159 --> 00:27:56,720
test environment

00:27:54,000 --> 00:27:57,840
and we wanted and had to play around

00:27:56,720 --> 00:28:00,480
with it and learn

00:27:57,840 --> 00:28:02,399
things how does it behave and it was

00:28:00,480 --> 00:28:03,279
very cool to see that all of our

00:28:02,399 --> 00:28:06,320
assumptions

00:28:03,279 --> 00:28:08,880
um were fulfilled so

00:28:06,320 --> 00:28:09,760
it was very cool to see that things just

00:28:08,880 --> 00:28:13,200
work

00:28:09,760 --> 00:28:13,840
we also take a look into the metrics and

00:28:13,200 --> 00:28:16,960
everything

00:28:13,840 --> 00:28:17,840
uh how is it running how about latencies

00:28:16,960 --> 00:28:20,880
between

00:28:17,840 --> 00:28:22,640
the data centers and the vpn tunnels how

00:28:20,880 --> 00:28:25,360
does it work and it turned out

00:28:22,640 --> 00:28:27,760
yeah it was uh pretty fine uh so there

00:28:25,360 --> 00:28:30,960
wasn't a big latency added

00:28:27,760 --> 00:28:33,919
at all so test environment

00:28:30,960 --> 00:28:35,600
yeah was was very nice and when we did

00:28:33,919 --> 00:28:38,960
it over a longer time frame so

00:28:35,600 --> 00:28:41,440
it was not just done in two hours or a

00:28:38,960 --> 00:28:44,080
day or whatever

00:28:41,440 --> 00:28:44,480
for the pre and for the prod migration

00:28:44,080 --> 00:28:46,880
we

00:28:44,480 --> 00:28:48,960
are decided to do it differently so we

00:28:46,880 --> 00:28:51,840
had the test environment to learn

00:28:48,960 --> 00:28:53,360
and for the pre and protected

00:28:51,840 --> 00:28:54,080
environments we wanted to take it

00:28:53,360 --> 00:28:56,840
literally

00:28:54,080 --> 00:28:58,159
so pre-environment is our pre-production

00:28:56,840 --> 00:29:01,600
environment

00:28:58,159 --> 00:29:04,960
and to ensure that we have

00:29:01,600 --> 00:29:08,080
no configuration drift we had set up

00:29:04,960 --> 00:29:10,640
the free hardware and the prod hardware

00:29:08,080 --> 00:29:13,440
so the virtual hardware of course

00:29:10,640 --> 00:29:14,080
at the same time to just do it exactly

00:29:13,440 --> 00:29:17,440
in the same

00:29:14,080 --> 00:29:18,960
way in this exact order not to fall into

00:29:17,440 --> 00:29:22,480
some some pitfalls

00:29:18,960 --> 00:29:25,520
and the migration of our

00:29:22,480 --> 00:29:27,600
applications databases and whatever in

00:29:25,520 --> 00:29:30,880
the pre-environment environment

00:29:27,600 --> 00:29:31,120
we did it in a manner as we wanted to do

00:29:30,880 --> 00:29:32,720
it

00:29:31,120 --> 00:29:34,640
also in the product environment so we

00:29:32,720 --> 00:29:37,760
wanted to do it

00:29:34,640 --> 00:29:41,600
all yeah not at once but all in

00:29:37,760 --> 00:29:44,720
in one go so we spent just one day

00:29:41,600 --> 00:29:47,760
to follow through our plan how to

00:29:44,720 --> 00:29:50,880
migrate and when to migrate what and

00:29:47,760 --> 00:29:54,240
we just did it without any downtime for

00:29:50,880 --> 00:29:57,120
our prepared environment because

00:29:54,240 --> 00:29:58,799
all the technology we used there was

00:29:57,120 --> 00:29:59,520
just made for this all the service

00:29:58,799 --> 00:30:04,080
discovery

00:29:59,520 --> 00:30:04,080
is just exactly made for this

00:30:04,399 --> 00:30:08,480
so if you do a test environment and

00:30:06,240 --> 00:30:11,279
doing pre-environment

00:30:08,480 --> 00:30:11,679
then you some time have to do prod as

00:30:11,279 --> 00:30:14,880
well

00:30:11,679 --> 00:30:17,600
and well what was stopping us

00:30:14,880 --> 00:30:19,120
so we had very good tests we had double

00:30:17,600 --> 00:30:22,320
checked everything

00:30:19,120 --> 00:30:25,120
the infrastructure was

00:30:22,320 --> 00:30:26,720
besides of some sizing of gce instances

00:30:25,120 --> 00:30:29,760
for example

00:30:26,720 --> 00:30:32,240
completely identical and

00:30:29,760 --> 00:30:32,960
yeah it turned out with all this

00:30:32,240 --> 00:30:36,480
information

00:30:32,960 --> 00:30:38,720
we could just get some time slot

00:30:36,480 --> 00:30:41,279
and some some date where to do the

00:30:38,720 --> 00:30:41,279
migration

00:30:41,440 --> 00:30:45,760
and then came the day of our prod

00:30:44,080 --> 00:30:48,960
migration

00:30:45,760 --> 00:30:52,240
so what did we do yeah we checked

00:30:48,960 --> 00:30:55,039
everything stopped the external traffic

00:30:52,240 --> 00:30:57,840
failover to then according to our run

00:30:55,039 --> 00:31:00,080
book or our plan

00:30:57,840 --> 00:31:02,320
fell over the postgres machines created

00:31:00,080 --> 00:31:04,799
new replica at google

00:31:02,320 --> 00:31:05,840
started to migrate the nomad jobs so the

00:31:04,799 --> 00:31:07,840
docker containers

00:31:05,840 --> 00:31:09,760
from a to b so from the old hosting

00:31:07,840 --> 00:31:13,200
provider to the google provider

00:31:09,760 --> 00:31:16,240
or google data center migrated

00:31:13,200 --> 00:31:19,919
our solar radis elastic search

00:31:16,240 --> 00:31:22,080
instances over to google reconfigured

00:31:19,919 --> 00:31:24,799
the external dns

00:31:22,080 --> 00:31:25,600
and started the relocation of our kafka

00:31:24,799 --> 00:31:28,799
topics

00:31:25,600 --> 00:31:32,799
in two batches so as i explained before

00:31:28,799 --> 00:31:35,919
we knew what to do first so we did it

00:31:32,799 --> 00:31:37,360
yeah like in pareto split so all smaller

00:31:35,919 --> 00:31:38,799
topics first that were pretty

00:31:37,360 --> 00:31:40,880
interesting and the

00:31:38,799 --> 00:31:43,039
bigger topics just to him at the very

00:31:40,880 --> 00:31:46,080
last

00:31:43,039 --> 00:31:50,000
because we can just wait for it

00:31:46,080 --> 00:31:51,600
and after most of the kafka topics for

00:31:50,000 --> 00:31:55,200
example were done

00:31:51,600 --> 00:31:57,360
we did some testing so

00:31:55,200 --> 00:31:59,200
testing without customers is everything

00:31:57,360 --> 00:32:02,480
possible can we do a

00:31:59,200 --> 00:32:04,159
out can we whatever functional testing

00:32:02,480 --> 00:32:06,559
taking a look at the metrics taking a

00:32:04,159 --> 00:32:08,960
look at the logs

00:32:06,559 --> 00:32:10,480
and then switched on the external

00:32:08,960 --> 00:32:12,880
traffic again

00:32:10,480 --> 00:32:14,240
have happily welcome dear customers you

00:32:12,880 --> 00:32:17,440
can shop again

00:32:14,240 --> 00:32:19,519
and we were done after 6 hours and 25

00:32:17,440 --> 00:32:23,039
minutes

00:32:19,519 --> 00:32:24,799
to be honest kafka took just 50 hours

00:32:23,039 --> 00:32:26,960
and 50 minutes

00:32:24,799 --> 00:32:28,799
to complete with the complete

00:32:26,960 --> 00:32:30,960
reassignment of topics

00:32:28,799 --> 00:32:32,000
but the main part of this whole

00:32:30,960 --> 00:32:35,279
migration

00:32:32,000 --> 00:32:36,559
was just done inside of six hours and 25

00:32:35,279 --> 00:32:39,760
minutes

00:32:36,559 --> 00:32:43,279
and the wonder full part of this is

00:32:39,760 --> 00:32:45,200
that we just did it without any

00:32:43,279 --> 00:32:46,399
external interaction so only the

00:32:45,200 --> 00:32:48,640
platform team

00:32:46,399 --> 00:32:50,440
just did this migration and we just

00:32:48,640 --> 00:32:53,440
moved everything to

00:32:50,440 --> 00:32:56,720
119 new gc instances

00:32:53,440 --> 00:32:59,519
200 micro services were moved

00:32:56,720 --> 00:33:00,080
138 databases migrated and shipped

00:32:59,519 --> 00:33:03,039
around

00:33:00,080 --> 00:33:03,519
five terabyte of production data so this

00:33:03,039 --> 00:33:06,399
was

00:33:03,519 --> 00:33:08,320
very very cool to achieve this in this

00:33:06,399 --> 00:33:10,720
short time period

00:33:08,320 --> 00:33:11,840
and this was the biggest single

00:33:10,720 --> 00:33:15,200
application

00:33:11,840 --> 00:33:15,840
migration inside river ever and this was

00:33:15,200 --> 00:33:18,399
just done

00:33:15,840 --> 00:33:19,200
in six hours so when i call it

00:33:18,399 --> 00:33:23,120
application

00:33:19,200 --> 00:33:25,600
well it's this ecom product this ecom

00:33:23,120 --> 00:33:26,799
environment you could also call it an

00:33:25,600 --> 00:33:30,159
application if you like

00:33:26,799 --> 00:33:33,760
for externals so just

00:33:30,159 --> 00:33:36,080
completed this one small platform team

00:33:33,760 --> 00:33:37,840
just migrated a complete prod

00:33:36,080 --> 00:33:40,880
environment

00:33:37,840 --> 00:33:44,000
just single-handedly without

00:33:40,880 --> 00:33:46,399
any issues from a hosting provider to

00:33:44,000 --> 00:33:46,399
google

00:33:46,480 --> 00:33:50,399
yes we switched off external traffic but

00:33:48,799 --> 00:33:53,519
only to maximize

00:33:50,399 --> 00:33:55,039
the data consistency of our data we

00:33:53,519 --> 00:33:58,000
didn't want to deal with

00:33:55,039 --> 00:33:59,679
any split brain situations at all we

00:33:58,000 --> 00:34:02,159
wanted to be 100 percent

00:33:59,679 --> 00:34:03,519
sure on this and this was definitely

00:34:02,159 --> 00:34:07,039
something that we could

00:34:03,519 --> 00:34:08,480
take and now we have everything in code

00:34:07,039 --> 00:34:11,280
so every firewall rule

00:34:08,480 --> 00:34:12,079
every uh instance configuration what not

00:34:11,280 --> 00:34:15,440
is every

00:34:12,079 --> 00:34:20,639
where start in tagger form and

00:34:15,440 --> 00:34:23,440
it is really nice to see this

00:34:20,639 --> 00:34:24,320
so all this was heavily powered by

00:34:23,440 --> 00:34:27,200
hashicorp

00:34:24,320 --> 00:34:27,679
console the console was just the key

00:34:27,200 --> 00:34:29,679
thing

00:34:27,679 --> 00:34:30,800
in in this migration with with this

00:34:29,679 --> 00:34:33,040
dynamic setup

00:34:30,800 --> 00:34:34,159
and dynamic configuration this really

00:34:33,040 --> 00:34:37,119
enabled us

00:34:34,159 --> 00:34:37,760
to migrate uh this environment and to do

00:34:37,119 --> 00:34:40,960
this

00:34:37,760 --> 00:34:44,720
stunt and yeah

00:34:40,960 --> 00:34:47,119
a stunt exactly like this it only took

00:34:44,720 --> 00:34:48,560
a bit longer like the stable class it

00:34:47,119 --> 00:34:52,320
was six hours but

00:34:48,560 --> 00:34:54,800
hey it was uh really comparable

00:34:52,320 --> 00:34:55,679
and we are really proud to have done it

00:34:54,800 --> 00:34:58,000
this way

00:34:55,679 --> 00:35:00,160
without any further external interaction

00:34:58,000 --> 00:35:02,720
i i really love this and

00:35:00,160 --> 00:35:03,839
uh thank you very much to this platform

00:35:02,720 --> 00:35:07,680
team who achieved

00:35:03,839 --> 00:35:10,160
this great task thank you very much

00:35:07,680 --> 00:35:11,359
and now please i'm open for your

00:35:10,160 --> 00:35:13,920
questions

00:35:11,359 --> 00:35:16,400
uh tell me what do you think of this do

00:35:13,920 --> 00:35:18,720
you already use console would you like

00:35:16,400 --> 00:35:21,839
to use console

00:35:18,720 --> 00:35:24,880
need any information just ask me

00:35:21,839 --> 00:35:31,839
i'm here for you in the chat thank you

00:35:24,880 --> 00:35:31,839
and bye

00:35:32,580 --> 00:35:37,769
[Music]

00:35:41,839 --> 00:35:43,920

YouTube URL: https://www.youtube.com/watch?v=JqxohEuxl2E


