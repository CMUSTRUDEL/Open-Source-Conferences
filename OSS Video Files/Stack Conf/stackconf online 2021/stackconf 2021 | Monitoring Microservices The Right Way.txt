Title: stackconf 2021 | Monitoring Microservices The Right Way
Publication date: 2021-06-24
Playlist: stackconf online 2021
Description: 
	by Dotan Horovits

Modern systems today are far more complex to monitor. Microservices combined with containerized deployment results in highly dynamic systems with many moving parts across multiple layers. These systems emit massive amounts of highly dimensional telemetry data from hardware and the operating system, through Docker and Kubernetes, all the way to application and its databases, web proxies and other frameworks. Many have come to realize that the commonly prescribed Graphite+StatsD monitoring stack is no longer sufficient to cover their backs. New requirements need to be considered when choosing a monitoring solution for the job, including scalability, query flexibility and metrics collection. In this talk Horovits will look at the characteristics of modern systems and what to look for in a good monitoring system. He will also discuss the common open source tools, from the days of Graphite and StatsD to the currently dominant Prometheus. This talk will put you on the right track for choosing the right monitoring solution for your needs.


NETWAYS
Konferenzen: https://www.netways.de/events
Schulungen: https://www.netways.de/schulungen
Shop: https://shop.netways.de/
Blog: http://blog.netways.de/
NWS: https://nws.netways.de 

Webinare
Archiv Link: https://www.netways.de/netways/webinare/

Social Media
SlideShare: http://de.slideshare.net/netways
YouTube: https://www.netways.de/youtube
Facebook: https://www.facebook.com/netways
Twitter: https://twitter.com/netways
Instagram: https://www.instagram.com/netwaysgmbh/


Musik: https://www.frametraxx.de/
Captions: 
	00:00:05,660 --> 00:00:13,119
[Applause]

00:00:06,120 --> 00:00:13,119
[Music]

00:00:14,480 --> 00:00:18,320
hello everyone

00:00:15,679 --> 00:00:19,600
glad to be here at stacconf 2021 and

00:00:18,320 --> 00:00:23,600
today i'd like to talk to you about

00:00:19,600 --> 00:00:25,439
monitoring microservices the right way

00:00:23,600 --> 00:00:27,359
a word about myself my name is dutan

00:00:25,439 --> 00:00:28,480
horvitz i'm a developer advocate at

00:00:27,359 --> 00:00:30,320
logs.io

00:00:28,480 --> 00:00:33,040
i've been around as a developer as a

00:00:30,320 --> 00:00:36,000
solutions architect a product manager

00:00:33,040 --> 00:00:38,079
i'm an advocate of open source software

00:00:36,000 --> 00:00:40,960
open standards open communities

00:00:38,079 --> 00:00:43,360
in general and the cncf cloud native

00:00:40,960 --> 00:00:45,680
computing foundation in particular

00:00:43,360 --> 00:00:47,520
i co-organized a local cncf chapter in

00:00:45,680 --> 00:00:49,200
tel aviv cloud native tel aviv so if

00:00:47,520 --> 00:00:52,160
you're around do uh join one of our

00:00:49,200 --> 00:00:53,760
monthly meetups uh i also run a podcast

00:00:52,160 --> 00:00:56,160
open observability talks

00:00:53,760 --> 00:00:58,320
about open source and devops and

00:00:56,160 --> 00:01:01,280
observability so if your podcast fans

00:00:58,320 --> 00:01:02,800
do check it out in your favorite app and

00:01:01,280 --> 00:01:05,040
in general you can find me anywhere at

00:01:02,800 --> 00:01:08,320
horvitz

00:01:05,040 --> 00:01:10,159
my company logs.io provides a sas

00:01:08,320 --> 00:01:10,720
platform for cloud native observability

00:01:10,159 --> 00:01:12,320
which

00:01:10,720 --> 00:01:14,240
essentially means that you can just send

00:01:12,320 --> 00:01:14,799
your logs your metrics your traces to

00:01:14,240 --> 00:01:18,080
one

00:01:14,799 --> 00:01:20,320
centralized managed place for

00:01:18,080 --> 00:01:21,600
indexing and storage and visualization

00:01:20,320 --> 00:01:24,000
and the beauty is that it's

00:01:21,600 --> 00:01:25,119
all based on a popular open source that

00:01:24,000 --> 00:01:27,680
you probably

00:01:25,119 --> 00:01:28,799
use such as elasticsearch cabana

00:01:27,680 --> 00:01:31,119
prometheus

00:01:28,799 --> 00:01:33,360
jaeger and so on if you're interested

00:01:31,119 --> 00:01:35,360
check out the url here at the bottom and

00:01:33,360 --> 00:01:37,520
especially for the participants of

00:01:35,360 --> 00:01:40,799
stackon for 2021

00:01:37,520 --> 00:01:44,479
worth your while with that

00:01:40,799 --> 00:01:48,000
let's uh check out how to monitor

00:01:44,479 --> 00:01:51,520
our systems modern microservices

00:01:48,000 --> 00:01:55,040
systems but before that let's

00:01:51,520 --> 00:02:00,000
recap on how we used to monitor

00:01:55,040 --> 00:02:00,000
our systems actually not so long ago

00:02:01,759 --> 00:02:06,079
so the popular combination for

00:02:03,600 --> 00:02:08,959
monitoring was stating graphite

00:02:06,079 --> 00:02:09,599
both open source tools our symbols were

00:02:08,959 --> 00:02:12,400
our

00:02:09,599 --> 00:02:13,120
applications were relatively simple

00:02:12,400 --> 00:02:16,319
largely

00:02:13,120 --> 00:02:19,360
based on homegrown code that pushed

00:02:16,319 --> 00:02:22,239
metrics to statsd over udp

00:02:19,360 --> 00:02:22,959
statsd server aggregated the metrics and

00:02:22,239 --> 00:02:26,480
send them

00:02:22,959 --> 00:02:28,959
over tcp to graphite backend for

00:02:26,480 --> 00:02:29,920
storing and visualization you can see

00:02:28,959 --> 00:02:32,720
here how a

00:02:29,920 --> 00:02:34,800
visualization of graphite metrics would

00:02:32,720 --> 00:02:37,760
look like on the screen

00:02:34,800 --> 00:02:38,480
and also uh the metrics how metrics uh

00:02:37,760 --> 00:02:41,680
look like

00:02:38,480 --> 00:02:45,040
that's the uh common you know famous

00:02:41,680 --> 00:02:46,879
hierarchy dot notation of graphite uh

00:02:45,040 --> 00:02:48,080
representing essentially how our

00:02:46,879 --> 00:02:51,120
application

00:02:48,080 --> 00:02:52,879
is deployed in the environment so

00:02:51,120 --> 00:02:55,280
in this example here we have pro

00:02:52,879 --> 00:02:58,400
production inside it the web server

00:02:55,280 --> 00:03:00,480
in it the html service and in it the

00:02:58,400 --> 00:03:03,200
respective

00:03:00,480 --> 00:03:06,640
metrics request total count it could be

00:03:03,200 --> 00:03:09,840
error total count or any other

00:03:06,640 --> 00:03:10,640
the main takeaways from the way we use

00:03:09,840 --> 00:03:13,760
to monitor

00:03:10,640 --> 00:03:15,040
are first that metric collection was

00:03:13,760 --> 00:03:17,120
done in push mode

00:03:15,040 --> 00:03:19,280
so each and every piece of our

00:03:17,120 --> 00:03:22,159
application

00:03:19,280 --> 00:03:24,720
knew that stats the server and was

00:03:22,159 --> 00:03:28,480
configured to push to that

00:03:24,720 --> 00:03:31,920
server secondly the hierarchy notation

00:03:28,480 --> 00:03:33,760
and in general the way that we interact

00:03:31,920 --> 00:03:34,720
with the metrics and query the metrics

00:03:33,760 --> 00:03:37,920
is based on the

00:03:34,720 --> 00:03:41,280
hierarchy notation each each dimension

00:03:37,920 --> 00:03:43,760
of our metric is a part of the of a

00:03:41,280 --> 00:03:46,799
hierarchy

00:03:43,760 --> 00:03:49,200
and also that a lot of

00:03:46,799 --> 00:03:49,920
our applications was based on homegrown

00:03:49,200 --> 00:03:51,840
code which

00:03:49,920 --> 00:03:53,680
meant that it was easier to instrument

00:03:51,840 --> 00:03:56,159
and control the way that it exposed

00:03:53,680 --> 00:03:56,159
metrics

00:03:56,480 --> 00:04:00,480
but then came microservices and cloud

00:03:59,200 --> 00:04:03,519
native architectures and

00:04:00,480 --> 00:04:04,239
things started getting missing so let's

00:04:03,519 --> 00:04:05,760
look at

00:04:04,239 --> 00:04:08,000
the changes in the architecture and how

00:04:05,760 --> 00:04:10,319
it impacted the

00:04:08,000 --> 00:04:12,080
the monitoring and then also we'll see

00:04:10,319 --> 00:04:15,439
the solutions and what you need to do in

00:04:12,080 --> 00:04:18,320
order to meet these challenges

00:04:15,439 --> 00:04:20,239
the first change was the move to the

00:04:18,320 --> 00:04:23,199
shift to microservices

00:04:20,239 --> 00:04:25,520
and first the shift brought about a

00:04:23,199 --> 00:04:26,080
surge in the number of discrete entities

00:04:25,520 --> 00:04:28,960
that

00:04:26,080 --> 00:04:29,919
we need to monitor if you look about on

00:04:28,960 --> 00:04:33,680
amazon or

00:04:29,919 --> 00:04:36,560
netflix on uber or others they

00:04:33,680 --> 00:04:38,479
talk about thousands if not more

00:04:36,560 --> 00:04:41,199
proprietary microservices

00:04:38,479 --> 00:04:42,000
but even the most modest application

00:04:41,199 --> 00:04:44,160
today

00:04:42,000 --> 00:04:46,560
and what used to be one monolith is

00:04:44,160 --> 00:04:48,560
today dozens or more microservices

00:04:46,560 --> 00:04:50,160
each microservice is a cluster of

00:04:48,560 --> 00:04:53,360
multiple instances

00:04:50,160 --> 00:04:55,360
each service potentially running its own

00:04:53,360 --> 00:04:57,440
programming language and database

00:04:55,360 --> 00:04:59,680
each one independently deployed and

00:04:57,440 --> 00:05:01,680
scaled and upgraded

00:04:59,680 --> 00:05:03,680
and these systems are highly dynamic

00:05:01,680 --> 00:05:05,919
volatile ephemeral you know things

00:05:03,680 --> 00:05:07,840
spinning up tearing down scaling in and

00:05:05,919 --> 00:05:11,280
out

00:05:07,840 --> 00:05:15,680
so from monitoring perspective

00:05:11,280 --> 00:05:18,240
first we need to monitor many many

00:05:15,680 --> 00:05:18,960
entities much more than we used to but

00:05:18,240 --> 00:05:23,199
also

00:05:18,960 --> 00:05:25,759
a very diverse very dynamic in nature

00:05:23,199 --> 00:05:26,400
which means high volume of metrics with

00:05:25,759 --> 00:05:29,360
with

00:05:26,400 --> 00:05:31,120
high many dimensions high cardinality

00:05:29,360 --> 00:05:34,080
matrix as it's called

00:05:31,120 --> 00:05:35,759
so the trend the shift to microservices

00:05:34,080 --> 00:05:38,160
disrupted the

00:05:35,759 --> 00:05:38,880
monitoring in one way and then came

00:05:38,160 --> 00:05:41,919
about

00:05:38,880 --> 00:05:45,039
cloud native architecture and kubernetes

00:05:41,919 --> 00:05:47,039
and added more complication

00:05:45,039 --> 00:05:48,160
because now i need to monitor

00:05:47,039 --> 00:05:50,840
applications spanning

00:05:48,160 --> 00:05:52,080
multiple containers and pods and

00:05:50,840 --> 00:05:54,639
namespaces

00:05:52,080 --> 00:05:57,360
deployment versions potentially over

00:05:54,639 --> 00:05:59,120
fleets of clusters

00:05:57,360 --> 00:06:01,680
and this introduced many additional

00:05:59,120 --> 00:06:04,479
dimensions for my metrics

00:06:01,680 --> 00:06:05,199
now i can ask about my micro service

00:06:04,479 --> 00:06:10,240
performance

00:06:05,199 --> 00:06:13,440
per pod per node per version etc

00:06:10,240 --> 00:06:17,039
in addition the docker

00:06:13,440 --> 00:06:20,560
and kubernetes or all the the container

00:06:17,039 --> 00:06:23,199
framework are critical systems that

00:06:20,560 --> 00:06:25,199
need to be monitored as well uh for

00:06:23,199 --> 00:06:28,240
example here on the screen you can see

00:06:25,199 --> 00:06:30,800
uh examples of kubernetes control plane

00:06:28,240 --> 00:06:31,360
services whether on the node level such

00:06:30,800 --> 00:06:33,680
as

00:06:31,360 --> 00:06:35,120
a cubelet and the k proxy or on the

00:06:33,680 --> 00:06:38,880
master node such as the

00:06:35,120 --> 00:06:38,880
api the lcd the uh

00:06:39,759 --> 00:06:43,919
or the manager and so on each and every

00:06:42,000 --> 00:06:46,080
one of these

00:06:43,919 --> 00:06:47,520
control plane services by kubernetes i

00:06:46,080 --> 00:06:49,360
need to make sure that they they're

00:06:47,520 --> 00:06:51,840
up and running and performing and

00:06:49,360 --> 00:06:54,240
responding and so on

00:06:51,840 --> 00:06:55,759
so microservices and cloud native

00:06:54,240 --> 00:06:57,440
architectures

00:06:55,759 --> 00:07:00,319
and the last piece that i would like to

00:06:57,440 --> 00:07:03,199
talk about is the third parties

00:07:00,319 --> 00:07:04,720
uh and again we we did use third parties

00:07:03,199 --> 00:07:08,720
uh even before yeah it's not

00:07:04,720 --> 00:07:10,560
something new but i think with the uh

00:07:08,720 --> 00:07:11,840
micro service architectures in the cloud

00:07:10,560 --> 00:07:14,479
native realm

00:07:11,840 --> 00:07:15,680
it got a significant boost and the fact

00:07:14,479 --> 00:07:18,880
being that today's

00:07:15,680 --> 00:07:19,840
systems use many libraries and tools and

00:07:18,880 --> 00:07:23,280
frameworks

00:07:19,840 --> 00:07:25,440
for building uh software uh including

00:07:23,280 --> 00:07:28,080
you know the web servers and the

00:07:25,440 --> 00:07:29,360
nosql databases and the sql database and

00:07:28,080 --> 00:07:31,120
the api gateway

00:07:29,360 --> 00:07:32,400
and the message broker and the queue and

00:07:31,120 --> 00:07:34,479
and so on

00:07:32,400 --> 00:07:36,800
uh for example here on the screen you

00:07:34,479 --> 00:07:39,599
can see a typical

00:07:36,800 --> 00:07:40,400
data processing pipeline and just see

00:07:39,599 --> 00:07:43,680
how many

00:07:40,400 --> 00:07:46,400
uh you know open source

00:07:43,680 --> 00:07:47,680
frameworks or cloud services are

00:07:46,400 --> 00:07:50,479
potentially involved

00:07:47,680 --> 00:07:51,919
in just running through that data

00:07:50,479 --> 00:07:54,080
processing pipeline

00:07:51,919 --> 00:07:56,840
and that's just the data processing out

00:07:54,080 --> 00:08:00,639
of the entire system

00:07:56,840 --> 00:08:03,840
so uh we need to monitor these

00:08:00,639 --> 00:08:06,400
third-party tools and this tech stack

00:08:03,840 --> 00:08:09,199
keeps updating in an increasing pace you

00:08:06,400 --> 00:08:12,240
know the the chase after the latest

00:08:09,199 --> 00:08:15,280
tech stack is becoming crazy

00:08:12,240 --> 00:08:17,280
so monitoring systems today need to

00:08:15,280 --> 00:08:18,960
provide integration with a large and

00:08:17,280 --> 00:08:21,199
very dynamic ecosystem

00:08:18,960 --> 00:08:24,319
of third-party platforms to provide

00:08:21,199 --> 00:08:24,319
complete observability

00:08:25,440 --> 00:08:30,560
so just to recap on the challenges

00:08:28,000 --> 00:08:32,479
before we move on to the solutions

00:08:30,560 --> 00:08:34,399
first is microservices that brought

00:08:32,479 --> 00:08:36,560
about many many

00:08:34,399 --> 00:08:37,680
entities that we need to monitor high

00:08:36,560 --> 00:08:40,800
volume of metrics

00:08:37,680 --> 00:08:42,640
with many dimensions high cardinality

00:08:40,800 --> 00:08:45,200
metrics

00:08:42,640 --> 00:08:47,519
then cloud native introduced more layers

00:08:45,200 --> 00:08:50,160
more virtualization and also adding

00:08:47,519 --> 00:08:52,080
additional dimensions you know per pod

00:08:50,160 --> 00:08:54,640
per node per version etc

00:08:52,080 --> 00:08:56,000
and also new critical systems to monitor

00:08:54,640 --> 00:08:58,160
you know the

00:08:56,000 --> 00:08:59,360
docker uh container runtime and

00:08:58,160 --> 00:09:03,120
kubernetes

00:08:59,360 --> 00:09:06,080
uh are also in need of monitoring

00:09:03,120 --> 00:09:07,120
uh and thirdly uh the large and dynamic

00:09:06,080 --> 00:09:08,959
ecosystem of

00:09:07,120 --> 00:09:11,760
third-party platforms we use in our

00:09:08,959 --> 00:09:14,000
system uh whether open source or cloud

00:09:11,760 --> 00:09:16,399
cloud services that we need to monitor

00:09:14,000 --> 00:09:16,399
as well

00:09:19,040 --> 00:09:23,680
and new challenges obviously call for uh

00:09:21,839 --> 00:09:26,480
new capabilities

00:09:23,680 --> 00:09:27,440
and uh in the rest of the uh talk today

00:09:26,480 --> 00:09:30,560
i'd like to

00:09:27,440 --> 00:09:31,600
devote to the requirements that you'd be

00:09:30,560 --> 00:09:34,720
looking for

00:09:31,600 --> 00:09:35,920
when designing your monitoring system to

00:09:34,720 --> 00:09:38,320
make sure that you can meet these

00:09:35,920 --> 00:09:40,560
challenges

00:09:38,320 --> 00:09:43,360
so the first requirement is flexible

00:09:40,560 --> 00:09:46,880
querying over high cardinality

00:09:43,360 --> 00:09:49,839
you saw before the hierarchy notation

00:09:46,880 --> 00:09:51,519
the dot notation and it worked well for

00:09:49,839 --> 00:09:54,560
static machine-centric

00:09:51,519 --> 00:09:57,120
metrics but it turns out to be too

00:09:54,560 --> 00:09:57,760
strict and too cumbersome even to

00:09:57,120 --> 00:10:00,720
express

00:09:57,760 --> 00:10:01,680
the dynamic services metrics with high

00:10:00,720 --> 00:10:04,800
cardinality

00:10:01,680 --> 00:10:08,079
that we see we saw before for today's

00:10:04,800 --> 00:10:09,839
uh modern systems uh if we go back to

00:10:08,079 --> 00:10:12,320
the example that i showed before

00:10:09,839 --> 00:10:14,640
uh of the web server so we had a web

00:10:12,320 --> 00:10:18,000
server in our environment uh and

00:10:14,640 --> 00:10:21,920
so we saw the the request total for for

00:10:18,000 --> 00:10:23,760
that uh uh web server

00:10:21,920 --> 00:10:25,680
but today in the microservice

00:10:23,760 --> 00:10:29,040
architecture that will probably be

00:10:25,680 --> 00:10:31,600
a microservice that is clustered and

00:10:29,040 --> 00:10:32,480
spans across multiple nodes in multiple

00:10:31,600 --> 00:10:35,519
pods

00:10:32,480 --> 00:10:38,480
so this query may be a query

00:10:35,519 --> 00:10:40,640
i may want to query about the metric on

00:10:38,480 --> 00:10:43,839
a specific pod

00:10:40,640 --> 00:10:46,480
or on all the pods on a specific node

00:10:43,839 --> 00:10:47,120
or maybe on across the the entire

00:10:46,480 --> 00:10:49,920
service

00:10:47,120 --> 00:10:50,560
spanning multiple nodes so each one of

00:10:49,920 --> 00:10:53,519
these

00:10:50,560 --> 00:10:54,240
is a different metric in the hierarchy

00:10:53,519 --> 00:10:55,839
notation

00:10:54,240 --> 00:10:58,480
because in hierarchy notation

00:10:55,839 --> 00:10:59,200
introducing a new dimension effectively

00:10:58,480 --> 00:11:01,200
creates

00:10:59,200 --> 00:11:02,640
a new metric because you have a dot

00:11:01,200 --> 00:11:05,600
something dot something it's just

00:11:02,640 --> 00:11:06,320
a new string the entire concatenation is

00:11:05,600 --> 00:11:09,040
defines

00:11:06,320 --> 00:11:09,680
the metric so a new dimension which is a

00:11:09,040 --> 00:11:11,680
new

00:11:09,680 --> 00:11:12,800
piece of the of the chain of the

00:11:11,680 --> 00:11:15,600
concatenation

00:11:12,800 --> 00:11:17,760
is a new metric in effect which makes it

00:11:15,600 --> 00:11:21,040
quite difficult to expose

00:11:17,760 --> 00:11:21,040
highly dimensional data

00:11:21,120 --> 00:11:25,279
it also is difficult to quit to do query

00:11:23,760 --> 00:11:27,360
based aggregation so

00:11:25,279 --> 00:11:28,480
if you want to do a group by something

00:11:27,360 --> 00:11:31,680
that i haven't

00:11:28,480 --> 00:11:33,440
planned on before and pre-aggregated it

00:11:31,680 --> 00:11:34,640
may be very tricky in the hierarchy

00:11:33,440 --> 00:11:36,959
notation

00:11:34,640 --> 00:11:38,480
if you think for example if you have

00:11:36,959 --> 00:11:42,079
multiple servers and you want to

00:11:38,480 --> 00:11:43,760
group by error code

00:11:42,079 --> 00:11:45,600
do you do that in the hierarchy model

00:11:43,760 --> 00:11:49,040
it's quite complex

00:11:45,600 --> 00:11:52,079
so all of these drove the shift to more

00:11:49,040 --> 00:11:52,800
flexible uh query the need for flexible

00:11:52,079 --> 00:11:56,480
querying

00:11:52,800 --> 00:11:57,120
and that means from a hierarchy model to

00:11:56,480 --> 00:11:59,279
the

00:11:57,120 --> 00:12:00,639
labeling model a living model is

00:11:59,279 --> 00:12:03,360
essentially a

00:12:00,639 --> 00:12:04,800
key value pair that we cannot get pairs

00:12:03,360 --> 00:12:06,800
that we can attach to the

00:12:04,800 --> 00:12:08,240
metric i have the metric name then i

00:12:06,800 --> 00:12:10,399
have a key value pair

00:12:08,240 --> 00:12:12,000
of the different they meant the keys or

00:12:10,399 --> 00:12:15,120
the different dimensions

00:12:12,000 --> 00:12:16,399
and then um it was not by the way nicely

00:12:15,120 --> 00:12:20,240
introduced

00:12:16,399 --> 00:12:22,240
by the prom ql query language and it was

00:12:20,240 --> 00:12:24,800
brought about by prometheus open source

00:12:22,240 --> 00:12:27,920
project but since then it's been adopted

00:12:24,800 --> 00:12:30,560
by many others and that's definitely the

00:12:27,920 --> 00:12:31,200
the going forward part so if i look at

00:12:30,560 --> 00:12:33,920
the uh

00:12:31,200 --> 00:12:35,200
same example we've looked at before in

00:12:33,920 --> 00:12:37,200
the labeling model

00:12:35,200 --> 00:12:38,639
i would look now at an http request

00:12:37,200 --> 00:12:41,600
total metric

00:12:38,639 --> 00:12:43,040
and then i can state which labels i'm

00:12:41,600 --> 00:12:46,240
interested in so if i want

00:12:43,040 --> 00:12:48,240
uh to query by on a specific pod

00:12:46,240 --> 00:12:49,680
then i would state for example server

00:12:48,240 --> 00:12:51,680
equals pod seven

00:12:49,680 --> 00:12:53,120
or if i want across the entire service i

00:12:51,680 --> 00:12:55,680
would just say state

00:12:53,120 --> 00:12:57,839
service equals engines and that's

00:12:55,680 --> 00:13:01,519
obviously a very uh

00:12:57,839 --> 00:13:03,120
small example real life web applications

00:13:01,519 --> 00:13:04,959
where we have the

00:13:03,120 --> 00:13:06,320
web server software you know engines and

00:13:04,959 --> 00:13:08,959
apache and you have

00:13:06,320 --> 00:13:11,040
multiple environments like production

00:13:08,959 --> 00:13:14,720
and staging and so on and you have

00:13:11,040 --> 00:13:15,440
the http methods the post and get and

00:13:14,720 --> 00:13:17,760
the http

00:13:15,440 --> 00:13:20,560
response code and the error code and the

00:13:17,760 --> 00:13:24,720
multiple endpoints and so on

00:13:20,560 --> 00:13:27,040
any ad hoc query that i would like to

00:13:24,720 --> 00:13:28,800
ask my system like what is the total

00:13:27,040 --> 00:13:32,399
number of requests per

00:13:28,800 --> 00:13:34,480
web server in

00:13:32,399 --> 00:13:36,079
pod in production or what's the number

00:13:34,480 --> 00:13:39,199
of http errors using

00:13:36,079 --> 00:13:41,199
engine server for a specific endpoint in

00:13:39,199 --> 00:13:42,000
staging or what's the slowest post

00:13:41,199 --> 00:13:46,000
requests

00:13:42,000 --> 00:13:49,440
segmented by endpoint url all of these

00:13:46,000 --> 00:13:53,440
become very easy to query

00:13:49,440 --> 00:13:55,360
ad hoc based on labeling model

00:13:53,440 --> 00:13:57,279
so flexible querying and the labeling

00:13:55,360 --> 00:13:59,360
model is definitely a

00:13:57,279 --> 00:14:02,320
requirement that you would like in your

00:13:59,360 --> 00:14:04,880
monitoring system

00:14:02,320 --> 00:14:06,320
next up is metric scraping and auto

00:14:04,880 --> 00:14:09,600
discovery

00:14:06,320 --> 00:14:12,880
we saw in statsd days that

00:14:09,600 --> 00:14:14,800
the application pushed metrics to the

00:14:12,880 --> 00:14:16,880
statsd server

00:14:14,800 --> 00:14:18,399
but systems today have many

00:14:16,880 --> 00:14:20,160
microservices and

00:14:18,399 --> 00:14:21,440
third-party frameworks as we've seen

00:14:20,160 --> 00:14:23,120
before

00:14:21,440 --> 00:14:25,199
how would you configure each

00:14:23,120 --> 00:14:26,959
microservice and a third party

00:14:25,199 --> 00:14:28,240
to push metrics to your monitoring

00:14:26,959 --> 00:14:31,360
backend

00:14:28,240 --> 00:14:33,519
it can be quite quite complex

00:14:31,360 --> 00:14:37,120
would it be great if our monitoring

00:14:33,519 --> 00:14:40,399
service can discover the services itself

00:14:37,120 --> 00:14:40,399
and pull the metrics from them

00:14:40,880 --> 00:14:45,839
that's exactly what prometheus open

00:14:42,480 --> 00:14:45,839
source project introduced

00:14:46,160 --> 00:14:49,600
with the auto discovery and the metric

00:14:48,720 --> 00:14:51,600
scraping

00:14:49,600 --> 00:14:53,680
prometheus by the way is an open source

00:14:51,600 --> 00:14:56,160
under the cncf

00:14:53,680 --> 00:14:57,040
it's in fact the second project to have

00:14:56,160 --> 00:15:00,079
graduated

00:14:57,040 --> 00:15:04,800
from the cncf after kubernetes itself

00:15:00,079 --> 00:15:08,079
and prometheus can essentially

00:15:04,800 --> 00:15:10,399
find or discover these components

00:15:08,079 --> 00:15:12,160
these services these targets in

00:15:10,399 --> 00:15:14,399
prometheus terms

00:15:12,160 --> 00:15:15,920
within the system and we've seen in

00:15:14,399 --> 00:15:18,480
microservices it could be

00:15:15,920 --> 00:15:20,880
many many and very diverse and also

00:15:18,480 --> 00:15:23,519
third parties and so on

00:15:20,880 --> 00:15:24,079
and then it can pull metrics off of

00:15:23,519 --> 00:15:27,279
these

00:15:24,079 --> 00:15:30,959
targets or scrape in prometheus terms

00:15:27,279 --> 00:15:30,959
scrape metrics off of these targets

00:15:31,360 --> 00:15:36,079
instead of them having to push it to

00:15:33,440 --> 00:15:36,079
prometheus

00:15:36,480 --> 00:15:41,680
we can do that thanks to open metrics

00:15:39,759 --> 00:15:43,120
which is an open standard for

00:15:41,680 --> 00:15:45,759
transmitting metrics or

00:15:43,120 --> 00:15:48,000
an exposition format a way to expose

00:15:45,759 --> 00:15:51,040
metrics in a standard way

00:15:48,000 --> 00:15:53,680
that has become quite standard

00:15:51,040 --> 00:15:54,399
across the industry it's standard under

00:15:53,680 --> 00:15:57,519
the uh

00:15:54,399 --> 00:15:59,519
cncf sandbox project it's uh based on

00:15:57,519 --> 00:16:02,639
prometheus format so it's pretty

00:15:59,519 --> 00:16:04,720
established and mature uh it's uh

00:16:02,639 --> 00:16:05,920
these days being proposed as a standard

00:16:04,720 --> 00:16:09,279
under the ietf

00:16:05,920 --> 00:16:10,800
a real formal standard but the most

00:16:09,279 --> 00:16:13,920
important thing here is that

00:16:10,800 --> 00:16:16,560
many many tools and frameworks today

00:16:13,920 --> 00:16:17,279
expose out-of-the-box metrics using this

00:16:16,560 --> 00:16:20,399
format

00:16:17,279 --> 00:16:22,880
so you know kafka and rabbit mq and

00:16:20,399 --> 00:16:26,480
 and my sequin elasticsearch

00:16:22,880 --> 00:16:28,959
apache engines jenkins jira github

00:16:26,480 --> 00:16:32,320
you know cloud services i don't know aws

00:16:28,959 --> 00:16:35,279
ecs and azure health and others so

00:16:32,320 --> 00:16:36,720
whatever third parties you use in your

00:16:35,279 --> 00:16:38,560
system architecture

00:16:36,720 --> 00:16:40,079
you're more than likely to find that

00:16:38,560 --> 00:16:43,920
they already have

00:16:40,079 --> 00:16:46,880
a built-in uh exposition of

00:16:43,920 --> 00:16:47,360
exposed out-of-the-box these in this

00:16:46,880 --> 00:16:51,839
format

00:16:47,360 --> 00:16:51,839
their metrics so um

00:16:52,160 --> 00:16:56,320
and that's really essential because the

00:16:55,040 --> 00:16:57,759
key here as we've seen with the

00:16:56,320 --> 00:17:00,560
challenges before

00:16:57,759 --> 00:17:01,920
uh that this large ecosystem is key when

00:17:00,560 --> 00:17:04,000
dealing in such

00:17:01,920 --> 00:17:05,919
with such diverse and dynamic systems as

00:17:04,000 --> 00:17:08,799
we uh today face

00:17:05,919 --> 00:17:11,039
um now prometheus being you know

00:17:08,799 --> 00:17:13,360
together with kubernetes under cncf then

00:17:11,039 --> 00:17:16,400
not surprisingly that prometheus has a

00:17:13,360 --> 00:17:18,559
native integration with kubernetes

00:17:16,400 --> 00:17:19,839
both for uh the auto discovery the

00:17:18,559 --> 00:17:21,039
service auto discovery and for the

00:17:19,839 --> 00:17:24,000
scraping it can

00:17:21,039 --> 00:17:24,319
uh uh integrate with the kubernetes api

00:17:24,000 --> 00:17:26,799
with

00:17:24,319 --> 00:17:27,679
cube state metrics uh and so on it can

00:17:26,799 --> 00:17:30,480
also

00:17:27,679 --> 00:17:32,080
integrate with uh the broader cloud

00:17:30,480 --> 00:17:35,600
native ecosystem for example

00:17:32,080 --> 00:17:36,000
it can integrate with console to get the

00:17:35,600 --> 00:17:38,559
uh

00:17:36,000 --> 00:17:39,120
service discovery and the configuration

00:17:38,559 --> 00:17:41,840
and and

00:17:39,120 --> 00:17:44,480
similar so uh and as a and also service

00:17:41,840 --> 00:17:47,840
discovery services from from cloud

00:17:44,480 --> 00:17:51,919
services so it's very much

00:17:47,840 --> 00:17:51,919
plug and play in many of your ecosystem

00:17:53,280 --> 00:17:57,600
and the third requirement that you would

00:17:55,520 --> 00:17:59,440
like to uh

00:17:57,600 --> 00:18:00,880
in your system in your monitoring system

00:17:59,440 --> 00:18:03,120
is scalability

00:18:00,880 --> 00:18:04,640
we've seen that systems emit these days

00:18:03,120 --> 00:18:06,720
massive amounts of

00:18:04,640 --> 00:18:10,240
sorry high cardinality metrics which

00:18:06,720 --> 00:18:12,480
means high volume of time series data

00:18:10,240 --> 00:18:13,679
and prometheus with all its virtues that

00:18:12,480 --> 00:18:18,160
we've seen here

00:18:13,679 --> 00:18:21,520
is by design a single node installation

00:18:18,160 --> 00:18:23,600
so it can scale horizontally

00:18:21,520 --> 00:18:25,840
one option for scaling out prometheus

00:18:23,600 --> 00:18:26,799
horizontally is to compose prometheus

00:18:25,840 --> 00:18:30,000
instances into a

00:18:26,799 --> 00:18:32,320
federated architecture shardio metrics

00:18:30,000 --> 00:18:34,000
and so on there are lots of information

00:18:32,320 --> 00:18:36,799
on the web about that

00:18:34,000 --> 00:18:38,799
but that can become quite complicated to

00:18:36,799 --> 00:18:40,840
manage

00:18:38,799 --> 00:18:43,360
another option is to write to a

00:18:40,840 --> 00:18:46,000
long-term storage back-end

00:18:43,360 --> 00:18:48,480
prometheus has a built-in capability to

00:18:46,000 --> 00:18:52,000
remote right to a backend store

00:18:48,480 --> 00:18:54,640
and there are other quite a few

00:18:52,000 --> 00:18:56,160
time series databases that can serve as

00:18:54,640 --> 00:18:59,600
long-term storage for

00:18:56,160 --> 00:19:02,400
prometheus offering compatibility with

00:18:59,600 --> 00:19:03,280
prometheus apis of course and with the

00:19:02,400 --> 00:19:05,200
prom ql

00:19:03,280 --> 00:19:06,480
pro query language so you can query

00:19:05,200 --> 00:19:09,120
across the

00:19:06,480 --> 00:19:12,080
prometheus instances and the long-term

00:19:09,120 --> 00:19:12,080
storage uniformly

00:19:12,960 --> 00:19:16,240
many of these are open source solutions

00:19:14,640 --> 00:19:19,760
such as the m3 that

00:19:16,240 --> 00:19:20,559
we use and influx db and thanos and

00:19:19,760 --> 00:19:23,440
cortex

00:19:20,559 --> 00:19:25,200
by the way both thanos and cortex are

00:19:23,440 --> 00:19:29,760
projects under the

00:19:25,200 --> 00:19:32,400
cncf um and they have a lot of

00:19:29,760 --> 00:19:34,000
core you know interaction between them

00:19:32,400 --> 00:19:37,120
and also of course the proprietary

00:19:34,000 --> 00:19:38,000
services such as logs.i now it's

00:19:37,120 --> 00:19:40,080
important to mention

00:19:38,000 --> 00:19:41,280
that not all the long-term storage

00:19:40,080 --> 00:19:43,840
solutions uh

00:19:41,280 --> 00:19:44,480
use remote right from prometheus thanos

00:19:43,840 --> 00:19:47,919
for instance

00:19:44,480 --> 00:19:49,520
uh offers a pull mode from prometheus

00:19:47,919 --> 00:19:51,679
but the important thing is that

00:19:49,520 --> 00:19:54,960
prometheus still

00:19:51,679 --> 00:19:58,000
still serves as the scraping agent

00:19:54,960 --> 00:20:00,000
so we still leverage the seamless

00:19:58,000 --> 00:20:02,080
integration and the ecosystem and the

00:20:00,000 --> 00:20:03,919
scraping capabilities and the question

00:20:02,080 --> 00:20:05,280
being whether prometheus then remote

00:20:03,919 --> 00:20:07,039
rights to the long-term storage or

00:20:05,280 --> 00:20:11,200
whether the long-term storage

00:20:07,039 --> 00:20:11,200
pulls off of the prometheus instance

00:20:12,320 --> 00:20:17,919
so let's summarize what we've seen here

00:20:16,640 --> 00:20:20,400
monitoring microservices and

00:20:17,919 --> 00:20:23,440
cloud-native systems introduces

00:20:20,400 --> 00:20:27,200
challenges with high volume of

00:20:23,440 --> 00:20:29,600
metrics with a large diversity

00:20:27,200 --> 00:20:30,720
with the many dimensions the high

00:20:29,600 --> 00:20:34,159
cardinality

00:20:30,720 --> 00:20:37,520
of the metrics highly distributed

00:20:34,159 --> 00:20:39,760
with many third-party tools and

00:20:37,520 --> 00:20:42,400
frameworks involved

00:20:39,760 --> 00:20:43,280
and in order to monitor these and meet

00:20:42,400 --> 00:20:47,200
these challenges

00:20:43,280 --> 00:20:50,159
we need first flexible query

00:20:47,200 --> 00:20:51,200
and the key here as we've seen is a

00:20:50,159 --> 00:20:54,400
labeling model

00:20:51,200 --> 00:20:57,440
so having key value pairs attached

00:20:54,400 --> 00:20:59,440
to each metric each key is

00:20:57,440 --> 00:21:00,640
each label is a dimension and then we

00:20:59,440 --> 00:21:02,559
can query

00:21:00,640 --> 00:21:05,600
high cardinality metrics metrics with

00:21:02,559 --> 00:21:07,520
many dimensions very easily but just

00:21:05,600 --> 00:21:09,679
specifying the relevant labels we're

00:21:07,520 --> 00:21:12,080
interested in

00:21:09,679 --> 00:21:12,880
secondly uh we would be looking for

00:21:12,080 --> 00:21:15,120
efficient

00:21:12,880 --> 00:21:16,960
metric scraping with auto discovery so

00:21:15,120 --> 00:21:18,960
that our monitoring system can

00:21:16,960 --> 00:21:22,640
automatically

00:21:18,960 --> 00:21:26,080
discover the services the components

00:21:22,640 --> 00:21:28,840
of our of our system and then pull

00:21:26,080 --> 00:21:30,000
or scrape the metrics off of them in a

00:21:28,840 --> 00:21:33,360
uniform

00:21:30,000 --> 00:21:36,000
manner and thirdly scalability so that

00:21:33,360 --> 00:21:36,000
we can actually

00:21:36,400 --> 00:21:39,840
handle large volumes of metrics stall

00:21:38,880 --> 00:21:42,720
them

00:21:39,840 --> 00:21:44,400
efficiently and also over over long

00:21:42,720 --> 00:21:45,280
periods of time so that i can actually

00:21:44,400 --> 00:21:49,360
query

00:21:45,280 --> 00:21:51,440
uh things that are metrics months away

00:21:49,360 --> 00:21:53,760
or even a year or two away

00:21:51,440 --> 00:21:54,720
with just as ease as i would do in the

00:21:53,760 --> 00:21:59,360
in the past

00:21:54,720 --> 00:22:01,919
weeks and as an open source advocate i'm

00:21:59,360 --> 00:22:02,480
more than happy to say that open source

00:22:01,919 --> 00:22:05,760
is taking

00:22:02,480 --> 00:22:09,679
a lead here we've seen prometheus as a

00:22:05,760 --> 00:22:12,559
leading tool for uh for

00:22:09,679 --> 00:22:13,280
scraping and discovering and scraping

00:22:12,559 --> 00:22:15,840
metrics

00:22:13,280 --> 00:22:16,559
we talked about openmetrics as a

00:22:15,840 --> 00:22:18,880
standard an

00:22:16,559 --> 00:22:21,120
open source standard for exposing

00:22:18,880 --> 00:22:23,120
metrics across the ecosystem

00:22:21,120 --> 00:22:24,159
we talked about time series databases

00:22:23,120 --> 00:22:27,280
that can provide

00:22:24,159 --> 00:22:29,520
long-term storage for to scale

00:22:27,280 --> 00:22:32,640
prometheus and to and now allow

00:22:29,520 --> 00:22:34,720
querying over long periods of time

00:22:32,640 --> 00:22:37,200
and mentioned also the seamless

00:22:34,720 --> 00:22:40,720
integration with kubernetes and the

00:22:37,200 --> 00:22:43,840
cloud-native ecosystem

00:22:40,720 --> 00:22:44,960
if you'd like to read more about this

00:22:43,840 --> 00:22:48,400
topic then

00:22:44,960 --> 00:22:49,919
i i wrote an article about that so do

00:22:48,400 --> 00:22:52,720
check it out i'll post it here on the

00:22:49,919 --> 00:22:56,720
chat as well there's also a great

00:22:52,720 --> 00:23:00,000
episode of open observability talks that

00:22:56,720 --> 00:23:04,559
specifies very well how to

00:23:00,000 --> 00:23:07,440
actually deploy and monitor real systems

00:23:04,559 --> 00:23:09,360
these days and of course the relevant

00:23:07,440 --> 00:23:11,039
communities the prometheus community the

00:23:09,360 --> 00:23:12,640
open metrics community and others

00:23:11,039 --> 00:23:14,080
open telemetry by the way another

00:23:12,640 --> 00:23:16,559
interesting discussions

00:23:14,080 --> 00:23:17,440
uh because things are moved so fast that

00:23:16,559 --> 00:23:19,120
oftentimes

00:23:17,440 --> 00:23:21,120
the latest and greatest you'll just find

00:23:19,120 --> 00:23:23,600
in the uh on the community discussion so

00:23:21,120 --> 00:23:25,280
do check them out and of course feel

00:23:23,600 --> 00:23:27,120
free to reach out to me

00:23:25,280 --> 00:23:28,480
with any questions or comment i'd be

00:23:27,120 --> 00:23:31,280
more than happy to

00:23:28,480 --> 00:23:33,520
touch base and discuss further so you

00:23:31,280 --> 00:23:35,840
can find me at horvitz

00:23:33,520 --> 00:23:51,840
i've been dothan horvitz and thank you

00:23:35,840 --> 00:23:51,840
very much for listening

00:23:54,240 --> 00:23:56,320

YouTube URL: https://www.youtube.com/watch?v=5DiUC-YB9fY


