Title: PromCon 2016: 10x Improvement for Kubernetes Performance - Jonathan Boulle
Publication date: 2016-09-04
Playlist: PromCon 2016
Description: 
	* Abstract:

Scalability is one of the important factors that make a successful distributed system. At CoreOS, we love Kubernetes and help drive the project forward through upstream contributions. Learn how we analyzed Kubernetes cluster performance to gain insight into cluster behavior under large workloads and learn where there are performance issues. See how we used Prometheus for improving Kubernetes scheduler performance by 10x.

* Speaker biography:

Jonathan Boulle is Head of Containers and Berlin site lead at CoreOS.

Jonathan Boulle works at CoreOS on all things distributed and all things contained. He's contributed heavily to etcd and fleet and lead the development of the App Container (appc) specification and rkt, the first appc runtime. He is actively involved in the upstream Kubernetes project. Prior to CoreOS, he worked at Twitter on their cluster management platform based on Mesos and Aurora. He's passionate about Linux, F/OSS, the Oxford comma, and developing well-defined systems that scale.

* Slides:

https://github.com/jonboulle/presentations/blob/master/2016-08_Promcon_Metrics_Matter_10x_Improvement_for_Kubernetes_Performance.pdf

* PromCon website:

https://promcon.io/
Captions: 
	00:00:00,000 --> 00:00:04,500
and so my name is jonathan i work at

00:00:01,439 --> 00:00:05,490
chorus as a bit of disclosure i don't

00:00:04,500 --> 00:00:07,440
actually work on the kubernetes

00:00:05,490 --> 00:00:08,670
performance team at core OS but they're

00:00:07,440 --> 00:00:09,660
all in SF and they weren't be able to

00:00:08,670 --> 00:00:11,790
make it so i'm just going to be talking

00:00:09,660 --> 00:00:14,460
kind of on their behalf but i have been

00:00:11,790 --> 00:00:15,809
involved with this stuff a bit and this

00:00:14,460 --> 00:00:18,029
also the title of this talk is a little

00:00:15,809 --> 00:00:20,220
bit of a trap a trick everyone loves

00:00:18,029 --> 00:00:21,810
like you know 10x employment performance

00:00:20,220 --> 00:00:23,460
improvements that sounds great but

00:00:21,810 --> 00:00:25,289
actually I want to use this as well to

00:00:23,460 --> 00:00:26,430
kind of argue that why you know metrics

00:00:25,289 --> 00:00:27,660
are important and why you should use

00:00:26,430 --> 00:00:30,630
more of them and use more of them with

00:00:27,660 --> 00:00:33,960
systems like Google IDs it's the first

00:00:30,630 --> 00:00:35,100
kind of like Styles a little again

00:00:33,960 --> 00:00:36,360
trying to kind of persuade you like why

00:00:35,100 --> 00:00:37,590
metrics are important why we should

00:00:36,360 --> 00:00:39,629
instrument everything like the

00:00:37,590 --> 00:00:41,280
Prometheus guys keep telling us and then

00:00:39,629 --> 00:00:42,840
the first like obviously the main two

00:00:41,280 --> 00:00:44,610
reasons that people often think about

00:00:42,840 --> 00:00:46,140
coming from you know DevOps or coming

00:00:44,610 --> 00:00:49,079
from older systems is like first is

00:00:46,140 --> 00:00:51,539
health monitoring right so that allows

00:00:49,079 --> 00:00:53,010
you to you know easily kind of measure

00:00:51,539 --> 00:00:54,629
how healthy your system is performing

00:00:53,010 --> 00:00:58,020
whether it like an individual service

00:00:54,629 --> 00:00:59,699
level or maybe at a higher level within

00:00:58,020 --> 00:01:01,770
kind of your stack I'm and then also to

00:00:59,699 --> 00:01:03,210
do different alerting based on that and

00:01:01,770 --> 00:01:05,040
you know time series based loading is

00:01:03,210 --> 00:01:07,890
gobs a bit more powerful than you know

00:01:05,040 --> 00:01:09,180
older kind of more sort of static

00:01:07,890 --> 00:01:12,240
systems like things like nagios and

00:01:09,180 --> 00:01:13,890
stuff with simple simple checks but kind

00:01:12,240 --> 00:01:15,600
of looking trying to move beyond that to

00:01:13,890 --> 00:01:17,490
seeing one of the things that you know

00:01:15,600 --> 00:01:19,860
this new metrics kind of world allows us

00:01:17,490 --> 00:01:21,240
to do something that I think someone

00:01:19,860 --> 00:01:22,710
touched on a talk earlier today it was

00:01:21,240 --> 00:01:24,630
maybe or is also having a conversation

00:01:22,710 --> 00:01:26,939
but someone about it is you know you can

00:01:24,630 --> 00:01:28,080
start to use metrics maybe for sort of

00:01:26,939 --> 00:01:29,850
business inside things I mean obviously

00:01:28,080 --> 00:01:31,439
people do use metrics for business

00:01:29,850 --> 00:01:34,110
inside but even even something like

00:01:31,439 --> 00:01:35,850
prometheus maybe if you're trying to

00:01:34,110 --> 00:01:37,829
like capacity plan you can get an idea

00:01:35,850 --> 00:01:40,229
from the kind of metrics that breathe

00:01:37,829 --> 00:01:41,490
Prometheus can expose about you know at

00:01:40,229 --> 00:01:42,899
what your utilization is like in your

00:01:41,490 --> 00:01:44,280
classroom when you might not start to

00:01:42,899 --> 00:01:46,409
need to think about getting more servers

00:01:44,280 --> 00:01:48,479
or similarly you could put even if you

00:01:46,409 --> 00:01:51,290
want to track things like you know how

00:01:48,479 --> 00:01:55,680
how users are interacting with your apps

00:01:51,290 --> 00:01:57,329
auto-tuning is kind of a new area i'd

00:01:55,680 --> 00:01:59,430
say we're seeing this start to be

00:01:57,329 --> 00:02:01,110
explored in cuban at ease for example

00:01:59,430 --> 00:02:02,909
and the idea here is that you know once

00:02:01,110 --> 00:02:04,619
you have the system running you want to

00:02:02,909 --> 00:02:07,590
be able to automatically adjust state so

00:02:04,619 --> 00:02:10,289
maybe at you know expand resource usage

00:02:07,590 --> 00:02:11,730
on the fly automatically in response to

00:02:10,289 --> 00:02:12,610
demand or in response to system

00:02:11,730 --> 00:02:14,770
performance

00:02:12,610 --> 00:02:16,480
so your auto scaling on Cuban 80's would

00:02:14,770 --> 00:02:18,760
be a good example of that I mean metrics

00:02:16,480 --> 00:02:20,620
is really a key part to that to such a

00:02:18,760 --> 00:02:21,760
system and then finally the one I want

00:02:20,620 --> 00:02:23,080
to kind of focus on today is you know

00:02:21,760 --> 00:02:24,820
debugging with metrics and this is also

00:02:23,080 --> 00:02:27,520
sort of touched on in the last talk as

00:02:24,820 --> 00:02:29,380
well now d has just to see what you know

00:02:27,520 --> 00:02:31,210
once we start maybe we start using

00:02:29,380 --> 00:02:32,530
metrics for like monitoring and help and

00:02:31,210 --> 00:02:33,820
healthful loading and stuff but now we

00:02:32,530 --> 00:02:36,040
start to see what kind of things other

00:02:33,820 --> 00:02:38,140
kind of bonuses that can provide and how

00:02:36,040 --> 00:02:39,730
we can use it to to kind of do debugging

00:02:38,140 --> 00:02:42,580
a bit better than the kind of old

00:02:39,730 --> 00:02:45,850
starways again so this is kind of a case

00:02:42,580 --> 00:02:47,950
study of like how we used metrics and

00:02:45,850 --> 00:02:49,720
among other things to improve the Cuban

00:02:47,950 --> 00:02:51,790
ad scheduler performance by a good

00:02:49,720 --> 00:02:54,130
factor so in a way this is a talk about

00:02:51,790 --> 00:02:55,989
like how to use Prometheus but it's also

00:02:54,130 --> 00:02:58,030
as you'll see it to talk about you know

00:02:55,989 --> 00:02:59,470
how we didn't use Prometheus as well as

00:02:58,030 --> 00:03:01,540
we as we maybe we could have or should

00:02:59,470 --> 00:03:03,370
have and how we some things we kind of

00:03:01,540 --> 00:03:07,030
learn from that and sort of fed back

00:03:03,370 --> 00:03:09,340
into projects looking forward so again

00:03:07,030 --> 00:03:11,440
to set the context we we sort of setup I

00:03:09,340 --> 00:03:14,320
think about a year a year ago we set up

00:03:11,440 --> 00:03:17,110
a team at core OS focused on Cooper

00:03:14,320 --> 00:03:18,489
nerdy scalability since we you know we

00:03:17,110 --> 00:03:20,799
do a lot with Cuban ideas at core OS a

00:03:18,489 --> 00:03:21,970
big fans and the idea of this team was

00:03:20,799 --> 00:03:24,489
that we want to make sure that

00:03:21,970 --> 00:03:26,590
communities can scale you know pretty

00:03:24,489 --> 00:03:28,630
really well particularly compared to

00:03:26,590 --> 00:03:29,860
kind of competing schedule a sort of

00:03:28,630 --> 00:03:32,350
container scheduling systems out there

00:03:29,860 --> 00:03:33,459
and the first step in you know being

00:03:32,350 --> 00:03:34,989
able to scale something is just kind of

00:03:33,459 --> 00:03:36,459
understanding the performance so the

00:03:34,989 --> 00:03:38,080
first thing that the first kind of task

00:03:36,459 --> 00:03:39,640
of this team was just like testing

00:03:38,080 --> 00:03:42,310
communities and understanding how it

00:03:39,640 --> 00:03:43,570
performs and you know start to kind of

00:03:42,310 --> 00:03:46,060
think about where maybe maybe some of

00:03:43,570 --> 00:03:47,500
the bottlenecks are for anyone who's not

00:03:46,060 --> 00:03:49,299
really familiar with communities

00:03:47,500 --> 00:03:51,790
architecture this is it at a very high

00:03:49,299 --> 00:03:52,780
level you have sort of the central in

00:03:51,790 --> 00:03:54,489
them in the middle you have these sort

00:03:52,780 --> 00:03:55,989
of scheduler what we call a control

00:03:54,489 --> 00:03:57,640
plane so that's the scheduler

00:03:55,989 --> 00:03:59,380
responsible for deciding where to put

00:03:57,640 --> 00:04:00,940
containers in the cluster in the API

00:03:59,380 --> 00:04:02,200
server which is like your first port of

00:04:00,940 --> 00:04:03,940
entry for all your interactions with

00:04:02,200 --> 00:04:06,549
Cuban at ease I mean how the community

00:04:03,940 --> 00:04:08,320
is sort of interacts internally and then

00:04:06,549 --> 00:04:11,709
the backing store for for Cuba native

00:04:08,320 --> 00:04:12,880
operations is currently set CD generally

00:04:11,709 --> 00:04:14,769
that runs sort of separately from the

00:04:12,880 --> 00:04:15,850
control plane and then finally you know

00:04:14,769 --> 00:04:17,950
the bulk of kind of your cluster

00:04:15,850 --> 00:04:19,989
generally will be you know your worker

00:04:17,950 --> 00:04:21,220
nodes or your coup blitz and those are

00:04:19,989 --> 00:04:23,260
you know those are pretty dumb

00:04:21,220 --> 00:04:24,820
relatively speaking those are just

00:04:23,260 --> 00:04:25,200
getting work assigned to them and then

00:04:24,820 --> 00:04:26,970
actually

00:04:25,200 --> 00:04:28,380
the containers you know that's where at

00:04:26,970 --> 00:04:30,810
the end of the day your applications run

00:04:28,380 --> 00:04:33,060
but most of kind of a special magic and

00:04:30,810 --> 00:04:35,070
logic and communities happens you know

00:04:33,060 --> 00:04:38,160
arguably happens in the scheduler in the

00:04:35,070 --> 00:04:39,840
API sober so initial focus in the google

00:04:38,160 --> 00:04:41,640
ad scaly scalability team is on this

00:04:39,840 --> 00:04:42,720
control plane and that's because you

00:04:41,640 --> 00:04:45,180
know the worker nodes are relatively

00:04:42,720 --> 00:04:47,160
independent whereas the control plane is

00:04:45,180 --> 00:04:49,080
involved in everything in the cluster so

00:04:47,160 --> 00:04:50,610
every time a you schedule an application

00:04:49,080 --> 00:04:52,770
every time application needs to be

00:04:50,610 --> 00:04:54,660
rescheduled or you want to scale up an

00:04:52,770 --> 00:04:56,190
application that needs to go through the

00:04:54,660 --> 00:04:58,650
control plane whereas it's only going to

00:04:56,190 --> 00:05:01,050
touch you know relatively independent

00:04:58,650 --> 00:05:02,850
number of worker nodes if a worker node

00:05:01,050 --> 00:05:04,590
you know is performing badly in itself

00:05:02,850 --> 00:05:06,060
that's only going to effect it's going

00:05:04,590 --> 00:05:07,470
to have the ice the damage from that

00:05:06,060 --> 00:05:08,610
kind of isolated it's only going to

00:05:07,470 --> 00:05:11,340
affect the new pods that are running on

00:05:08,610 --> 00:05:14,010
that note so the first thing we did was

00:05:11,340 --> 00:05:15,450
use this tool called khoob mark which

00:05:14,010 --> 00:05:17,940
was developed by the Cuban 80s community

00:05:15,450 --> 00:05:19,530
the idea is just to be able to rather

00:05:17,940 --> 00:05:20,970
than having to spin up you know thousand

00:05:19,530 --> 00:05:23,070
no cluster to be able to test stuff at

00:05:20,970 --> 00:05:25,920
scale we want to be able to simulate the

00:05:23,070 --> 00:05:27,690
worker nodes excuse me so we can test

00:05:25,920 --> 00:05:29,850
larger clusters and really you know

00:05:27,690 --> 00:05:32,460
focus on on this control plane and so

00:05:29,850 --> 00:05:33,510
the initial question that we focused on

00:05:32,460 --> 00:05:37,080
was you know how long does it take to

00:05:33,510 --> 00:05:38,700
run 30 30 pods on every single node in a

00:05:37,080 --> 00:05:42,390
cluster so we started just with a

00:05:38,700 --> 00:05:44,210
hundred node cluster so 300 pots 33,000

00:05:42,390 --> 00:05:46,890
pods how long does that take to schedule

00:05:44,210 --> 00:05:48,390
and then you know the first way that we

00:05:46,890 --> 00:05:50,520
started looking at this with the sort of

00:05:48,390 --> 00:05:51,900
the output from could mark was just kind

00:05:50,520 --> 00:05:53,310
of you know going through the going

00:05:51,900 --> 00:05:55,950
through the could mark and seeing you

00:05:53,310 --> 00:05:58,970
know with time how many pods get created

00:05:55,950 --> 00:06:02,370
and how many are still pending and so on

00:05:58,970 --> 00:06:04,770
so the initial results are shown a

00:06:02,370 --> 00:06:06,210
pretty linear rate of pod creation but

00:06:04,770 --> 00:06:10,020
pretty what we thought seemed pretty

00:06:06,210 --> 00:06:12,300
slow like 20 20 plugs a second there was

00:06:10,020 --> 00:06:13,650
no there's no delay between the pod kind

00:06:12,300 --> 00:06:15,180
of being created being submitted to the

00:06:13,650 --> 00:06:16,590
schedule and being created and then the

00:06:15,180 --> 00:06:18,450
delay between it being actually running

00:06:16,590 --> 00:06:20,370
on the system and that's the the dotted

00:06:18,450 --> 00:06:21,780
line just under here so it's very quick

00:06:20,370 --> 00:06:24,360
for a pod once it's been created to

00:06:21,780 --> 00:06:26,220
actually get to get to be running so we

00:06:24,360 --> 00:06:28,110
suspected that you know this is probably

00:06:26,220 --> 00:06:30,570
a place to start looking at performance

00:06:28,110 --> 00:06:33,780
it seemed like a bottleneck so place we

00:06:30,570 --> 00:06:35,700
wanted to kind of start investigating so

00:06:33,780 --> 00:06:37,290
the first place to start pretty stupid

00:06:35,700 --> 00:06:39,030
but it's oh it's the kind of the easy

00:06:37,290 --> 00:06:40,710
easy way easy place to start looking

00:06:39,030 --> 00:06:42,780
is you know hey I've got a performance

00:06:40,710 --> 00:06:44,190
problem if something seems slow maybe

00:06:42,780 --> 00:06:45,870
I'm just using too many resources maybe

00:06:44,190 --> 00:06:49,080
the hardware's you know tapping out the

00:06:45,870 --> 00:06:50,910
hard way easy way to kind of jump in

00:06:49,080 --> 00:06:54,030
look at that was using you know H type

00:06:50,910 --> 00:06:55,530
or top but to see how the KU vanetti

00:06:54,030 --> 00:06:57,540
scheduler components the control plane

00:06:55,530 --> 00:06:59,910
components are doing so you know fired

00:06:57,540 --> 00:07:02,130
up top and we see you know some some

00:06:59,910 --> 00:07:03,630
reasonable CPU usage but nothing extreme

00:07:02,130 --> 00:07:05,520
we weren't seeing you know we had 16

00:07:03,630 --> 00:07:08,220
cause most of them are still mostly

00:07:05,520 --> 00:07:11,340
sitting idle not nothing nothing too

00:07:08,220 --> 00:07:12,750
outrageous incidentally oh so we could

00:07:11,340 --> 00:07:14,760
rule out how we're resource starvation

00:07:12,750 --> 00:07:16,830
as kind of a cause in this case and i'll

00:07:14,760 --> 00:07:18,180
come back to might be wonder i'm using

00:07:16,830 --> 00:07:20,550
top i'll come back to that a bit later

00:07:18,180 --> 00:07:21,930
on so this suggests again the

00:07:20,550 --> 00:07:23,490
bottlenecks somewhere and you know in

00:07:21,930 --> 00:07:25,560
the software that we're working with in

00:07:23,490 --> 00:07:28,850
the control plane but cooper net is a

00:07:25,560 --> 00:07:31,080
huge codebase it's very quite complex

00:07:28,850 --> 00:07:33,930
you know finding bottlenecks in such

00:07:31,080 --> 00:07:34,980
complex code is never really trivial so

00:07:33,930 --> 00:07:36,960
you know how do we know where to start

00:07:34,980 --> 00:07:39,660
well this is where Prometheus came in

00:07:36,960 --> 00:07:41,040
for us fortunately with you know with

00:07:39,660 --> 00:07:43,170
all these prometheus sorry with all

00:07:41,040 --> 00:07:44,820
these communities API calls communities

00:07:43,170 --> 00:07:46,860
actually already exposed metrics that

00:07:44,820 --> 00:07:49,170
sort of provided some information around

00:07:46,860 --> 00:07:51,600
you know latency and response time and

00:07:49,170 --> 00:07:53,340
stuff around these api calls so we could

00:07:51,600 --> 00:07:54,660
start to look at those metrics to you

00:07:53,340 --> 00:07:57,420
know get it get a clue for where to kind

00:07:54,660 --> 00:07:59,220
of dig in further so we you know we

00:07:57,420 --> 00:08:00,870
reran the the good marks we again and

00:07:59,220 --> 00:08:02,130
this time like keeping a closer eye on

00:08:00,870 --> 00:08:03,600
org actually looking at the metrics

00:08:02,130 --> 00:08:06,300
monitoring the scheduler's metrics in

00:08:03,600 --> 00:08:09,090
particular and you know that was it

00:08:06,300 --> 00:08:10,650
wasn't great but it also wasn't didn't

00:08:09,090 --> 00:08:12,630
seem to be the culprit because you know

00:08:10,650 --> 00:08:14,820
the schedule is latency was only seven

00:08:12,630 --> 00:08:16,320
milliseconds which you know mere meant

00:08:14,820 --> 00:08:17,460
it could schedule about 140 pods a

00:08:16,320 --> 00:08:18,900
second which was a lot better than what

00:08:17,460 --> 00:08:19,920
we were seeing so this suggested you

00:08:18,900 --> 00:08:23,220
know the schedule is not actually the

00:08:19,920 --> 00:08:25,650
place to start not the primary botany so

00:08:23,220 --> 00:08:26,640
you know the next step was actually

00:08:25,650 --> 00:08:29,490
involved you know looking at some other

00:08:26,640 --> 00:08:30,720
metrics and unfortunately a sort of

00:08:29,490 --> 00:08:32,580
adding different logs and print

00:08:30,720 --> 00:08:33,900
statements and things partly because we

00:08:32,580 --> 00:08:36,690
didn't have enough exposed metrics at

00:08:33,900 --> 00:08:37,830
this stage so the reason that the way

00:08:36,690 --> 00:08:39,750
that we found the first bottleneck

00:08:37,830 --> 00:08:41,760
really was actually adding more log

00:08:39,750 --> 00:08:43,410
debugging and through this kind of

00:08:41,760 --> 00:08:45,080
isolated contention to the replication

00:08:43,410 --> 00:08:47,640
controller which is responsible for

00:08:45,080 --> 00:08:49,590
spinning up you know responsible for

00:08:47,640 --> 00:08:51,820
spinning up the different applicant

00:08:49,590 --> 00:08:54,200
replicas of a pot

00:08:51,820 --> 00:08:55,940
there was some pretty low hanging fruit

00:08:54,200 --> 00:08:57,200
in the replication controller so there

00:08:55,940 --> 00:08:58,820
was an easy place to start there were a

00:08:57,200 --> 00:09:00,440
couple of actually rate limiters in the

00:08:58,820 --> 00:09:01,760
in the client code because you know I

00:09:00,440 --> 00:09:04,210
guess someone who had first written the

00:09:01,760 --> 00:09:07,550
code it said we don't overwhelm the API

00:09:04,210 --> 00:09:09,230
and so you know removing those gained a

00:09:07,550 --> 00:09:11,180
bit of it gave a bit of a game and then

00:09:09,230 --> 00:09:13,460
there was a little inefficient code path

00:09:11,180 --> 00:09:15,560
that was a kind of a quick fix and it's

00:09:13,460 --> 00:09:17,000
kind of ironically the code path that

00:09:15,560 --> 00:09:18,620
ended up having quite a significant

00:09:17,000 --> 00:09:20,450
impact was a very very simple thing it

00:09:18,620 --> 00:09:21,830
was metrics registration and just the

00:09:20,450 --> 00:09:23,690
simple fact of doing this metrics

00:09:21,830 --> 00:09:25,910
registration you know in every request

00:09:23,690 --> 00:09:27,290
rather than registering once and

00:09:25,910 --> 00:09:28,550
allowing the operation to cache itself

00:09:27,290 --> 00:09:31,040
actually ended up having quite a big

00:09:28,550 --> 00:09:33,650
impact on scheduling performance so

00:09:31,040 --> 00:09:36,830
metrics aren't always great but you know

00:09:33,650 --> 00:09:38,450
did help a lot we went you know with

00:09:36,830 --> 00:09:40,010
that same hundred no cluster scenario we

00:09:38,450 --> 00:09:41,750
went managed to go from 20 points a

00:09:40,010 --> 00:09:43,070
second up to you know more than over 300

00:09:41,750 --> 00:09:45,830
plots a second so that really seemed to

00:09:43,070 --> 00:09:48,410
seem like a great start ideally that

00:09:45,830 --> 00:09:50,120
would be the end of the story but that's

00:09:48,410 --> 00:09:51,940
just kind of showing the improvement

00:09:50,120 --> 00:09:53,630
initially improvement in the right

00:09:51,940 --> 00:09:55,430
because you can see is still kind of

00:09:53,630 --> 00:09:57,950
quite linear over time but much higher

00:09:55,430 --> 00:09:59,420
throughput so then we wanted to you know

00:09:57,950 --> 00:10:00,800
take it up to the next step from 100 no

00:09:59,420 --> 00:10:02,839
cluster and move to a to a thousand no

00:10:00,800 --> 00:10:04,670
cluster in this case you know we weren't

00:10:02,839 --> 00:10:06,920
so lucky that performance deteriorated

00:10:04,670 --> 00:10:08,900
pretty dramatically we ended up going to

00:10:06,920 --> 00:10:10,610
a you know total kind of POD throughput

00:10:08,900 --> 00:10:12,260
of getting a pod running of just five a

00:10:10,610 --> 00:10:14,959
second which was kind of worse than than

00:10:12,260 --> 00:10:17,209
when we started and actually the rate of

00:10:14,959 --> 00:10:19,160
creating the pods was quite good

00:10:17,209 --> 00:10:20,180
creating the pod entities but actually

00:10:19,160 --> 00:10:21,950
getting them getting them running

00:10:20,180 --> 00:10:25,459
getting them replicated out there was

00:10:21,950 --> 00:10:27,080
remained quite slow so you can see here

00:10:25,459 --> 00:10:28,400
like on the sort of line on the left is

00:10:27,080 --> 00:10:30,080
then being created so that's happening

00:10:28,400 --> 00:10:31,790
quite fast but in getting gained him

00:10:30,080 --> 00:10:33,410
actually running was was was slow and

00:10:31,790 --> 00:10:35,779
sort of the latency there was was

00:10:33,410 --> 00:10:37,310
deteriorating this is the point at which

00:10:35,779 --> 00:10:38,540
the scheduler component started to

00:10:37,310 --> 00:10:40,550
become more suspicious because the

00:10:38,540 --> 00:10:41,900
latency started quite high if you

00:10:40,550 --> 00:10:43,700
remember when we started it was down at

00:10:41,900 --> 00:10:45,620
I think was seven milliseconds now it's

00:10:43,700 --> 00:10:46,970
kind of up to 60 and then over time as

00:10:45,620 --> 00:10:48,920
the cluster grew or is the number of

00:10:46,970 --> 00:10:50,870
schedule pods grow in the cluster that

00:10:48,920 --> 00:10:54,350
the latency was deteriorating even worse

00:10:50,870 --> 00:10:57,500
so you know the metrics helped here help

00:10:54,350 --> 00:10:58,880
focus on the scheduler so we thought we

00:10:57,500 --> 00:11:00,890
kind of isolated the next problem there

00:10:58,880 --> 00:11:02,900
but again like even within the schedule

00:11:00,890 --> 00:11:04,610
is quite a complex code base and we kind

00:11:02,900 --> 00:11:05,420
of reached a point where we need to dive

00:11:04,610 --> 00:11:08,330
in more in

00:11:05,420 --> 00:11:09,830
in much more detail the other problem

00:11:08,330 --> 00:11:11,260
that we had There was that could mark

00:11:09,830 --> 00:11:14,570
you know I mentioned this was taking

00:11:11,260 --> 00:11:16,040
five thousand seconds so Kumar crimes

00:11:14,570 --> 00:11:18,380
were taking a couple of hours each test

00:11:16,040 --> 00:11:19,880
so you know it's really a little bit too

00:11:18,380 --> 00:11:23,630
slow to kind of practically be able to

00:11:19,880 --> 00:11:25,040
debug this you know more quickly so in

00:11:23,630 --> 00:11:26,150
an what we ended up doing was kind of

00:11:25,040 --> 00:11:27,890
creating this sort of lightweight

00:11:26,150 --> 00:11:30,320
version of the benchmarking tool that

00:11:27,890 --> 00:11:34,700
there's more details in the link in the

00:11:30,320 --> 00:11:36,290
slides to be able to you know simulate

00:11:34,700 --> 00:11:38,330
this more quickly and then kind of boil

00:11:36,290 --> 00:11:40,280
it down to you know the fundamental

00:11:38,330 --> 00:11:41,980
contentious bit boil it down to a go

00:11:40,280 --> 00:11:44,450
unit test that we could run repeatedly

00:11:41,980 --> 00:11:46,160
very quickly and analyze the performance

00:11:44,450 --> 00:11:49,820
there and trying to analyze you know the

00:11:46,160 --> 00:11:52,070
slow performing parts of that you know

00:11:49,820 --> 00:11:53,960
after a bunch of go profiling and kind

00:11:52,070 --> 00:11:56,270
of graphing it out and diving into it

00:11:53,960 --> 00:11:58,970
more detail actually isolated a few a

00:11:56,270 --> 00:12:02,360
few a few a few performance hotspots

00:11:58,970 --> 00:12:03,650
this is a one example ended up that you

00:12:02,360 --> 00:12:04,970
know actually most of the time the

00:12:03,650 --> 00:12:07,400
schedule was spinning was just doing

00:12:04,970 --> 00:12:10,520
this in a round rounding operation on a

00:12:07,400 --> 00:12:11,930
number on a value that that was you know

00:12:10,520 --> 00:12:15,920
buried away somewhere in the google ad s

00:12:11,930 --> 00:12:18,740
codebase so spend some time you know

00:12:15,920 --> 00:12:19,940
working on little fixes to those a lot

00:12:18,740 --> 00:12:21,830
of them ended up being very very simple

00:12:19,940 --> 00:12:24,080
actually I'm just optimizing some of

00:12:21,830 --> 00:12:26,450
these things you know put up a number of

00:12:24,080 --> 00:12:29,030
poor request to Tacuba Nettie's most of

00:12:26,450 --> 00:12:30,620
them got merged and it was very

00:12:29,030 --> 00:12:32,360
successful in the end we ended up you

00:12:30,620 --> 00:12:34,580
know the average scheduling latency

00:12:32,360 --> 00:12:35,990
drops pretty significantly but but even

00:12:34,580 --> 00:12:37,670
more significantly the sort of total

00:12:35,990 --> 00:12:39,470
time of scheduling be able to schedule

00:12:37,670 --> 00:12:42,250
30,000 pubs in two thousand nose dropped

00:12:39,470 --> 00:12:45,800
this is the magical 10x from you know

00:12:42,250 --> 00:12:48,200
8787 80 seconds down to two just 587

00:12:45,800 --> 00:12:49,490
seconds and all this is true achieved

00:12:48,200 --> 00:12:52,100
through a pretty small amount of code

00:12:49,490 --> 00:12:54,320
yet just for pull requests none of them

00:12:52,100 --> 00:12:56,270
particularly significant some some sort

00:12:54,320 --> 00:12:59,170
of minor minor fixes that could really

00:12:56,270 --> 00:13:01,190
bought a lot of a lot of benefit and

00:12:59,170 --> 00:13:02,870
this is kind of through a combination of

00:13:01,190 --> 00:13:04,700
tools you know we use Prometheus a bit

00:13:02,870 --> 00:13:06,350
to kind of help us guide us in the right

00:13:04,700 --> 00:13:08,120
direction but then when we need more

00:13:06,350 --> 00:13:10,370
detail we would drive into things like

00:13:08,120 --> 00:13:12,650
paper off using paper off to analyze the

00:13:10,370 --> 00:13:14,150
girl run times performance we sort of

00:13:12,650 --> 00:13:16,640
had our own plotting thing which I took

00:13:14,150 --> 00:13:17,100
joy in a second and using top to look at

00:13:16,640 --> 00:13:19,940
the system

00:13:17,100 --> 00:13:22,020
metrics Solan I was a pretty good result

00:13:19,940 --> 00:13:23,790
but you know want to talk a little bit

00:13:22,020 --> 00:13:25,290
more about like what we didn't really do

00:13:23,790 --> 00:13:26,700
right and why how we could have done

00:13:25,290 --> 00:13:28,530
better in just in terms of how we went

00:13:26,700 --> 00:13:30,330
through this process we really didn't

00:13:28,530 --> 00:13:31,440
use metrics and Prometheus in the way

00:13:30,330 --> 00:13:33,600
that Prometheus kind of thinks about

00:13:31,440 --> 00:13:35,010
them as much as we could so you know for

00:13:33,600 --> 00:13:37,380
example the first kind of obvious thing

00:13:35,010 --> 00:13:38,970
was like jumping on a machine and using

00:13:37,380 --> 00:13:41,150
top this is not really a production way

00:13:38,970 --> 00:13:42,990
of you know analyzing performance

00:13:41,150 --> 00:13:45,180
instead you know would have been better

00:13:42,990 --> 00:13:46,680
to you know to expose this I mean

00:13:45,180 --> 00:13:48,270
hopefully this is already exposed and

00:13:46,680 --> 00:13:49,650
you'll Prometheus environment but to to

00:13:48,270 --> 00:13:51,390
be able to analyze system level metrics

00:13:49,650 --> 00:13:54,570
you know with something like prometheus

00:13:51,390 --> 00:13:55,980
over time similarly with this you know

00:13:54,570 --> 00:13:58,020
the cube March test we were actually

00:13:55,980 --> 00:13:59,910
doing a lot of kind of log scraping and

00:13:58,020 --> 00:14:00,960
ended up generating these these graphs

00:13:59,910 --> 00:14:02,370
that I've been showing their kind of

00:14:00,960 --> 00:14:04,010
this custom graphing tool that was

00:14:02,370 --> 00:14:06,120
created for this but you know really

00:14:04,010 --> 00:14:07,980
would be better to expose the stuff

00:14:06,120 --> 00:14:09,450
directly and take advantage of all the

00:14:07,980 --> 00:14:11,190
stuff that you already get and like the

00:14:09,450 --> 00:14:12,750
Prometheus ecosystem like we could graph

00:14:11,190 --> 00:14:13,950
the stuff with Gravano and do more you

00:14:12,750 --> 00:14:16,740
know advanced kind of queries and

00:14:13,950 --> 00:14:18,750
analysis with prom cool so even though

00:14:16,740 --> 00:14:20,310
we know previous helped us in this in

00:14:18,750 --> 00:14:22,080
this exercise we definitely didn't use

00:14:20,310 --> 00:14:24,330
it I would say as we should have as much

00:14:22,080 --> 00:14:27,000
as we should and subsequently you know

00:14:24,330 --> 00:14:28,530
after this experience we've started

00:14:27,000 --> 00:14:30,360
we've been in the process of adding a

00:14:28,530 --> 00:14:32,370
lot more metrics to to some of our own

00:14:30,360 --> 00:14:35,370
services like Etsy d particularly the

00:14:32,370 --> 00:14:37,380
new at CD v3 API and then in the v3

00:14:35,370 --> 00:14:41,910
store and then also in Cuba Nettie's

00:14:37,380 --> 00:14:43,410
itself so why a matrix better why do I

00:14:41,910 --> 00:14:45,570
said you know those things are wrong a

00:14:43,410 --> 00:14:48,780
couple of couple of quick reasons to

00:14:45,570 --> 00:14:50,660
kind of I guess reiterate it simplifies

00:14:48,780 --> 00:14:53,580
the lower level analysis because you can

00:14:50,660 --> 00:14:54,960
cut start to extract information you

00:14:53,580 --> 00:14:56,520
know about your code how it's running

00:14:54,960 --> 00:14:58,260
without actually having to dive into the

00:14:56,520 --> 00:15:00,150
code and add print statements and you

00:14:58,260 --> 00:15:03,090
know starrett stare at particular things

00:15:00,150 --> 00:15:05,370
you can really you know get in from

00:15:03,090 --> 00:15:08,220
interesting information and be able to

00:15:05,370 --> 00:15:09,840
point to a pretty specific area if you

00:15:08,220 --> 00:15:12,540
instrument thoughtfully you know while

00:15:09,840 --> 00:15:13,620
you're writing the code just as may be

00:15:12,540 --> 00:15:15,120
just as important there's no need to

00:15:13,620 --> 00:15:16,380
necessarily reproduce the problem so

00:15:15,120 --> 00:15:17,460
rather than you're having to add debug

00:15:16,380 --> 00:15:19,740
statements and then go through it again

00:15:17,460 --> 00:15:22,290
and make sure it happens again you know

00:15:19,740 --> 00:15:23,250
if you have a sort of metrics set up

00:15:22,290 --> 00:15:24,660
you're already collecting this

00:15:23,250 --> 00:15:27,000
information it's already there so you

00:15:24,660 --> 00:15:28,860
already you know the time that the

00:15:27,000 --> 00:15:30,180
problem occurred or the initial time

00:15:28,860 --> 00:15:30,720
that you you know produce this poor

00:15:30,180 --> 00:15:31,980
performance

00:15:30,720 --> 00:15:34,500
you should be able to go and already get

00:15:31,980 --> 00:15:36,720
that information you don't need to yeah

00:15:34,500 --> 00:15:38,910
riri cabal your code or attach your live

00:15:36,720 --> 00:15:40,470
debugger oh and you know which obviously

00:15:38,910 --> 00:15:42,930
has its own effect on performance and

00:15:40,470 --> 00:15:44,550
code and runtime and such and then you

00:15:42,930 --> 00:15:46,920
get this you know nice dynamic power of

00:15:44,550 --> 00:15:48,779
the prom quill and reach for example and

00:15:46,920 --> 00:15:50,160
the different graphing ways you can you

00:15:48,779 --> 00:15:52,829
can interact with it you don't have

00:15:50,160 --> 00:15:54,779
these sort of things we result from with

00:15:52,829 --> 00:15:57,540
p prof like these SVG graphs we sort of

00:15:54,779 --> 00:15:58,560
looked at earlier and then you know

00:15:57,540 --> 00:15:59,850
that's kind of at the lower level of

00:15:58,560 --> 00:16:01,110
debugging but then at the higher level

00:15:59,850 --> 00:16:03,000
you get these you know all these really

00:16:01,110 --> 00:16:04,980
nice features that kind of stepping back

00:16:03,000 --> 00:16:06,600
and try to analyze performance at a

00:16:04,980 --> 00:16:08,579
higher level you get you know being able

00:16:06,600 --> 00:16:10,139
to look at things across a fleet so

00:16:08,579 --> 00:16:11,579
seeing how about how many different

00:16:10,139 --> 00:16:13,230
versions of your software performing

00:16:11,579 --> 00:16:15,360
across the cluster or in different

00:16:13,230 --> 00:16:16,980
scenarios different hardware platforms

00:16:15,360 --> 00:16:18,540
for example i'm you get you can

00:16:16,980 --> 00:16:20,459
aggregate information from you know many

00:16:18,540 --> 00:16:22,069
different for example many different

00:16:20,459 --> 00:16:25,319
runs of a particular piece of software

00:16:22,069 --> 00:16:27,149
you have that history provided you're

00:16:25,319 --> 00:16:28,589
keeping history so you can kind of look

00:16:27,149 --> 00:16:30,959
back and compare with different

00:16:28,589 --> 00:16:32,129
scenarios and then you know maybe one of

00:16:30,959 --> 00:16:33,750
the most more important things as well

00:16:32,129 --> 00:16:35,089
pretty clean distributed systems as you

00:16:33,750 --> 00:16:37,259
can see correlations more easily

00:16:35,089 --> 00:16:39,120
correlations between different metrics

00:16:37,259 --> 00:16:41,100
within within the same piece of software

00:16:39,120 --> 00:16:43,829
or maybe within different components so

00:16:41,100 --> 00:16:45,089
you know scheduler metric in the case of

00:16:43,829 --> 00:16:46,769
the screw benetti's control plane

00:16:45,089 --> 00:16:52,439
correlating with you know some API

00:16:46,769 --> 00:16:54,930
metrics exposed by the API server so

00:16:52,439 --> 00:16:56,250
yeah I think metrics are great we should

00:16:54,930 --> 00:16:58,050
use more of them we think you should use

00:16:56,250 --> 00:16:59,220
more of them you know then i'll provide

00:16:58,050 --> 00:17:01,170
all the answers at the end of the day

00:16:59,220 --> 00:17:02,490
even if we we had instrumented this

00:17:01,170 --> 00:17:03,660
stuff much more thoroughly from the

00:17:02,490 --> 00:17:05,130
beginning you know we would still

00:17:03,660 --> 00:17:07,199
probably neat at the end of the day we

00:17:05,130 --> 00:17:08,669
would still need to dive in to you know

00:17:07,199 --> 00:17:10,319
to find out some of these problems like

00:17:08,669 --> 00:17:11,669
some of the issues that we ended up

00:17:10,319 --> 00:17:13,530
finding where it comes down to a simple

00:17:11,669 --> 00:17:14,640
you know maths function the

00:17:13,530 --> 00:17:16,230
implementation of particular mass

00:17:14,640 --> 00:17:18,689
function you know realistically probably

00:17:16,230 --> 00:17:19,799
not going to be instrumenting every

00:17:18,689 --> 00:17:22,140
single call like that it's just not

00:17:19,799 --> 00:17:24,120
practical and doesn't necessarily help

00:17:22,140 --> 00:17:27,120
you in finding those really critical hot

00:17:24,120 --> 00:17:28,319
spots but you know it really gives you a

00:17:27,120 --> 00:17:29,789
good starting point for where to where

00:17:28,319 --> 00:17:31,080
to start looking and that's how that's

00:17:29,789 --> 00:17:32,340
part of the reason why it was so helpful

00:17:31,080 --> 00:17:34,049
to us in this process you know

00:17:32,340 --> 00:17:36,090
approaching a really big complex project

00:17:34,049 --> 00:17:37,679
like Cooper Nettie's distributed stem

00:17:36,090 --> 00:17:39,149
with these different moving parts and

00:17:37,679 --> 00:17:41,370
wanting to you know answer these

00:17:39,149 --> 00:17:42,299
high-level have these sort of high-level

00:17:41,370 --> 00:17:44,460
questions of how can we improve

00:17:42,299 --> 00:17:46,230
performance for the system as a whole

00:17:44,460 --> 00:17:47,760
metrics are really critical to to help

00:17:46,230 --> 00:17:50,850
point us you know point us in the right

00:17:47,760 --> 00:17:54,330
direction that's that's pretty much my

00:17:50,850 --> 00:17:55,470
talk but we love previous and I just

00:17:54,330 --> 00:17:56,940
wanted a quick shout-out on if anyone's

00:17:55,470 --> 00:17:59,010
interested in working on previous two

00:17:56,940 --> 00:18:00,180
we're hiring a build it and bill in so

00:17:59,010 --> 00:18:12,320
feel free to talk to me right now on the

00:18:00,180 --> 00:18:12,320
team and thank you so questions

00:18:13,929 --> 00:18:17,409
no questions

00:18:19,200 --> 00:18:26,130
okay that's the first I'm very

00:18:21,750 --> 00:18:32,309
disappointed in you okay well we're I'm

00:18:26,130 --> 00:18:34,409
the longer disappointed in you I have

00:18:32,309 --> 00:18:35,429
you noticed like the lack of metrics and

00:18:34,409 --> 00:18:37,440
Cooper Nettie's and have you started to

00:18:35,429 --> 00:18:40,260
contribute more metric implementation

00:18:37,440 --> 00:18:41,340
back and now do you have Fabia and

00:18:40,260 --> 00:18:42,630
Frederick would be a better place to

00:18:41,340 --> 00:18:43,830
talks about specifics because they've

00:18:42,630 --> 00:18:46,830
actually put up some p oz in the last

00:18:43,830 --> 00:18:49,250
few weeks about Frederick do you want to

00:18:46,830 --> 00:18:49,250
talk to that

00:18:58,720 --> 00:19:04,270
we actually started the special interest

00:19:02,500 --> 00:19:08,380
group in corona d second fermentation

00:19:04,270 --> 00:19:11,980
which has a meeting today where we

00:19:08,380 --> 00:19:14,400
discuss which kind of metrics make sense

00:19:11,980 --> 00:19:17,080
to instrument within coronaries and

00:19:14,400 --> 00:19:29,200
every basically anyone interested in

00:19:17,080 --> 00:19:30,850
that can join and pitch in yeah if

00:19:29,200 --> 00:19:32,500
anyone is interested in conatus metrics

00:19:30,850 --> 00:19:34,480
there will be the first meeting at six

00:19:32,500 --> 00:19:36,690
early today so we will stay here and

00:19:34,480 --> 00:19:39,250
just join the video call so come on John

00:19:36,690 --> 00:19:42,760
but yeah you get free drinks otherwise

00:19:39,250 --> 00:19:46,720
so so to be clear your choices i er to

00:19:42,760 --> 00:19:48,370
go to the monitoring sig and where corn

00:19:46,720 --> 00:19:51,070
west people be present or go to the

00:19:48,370 --> 00:19:54,659
beers sponsored by coral s so they've

00:19:51,070 --> 00:19:54,659
really they've got you both ways so

00:19:57,390 --> 00:20:03,070
another question just a remark

00:20:00,000 --> 00:20:05,470
Prometheus metrics expose parameters

00:20:03,070 --> 00:20:07,720
itself they are not great anyway we have

00:20:05,470 --> 00:20:10,840
issues in Europe to fix them but they're

00:20:07,720 --> 00:20:13,059
often driven by exactly what you did or

00:20:10,840 --> 00:20:14,770
should have done like perform we were

00:20:13,059 --> 00:20:16,299
coding Prometheus I wanted to optimize

00:20:14,770 --> 00:20:18,490
it and then we had the metrics we need

00:20:16,299 --> 00:20:21,100
it right now that's also like you always

00:20:18,490 --> 00:20:23,500
use P prof of course as well but the

00:20:21,100 --> 00:20:24,820
combination like like people of is

00:20:23,500 --> 00:20:28,299
always the snapshot you mentioned that

00:20:24,820 --> 00:20:30,789
and for example like chunk ops to like

00:20:28,299 --> 00:20:31,929
connect this to my talk or you could see

00:20:30,789 --> 00:20:34,120
what is actually happening with the

00:20:31,929 --> 00:20:36,610
chunks and then just get a graph over

00:20:34,120 --> 00:20:40,000
like the last day what has my promises

00:20:36,610 --> 00:20:42,220
so ever done when it's really great to

00:20:40,000 --> 00:20:45,330
like know what the bottlenecks around

00:20:42,220 --> 00:20:45,330
where your resources upward

00:20:45,789 --> 00:20:49,989
another interesting thing I find and

00:20:48,129 --> 00:20:51,940
because I'm always telling people that

00:20:49,989 --> 00:20:54,549
all these tools complimentary like you

00:20:51,940 --> 00:20:56,649
need your metrics to as you said figure

00:20:54,549 --> 00:20:58,359
out where to start looking you need your

00:20:56,649 --> 00:21:00,489
logs then you're profiling tools your

00:20:58,359 --> 00:21:02,200
debugger is your DTrace to then dig into

00:21:00,489 --> 00:21:03,999
the detail or lots of people talking

00:21:02,200 --> 00:21:06,090
about can I have one dashboard or one

00:21:03,999 --> 00:21:11,080
system that will do all of this for me

00:21:06,090 --> 00:21:14,139
yeah let me know if anyone finds one ok

00:21:11,080 --> 00:21:17,100
so at this point we are at let's see

00:21:14,139 --> 00:21:17,100

YouTube URL: https://www.youtube.com/watch?v=HS-a_RG7iX0


