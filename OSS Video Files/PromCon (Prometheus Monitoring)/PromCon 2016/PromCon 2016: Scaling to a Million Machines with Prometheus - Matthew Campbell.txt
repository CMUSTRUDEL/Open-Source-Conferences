Title: PromCon 2016: Scaling to a Million Machines with Prometheus - Matthew Campbell
Publication date: 2016-09-04
Playlist: PromCon 2016
Description: 
	* Abstract:

Digital Ocean hosts a public facing monitoring platform that runs on top of Prometheus. We will discuss how we scale Prometheus up to millions of virtual machines. How we manage and maintain the largest Prometheus cluster in the world. Also how we do multi-tenancy in Prometheus.

* Speaker biography:

Matthew Campbell is a Microservices scalability expert at DigitalOcean where he builds the future of cloud services. He is writing a book called "Microservices in Go" for O'Reilly. He recently presented at GothamGo, Velocity NYC, and GopherCon India, and blogs at kanwisher.com. Matthew was a founder of Errplane and Langfight. In the past he worked at Thomson Reuters, Bloomberg, Gucci, and Cartoon network. He spoke at GopherConIndia 2015.

* Slides:

http://www.slideshare.net/MatthewCampbell7/breaking-prometheus-promcon-berlin-16
Captions: 
	00:00:00,060 --> 00:00:06,629
this is Matthew from digitalocean last

00:00:03,810 --> 00:00:08,429
year at velocity Julius I velocity New

00:00:06,629 --> 00:00:10,740
York there many velocities we visited

00:00:08,429 --> 00:00:12,809
the digitalocean office and that was the

00:00:10,740 --> 00:00:15,150
first time we realized somebody is out

00:00:12,809 --> 00:00:17,880
there that runs a bigger Prometheus

00:00:15,150 --> 00:00:19,590
setup and SoundCloud that's pretty nice

00:00:17,880 --> 00:00:22,650
because we always consider ourselves the

00:00:19,590 --> 00:00:24,810
biggest user of Prometheus but then now

00:00:22,650 --> 00:00:26,189
there are many bigger uses I guess some

00:00:24,810 --> 00:00:27,900
are doing weird things but at

00:00:26,189 --> 00:00:29,939
digitalocean scales to millions of

00:00:27,900 --> 00:00:37,380
machines and that what Matthew will tell

00:00:29,939 --> 00:00:39,960
us about awesome thanks yeah so the

00:00:37,380 --> 00:00:42,000
original top title was about scaling for

00:00:39,960 --> 00:00:43,379
me theists to a million machines but I

00:00:42,000 --> 00:00:45,980
thought I would change it to how he

00:00:43,379 --> 00:00:48,210
broke Prometheus in every way possible I

00:00:45,980 --> 00:00:50,160
think it's a bit more fun to talk about

00:00:48,210 --> 00:00:53,579
how he broke things and how we got past

00:00:50,160 --> 00:00:55,590
all the breaking just as a show a hand

00:00:53,579 --> 00:00:57,570
here who here has like a million things

00:00:55,590 --> 00:01:00,420
that they're monitoring with Prometheus

00:00:57,570 --> 00:01:04,320
oh there's one oh there's two other

00:01:00,420 --> 00:01:06,350
people oh well I mean I'm talking

00:01:04,320 --> 00:01:10,670
millions of scrape targets

00:01:06,350 --> 00:01:15,930
okay how about tens of thousands

00:01:10,670 --> 00:01:18,960
thousands okay we got like four yes all

00:01:15,930 --> 00:01:20,490
right and then hundreds I would expect

00:01:18,960 --> 00:01:23,460
at least the whole audience should have

00:01:20,490 --> 00:01:25,170
hundreds okay cool so hopefully next

00:01:23,460 --> 00:01:27,330
year we'll have a lot more people that

00:01:25,170 --> 00:01:29,750
will have much larger installations and

00:01:27,330 --> 00:01:33,060
we'll be breaking new ground here so

00:01:29,750 --> 00:01:36,360
just really quick about me I work on the

00:01:33,060 --> 00:01:38,070
time series team at digitalocean I live

00:01:36,360 --> 00:01:39,900
in Bangkok so I may have traveled the

00:01:38,070 --> 00:01:45,090
farthest anybody here travel farther

00:01:39,900 --> 00:01:46,829
than Bangkok to get here oh Indonesia

00:01:45,090 --> 00:01:52,799
all right awesome

00:01:46,829 --> 00:01:54,840
so I I didn't win this one yeah so I

00:01:52,799 --> 00:01:58,140
kind of want to structure this talk as

00:01:54,840 --> 00:02:00,810
kind of the dark days of dizzier ocean

00:01:58,140 --> 00:02:03,030
before we found Prometheus to where we

00:02:00,810 --> 00:02:05,460
are now where we have a million

00:02:03,030 --> 00:02:07,229
Prometheus servers so when I originally

00:02:05,460 --> 00:02:09,569
started at digitalocean

00:02:07,229 --> 00:02:12,569
every single team had a different

00:02:09,569 --> 00:02:13,630
monitoring and metric solution there was

00:02:12,569 --> 00:02:16,300
teams that were running

00:02:13,630 --> 00:02:19,420
by their teams they were running in flux

00:02:16,300 --> 00:02:23,410
DB and for some godforsaken reason we

00:02:19,420 --> 00:02:26,050
had a 50 node open TS DB cluster which

00:02:23,410 --> 00:02:28,000
somehow I am inherited when I started

00:02:26,050 --> 00:02:34,030
there so one of the first things we did

00:02:28,000 --> 00:02:36,580
was just turn that off so we started

00:02:34,030 --> 00:02:38,020
using this Prometheus thing so my

00:02:36,580 --> 00:02:40,300
managers of time Ian he's gonna give a

00:02:38,020 --> 00:02:42,370
talk later is he's like this Prometheus

00:02:40,300 --> 00:02:46,390
thing looks pretty cool so we started to

00:02:42,370 --> 00:02:48,870
use it and it was really small at the

00:02:46,390 --> 00:02:51,880
beginning we were just kind of

00:02:48,870 --> 00:02:55,420
monitoring a few micro services here and

00:02:51,880 --> 00:02:58,570
there and basically each team had their

00:02:55,420 --> 00:03:00,790
own Prometheus server and each team

00:02:58,570 --> 00:03:03,160
would manually set up their Prometheus

00:03:00,790 --> 00:03:05,140
server with manual targets they would

00:03:03,160 --> 00:03:06,940
scrape each one of their nodes every

00:03:05,140 --> 00:03:10,210
time a new micro service would come

00:03:06,940 --> 00:03:13,450
online they would just add a new node to

00:03:10,210 --> 00:03:15,820
their to their manual configs all right

00:03:13,450 --> 00:03:18,060
and as you can imagine like this was

00:03:15,820 --> 00:03:20,230
like kind of a very untenable thing

00:03:18,060 --> 00:03:22,330
because all of a sudden we were in

00:03:20,230 --> 00:03:24,130
charge of metrics but there were a

00:03:22,330 --> 00:03:28,090
hundred different metric servers running

00:03:24,130 --> 00:03:30,460
in the company so what were all the

00:03:28,090 --> 00:03:32,500
different kinds of problems so like once

00:03:30,460 --> 00:03:35,080
a group got a Prometheus server up they

00:03:32,500 --> 00:03:36,400
never upgraded the version so if you

00:03:35,080 --> 00:03:38,020
wanted to run queries you would have

00:03:36,400 --> 00:03:40,840
different query functions on every

00:03:38,020 --> 00:03:42,490
single group server and then when we

00:03:40,840 --> 00:03:44,320
were startup so we're vastly quickly

00:03:42,490 --> 00:03:46,690
moving we would just lose Prometheus

00:03:44,320 --> 00:03:49,570
servers and we would refine them when we

00:03:46,690 --> 00:03:51,310
had issues or if the ops team was having

00:03:49,570 --> 00:03:53,260
a problem we just wouldn't know where

00:03:51,310 --> 00:03:58,330
where to look to find all the different

00:03:53,260 --> 00:04:00,640
metrics so we slowly tried to like fix

00:03:58,330 --> 00:04:05,440
this problem and this was like kind of

00:04:00,640 --> 00:04:08,350
the first step and what we found was

00:04:05,440 --> 00:04:10,870
console and Prometheus who here is using

00:04:08,350 --> 00:04:13,090
console in Prometheus yes

00:04:10,870 --> 00:04:15,550
like quarter of the audience hopefully I

00:04:13,090 --> 00:04:19,390
can convince to get like 3/4 of the

00:04:15,550 --> 00:04:22,360
audience that'd be awesome so the first

00:04:19,390 --> 00:04:25,450
step was was linking console with

00:04:22,360 --> 00:04:27,550
Prometheus which was like was really

00:04:25,450 --> 00:04:29,860
fundamental so basically what we did

00:04:27,550 --> 00:04:33,460
we got a set of centralized Prometheus

00:04:29,860 --> 00:04:35,710
servers we call our Pandora and we

00:04:33,460 --> 00:04:37,629
linked every cent every time a new micro

00:04:35,710 --> 00:04:40,330
service came up in the company they

00:04:37,629 --> 00:04:42,729
would have a console entry for the

00:04:40,330 --> 00:04:46,319
metrics endpoint so that way we only

00:04:42,729 --> 00:04:49,060
added the configs once into console and

00:04:46,319 --> 00:04:51,280
the Prometheus would get all the micro

00:04:49,060 --> 00:04:52,509
service instances because a lot of times

00:04:51,280 --> 00:04:55,120
we would be spinning up and down

00:04:52,509 --> 00:04:57,849
machines the ops teams can also if we

00:04:55,120 --> 00:05:00,099
have like a failed micro service they

00:04:57,849 --> 00:05:01,900
might have to spin up new boxes and that

00:05:00,099 --> 00:05:04,659
way the metrics are still being flowing

00:05:01,900 --> 00:05:07,719
into the Prometheus server so this is

00:05:04,659 --> 00:05:09,819
kind of like a diagram of how we started

00:05:07,719 --> 00:05:13,150
to get there so now at this point we

00:05:09,819 --> 00:05:14,710
have maybe about a dozen teams and we're

00:05:13,150 --> 00:05:17,979
monitoring a dozen different micro

00:05:14,710 --> 00:05:19,629
services with one Prometheus server now

00:05:17,979 --> 00:05:22,060
it's a rather large Prometheus server

00:05:19,629 --> 00:05:26,310
but it was it was still one and we could

00:05:22,060 --> 00:05:28,389
we could scale it out pretty quickly so

00:05:26,310 --> 00:05:31,000
everybody in the company was really

00:05:28,389 --> 00:05:32,830
happy so we were we were slowly now like

00:05:31,000 --> 00:05:34,810
there was a clear movement now that

00:05:32,830 --> 00:05:36,460
pretty much all the other metric systems

00:05:34,810 --> 00:05:39,550
were going away and we were going to

00:05:36,460 --> 00:05:44,740
move everything onto Prometheus ok just

00:05:39,550 --> 00:05:48,729
this one hammer for everything right so

00:05:44,740 --> 00:05:52,509
the stage two so the head of the ops

00:05:48,729 --> 00:05:53,949
team came to me and they said ok we want

00:05:52,509 --> 00:05:56,169
to monitor every single one of our

00:05:53,949 --> 00:05:59,409
hypervisors so digitalocean is a cloud

00:05:56,169 --> 00:06:01,569
and basically we resell virtual machines

00:05:59,409 --> 00:06:04,389
so we have tens of thousands of servers

00:06:01,569 --> 00:06:06,729
across about 11 data centers and we

00:06:04,389 --> 00:06:08,680
wanted to take Prometheus from now from

00:06:06,729 --> 00:06:11,080
like a hundred micro services that were

00:06:08,680 --> 00:06:12,969
monitoring to ten twenty thousand

00:06:11,080 --> 00:06:16,479
servers that we wanted to get metrics

00:06:12,969 --> 00:06:20,529
from we said cool we'll try it out and

00:06:16,479 --> 00:06:22,150
see what happens right so the first kind

00:06:20,529 --> 00:06:23,770
of the first way we experimented with

00:06:22,150 --> 00:06:26,319
this this this diagram is a bit hard to

00:06:23,770 --> 00:06:28,840
read but basically what we did was we we

00:06:26,319 --> 00:06:32,319
set up a single Prometheus server per

00:06:28,840 --> 00:06:34,389
per data center and we had that

00:06:32,319 --> 00:06:36,729
Prometheus server scrape every single

00:06:34,389 --> 00:06:39,639
physical node in the region using node

00:06:36,729 --> 00:06:41,080
exporter right and for some of the

00:06:39,639 --> 00:06:42,909
smaller regions

00:06:41,080 --> 00:06:45,909
this worked fine so if we had a thousand

00:06:42,909 --> 00:06:49,449
machines 2,000 machines we were able to

00:06:45,909 --> 00:06:51,490
actually scrape it some of the larger

00:06:49,449 --> 00:06:52,719
regions like our New York data center we

00:06:51,490 --> 00:06:55,449
actually have three data centers in New

00:06:52,719 --> 00:06:57,430
York we couldn't get 10,000 machines

00:06:55,449 --> 00:07:00,129
onto a single Prometheus server

00:06:57,430 --> 00:07:01,689
obviously so we're just like okay

00:07:00,129 --> 00:07:04,990
what what are all the kind of tricks

00:07:01,689 --> 00:07:06,699
that we can do so obviously the first

00:07:04,990 --> 00:07:08,349
thing we did was lower the retention

00:07:06,699 --> 00:07:10,930
window we're just gonna keep three days

00:07:08,349 --> 00:07:14,020
of data right like that's that's the

00:07:10,930 --> 00:07:16,389
first one we started to add new flags to

00:07:14,020 --> 00:07:18,159
the node exporter so we could drop

00:07:16,389 --> 00:07:20,319
metrics that weren't important

00:07:18,159 --> 00:07:22,419
so you see you'll see some of our polar

00:07:20,319 --> 00:07:24,190
crests on that and then we kept buying

00:07:22,419 --> 00:07:26,949
larger and larger machines for the

00:07:24,190 --> 00:07:28,750
single Prometheus instance until we kind

00:07:26,949 --> 00:07:33,219
of maxed out the machine that we could

00:07:28,750 --> 00:07:34,930
actually buy for the Prometheus and we

00:07:33,219 --> 00:07:36,729
still couldn't get our New York region

00:07:34,930 --> 00:07:38,020
so all of our other regions we could we

00:07:36,729 --> 00:07:39,610
could Wow

00:07:38,020 --> 00:07:45,550
all about a couple of the regions we

00:07:39,610 --> 00:07:48,639
could do so we said before we do that

00:07:45,550 --> 00:07:50,919
the the other thing is is the other

00:07:48,639 --> 00:07:52,509
thing we found out was that tuning is

00:07:50,919 --> 00:07:54,279
incredibly important there's actually a

00:07:52,509 --> 00:07:56,440
good tuning dock on the website that

00:07:54,279 --> 00:08:02,080
you'll skip but if you're running up

00:07:56,440 --> 00:08:03,610
Prometheus I didn't see it I actually

00:08:02,080 --> 00:08:04,930
had to have the Prometheus guys come to

00:08:03,610 --> 00:08:08,889
our office and tell us that there was a

00:08:04,930 --> 00:08:11,379
tuning guide so yes this guy sat with me

00:08:08,889 --> 00:08:14,349
so that was the only reason we were able

00:08:11,379 --> 00:08:16,060
to scale it initially so whatever I

00:08:14,349 --> 00:08:17,500
recommend if you're running Prometheus

00:08:16,060 --> 00:08:19,599
on a server that's more than eight

00:08:17,500 --> 00:08:21,759
gigabytes of RAM which I hope you

00:08:19,599 --> 00:08:24,039
probably are you really have to tweak

00:08:21,759 --> 00:08:25,690
each one of these settings otherwise

00:08:24,039 --> 00:08:28,539
you're not gonna use the full machine

00:08:25,690 --> 00:08:30,159
and there's really good guides I'm not

00:08:28,539 --> 00:08:32,019
gonna go deep into it but if you want to

00:08:30,159 --> 00:08:34,779
grab me I'm happy to show you like how

00:08:32,019 --> 00:08:36,250
we configure these things and how we how

00:08:34,779 --> 00:08:41,079
we actually optimized a lot of these

00:08:36,250 --> 00:08:43,329
tuning options so the first thing we did

00:08:41,079 --> 00:08:45,610
was so now that we wanted to tackle the

00:08:43,329 --> 00:08:46,630
big regions we started to shard the

00:08:45,610 --> 00:08:48,310
Prometheus servers

00:08:46,630 --> 00:08:52,209
is anybody here using sharding with

00:08:48,310 --> 00:08:54,130
Prometheus ok only about four people

00:08:52,209 --> 00:08:57,759
five people

00:08:54,130 --> 00:08:59,440
okay yeah no I understand it's it's

00:08:57,759 --> 00:09:02,920
still a bit rough it's still a bit rough

00:08:59,440 --> 00:09:05,290
on the sharding so if you're not

00:09:02,920 --> 00:09:08,529
familiar with sharding essentially in

00:09:05,290 --> 00:09:10,720
prometheus you can configure a label

00:09:08,529 --> 00:09:13,029
that you want the Prometheus servers to

00:09:10,720 --> 00:09:15,519
shard against so what we ended up doing

00:09:13,029 --> 00:09:17,440
was having something like chef deploy

00:09:15,519 --> 00:09:19,750
multiple Prometheus servers and then

00:09:17,440 --> 00:09:22,779
tell each Prometheus server which node

00:09:19,750 --> 00:09:24,759
in the shard it was and we were able to

00:09:22,779 --> 00:09:26,740
spin up like three Prometheus servers in

00:09:24,759 --> 00:09:29,470
the region and we were able to scrape

00:09:26,740 --> 00:09:31,269
about 10,000 hypervisors and this

00:09:29,470 --> 00:09:34,089
actually worked we were able to scale

00:09:31,269 --> 00:09:37,810
this and we were able to get this on to

00:09:34,089 --> 00:09:38,920
about three servers right but then here

00:09:37,810 --> 00:09:41,889
comes the problem

00:09:38,920 --> 00:09:43,810
there's no way to query the data if you

00:09:41,889 --> 00:09:48,160
don't know where the data is on the

00:09:43,810 --> 00:09:50,319
different shards so we wrote this thing

00:09:48,160 --> 00:09:53,110
in an afternoon literally called the

00:09:50,319 --> 00:09:56,769
Prometheus proxy and basically the

00:09:53,110 --> 00:09:59,439
Prometheus proxy is a sharding proxy so

00:09:56,769 --> 00:10:02,230
for our internal users that digital

00:09:59,439 --> 00:10:04,480
ocean we have a giant graph on instance

00:10:02,230 --> 00:10:06,759
and the graph Donna instance points that

00:10:04,480 --> 00:10:08,680
the Prometheus proxy and the Prometheus

00:10:06,759 --> 00:10:11,170
proxy will determine will actually parse

00:10:08,680 --> 00:10:15,449
the Prometheus queries and determine

00:10:11,170 --> 00:10:17,920
which shard to get the data from and

00:10:15,449 --> 00:10:20,290
that's how we are actually able to like

00:10:17,920 --> 00:10:22,209
scale out and we could we could spin up

00:10:20,290 --> 00:10:23,860
like six or eight different Prometheus

00:10:22,209 --> 00:10:25,630
servers and the Prometheus proxy would

00:10:23,860 --> 00:10:27,939
handle it and graph on I wouldn't have

00:10:25,630 --> 00:10:33,819
to know because graph anna has no idea

00:10:27,939 --> 00:10:35,439
of charting or anything like that so it

00:10:33,819 --> 00:10:37,980
was really cool so if you took like a

00:10:35,439 --> 00:10:40,149
random Prometheus query like this

00:10:37,980 --> 00:10:42,130
basically what would the Prometheus

00:10:40,149 --> 00:10:44,740
proxy would do would be break down each

00:10:42,130 --> 00:10:47,019
part of the Prometheus query and it

00:10:44,740 --> 00:10:49,899
would pick a label so for instance we

00:10:47,019 --> 00:10:52,120
were using the instance label and it

00:10:49,899 --> 00:10:54,370
would actually determine which of the

00:10:52,120 --> 00:10:56,259
nodes of the shard for that instance it

00:10:54,370 --> 00:10:57,630
would go to and that way you could have

00:10:56,259 --> 00:11:00,310
things that didn't know anything about

00:10:57,630 --> 00:11:06,550
Prometheus sharding could use could use

00:11:00,310 --> 00:11:08,120
it so then the next issue is where this

00:11:06,550 --> 00:11:09,950
was good so we could finally

00:11:08,120 --> 00:11:12,650
monitor all of our data centers with

00:11:09,950 --> 00:11:14,839
Prometheus we could have 10,000

00:11:12,650 --> 00:11:17,570
hypervisors monitored and we are like

00:11:14,839 --> 00:11:19,610
cool now I want to alert I want to alert

00:11:17,570 --> 00:11:23,210
cuz we hated Nadia's I don't know it

00:11:19,610 --> 00:11:26,750
does anybody here hate not yes yes whole

00:11:23,210 --> 00:11:32,089
room hits not just that was kind of a

00:11:26,750 --> 00:11:34,040
loaded question so we were like maybe we

00:11:32,089 --> 00:11:36,320
could get rid of this Nadia's thing if

00:11:34,040 --> 00:11:37,880
we just used the alert manager and we

00:11:36,320 --> 00:11:40,790
did things like could we get alerting on

00:11:37,880 --> 00:11:41,990
this space so at least a year ago when

00:11:40,790 --> 00:11:43,940
we initially looked at the alerting

00:11:41,990 --> 00:11:46,940
manager there was no really good way of

00:11:43,940 --> 00:11:48,980
doing a high availability set up and for

00:11:46,940 --> 00:11:52,910
our set up there is no option to not

00:11:48,980 --> 00:11:55,430
have high availability so what we ended

00:11:52,910 --> 00:11:58,339
up doing was we took a fork of the alert

00:11:55,430 --> 00:12:01,880
manager and we backed it with just a my

00:11:58,339 --> 00:12:04,070
sequel database it was really simple so

00:12:01,880 --> 00:12:06,110
basically we have the alert manager

00:12:04,070 --> 00:12:08,720
behind a load balancer and every alert

00:12:06,110 --> 00:12:10,580
goes into the my sequel database and if

00:12:08,720 --> 00:12:13,279
it's already there it can do D duping

00:12:10,580 --> 00:12:15,560
and that way we can have multiple alert

00:12:13,279 --> 00:12:18,200
managers going and then the clusters not

00:12:15,560 --> 00:12:19,730
going to fall over I think I looked at

00:12:18,200 --> 00:12:21,890
some of the new commits it looks like

00:12:19,730 --> 00:12:23,480
there's actually now an interface to do

00:12:21,890 --> 00:12:26,600
this properly when we originally did

00:12:23,480 --> 00:12:28,700
this we've really had a hack it in and I

00:12:26,600 --> 00:12:30,290
think there's like a mesh interface that

00:12:28,700 --> 00:12:33,680
somebody's adding which looks kind of

00:12:30,290 --> 00:12:34,880
cool but this was actually able that we

00:12:33,680 --> 00:12:36,470
actually were able to get this into

00:12:34,880 --> 00:12:39,050
production and work pretty well and we

00:12:36,470 --> 00:12:41,240
could run this at a scale of you know

00:12:39,050 --> 00:12:50,510
thousands of alerts an hour no problem

00:12:41,240 --> 00:12:52,279
you know yes so just to go back so now

00:12:50,510 --> 00:12:55,520
we've been running the sharding for like

00:12:52,279 --> 00:12:57,529
a month or two and now we start to see

00:12:55,520 --> 00:13:00,650
all the real kind of gotchas with the

00:12:57,529 --> 00:13:02,839
sharding right what happens when you

00:13:00,650 --> 00:13:04,520
need to grow the cluster so as we were

00:13:02,839 --> 00:13:07,459
growing as a business we needed more

00:13:04,520 --> 00:13:09,830
Prometheus servers well if you want to

00:13:07,459 --> 00:13:11,240
add a shard you've now just lost all of

00:13:09,830 --> 00:13:12,740
your history right because the

00:13:11,240 --> 00:13:16,760
distribution of the shard you have

00:13:12,740 --> 00:13:18,740
mission data so we said okay the way

00:13:16,760 --> 00:13:20,900
we'll solve it is we'll just spin up ten

00:13:18,740 --> 00:13:21,350
Prometheus servers instead of three in

00:13:20,900 --> 00:13:24,020
every

00:13:21,350 --> 00:13:25,970
region so we ended up having huge

00:13:24,020 --> 00:13:28,870
amounts of these kind of like wasted

00:13:25,970 --> 00:13:31,250
Prometheus servers kind of laying around

00:13:28,870 --> 00:13:34,700
the other thing is is if we lost the

00:13:31,250 --> 00:13:36,560
single node we were losing data and we

00:13:34,700 --> 00:13:38,540
still had very limited data windows so

00:13:36,560 --> 00:13:42,500
we were still operating in under like a

00:13:38,540 --> 00:13:44,630
week of data for most of our servers so

00:13:42,500 --> 00:13:50,360
it's the sharding still didn't solve a

00:13:44,630 --> 00:13:51,890
lot of our problems okay so then we were

00:13:50,360 --> 00:13:55,940
like okay how could we at least solve

00:13:51,890 --> 00:13:58,820
the data integrity issue so we took it

00:13:55,940 --> 00:14:02,120
we took a step back and we said what if

00:13:58,820 --> 00:14:06,050
we store all the metrics into Kafka

00:14:02,120 --> 00:14:08,120
before they come into prometheus so we

00:14:06,050 --> 00:14:11,030
rebuilt a new version of the push

00:14:08,120 --> 00:14:13,060
gateway and a special version of

00:14:11,030 --> 00:14:15,380
Prometheus that was a scraper only and

00:14:13,060 --> 00:14:20,030
we had both the pushed gateway and the

00:14:15,380 --> 00:14:21,860
scraper push data into a Kafka and we

00:14:20,030 --> 00:14:23,660
did a special fork of Prometheus that

00:14:21,860 --> 00:14:26,300
was actually reading the data from Kafka

00:14:23,660 --> 00:14:28,190
into memory so if we wanted to rebuild

00:14:26,300 --> 00:14:30,110
that if we wanted to rebuild the

00:14:28,190 --> 00:14:32,120
Prometheus instance we could rebuild the

00:14:30,110 --> 00:14:33,920
data from Kafka or if we wanted to

00:14:32,120 --> 00:14:37,580
rebuild the shard cluster we could

00:14:33,920 --> 00:14:39,470
rebuild it from the whole Kafka so this

00:14:37,580 --> 00:14:41,150
was a really kind of a fun experiment we

00:14:39,470 --> 00:14:43,160
never actually went live with this

00:14:41,150 --> 00:14:44,720
because we ended up going with a much

00:14:43,160 --> 00:14:48,440
better solution but it was kind of an

00:14:44,720 --> 00:14:52,150
interesting half step of getting being

00:14:48,440 --> 00:14:52,150
having some kind of a reliability

00:14:54,190 --> 00:15:00,280
so kind of the main project that I've

00:14:56,830 --> 00:15:03,670
been tasked for was giving metrics to

00:15:00,280 --> 00:15:05,500
our customers so we have we have a we

00:15:03,670 --> 00:15:07,330
have something in the low millions of

00:15:05,500 --> 00:15:09,790
customers right now like uh actual

00:15:07,330 --> 00:15:11,890
virtual machines and for each virtual

00:15:09,790 --> 00:15:13,720
machine we need to give very standard

00:15:11,890 --> 00:15:15,460
metrics and in fact we want to give like

00:15:13,720 --> 00:15:18,820
custom metrics and stuff to our end

00:15:15,460 --> 00:15:21,820
users so we said we really wanted to

00:15:18,820 --> 00:15:24,490
power this whole UI with Prometheus and

00:15:21,820 --> 00:15:27,100
we said okay how can we move from the

00:15:24,490 --> 00:15:29,410
tens of thousands to a million servers

00:15:27,100 --> 00:15:31,270
with Prometheus and at the current

00:15:29,410 --> 00:15:32,740
moment with the Chardon solution it just

00:15:31,270 --> 00:15:34,510
didn't seem like that was going to be a

00:15:32,740 --> 00:15:38,200
possible it was going to really be

00:15:34,510 --> 00:15:39,520
possible so we said we were looking we

00:15:38,200 --> 00:15:40,690
said oh we're gonna have to dump

00:15:39,520 --> 00:15:45,130
Prometheus we're gonna have to do

00:15:40,690 --> 00:15:48,970
something else so we started to dig into

00:15:45,130 --> 00:15:53,020
it and up and we started to build some

00:15:48,970 --> 00:15:55,870
new tools we built a special version of

00:15:53,020 --> 00:15:59,350
the node exporter but it's a reverse

00:15:55,870 --> 00:16:01,450
node exporter and basically what it is

00:15:59,350 --> 00:16:03,370
is it's a node exporter that we can give

00:16:01,450 --> 00:16:06,070
the customers and it authenticates and

00:16:03,370 --> 00:16:07,270
it pushes data to digital ocean so if

00:16:06,070 --> 00:16:09,850
you want to get metrics about your

00:16:07,270 --> 00:16:11,320
virtual machine or your my sequel or

00:16:09,850 --> 00:16:13,240
stuff you install it on your virtual

00:16:11,320 --> 00:16:16,560
machine and it pushes into a special

00:16:13,240 --> 00:16:19,870
push gateway that is authenticated and

00:16:16,560 --> 00:16:21,400
the first reason we did this is that way

00:16:19,870 --> 00:16:24,130
we can out we can have like a funnel

00:16:21,400 --> 00:16:27,160
into Prometheus so we can have the push

00:16:24,130 --> 00:16:30,280
gateway actually be kind of like this

00:16:27,160 --> 00:16:31,990
push back into Prometheus I'm leaking

00:16:30,280 --> 00:16:34,000
authenticate because right now there's

00:16:31,990 --> 00:16:36,220
no really good way to do if you have

00:16:34,000 --> 00:16:38,080
untrusted Prometheus entries there's no

00:16:36,220 --> 00:16:43,630
good way to like have trust

00:16:38,080 --> 00:16:46,240
relationships we also built a special

00:16:43,630 --> 00:16:49,060
querying API on top of Prometheus

00:16:46,240 --> 00:16:52,150
where we built a gr PC and an HTTP

00:16:49,060 --> 00:16:54,670
server for customers and what we do is

00:16:52,150 --> 00:16:56,260
we limit the amount of Prometheus

00:16:54,670 --> 00:16:58,600
queries that they can actually go into

00:16:56,260 --> 00:17:00,190
the Prometheus back-end so we can still

00:16:58,600 --> 00:17:02,560
take advantage of all the nice query

00:17:00,190 --> 00:17:04,510
query capabilities without people

00:17:02,560 --> 00:17:06,089
potentially stealing data from different

00:17:04,510 --> 00:17:10,390
customs

00:17:06,089 --> 00:17:11,830
and I don't know if you saw Twitter but

00:17:10,390 --> 00:17:15,580
we just came out with this new thing

00:17:11,830 --> 00:17:17,470
called Vulcan basically Vulcan is kind

00:17:15,580 --> 00:17:20,200
of our Fork of Prometheus right now and

00:17:17,470 --> 00:17:22,270
the way to think of this is that Vulcan

00:17:20,200 --> 00:17:25,810
is kind of our experiment with

00:17:22,270 --> 00:17:28,000
Prometheus and what I hope is over time

00:17:25,810 --> 00:17:30,490
these pieces slowly but surely move back

00:17:28,000 --> 00:17:33,040
into the core Prometheus and that Vulcan

00:17:30,490 --> 00:17:35,590
just could go away but for now like this

00:17:33,040 --> 00:17:37,930
is kind of how we're we're actually

00:17:35,590 --> 00:17:41,440
scaling Prometheus and what what is

00:17:37,930 --> 00:17:43,480
Vulcan it's completely the Prometheus

00:17:41,440 --> 00:17:46,030
API in fact we use the entire code

00:17:43,480 --> 00:17:48,700
Prometheus codebase except that instead

00:17:46,030 --> 00:17:52,660
of a local store we use a Cassandra back

00:17:48,700 --> 00:17:55,390
store and instead of just regular we

00:17:52,660 --> 00:17:59,200
have regular scrapers but all the

00:17:55,390 --> 00:18:01,450
scrapers go into Kafka and we have

00:17:59,200 --> 00:18:04,540
special Kafka riders from Kafka into

00:18:01,450 --> 00:18:06,490
Cassandra so that way the Prometheus

00:18:04,540 --> 00:18:09,130
server can scale out with a centralized

00:18:06,490 --> 00:18:12,310
database and can actually scale up to

00:18:09,130 --> 00:18:14,140
like a million machines now right now

00:18:12,310 --> 00:18:17,350
we've had a like limit the amount of

00:18:14,140 --> 00:18:20,170
query types we need to do we can do

00:18:17,350 --> 00:18:22,330
against the Cassandra store but that's

00:18:20,170 --> 00:18:24,310
what we're gonna be scaling over time

00:18:22,330 --> 00:18:25,900
and I saw like some of the latest

00:18:24,310 --> 00:18:27,340
commits and master it looks like some

00:18:25,900 --> 00:18:30,160
other people must be working on some

00:18:27,340 --> 00:18:31,810
similar stuff because all the interfaces

00:18:30,160 --> 00:18:36,280
and local storage changed I was like ah

00:18:31,810 --> 00:18:37,840
that's quite odd so hopefully if you're

00:18:36,280 --> 00:18:41,770
working on something you should grab me

00:18:37,840 --> 00:18:43,300
maybe we can we can all collaborate so

00:18:41,770 --> 00:18:45,340
here's kind of a diagram of how our

00:18:43,300 --> 00:18:48,220
Cassandra back store works well this is

00:18:45,340 --> 00:18:51,330
a bit fuzzy but basically what's

00:18:48,220 --> 00:18:54,910
happening is in each region we have

00:18:51,330 --> 00:18:58,450
metrics scrapers and push gateways and

00:18:54,910 --> 00:19:01,270
they're all pushing data into a path

00:18:58,450 --> 00:19:03,130
cast or we have a special reader from

00:19:01,270 --> 00:19:07,240
the cat's core that's driving directly

00:19:03,130 --> 00:19:10,240
into Cassandra and then we have a

00:19:07,240 --> 00:19:11,980
special version of Prometheus that can

00:19:10,240 --> 00:19:15,880
read from the that can read from the

00:19:11,980 --> 00:19:17,050
Cassandra store so Prometheus really

00:19:15,880 --> 00:19:18,460
doesn't need to know anything about

00:19:17,050 --> 00:19:20,980
Kafka but it allows

00:19:18,460 --> 00:19:24,630
- it does and then it just it only does

00:19:20,980 --> 00:19:24,630
reading now so it does no writing

00:19:26,000 --> 00:19:33,260
and now we can start doing cool features

00:19:29,300 --> 00:19:35,510
like down sampling right and we do a

00:19:33,260 --> 00:19:39,230
really crazy half of how we do down

00:19:35,510 --> 00:19:41,930
sampling so what I do is I run it in

00:19:39,230 --> 00:19:44,540
memory version of Prometheus and I feed

00:19:41,930 --> 00:19:46,340
it the data from Kafka and I only keep

00:19:44,540 --> 00:19:48,200
it for the period I need and then I just

00:19:46,340 --> 00:19:50,240
make queries against it and I use those

00:19:48,200 --> 00:19:52,640
queries and I write them into Cassandra

00:19:50,240 --> 00:19:55,640
so the down sampling engine is actually

00:19:52,640 --> 00:19:57,410
Prometheus so it would be really easy to

00:19:55,640 --> 00:19:59,210
probably to take the down sampling we've

00:19:57,410 --> 00:20:02,270
done and kind of rebuild it into the

00:19:59,210 --> 00:20:05,060
core Prometheus server because we needed

00:20:02,270 --> 00:20:07,490
to get intervals of up to 18 months and

00:20:05,060 --> 00:20:13,850
at our size we couldn't we couldn't

00:20:07,490 --> 00:20:17,570
store raw data at that size the other

00:20:13,850 --> 00:20:20,600
kind of really fun thing is we built an

00:20:17,570 --> 00:20:21,560
in-memory alerting system and this is

00:20:20,600 --> 00:20:23,570
kind of something that I'm actually

00:20:21,560 --> 00:20:26,090
working on right now it's not this part

00:20:23,570 --> 00:20:27,770
it's not open source yet but I really

00:20:26,090 --> 00:20:28,970
wanted the alerting system to be really

00:20:27,770 --> 00:20:30,890
high availability because we're gonna

00:20:28,970 --> 00:20:32,990
start giving it to customers so

00:20:30,890 --> 00:20:35,300
customers can say like if the disk of

00:20:32,990 --> 00:20:36,980
their virtual machine fails so now we

00:20:35,300 --> 00:20:39,800
have millions of machines that we need

00:20:36,980 --> 00:20:42,590
to alert on so what we've done is we

00:20:39,800 --> 00:20:46,730
take a very standard in-memory version

00:20:42,590 --> 00:20:49,460
of Prometheus and we have it scrape a

00:20:46,730 --> 00:20:51,590
Kafka source into memory and we just

00:20:49,460 --> 00:20:54,050
have standard alerting rules in there

00:20:51,590 --> 00:20:56,660
and the alerting role is just push into

00:20:54,050 --> 00:20:59,420
another Kafka system so we can actually

00:20:56,660 --> 00:21:02,270
reuse a huge portion of the actual of

00:20:59,420 --> 00:21:04,700
the Prometheus system and make it high

00:21:02,270 --> 00:21:09,280
availability because each chunk is just

00:21:04,700 --> 00:21:09,280
reading a different cat gita' partition

00:21:11,960 --> 00:21:20,340
so what what is the future some of the

00:21:17,370 --> 00:21:23,970
things that I would like I would love to

00:21:20,340 --> 00:21:26,580
see is new scraping sources inside of

00:21:23,970 --> 00:21:28,170
prometheus so right now it's quite

00:21:26,580 --> 00:21:31,620
difficult to scrape things that aren't

00:21:28,170 --> 00:21:33,570
HTTP so I would love to see a way for

00:21:31,620 --> 00:21:34,800
things like casaya for Kafka to be

00:21:33,570 --> 00:21:37,710
actually scraped into the core

00:21:34,800 --> 00:21:42,030
prometheus server that would actually

00:21:37,710 --> 00:21:43,800
open up a lot of opportunities one of

00:21:42,030 --> 00:21:48,660
the biggest issues we've seen is per

00:21:43,800 --> 00:21:50,460
series expire ease of time series it

00:21:48,660 --> 00:21:52,350
seems like the plugin storage stuff is

00:21:50,460 --> 00:21:53,640
being worked on I know some people on

00:21:52,350 --> 00:21:56,190
the mailing list have been talking about

00:21:53,640 --> 00:21:57,930
using remote storage but we ended up

00:21:56,190 --> 00:21:59,550
using the local interface and we found

00:21:57,930 --> 00:22:02,190
that that ended up working a lot better

00:21:59,550 --> 00:22:07,080
for us and really having high

00:22:02,190 --> 00:22:10,500
availability alerting so to kind of wrap

00:22:07,080 --> 00:22:11,880
it up I talked a lot of kind of about

00:22:10,500 --> 00:22:14,580
the future of where we're going with

00:22:11,880 --> 00:22:16,440
Cassandra and this but for the vast

00:22:14,580 --> 00:22:18,750
majority of the people here I don't

00:22:16,440 --> 00:22:20,070
think you need that yet maybe you'll

00:22:18,750 --> 00:22:22,110
need that in a year or two when you've

00:22:20,070 --> 00:22:24,060
really grown with the Prometheus but I

00:22:22,110 --> 00:22:26,130
kind of wanted to illustrate like the

00:22:24,060 --> 00:22:29,160
different scales that we had and things

00:22:26,130 --> 00:22:32,160
like the sharding or a lesser spent

00:22:29,160 --> 00:22:34,170
Federation should be able to handle much

00:22:32,160 --> 00:22:36,180
smaller loads of even thousands of

00:22:34,170 --> 00:22:37,950
servers so you don't have to feel like

00:22:36,180 --> 00:22:42,450
you have to do a very custom solution

00:22:37,950 --> 00:22:45,350
yet but we're always looking for help

00:22:42,450 --> 00:22:48,920
we've just open sourced our Vulcan so

00:22:45,350 --> 00:22:53,390
please please do some complete reading

00:22:48,920 --> 00:22:53,390
and that's it

00:22:59,900 --> 00:23:07,480
all right that was a lot of stuff I

00:23:02,600 --> 00:23:07,480
guess there are some questions okay

00:23:09,820 --> 00:23:16,070
could you describe the cassandra' schema

00:23:12,500 --> 00:23:17,630
please could you describe the cassandra'

00:23:16,070 --> 00:23:20,960
schema that you use to store points on

00:23:17,630 --> 00:23:21,490
the ridge points oh this is sander

00:23:20,960 --> 00:23:24,290
schemer

00:23:21,490 --> 00:23:26,809
yes so the Cassander schema basically

00:23:24,290 --> 00:23:28,670
right now it's really simple we

00:23:26,809 --> 00:23:30,440
basically have just a couple tables we

00:23:28,670 --> 00:23:32,500
have a table that describes all the time

00:23:30,440 --> 00:23:36,500
series and then we have another one that

00:23:32,500 --> 00:23:39,110
per time series key we actually have all

00:23:36,500 --> 00:23:41,059
the the data points for that time series

00:23:39,110 --> 00:23:43,580
and then we break it down by time

00:23:41,059 --> 00:23:47,140
periods for down sampling we actually

00:23:43,580 --> 00:23:47,140
have separate tables for down sampling

00:23:51,040 --> 00:23:57,370
you spoke about Vulcan your Fork of

00:23:53,870 --> 00:24:00,290
Prometheus how do you handle it with

00:23:57,370 --> 00:24:02,390
Prometheus upstream do you merge changes

00:24:00,290 --> 00:24:04,809
that are in parameters into Vulcan to

00:24:02,390 --> 00:24:08,830
keep compatibility or is Vulcan

00:24:04,809 --> 00:24:11,780
completely separate for now yeah so

00:24:08,830 --> 00:24:15,770
we've not been great about this because

00:24:11,780 --> 00:24:17,300
I was trying to pull down stream the

00:24:15,770 --> 00:24:19,730
other day and all the interface has

00:24:17,300 --> 00:24:22,970
changed but we are we are trying to keep

00:24:19,730 --> 00:24:25,280
ours at least downstream of Prometheus

00:24:22,970 --> 00:24:27,170
so that way like when we start to want

00:24:25,280 --> 00:24:29,960
to contribute stuff backwards we'll be

00:24:27,170 --> 00:24:31,880
in a good shape we haven't moved

00:24:29,960 --> 00:24:33,650
anything backwards because this has been

00:24:31,880 --> 00:24:35,750
a large experiment we weren't even

00:24:33,650 --> 00:24:38,650
really sure if this was going to work at

00:24:35,750 --> 00:24:38,650
all so actually

00:24:40,820 --> 00:24:47,120
hi can you talk more about the push

00:24:44,879 --> 00:24:49,830
gateway and how that factored into

00:24:47,120 --> 00:24:53,220
authenticating for multi-tenancy the

00:24:49,830 --> 00:24:56,639
comments right so that yes so the

00:24:53,220 --> 00:24:58,590
problem we have is that the standard

00:24:56,639 --> 00:25:00,990
push gateway allows you to send any

00:24:58,590 --> 00:25:04,019
metric in right so one customer could

00:25:00,990 --> 00:25:07,110
pretend to be another customer right so

00:25:04,019 --> 00:25:10,289
what we do is we have we have a API

00:25:07,110 --> 00:25:12,029
token that the agent sends to the our

00:25:10,289 --> 00:25:15,149
special version of the push gateway and

00:25:12,029 --> 00:25:18,179
it limits the metrics you send to only

00:25:15,149 --> 00:25:21,120
machines that you own so we actually tag

00:25:18,179 --> 00:25:23,039
the the machine ID so that way they

00:25:21,120 --> 00:25:25,909
can't they can only they can only spam

00:25:23,039 --> 00:25:25,909
their own metrics

00:25:29,040 --> 00:25:34,390
I had a question about the reverse node

00:25:32,380 --> 00:25:36,190
exporter is that something you is in the

00:25:34,390 --> 00:25:39,780
Vulcan source or is that open source at

00:25:36,190 --> 00:25:42,760
all Carlos did we open source that yeah

00:25:39,780 --> 00:25:45,100
we're well it's on the list at the open

00:25:42,760 --> 00:25:47,710
source it's not part of Vulcan because

00:25:45,100 --> 00:25:50,650
it actually works with the standard it

00:25:47,710 --> 00:25:52,480
works with standard Prometheus server so

00:25:50,650 --> 00:26:04,810
we it's definitely like very high on our

00:25:52,480 --> 00:26:10,060
list to open source either um what do

00:26:04,810 --> 00:26:14,140
you shot on in Kafka right now we're

00:26:10,060 --> 00:26:16,210
doing it on customer but the other way

00:26:14,140 --> 00:26:18,280
you would do it is probably machine so

00:26:16,210 --> 00:26:21,220
typically for us we never need to do

00:26:18,280 --> 00:26:22,810
queries across customers and internally

00:26:21,220 --> 00:26:26,770
we never need to do cruise across

00:26:22,810 --> 00:26:28,060
machines typically there is some I think

00:26:26,770 --> 00:26:32,560
somebody in the front here and the

00:26:28,060 --> 00:26:34,810
second rather you said that imprinting

00:26:32,560 --> 00:26:36,880
using the local storage interface was

00:26:34,810 --> 00:26:39,460
easier than the remote storage interface

00:26:36,880 --> 00:26:40,720
did you use the remote storage interface

00:26:39,460 --> 00:26:43,630
and can you talk a bit more about that

00:26:40,720 --> 00:26:46,810
what problems you encountered all right

00:26:43,630 --> 00:26:50,080
so we kind of wanted our server to be

00:26:46,810 --> 00:26:52,510
very well integrated into Prometheus the

00:26:50,080 --> 00:26:54,310
problem is we weren't

00:26:52,510 --> 00:26:56,950
I think the remote storage interface is

00:26:54,310 --> 00:26:58,840
really for writing there's not a great

00:26:56,950 --> 00:27:00,790
story on how to read from the remote

00:26:58,840 --> 00:27:04,180
storage interface and we mostly wanted

00:27:00,790 --> 00:27:05,380
to read so we really needed to use the

00:27:04,180 --> 00:27:07,510
local storage was really the only

00:27:05,380 --> 00:27:10,450
interface that could could do all the

00:27:07,510 --> 00:27:15,760
functionality that's actually no story

00:27:10,450 --> 00:27:18,130
of I'm reading from I want to ask

00:27:15,760 --> 00:27:19,600
instead of using authentication and some

00:27:18,130 --> 00:27:21,040
of the other things you mentioned it

00:27:19,600 --> 00:27:25,470
wasn't a possibility to chest

00:27:21,040 --> 00:27:28,240
force per user customer label that could

00:27:25,470 --> 00:27:32,050
separate the data yes so we do have we

00:27:28,240 --> 00:27:34,210
do force a per customer label but we

00:27:32,050 --> 00:27:36,100
needed to get it still so that way like

00:27:34,210 --> 00:27:38,170
random people on the internet because

00:27:36,100 --> 00:27:39,970
it's internet facing service so you

00:27:38,170 --> 00:27:41,650
couldn't just spin up

00:27:39,970 --> 00:27:44,040
start writing random metrics to our

00:27:41,650 --> 00:27:44,040
employ

00:27:45,120 --> 00:27:52,480
yeah hi you mentioned that you had

00:27:49,290 --> 00:27:57,250
scrapers local to the data center

00:27:52,480 --> 00:28:00,690
pushing them into Vulcan I guess is that

00:27:57,250 --> 00:28:04,570
possible to purchase right into vanilla

00:28:00,690 --> 00:28:06,910
Prometheus set up as well right so let

00:28:04,570 --> 00:28:09,190
me make sure I understand so we have we

00:28:06,910 --> 00:28:11,020
in regular Prometheus you can run

00:28:09,190 --> 00:28:12,850
scrapers per data center and that's

00:28:11,020 --> 00:28:15,340
actually how we run our typical

00:28:12,850 --> 00:28:17,230
Prometheus setups and what you would

00:28:15,340 --> 00:28:19,990
normally do is you would just have

00:28:17,230 --> 00:28:22,030
something like graph on ax in the front

00:28:19,990 --> 00:28:23,530
that would kind of mask the fact that

00:28:22,030 --> 00:28:27,010
they're in different data centers and

00:28:23,530 --> 00:28:28,750
that's the best way and and we just do

00:28:27,010 --> 00:28:33,010
it one step further where our scrapers

00:28:28,750 --> 00:28:35,460
will dump to a Kafka okay and then it's

00:28:33,010 --> 00:28:38,559
not possible to feed that into a vanilla

00:28:35,460 --> 00:28:40,750
Prometheus that's why you so the regular

00:28:38,559 --> 00:28:42,309
Prometheus supports this - so like a

00:28:40,750 --> 00:28:46,360
regular Prometheus you can just scrape

00:28:42,309 --> 00:28:56,410
per region and and just use the UI to

00:28:46,360 --> 00:28:58,660
mask that okay thank you at the

00:28:56,410 --> 00:29:02,440
beginning of your talk you said you

00:28:58,660 --> 00:29:06,450
inherited a lot of open TCB things that

00:29:02,440 --> 00:29:10,330
you were just taught and how about the

00:29:06,450 --> 00:29:11,530
historical data in there yeah that's a

00:29:10,330 --> 00:29:14,650
good question

00:29:11,530 --> 00:29:17,470
so the cluster was kind of unmanaged so

00:29:14,650 --> 00:29:19,270
historical data we have very little

00:29:17,470 --> 00:29:22,000
pieces of historical data that are

00:29:19,270 --> 00:29:23,860
important to us things like CPU and

00:29:22,000 --> 00:29:25,929
memory usage aren't typically important

00:29:23,860 --> 00:29:28,059
to our business things like the wear of

00:29:25,929 --> 00:29:31,179
the servers are like if we have certain

00:29:28,059 --> 00:29:34,179
parts that are dying but we weren't

00:29:31,179 --> 00:29:36,010
tracking a lot of that in open TS TV and

00:29:34,179 --> 00:29:37,630
that was one of the big reasons why we

00:29:36,010 --> 00:29:39,940
needed to do this Kafka because there

00:29:37,630 --> 00:29:42,960
were pieces of data that were costing

00:29:39,940 --> 00:29:42,960
the business a lot of money

00:29:44,570 --> 00:29:52,889
okay any other questions last one how

00:29:51,720 --> 00:29:54,619
BIG's your Cassandra cluster I'm what

00:29:52,889 --> 00:29:56,999
kind of keep yes you pushing through it

00:29:54,619 --> 00:29:59,489
how big is your Cassandra cluster and

00:29:56,999 --> 00:30:03,539
what kind of QPS are you pushing through

00:29:59,489 --> 00:30:07,169
it so we have our initial cluster is

00:30:03,539 --> 00:30:08,519
just ten nodes but we're doing we're

00:30:07,169 --> 00:30:10,710
it's a pretty small metric set so

00:30:08,519 --> 00:30:14,369
basically we're only giving 40 metrics

00:30:10,710 --> 00:30:16,909
per machine and it's like about we're

00:30:14,369 --> 00:30:19,590
scaling up to about a million machines

00:30:16,909 --> 00:30:21,179
so that's still quite small as we start

00:30:19,590 --> 00:30:23,220
to roll out things like allowing the

00:30:21,179 --> 00:30:25,440
customers to add arbitrary metrics we're

00:30:23,220 --> 00:30:29,249
gonna have to scale out the cluster is

00:30:25,440 --> 00:30:31,619
exact GPS I have no idea we're not even

00:30:29,249 --> 00:30:38,940
barely scratching at end o Cassandra

00:30:31,619 --> 00:30:41,729
cluster okay anybody I can ask myself a

00:30:38,940 --> 00:30:43,889
question the the Prometheus proxy that

00:30:41,729 --> 00:30:46,349
just the shouting in the initial setup

00:30:43,889 --> 00:30:48,419
if you do anything that would aggregate

00:30:46,349 --> 00:30:52,379
across those shouting labels it would

00:30:48,419 --> 00:30:54,419
just it just errors and in our case we

00:30:52,379 --> 00:30:57,690
didn't really care too much because we

00:30:54,419 --> 00:30:59,519
were monitoring individual servers but

00:30:57,690 --> 00:31:03,690
it was it was the only option we had

00:30:59,519 --> 00:31:06,330
right okay unfortunately no distributed

00:31:03,690 --> 00:31:08,220
clear evaluation yet no I don't know

00:31:06,330 --> 00:31:13,049
have you have you seen any ways of doing

00:31:08,220 --> 00:31:17,879
that sir no no that's whatever okay

00:31:13,049 --> 00:31:22,940
anybody else it's not the case so we can

00:31:17,879 --> 00:31:22,940
move on thank you again

00:31:26,880 --> 00:31:28,940

YouTube URL: https://www.youtube.com/watch?v=likpVWB5Lvo


