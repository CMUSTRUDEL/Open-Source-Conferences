Title: PromCon 2016: Full Stack Metrics with Triton's Native Prometheus Support - Richard Kiene, Tim Gross
Publication date: 2016-09-04
Playlist: PromCon 2016
Description: 
	* Abstract

Triton is the open source data center automation solution that powers public cloud and private data centers world-wide. In early 2016, Joyent undertook a project to expose infrastructure and system for every instance — including infrastructure containers, Docker containers, and hardware virtual machines — using a Prometheus-compatible agent. The project, called Triton Container Monitor and described in our public request for discussion, https://github.com/joyent/rfd/blob/master/rfd/0027/README.md, takes advantage of container-native architecture, kstats, and other native introspection tools.

Additionally, developers need tools to expose up-stack application metrics to Prometheus. ContainerPilot, the open source, application-centric micro-orchestrator that makes it easy to build and operate portable application containers, now features a Prometheus-compatible telemetry interface to monitor application performance with user-defined probes. This interface brings monitoring directly to the container and eliminates the dependence on complex host monitoring for application-specific details.

In this session, Container Monitor lead engineer Richard Kiene will discuss the challenges and successes of implementing Prometheus as a native interface in Triton, and ContainerPilot lead engineer Tim Gross will demonstrate ContainerPilot telemetry and discuss the application-level decisions around defining metrics to drive application scaling.

* Speaker biographies:

Richard Kiene is a software engineer at Joyent. He has previously been chief of operations at an electronic publisher and senior network operator at a major telecoms carrier.

Tim Gross is a product manager at Joyent and the lead engineer on ContainerPilot. Formerly, Tim led operations at a streaming video startup where he had to scale his applications up and down 10x each day to handle peak loads without paying for unused infrastructure during idle times.

* Slides:

https://speakerdeck.com/tgross/triton

* PromCon website:

https://promcon.io/
Captions: 
	00:00:01,239 --> 00:00:07,839
and we work for joint which as just told

00:00:05,950 --> 00:00:10,390
we build the infrastructure and

00:00:07,839 --> 00:00:13,660
datacenter software we've been container

00:00:10,390 --> 00:00:16,359
native since 2006 we operate a public

00:00:13,660 --> 00:00:18,039
cloud with global data centers and the

00:00:16,359 --> 00:00:20,080
software that powers our cloud is open

00:00:18,039 --> 00:00:21,250
source so that you can run your private

00:00:20,080 --> 00:00:23,020
data center using the same software

00:00:21,250 --> 00:00:25,180
they're running in our public cloud and

00:00:23,020 --> 00:00:25,990
that's important because and it's

00:00:25,180 --> 00:00:27,340
important for me to tell you that

00:00:25,990 --> 00:00:29,230
because of what we're going to be

00:00:27,340 --> 00:00:30,250
talking about today I'm also gonna take

00:00:29,230 --> 00:00:31,720
this opportunity to point out that we

00:00:30,250 --> 00:00:34,329
were just recently acquired by Samsung

00:00:31,720 --> 00:00:35,739
and we're gonna be building new data

00:00:34,329 --> 00:00:37,450
centers and one of the largest docker

00:00:35,739 --> 00:00:38,620
installations in the world so if that

00:00:37,450 --> 00:00:42,399
sort of thing is interesting to you

00:00:38,620 --> 00:00:45,370
we're hiring okay let's take a step back

00:00:42,399 --> 00:00:47,320
a little bit away from prometheus per se

00:00:45,370 --> 00:00:49,030
and kind of get like one level up from

00:00:47,320 --> 00:00:50,559
that right so so with the rise of

00:00:49,030 --> 00:00:53,230
containers and this like model that we

00:00:50,559 --> 00:00:54,579
call micro services some of the we're

00:00:53,230 --> 00:00:56,379
finding that everybody is building

00:00:54,579 --> 00:00:57,879
distributed systems right the complexity

00:00:56,379 --> 00:01:00,160
of what we're trying to monitor has gone

00:00:57,879 --> 00:01:02,559
up a lot and these distributed systems

00:01:00,160 --> 00:01:04,299
are harder to observe and to debug than

00:01:02,559 --> 00:01:06,520
anything that we built say just 10 years

00:01:04,299 --> 00:01:10,299
ago and join CTO was saying things like

00:01:06,520 --> 00:01:12,490
this so and what's more a lot of the

00:01:10,299 --> 00:01:14,140
problems that we're facing are actually

00:01:12,490 --> 00:01:15,729
originating up stack from where we're

00:01:14,140 --> 00:01:17,290
monitoring right so like we get

00:01:15,729 --> 00:01:19,930
monitoring and alerts on like CPU

00:01:17,290 --> 00:01:21,909
percent dangers which is not actually

00:01:19,930 --> 00:01:23,439
the cause right it's the symptom that

00:01:21,909 --> 00:01:26,200
we're seeing and the cause is somewhere

00:01:23,439 --> 00:01:27,820
up stack it's it's it's in our in this

00:01:26,200 --> 00:01:29,740
really large pile of instructions that

00:01:27,820 --> 00:01:32,470
we have so our language runtimes our

00:01:29,740 --> 00:01:34,780
standard libraries the the application

00:01:32,470 --> 00:01:36,820
VM the application framework and of

00:01:34,780 --> 00:01:40,060
course are the crappy application code

00:01:36,820 --> 00:01:42,310
we all right so and because these

00:01:40,060 --> 00:01:43,570
problems are because these systems are

00:01:42,310 --> 00:01:45,430
distributed many of the most difficult

00:01:43,570 --> 00:01:47,439
problems to solve we're only ever going

00:01:45,430 --> 00:01:48,790
to see in production which means we need

00:01:47,439 --> 00:01:50,229
to be able to debug our production

00:01:48,790 --> 00:01:53,799
environments not just what's happening

00:01:50,229 --> 00:01:55,509
on my laptop and so to debug in

00:01:53,799 --> 00:01:58,540
production means debugging safely right

00:01:55,509 --> 00:02:00,549
so we can't if we if we if our systems

00:01:58,540 --> 00:02:03,159
crash when we debug them that doesn't

00:02:00,549 --> 00:02:05,500
work right that now we can tweak we will

00:02:03,159 --> 00:02:07,119
be afraid to debug in production also if

00:02:05,500 --> 00:02:09,220
observing the system changes its

00:02:07,119 --> 00:02:12,670
behavior which you know

00:02:09,220 --> 00:02:14,350
kind of a high galleon sort of sense or

00:02:12,670 --> 00:02:15,850
causes it to crash then we can't use it

00:02:14,350 --> 00:02:18,160
to accurately solve performance problems

00:02:15,850 --> 00:02:19,780
right so that means that observability

00:02:18,160 --> 00:02:21,190
is really the key to what mean that

00:02:19,780 --> 00:02:22,510
that's like the key element to what

00:02:21,190 --> 00:02:24,850
makes something production ready

00:02:22,510 --> 00:02:26,700
and so we're joint think that's kind of

00:02:24,850 --> 00:02:29,710
important and we have world-class

00:02:26,700 --> 00:02:32,650
observability tooling like Dietrich's

00:02:29,710 --> 00:02:34,240
and MDB and we built new tooling like

00:02:32,650 --> 00:02:36,130
our logging analysis system called

00:02:34,240 --> 00:02:37,540
thought to give us what we think is the

00:02:36,130 --> 00:02:38,770
best observer ability into what happens

00:02:37,540 --> 00:02:40,300
in our public cloud and for our

00:02:38,770 --> 00:02:41,950
customers to have that same level of

00:02:40,300 --> 00:02:47,050
observer ability in their private cloud

00:02:41,950 --> 00:02:48,220
installations so I want to so before we

00:02:47,050 --> 00:02:49,720
can kind of get in a container

00:02:48,220 --> 00:02:51,820
monitoring which is which is what

00:02:49,720 --> 00:02:52,630
Richards been working on lately I kind

00:02:51,820 --> 00:02:54,220
of want to give you a high-level

00:02:52,630 --> 00:02:55,720
overview of what the architecture that

00:02:54,220 --> 00:02:59,020
we're talking about is this is this is

00:02:55,720 --> 00:03:00,880
our Triton architecture so on Triton

00:02:59,020 --> 00:03:02,080
customer applications are running as

00:03:00,880 --> 00:03:04,000
containers and those are going to be

00:03:02,080 --> 00:03:06,970
either smart OS infrastructure

00:03:04,000 --> 00:03:08,560
containers or doctor-doctor containers

00:03:06,970 --> 00:03:13,390
and they're those containers are all

00:03:08,560 --> 00:03:16,870
running as Solaris zones so unlike say

00:03:13,390 --> 00:03:19,860
the container the containers on Linux

00:03:16,870 --> 00:03:24,280
Solaris zones have a decades-long

00:03:19,860 --> 00:03:26,470
battle-tested isolation for multi-tenant

00:03:24,280 --> 00:03:27,790
security we also get bare metal

00:03:26,470 --> 00:03:29,890
performance on that so there's no V

00:03:27,790 --> 00:03:31,530
there's no underlying VM and that

00:03:29,890 --> 00:03:33,220
isolation gives us the ability to have

00:03:31,530 --> 00:03:35,880
observability from outside without

00:03:33,220 --> 00:03:37,980
interfering with it

00:03:35,880 --> 00:03:39,300
so now of course you know we've been

00:03:37,980 --> 00:03:40,410
running a public cloud for a while right

00:03:39,300 --> 00:03:41,520
so and we're here to talk about

00:03:40,410 --> 00:03:43,500
something that Richards been working on

00:03:41,520 --> 00:03:44,520
today and it actually is not in

00:03:43,500 --> 00:03:45,840
production but of course there's an

00:03:44,520 --> 00:03:48,690
existing version and we call that cloud

00:03:45,840 --> 00:03:50,970
analytics and but it's not perfect right

00:03:48,690 --> 00:03:51,870
we and so here's some of them are the

00:03:50,970 --> 00:03:54,150
constraints that we're trying to work

00:03:51,870 --> 00:03:55,500
under here so in this system getting

00:03:54,150 --> 00:03:56,670
historical data out of it is a little

00:03:55,500 --> 00:03:59,610
bit cumbersome mostly just because of

00:03:56,670 --> 00:04:01,410
the way the API works the API is also

00:03:59,610 --> 00:04:03,990
awkward to work with lit for highly

00:04:01,410 --> 00:04:06,270
dimensional data and we want to improve

00:04:03,990 --> 00:04:07,770
our story around scalability and our

00:04:06,270 --> 00:04:10,260
scalability our story around

00:04:07,770 --> 00:04:12,420
availability we think it's good we think

00:04:10,260 --> 00:04:14,880
it can be better and this is kind of a

00:04:12,420 --> 00:04:16,350
subtle thing and this is where we think

00:04:14,880 --> 00:04:17,730
that we can differentiate from some of

00:04:16,350 --> 00:04:20,430
our competitors is that right now

00:04:17,730 --> 00:04:22,350
there's no real path to have end-user

00:04:20,430 --> 00:04:24,840
application metrics being fed into the

00:04:22,350 --> 00:04:28,290
same system that they would use to do

00:04:24,840 --> 00:04:30,990
the observations of their of their of

00:04:28,290 --> 00:04:32,760
the container runtime right so so if

00:04:30,990 --> 00:04:34,140
you're on say you know the leading cloud

00:04:32,760 --> 00:04:36,390
provider and usually you're using their

00:04:34,140 --> 00:04:38,400
metric system right now like you get a

00:04:36,390 --> 00:04:39,570
sample set of things that they've given

00:04:38,400 --> 00:04:40,800
you but there's no way to tie that

00:04:39,570 --> 00:04:45,240
together with your application metrics

00:04:40,800 --> 00:04:49,110
necessarily I'm gonna hand that off the

00:04:45,240 --> 00:04:50,460
Richard sweep all right so I'm kind of

00:04:49,110 --> 00:04:52,830
attacking the things that Tim was

00:04:50,460 --> 00:04:54,930
pointing out we came up with some some

00:04:52,830 --> 00:04:57,990
pretty good constraints as to to what we

00:04:54,930 --> 00:05:00,030
had to work with obviously it has to be

00:04:57,990 --> 00:05:02,340
multi tenant were a cloud

00:05:00,030 --> 00:05:04,830
we can't let consumers of this thing

00:05:02,340 --> 00:05:07,590
take down the cloud because that kills

00:05:04,830 --> 00:05:09,210
all the other people right and then we

00:05:07,590 --> 00:05:10,650
need to give people a way to like move

00:05:09,210 --> 00:05:12,450
off of what we've given them in the past

00:05:10,650 --> 00:05:14,100
with cloud analytics or move off of

00:05:12,450 --> 00:05:16,440
whatever they're running on their own or

00:05:14,100 --> 00:05:20,010
perhaps just integrate our stuff with

00:05:16,440 --> 00:05:21,720
theirs and and this kind of feeds into

00:05:20,010 --> 00:05:23,640
one of the greatest things is that we

00:05:21,720 --> 00:05:25,020
were building this for rainy day

00:05:23,640 --> 00:05:28,110
operation this is not sunny day

00:05:25,020 --> 00:05:29,910
operation what I mean by that is most of

00:05:28,110 --> 00:05:32,430
your server monitoring tools live in

00:05:29,910 --> 00:05:34,200
your server itself if your server is out

00:05:32,430 --> 00:05:38,040
of file descriptors or completely

00:05:34,200 --> 00:05:39,090
overloaded by some means you can't get

00:05:38,040 --> 00:05:40,500
the data thrown anymore you don't know

00:05:39,090 --> 00:05:42,500
if it's down because the network is down

00:05:40,500 --> 00:05:45,630
or if your containers down or what

00:05:42,500 --> 00:05:48,280
because ours operates on the principle

00:05:45,630 --> 00:05:50,650
of rainy day we live in the global zone

00:05:48,280 --> 00:05:52,330
which is somewhat unique to join it and

00:05:50,650 --> 00:05:53,830
allows us to actually tell you the

00:05:52,330 --> 00:06:00,180
difference is it down or is it just

00:05:53,830 --> 00:06:04,030
completely you know out of capacity so

00:06:00,180 --> 00:06:06,100
why pole well honestly there's a lot of

00:06:04,030 --> 00:06:08,050
really important reasons but one of the

00:06:06,100 --> 00:06:12,790
biggest ones is its kind of a cap

00:06:08,050 --> 00:06:15,190
problem for a public cloud if we're

00:06:12,790 --> 00:06:18,220
pushing to our customers we've got to

00:06:15,190 --> 00:06:19,570
know are you down are you overloaded we

00:06:18,220 --> 00:06:21,460
got to be able to back off we got to do

00:06:19,570 --> 00:06:22,870
all kinds of different things and and

00:06:21,460 --> 00:06:24,520
perhaps even keep a lot of state or

00:06:22,870 --> 00:06:26,530
coffered data so that you don't lose it

00:06:24,520 --> 00:06:29,470
because we had a problem or whatever

00:06:26,530 --> 00:06:32,050
else if if you're pulling data from us

00:06:29,470 --> 00:06:35,470
it's up to you and you have to manage

00:06:32,050 --> 00:06:37,390
your your availability your path to us

00:06:35,470 --> 00:06:39,669
and you can know whether you're up or

00:06:37,390 --> 00:06:41,680
down all of that it's it's just not

00:06:39,669 --> 00:06:43,960
tenable at least from our perspective to

00:06:41,680 --> 00:06:48,400
try and juggle all the balls in the air

00:06:43,960 --> 00:06:49,780
of pushing and then end-users can have

00:06:48,400 --> 00:06:51,370
multiple consumers right we don't have

00:06:49,780 --> 00:06:55,330
to deal with multiple push streams we

00:06:51,370 --> 00:07:00,940
can have a single agent availability for

00:06:55,330 --> 00:07:02,800
multiple people to use so why do we

00:07:00,940 --> 00:07:06,130
choose Prometheus kinda based on

00:07:02,800 --> 00:07:10,539
everything I just said it's it's pretty

00:07:06,130 --> 00:07:13,360
opinionated about being pull based it it

00:07:10,539 --> 00:07:15,220
doesn't have you know desires to be a

00:07:13,360 --> 00:07:18,250
giant database at least it doesn't sound

00:07:15,220 --> 00:07:20,410
like and so people can take from it and

00:07:18,250 --> 00:07:23,680
put it where they want we can implement

00:07:20,410 --> 00:07:25,510
their protocol the protocol and be an

00:07:23,680 --> 00:07:27,550
agent but not actually have to support

00:07:25,510 --> 00:07:29,740
it ourselves like you can run your own

00:07:27,550 --> 00:07:31,810
Prometheus server and talk to us and

00:07:29,740 --> 00:07:34,240
that kind of makes a nice demarcation

00:07:31,810 --> 00:07:39,280
point and then it's an open standard

00:07:34,240 --> 00:07:45,310
people can build on top of that alright

00:07:39,280 --> 00:07:47,410
so architecture this whole thing starts

00:07:45,310 --> 00:07:49,300
with the metric aging so there's one of

00:07:47,410 --> 00:07:52,060
these on every compute node in our cloud

00:07:49,300 --> 00:07:54,130
it actually lives in what I said is the

00:07:52,060 --> 00:07:56,590
global zone which kind of has the God

00:07:54,130 --> 00:07:58,810
view of everything that's going on it's

00:07:56,590 --> 00:08:01,360
an advantage of doing OS virtualization

00:07:58,810 --> 00:08:02,950
instead of hardware virtualization and

00:08:01,360 --> 00:08:06,580
it collects the metrics from the already

00:08:02,950 --> 00:08:10,180
built in metrics in the kernel and file

00:08:06,580 --> 00:08:13,390
system for smart OS which case that and

00:08:10,180 --> 00:08:14,979
ZFS so it's actually really nice because

00:08:13,390 --> 00:08:16,510
we don't have to do a bunch of our

00:08:14,979 --> 00:08:18,909
calculation and the agent that feeds

00:08:16,510 --> 00:08:20,650
them we kind of just move it out of the

00:08:18,909 --> 00:08:26,680
OS and the OS is doing the real hard

00:08:20,650 --> 00:08:29,229
work for keeping all that state so this

00:08:26,680 --> 00:08:30,970
is actually somewhat misleading the

00:08:29,229 --> 00:08:34,479
hypervisor and the metric agent are in

00:08:30,970 --> 00:08:37,479
the same place but kind of the get the

00:08:34,479 --> 00:08:41,820
gist there's lots of customers on one

00:08:37,479 --> 00:08:44,560
box and an agent itself that lives there

00:08:41,820 --> 00:08:48,339
what the agents actually doing is kind

00:08:44,560 --> 00:08:50,589
of approximating a node exporter so it

00:08:48,339 --> 00:08:52,540
takes the OS stats like you would think

00:08:50,589 --> 00:08:54,610
like a server monitor from New Relic or

00:08:52,540 --> 00:08:57,190
something it takes the the OS level

00:08:54,610 --> 00:09:00,670
stats and pushes them up obviously we

00:08:57,190 --> 00:09:02,170
can't have customers hitting our admin

00:09:00,670 --> 00:09:04,089
network in the global zone of each

00:09:02,170 --> 00:09:06,240
compute nodes so there's more to it than

00:09:04,089 --> 00:09:06,240
that

00:09:07,470 --> 00:09:14,020
right so every datacenter has many of

00:09:11,620 --> 00:09:19,000
these metric agents every scene gets its

00:09:14,020 --> 00:09:21,930
own metric agent and so the all

00:09:19,000 --> 00:09:25,390
important metric agent proxy comes in

00:09:21,930 --> 00:09:30,070
this is this is where customers talk to

00:09:25,390 --> 00:09:33,010
our Prometheus output essentially we

00:09:30,070 --> 00:09:35,110
proxy your servers request in and

00:09:33,010 --> 00:09:38,320
pretend as if we are one individual in

00:09:35,110 --> 00:09:41,290
that exporter so you will get into

00:09:38,320 --> 00:09:43,209
exactly how in a second this is H a

00:09:41,290 --> 00:09:45,310
across a every datacenter availability

00:09:43,209 --> 00:09:48,130
zone that we have you have one on a head

00:09:45,310 --> 00:09:49,930
node and then two on some others since

00:09:48,130 --> 00:09:53,560
it's stateless and just basically

00:09:49,930 --> 00:09:55,510
proxying requests and doing off it's

00:09:53,560 --> 00:09:58,470
pretty easy to scale out

00:09:55,510 --> 00:10:01,690
for redundancy and for for request rate

00:09:58,470 --> 00:10:04,300
and so it's a great choke point as well

00:10:01,690 --> 00:10:10,030
for rate limiting and making sure you

00:10:04,300 --> 00:10:13,240
are who you say you are so how do we

00:10:10,030 --> 00:10:17,140
pretend how do we act as if we are one

00:10:13,240 --> 00:10:19,930
known exporter can we build on something

00:10:17,140 --> 00:10:23,530
that a colleague and I built a little

00:10:19,930 --> 00:10:24,940
while ago called Triton C&S which is a

00:10:23,530 --> 00:10:28,360
container naming service which will

00:10:24,940 --> 00:10:31,810
automatically give you an a record based

00:10:28,360 --> 00:10:35,380
on some criteria for all of the vm's

00:10:31,810 --> 00:10:36,820
that you spin up in our cloud that's

00:10:35,380 --> 00:10:37,960
more detail than we need to get into

00:10:36,820 --> 00:10:41,260
here but essentially we're leveraging

00:10:37,960 --> 00:10:43,720
that so that every time you spin up a VM

00:10:41,260 --> 00:10:45,970
will actually create a cname to the

00:10:43,720 --> 00:10:46,660
metric proxy so you'd have like your VM

00:10:45,970 --> 00:10:49,090
UUID

00:10:46,660 --> 00:10:51,640
dot container monitor got Triton dot

00:10:49,090 --> 00:10:55,320
zone or something like that it need to

00:10:51,640 --> 00:10:58,570
bat slash metrics but that's kind of

00:10:55,320 --> 00:11:05,080
obtuse right we'll get around that in a

00:10:58,570 --> 00:11:06,360
minute so that ends up kind of looking

00:11:05,080 --> 00:11:09,940
like that

00:11:06,360 --> 00:11:12,640
one or more metric proxies the edge it

00:11:09,940 --> 00:11:14,290
knows which metric agent is where so it

00:11:12,640 --> 00:11:17,310
knows your where your VM is at so if

00:11:14,290 --> 00:11:20,020
you're looking for Thiam dead beef

00:11:17,310 --> 00:11:22,630
you're gonna it knows oh that's on CN

00:11:20,020 --> 00:11:25,660
one it'll reach out and reconstruct that

00:11:22,630 --> 00:11:27,040
request to CN one in a slightly

00:11:25,660 --> 00:11:29,740
different format which I can show you in

00:11:27,040 --> 00:11:32,560
a little while but it still maintains

00:11:29,740 --> 00:11:35,010
the Prometheus text format and slash

00:11:32,560 --> 00:11:35,010
metrics

00:11:39,650 --> 00:11:45,020
right so metric collection is either

00:11:42,470 --> 00:11:47,780
gonna happen with your Prometheus server

00:11:45,020 --> 00:11:49,160
wherever you put that or if you're not

00:11:47,780 --> 00:11:50,630
using Prometheus and you want to push it

00:11:49,160 --> 00:11:53,630
somewhere else you can spin up a

00:11:50,630 --> 00:11:55,760
forwarder essentially some app that we

00:11:53,630 --> 00:11:57,470
or you have written which speaks to

00:11:55,760 --> 00:12:00,700
Prometheus server and then shuttles it

00:11:57,470 --> 00:12:02,570
on to - I don't know fill in the blank

00:12:00,700 --> 00:12:11,270
metric system which you might already

00:12:02,570 --> 00:12:14,110
have graphite or whatever so that's kind

00:12:11,270 --> 00:12:14,110
of how that looks

00:12:15,280 --> 00:12:20,330
now when you add in the metric for der

00:12:17,980 --> 00:12:22,640
similar idea

00:12:20,330 --> 00:12:23,990
it's just pretending to be a Prometheus

00:12:22,640 --> 00:12:27,340
server and shuttling it out in a

00:12:23,990 --> 00:12:27,340
different format

00:12:29,500 --> 00:12:35,520
so I already kind of talked about this a

00:12:33,400 --> 00:12:38,200
little bit but when you launch container

00:12:35,520 --> 00:12:39,490
our VM API so we have a bunch of micro

00:12:38,200 --> 00:12:41,680
services that run all the different

00:12:39,490 --> 00:12:43,500
parts of our cloud so VM API pushes a

00:12:41,680 --> 00:12:46,270
change feed event change feeds another

00:12:43,500 --> 00:12:49,750
system that I built which essentially is

00:12:46,270 --> 00:12:53,260
a way of doing message bus without state

00:12:49,750 --> 00:12:55,630
having to do a time series delivery so

00:12:53,260 --> 00:12:58,720
it pushes that change feed to CNS CNS

00:12:55,630 --> 00:13:01,570
picks that up creates the cname and now

00:12:58,720 --> 00:13:04,510
all of a sudden you can query metric age

00:13:01,570 --> 00:13:12,310
of proxy as if it were a node exporter

00:13:04,510 --> 00:13:17,380
that you had access to kind of already

00:13:12,310 --> 00:13:18,610
touched on that so all right so we were

00:13:17,380 --> 00:13:20,950
talking about what I was talking about

00:13:18,610 --> 00:13:22,870
earlier that we wanted to give our

00:13:20,950 --> 00:13:24,310
customers an ability to say well what

00:13:22,870 --> 00:13:26,080
let's use the same system for

00:13:24,310 --> 00:13:28,500
application metrics that we want to have

00:13:26,080 --> 00:13:34,420
for the you know the container metrics

00:13:28,500 --> 00:13:36,190
and so I've been working at joint on a

00:13:34,420 --> 00:13:38,350
project called the autopilot pattern and

00:13:36,190 --> 00:13:39,970
it's a design pattern for having

00:13:38,350 --> 00:13:42,250
applications be self operating and self

00:13:39,970 --> 00:13:44,050
managing it's kind of the opposite of

00:13:42,250 --> 00:13:45,490
the kubernetes approach or the mazes

00:13:44,050 --> 00:13:46,750
approach where instead of saying like

00:13:45,490 --> 00:13:48,460
we're gonna put all the intelligence for

00:13:46,750 --> 00:13:49,810
how an application wires itself up in

00:13:48,460 --> 00:13:51,760
some kind of mega orchestration

00:13:49,810 --> 00:13:52,990
framework you actually push it down into

00:13:51,760 --> 00:13:55,750
the application level and let

00:13:52,990 --> 00:13:57,160
applications be self-assembling so they

00:13:55,750 --> 00:13:59,140
have enough intelligence to say well

00:13:57,160 --> 00:14:00,700
what are my dependencies and how do I

00:13:59,140 --> 00:14:06,070
find them and what do I need to do when

00:14:00,700 --> 00:14:07,360
things change there's a the problem with

00:14:06,070 --> 00:14:08,770
that the only problem with that model is

00:14:07,360 --> 00:14:10,060
of course that not all applications are

00:14:08,770 --> 00:14:12,520
capable of doing that and sometimes you

00:14:10,060 --> 00:14:13,630
need a little bit of help and so I've

00:14:12,520 --> 00:14:17,110
worked on a project called container

00:14:13,630 --> 00:14:18,790
pilot that is a micro Orchestrator so to

00:14:17,110 --> 00:14:21,370
speak that lives inside the container

00:14:18,790 --> 00:14:22,840
and so it is it acts as a paid one that

00:14:21,370 --> 00:14:24,760
it acts as the init system within the

00:14:22,840 --> 00:14:26,500
container it launches your application

00:14:24,760 --> 00:14:29,530
and then fires off a bunch of lifecycle

00:14:26,500 --> 00:14:31,270
hooks why that's interesting for this

00:14:29,530 --> 00:14:34,210
talk is that one of those hooks is

00:14:31,270 --> 00:14:35,860
telemetry sensors so as part of the as

00:14:34,210 --> 00:14:37,990
part of the container pilot

00:14:35,860 --> 00:14:40,660
configuration for your application you

00:14:37,990 --> 00:14:41,160
can give it a set of arbitrary sensors

00:14:40,660 --> 00:14:42,779
that live

00:14:41,160 --> 00:14:44,970
inside the container and those

00:14:42,779 --> 00:14:47,790
containers will be fed to a prometheus

00:14:44,970 --> 00:14:52,560
scraper a Prometheus capable end point

00:14:47,790 --> 00:14:55,529
that container pilot serves so because

00:14:52,560 --> 00:14:56,879
those containers have a CNS name and

00:14:55,529 --> 00:14:59,939
because container pilot exposes that

00:14:56,879 --> 00:15:01,740
Prometheus end point we can add as part

00:14:59,939 --> 00:15:03,600
of our discovery catalog we can add that

00:15:01,740 --> 00:15:05,399
and sorry and they're all registering

00:15:03,600 --> 00:15:08,490
themselves with the discovery catalog so

00:15:05,399 --> 00:15:09,870
for example common console RIT CD the in

00:15:08,490 --> 00:15:11,850
addition to you or Prometheus server

00:15:09,870 --> 00:15:13,560
using the Trident discovery plugin that

00:15:11,850 --> 00:15:15,480
we were taught that Richard was just

00:15:13,560 --> 00:15:17,879
pointing out you can also have it scrape

00:15:15,480 --> 00:15:20,220
your a console discovery discovery

00:15:17,879 --> 00:15:22,709
catalog and now you have both a source

00:15:20,220 --> 00:15:24,660
for application metrics and for the

00:15:22,709 --> 00:15:26,040
container metrics so I want to take a

00:15:24,660 --> 00:15:27,300
look at like what container pilot

00:15:26,040 --> 00:15:29,639
configuration looks like for something

00:15:27,300 --> 00:15:33,509
like that so this is this is an engine X

00:15:29,639 --> 00:15:34,800
container and if I kind of dive into

00:15:33,509 --> 00:15:36,899
this so at the top of this this is how

00:15:34,800 --> 00:15:39,329
we define services in in container pilot

00:15:36,899 --> 00:15:41,550
so the idea here is that we say we're

00:15:39,329 --> 00:15:44,610
talking to the console the discovery

00:15:41,550 --> 00:15:46,199
catalog at that that URL we have a pre

00:15:44,610 --> 00:15:50,490
start life cycle hook so like what do we

00:15:46,199 --> 00:15:52,949
do before we start a service block here

00:15:50,490 --> 00:15:55,290
which is I'm I'm going to advertise the

00:15:52,949 --> 00:15:57,029
nginx service into console and then this

00:15:55,290 --> 00:15:58,589
is how I'm gonna health check it and how

00:15:57,029 --> 00:16:00,269
long the heartbeats will last things

00:15:58,589 --> 00:16:01,529
like that and then what back ends this

00:16:00,269 --> 00:16:03,149
thing this particular application is

00:16:01,529 --> 00:16:06,269
interested in this in this case we just

00:16:03,149 --> 00:16:08,639
have an example and then what it's going

00:16:06,269 --> 00:16:10,470
to do when that changes if you want to

00:16:08,639 --> 00:16:11,970
see more of that I'd be happy to demo

00:16:10,470 --> 00:16:13,500
for anybody afterwards if it's not

00:16:11,970 --> 00:16:15,000
really interesting to the monitoring

00:16:13,500 --> 00:16:16,860
topic so if we keep going down a little

00:16:15,000 --> 00:16:20,399
bit so this is where the telemetry is

00:16:16,860 --> 00:16:21,870
design is is configured right so we have

00:16:20,399 --> 00:16:23,100
two different sensors here and you see

00:16:21,870 --> 00:16:24,930
that these are these are basically

00:16:23,100 --> 00:16:27,660
Prometheus configurations right so we've

00:16:24,930 --> 00:16:28,769
got the name for that and the help

00:16:27,660 --> 00:16:30,389
string and that's what will be in the

00:16:28,769 --> 00:16:34,079
text format these are two different

00:16:30,389 --> 00:16:35,819
gauge metrics and we see how those are

00:16:34,079 --> 00:16:37,680
generated right so there's a sensor and

00:16:35,819 --> 00:16:39,990
and these sensors can be arbitrarily

00:16:37,680 --> 00:16:41,610
retro this is just a bash bit right but

00:16:39,990 --> 00:16:44,160
they could be a C program it could be

00:16:41,610 --> 00:16:45,629
some kind of metric that is a particular

00:16:44,160 --> 00:16:47,459
to your application and the really

00:16:45,629 --> 00:16:48,899
important part here is that the met is

00:16:47,459 --> 00:16:50,790
that the sensor along with all this

00:16:48,899 --> 00:16:52,470
other behaviors actually ships inside

00:16:50,790 --> 00:16:53,910
the container right so your container is

00:16:52,470 --> 00:16:56,370
completely self can

00:16:53,910 --> 00:16:58,740
and rather than having it be the

00:16:56,370 --> 00:17:00,390
responsibility of some third party to

00:16:58,740 --> 00:17:03,330
figure out how to send how to measure

00:17:00,390 --> 00:17:06,180
your particular application so let's do

00:17:03,330 --> 00:17:08,720
a little demo here I'm going to switch

00:17:06,180 --> 00:17:08,720
off my mirror

00:17:27,780 --> 00:17:30,439
okay

00:17:31,410 --> 00:17:37,760
all right so what I've got here is in

00:17:35,670 --> 00:17:42,000
this terminal can everybody see that

00:17:37,760 --> 00:17:47,510
I've got a we're just gonna do doctor

00:17:42,000 --> 00:17:50,520
compose up so this is running against

00:17:47,510 --> 00:17:52,260
I've got a warning on that so this is

00:17:50,520 --> 00:17:53,520
running against a local version of

00:17:52,260 --> 00:17:54,810
trading that's running on a laptop and I

00:17:53,520 --> 00:17:56,610
don't have my certain set of properly so

00:17:54,810 --> 00:17:59,010
Python and composed is giving me a

00:17:56,610 --> 00:18:00,630
hassle so what this is doing is this is

00:17:59,010 --> 00:18:03,600
going and launching and we can kind of

00:18:00,630 --> 00:18:05,760
see here it's launching a series of

00:18:03,600 --> 00:18:07,080
containers so it and these are all

00:18:05,760 --> 00:18:09,300
defined by this doctor compose file

00:18:07,080 --> 00:18:12,000
that's here so I've got console I've got

00:18:09,300 --> 00:18:14,340
nginx and I've got an example back-end

00:18:12,000 --> 00:18:18,270
application so these come up they're

00:18:14,340 --> 00:18:21,450
going to start seeing that example okay

00:18:18,270 --> 00:18:22,770
so yep so what I'm gonna do here over

00:18:21,450 --> 00:18:26,940
here on this side is I'm going to ask

00:18:22,770 --> 00:18:29,420
the Triton API on this laptop for the IP

00:18:26,940 --> 00:18:29,420
of that

00:18:36,270 --> 00:18:41,900
copy that into browser here

00:18:43,140 --> 00:18:47,530
all right so this is the console

00:18:45,100 --> 00:18:49,840
configuration this is the time sorry

00:18:47,530 --> 00:18:50,920
this is the console UI so as these other

00:18:49,840 --> 00:18:53,320
containers come up they're going to be

00:18:50,920 --> 00:18:55,840
registering themselves and so the

00:18:53,320 --> 00:18:57,280
example one is come up first and you'll

00:18:55,840 --> 00:18:59,590
see here that it also has container

00:18:57,280 --> 00:19:01,240
pilot right so it's advertising itself

00:18:59,590 --> 00:19:03,280
as a service but because it has sensors

00:19:01,240 --> 00:19:05,260
it's also advertising a container pilot

00:19:03,280 --> 00:19:08,200
service to consoles so as these

00:19:05,260 --> 00:19:13,510
containers come up they'll all have IPs

00:19:08,200 --> 00:19:15,670
for a container pilot service so I bet

00:19:13,510 --> 00:19:18,750
I've now got nginx and example

00:19:15,670 --> 00:19:18,750
containers I should be able to

00:19:29,220 --> 00:19:34,560
all right so I'm gonna just hit my into

00:19:32,370 --> 00:19:36,270
next example so this is just like a this

00:19:34,560 --> 00:19:37,740
nginx is just proxying to a node.js

00:19:36,270 --> 00:19:39,770
application that does like fortune

00:19:37,740 --> 00:19:43,160
cookies right like very simple

00:19:39,770 --> 00:19:47,270
self-assembling application here and

00:19:43,160 --> 00:19:47,270
that also means that I have

00:19:52,269 --> 00:19:56,269
Prometheus metrics for that application

00:19:54,230 --> 00:19:58,279
so each of my containers will advertise

00:19:56,269 --> 00:20:01,480
this prometheus endpoint and now my

00:19:58,279 --> 00:20:03,769
Prometheus server will be able to use

00:20:01,480 --> 00:20:06,080
Khan will be able to use the console

00:20:03,769 --> 00:20:08,419
discovery plug-in to find those to find

00:20:06,080 --> 00:20:10,909
those nodes at those IPs but come here

00:20:08,419 --> 00:20:13,690
now you'll see that I have that

00:20:10,909 --> 00:20:15,950
container pilot IP and and that console

00:20:13,690 --> 00:20:18,620
my previous server will weave it a use

00:20:15,950 --> 00:20:25,490
that IP to reach to reach that metrics

00:20:18,620 --> 00:20:28,909
endpoint all right so kind of as a side

00:20:25,490 --> 00:20:31,580
effect of what we just spun up there we

00:20:28,909 --> 00:20:34,760
should see some some new targets in here

00:20:31,580 --> 00:20:37,850
and what I've got is the user that we

00:20:34,760 --> 00:20:40,389
use to spin up all Tim's demo has a

00:20:37,850 --> 00:20:43,269
Prometheus server on the same cloud that

00:20:40,389 --> 00:20:45,500
automatically picked up these three

00:20:43,269 --> 00:20:47,990
containers that Tim showed you in

00:20:45,500 --> 00:20:49,850
console via the Triton discovery plugin

00:20:47,990 --> 00:20:53,059
which look for our pull requests on that

00:20:49,850 --> 00:20:55,010
and the near future and so I I can't hit

00:20:53,059 --> 00:20:57,049
these because the DNS on my laptop is

00:20:55,010 --> 00:20:59,149
different than the DNS in the fake cloud

00:20:57,049 --> 00:21:01,340
what I can't show you is that also the

00:20:59,149 --> 00:21:02,960
the agents and the viens that are

00:21:01,340 --> 00:21:05,389
running in our cloud themselves are

00:21:02,960 --> 00:21:08,899
being are being monitored so this is

00:21:05,389 --> 00:21:11,240
actually seeing API the compute node API

00:21:08,899 --> 00:21:13,700
and its metrics so this is the same

00:21:11,240 --> 00:21:16,580
thing being fed for all of our stuff

00:21:13,700 --> 00:21:18,080
that customers containers would see as

00:21:16,580 --> 00:21:22,130
well which is what you're gonna see here

00:21:18,080 --> 00:21:24,169
if I wanted to be a little tricky

00:21:22,130 --> 00:21:26,210
I could actually break this down because

00:21:24,169 --> 00:21:29,240
I've got access to all the networks

00:21:26,210 --> 00:21:32,580
since this is a fake cloud and then I

00:21:29,240 --> 00:21:35,950
can just take this over here

00:21:32,580 --> 00:21:38,500
do that and we should get the metrics

00:21:35,950 --> 00:21:41,920
for one of Tim stones that's kind of

00:21:38,500 --> 00:21:44,260
showing you two things what you see here

00:21:41,920 --> 00:21:46,870
is actually the metric agent view of the

00:21:44,260 --> 00:21:50,620
world so it has its own admin accessible

00:21:46,870 --> 00:21:52,890
only IP you feed it the VM UUID that you

00:21:50,620 --> 00:21:56,500
care about well Mexico metric proxy does

00:21:52,890 --> 00:22:01,090
and slash metrics and it goes goes ahead

00:21:56,500 --> 00:22:03,040
and returns back that data the IP or the

00:22:01,090 --> 00:22:04,150
the you are eyes you see here are

00:22:03,040 --> 00:22:06,490
actually the ones that we would expose

00:22:04,150 --> 00:22:08,140
publicly so that's what you as if you

00:22:06,490 --> 00:22:09,880
were using our cloud or private cloud or

00:22:08,140 --> 00:22:11,320
whatever that's what you'd actually hit

00:22:09,880 --> 00:22:13,930
so that's kind of the translation that

00:22:11,320 --> 00:22:19,500
happens from that URI right there down

00:22:13,930 --> 00:22:19,500
to this one and then the round trip back

00:22:29,670 --> 00:22:33,270
so questions

00:22:40,090 --> 00:22:46,840
you describe the container environment

00:22:43,460 --> 00:22:49,400
that you provide a safe contained versus

00:22:46,840 --> 00:22:51,740
outside approach that kubernetes is

00:22:49,400 --> 00:22:55,190
using I don't understand where the

00:22:51,740 --> 00:22:59,090
differences are because to me it seemed

00:22:55,190 --> 00:23:01,790
like I could do very much inside the

00:22:59,090 --> 00:23:04,610
container or very much the same that you

00:23:01,790 --> 00:23:08,150
do show just now and you keep on ug so

00:23:04,610 --> 00:23:09,950
as well I think what you're mentioning

00:23:08,150 --> 00:23:11,270
is where I talk about the metric agent

00:23:09,950 --> 00:23:15,820
being able to live on the compute node

00:23:11,270 --> 00:23:19,280
itself is that what you're referencing

00:23:15,820 --> 00:23:21,650
probably okay so what I'm talking about

00:23:19,280 --> 00:23:24,320
is that since we do os-level

00:23:21,650 --> 00:23:26,180
virtualization we can have one agent

00:23:24,320 --> 00:23:27,740
that sits on each CN and looks in I

00:23:26,180 --> 00:23:29,240
imagine you could do that with with

00:23:27,740 --> 00:23:32,300
kubernetes if you're running it on bare

00:23:29,240 --> 00:23:35,210
metal if you're running kubernetes on a

00:23:32,300 --> 00:23:37,760
hypervisor and like the traditional like

00:23:35,210 --> 00:23:40,040
AWS style stuff that actually gets hard

00:23:37,760 --> 00:23:43,700
to get those metrics because your

00:23:40,040 --> 00:23:45,860
hypervisor is a lie because it could go

00:23:43,700 --> 00:23:47,840
down and not your your VM that

00:23:45,860 --> 00:23:49,610
kubernetes is running in right like

00:23:47,840 --> 00:23:51,110
you're still hoping that the underlying

00:23:49,610 --> 00:23:53,960
machine and hypervisor that's running

00:23:51,110 --> 00:23:55,550
that is around so that's kind of talking

00:23:53,960 --> 00:23:57,740
about if you're running kubernetes or

00:23:55,550 --> 00:23:59,960
docker on bare metal security

00:23:57,740 --> 00:24:04,510
implications of such things aside you

00:23:59,960 --> 00:24:04,510
could probably get away with this Thanks

00:24:07,460 --> 00:24:11,470

YouTube URL: https://www.youtube.com/watch?v=X8QV9HgPNbc


