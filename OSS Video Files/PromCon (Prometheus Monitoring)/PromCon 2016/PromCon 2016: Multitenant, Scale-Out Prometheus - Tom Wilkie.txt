Title: PromCon 2016: Multitenant, Scale-Out Prometheus - Tom Wilkie
Publication date: 2016-09-04
Playlist: PromCon 2016
Description: 
	* Abstract:

In this talk we'll present a prototype solution for multitenant, scale-out Prometheus.

Our solution turns a lot of the Prometheus architectural assumptions on its head, by marrying a scale-out PromQL query engine with a storage layer based on DynamoDB. We have disaggregated the Prometheus binary into a microservices-style architecture, with separate services for query, ingest, alerting rules and storage. By designing all these services as fungible replicas, this solution can be scaled out with ease and failure of any individual replica can be dealt with gracefully.

This multitenant, scale-out Prometheus service forms a core component of the Weave Service, a hosted management, monitoring and visualisation platform for microservice & containerised applications. This platform is built from 100% open source components, and we're working with the Prometheus community to contribute all the changes we've made back to Prometheus.

* Speaker biography:

Tom is a Software Engineer at Weaveworks. Previously he was at Google as a Site Reliability Manager for Google Analytics. Before that he was Founder, VP Engineering and CTO at Acunu, and before that a Software Engineer at XenSource. In his spare time, Tom likes to make craft beer and build home automation systems.

* Slides:

https://drive.google.com/file/d/0BwqTw528sZRIVGpIN1hTVExhdFE/view?usp=sharing

* PromCon website:

https://promcon.io/
Captions: 
	00:00:01,870 --> 00:00:09,170
thanks man and yes well Brad my name is

00:00:06,080 --> 00:00:11,780
from Rocky I work he works as a software

00:00:09,170 --> 00:00:13,400
engineer we have bunch of stuff but

00:00:11,780 --> 00:00:16,790
rather than 50 today I'm really talking

00:00:13,400 --> 00:00:18,349
about this project Frankenstein Julius's

00:00:16,790 --> 00:00:20,689
means I papers users actually working

00:00:18,349 --> 00:00:24,200
with me like for about two months so

00:00:20,689 --> 00:00:26,750
this is really automated possible first

00:00:24,200 --> 00:00:31,369
off i watch start with questions or

00:00:26,750 --> 00:00:33,050
anything but hands up right now put your

00:00:31,369 --> 00:00:38,630
hands down if you're not using

00:00:33,050 --> 00:00:40,250
containers and okay how about using you

00:00:38,630 --> 00:00:42,530
know I mean dr. containers out your

00:00:40,250 --> 00:00:44,140
hands out and in production you're not

00:00:42,530 --> 00:00:46,480
use an instruction put your hands down

00:00:44,140 --> 00:00:51,140
alright so a lot of a lot of contention

00:00:46,480 --> 00:00:53,440
ok so now I'll keep your hands up good

00:00:51,140 --> 00:00:56,390
now if you're using chicken essence

00:00:53,440 --> 00:00:58,010
comes down to not bother sorry let me

00:00:56,390 --> 00:00:59,360
get to some of it okay how about

00:00:58,010 --> 00:01:01,760
introduction becomes down if you're not

00:00:59,360 --> 00:01:04,840
using GV Nettie's introduction so

00:01:01,760 --> 00:01:07,580
someone has actually used to bash tamiya

00:01:04,840 --> 00:01:11,150
ok right now keep your hands up who's

00:01:07,580 --> 00:01:13,550
heard of week other than obviously plus

00:01:11,150 --> 00:01:20,120
the loader server again who's actually

00:01:13,550 --> 00:01:22,340
used it welcome okay great well that's

00:01:20,120 --> 00:01:25,400
that's audience participation over we go

00:01:22,340 --> 00:01:26,479
at cigna and the other question i get

00:01:25,400 --> 00:01:29,780
about this project is where does the

00:01:26,479 --> 00:01:31,280
name come from and so one evening I

00:01:29,780 --> 00:01:33,680
and said I'm weapons new exciting

00:01:31,280 --> 00:01:35,659
project it's called prometheus and the

00:01:33,680 --> 00:01:37,940
girls really cool stuff yeah even named

00:01:35,659 --> 00:01:39,440
how would she know a literary buff and

00:01:37,940 --> 00:01:41,570
she came up with Frankenstein because

00:01:39,440 --> 00:01:45,500
apparently the subtitle is a modern

00:01:41,570 --> 00:01:46,880
prometheus I think I was awfully like

00:01:45,500 --> 00:01:53,479
you know setting expectations really

00:01:46,880 --> 00:01:54,979
high so that's why the name and about 11

00:01:53,479 --> 00:01:57,350
cents for that company this is kind of

00:01:54,979 --> 00:01:59,509
our thing about being the best way to

00:01:57,350 --> 00:02:03,380
visualize manage monitor your cabinet

00:01:59,509 --> 00:02:05,229
applications so that kind of is a way we

00:02:03,380 --> 00:02:07,550
try and unify everything we do to do so

00:02:05,229 --> 00:02:08,959
and so we started off the journey

00:02:07,550 --> 00:02:12,709
started off of this thing with weave

00:02:08,959 --> 00:02:14,540
scope this is a visualization tool for

00:02:12,709 --> 00:02:16,670
travel a track patients with me soft

00:02:14,540 --> 00:02:18,310
ochre blah blah and if you can see them

00:02:16,670 --> 00:02:20,360
start gathering some metrics in scope

00:02:18,310 --> 00:02:22,730
we're doing basic then will you do

00:02:20,360 --> 00:02:24,769
anything special and I thought you know

00:02:22,730 --> 00:02:27,050
we'll leave this to the experts at some

00:02:24,769 --> 00:02:30,830
point then next we've got a positive

00:02:27,050 --> 00:02:33,140
version of scope could we cloud pretty

00:02:30,830 --> 00:02:34,970
cool right out of luck and you know

00:02:33,140 --> 00:02:36,769
finally we've got this launched dr. Fong

00:02:34,970 --> 00:02:38,420
two months ago and finally it's now time

00:02:36,769 --> 00:02:40,700
to focus on the metrics bit that we said

00:02:38,420 --> 00:02:42,950
we've been a long long time ago and the

00:02:40,700 --> 00:02:45,620
part of running the version we pass

00:02:42,950 --> 00:02:48,079
while user why we use QB nettings in

00:02:45,620 --> 00:02:49,880
production we use Prometheus to monitor

00:02:48,079 --> 00:02:51,980
that are running service we absolutely

00:02:49,880 --> 00:02:53,030
love it and we thought you know what we

00:02:51,980 --> 00:02:56,739
want to do is we want to make a version

00:02:53,030 --> 00:02:59,510
of this that our customers can use so

00:02:56,739 --> 00:03:02,810
that's really the short story that leads

00:02:59,510 --> 00:03:04,670
up to you know why not just run your own

00:03:02,810 --> 00:03:08,000
why why should we bother building the

00:03:04,670 --> 00:03:09,829
service well we quite like running top

00:03:08,000 --> 00:03:12,769
as a service as a developer because you

00:03:09,829 --> 00:03:14,359
get instant feedback you can understand

00:03:12,769 --> 00:03:15,769
what's happening with the service but

00:03:14,359 --> 00:03:18,320
we're really actually looking somewhat

00:03:15,769 --> 00:03:20,180
customers one of you well Julia's kind

00:03:18,320 --> 00:03:21,250
of touching this is beginning to meet

00:03:20,180 --> 00:03:23,870
this doesn't do any act

00:03:21,250 --> 00:03:26,030
we've cloud stuff that we've got does

00:03:23,870 --> 00:03:28,670
access control so are hosted version

00:03:26,030 --> 00:03:30,680
community education share it and so on

00:03:28,670 --> 00:03:32,480
also you know there's been some talk

00:03:30,680 --> 00:03:33,800
about attention periods I'm going to

00:03:32,480 --> 00:03:35,420
explain how we've achieved this later

00:03:33,800 --> 00:03:37,670
but with the hosted version of

00:03:35,420 --> 00:03:39,080
Prometheus we can have virtually

00:03:37,670 --> 00:03:40,580
infinite retention you know as a big

00:03:39,080 --> 00:03:42,920
bold promise and hopefully you'll

00:03:40,580 --> 00:03:44,360
believe me by the end of the talk I also

00:03:42,920 --> 00:03:45,860
want to provide a different story around

00:03:44,360 --> 00:03:47,150
durability in hey CheY I think

00:03:45,860 --> 00:03:50,600
Prometheus has an incredibly strong

00:03:47,150 --> 00:03:51,980
story about about hey CheY but I want a

00:03:50,600 --> 00:03:53,690
different one you know I I don't like

00:03:51,980 --> 00:03:55,880
the idea of a failure having a gap in my

00:03:53,690 --> 00:03:57,920
grass so I want you know I want

00:03:55,880 --> 00:04:00,110
reliability through scale outs Brian

00:03:57,920 --> 00:04:01,730
alluded to and eventually we haven't

00:04:00,110 --> 00:04:03,260
done this yet but eventually we want to

00:04:01,730 --> 00:04:05,330
offer you know better query performance

00:04:03,260 --> 00:04:07,580
by paralyzing queries across workers and

00:04:05,330 --> 00:04:09,560
this is especially going to be useful

00:04:07,580 --> 00:04:11,120
for very long term queries which kind of

00:04:09,560 --> 00:04:12,740
LinkedIn nicely to the to the point

00:04:11,120 --> 00:04:14,360
about infinite potential you know I want

00:04:12,740 --> 00:04:16,190
you to be able to go and say please tell

00:04:14,360 --> 00:04:18,200
me the CPU cpu usage of my cluster

00:04:16,190 --> 00:04:19,609
forever you know for the last however

00:04:18,200 --> 00:04:20,750
long you've been running it and have

00:04:19,609 --> 00:04:24,530
that returned in a reasonable amount of

00:04:20,750 --> 00:04:25,790
time so yeah are the virtually infinite

00:04:24,530 --> 00:04:27,200
pretension is something we can do

00:04:25,790 --> 00:04:30,860
because it's actually provided for us by

00:04:27,200 --> 00:04:32,480
AWS which will come on to you later so

00:04:30,860 --> 00:04:34,040
the API can practice so the requirements

00:04:32,480 --> 00:04:35,570
you know what's important what do we set

00:04:34,040 --> 00:04:36,680
out when we start designing this it was

00:04:35,570 --> 00:04:38,240
going to be a hundred percent API

00:04:36,680 --> 00:04:39,200
compatible with meteors in fact we

00:04:38,240 --> 00:04:41,240
really want to go a lot further than

00:04:39,200 --> 00:04:42,730
that we wanted to reuse as much of the

00:04:41,240 --> 00:04:46,070
Prometheus code as we as we could

00:04:42,730 --> 00:04:49,940
something I think we may be semi

00:04:46,070 --> 00:04:51,560
achieved um we wanted this we're a small

00:04:49,940 --> 00:04:53,169
company we're about 20 20 people in

00:04:51,560 --> 00:04:55,669
weave work split across three sites and

00:04:53,169 --> 00:04:57,860
that means you know we don't have a

00:04:55,669 --> 00:05:00,710
15-person sre team who are going to run

00:04:57,860 --> 00:05:03,290
this service for so it has to be super

00:05:00,710 --> 00:05:06,050
easy to operate and manage it has to be

00:05:03,290 --> 00:05:07,610
complete low cognitive load you know has

00:05:06,050 --> 00:05:09,710
to be really really easy also you know

00:05:07,610 --> 00:05:10,880
we're really optimistic we think we're

00:05:09,710 --> 00:05:13,640
getting at tens of thousands of users

00:05:10,880 --> 00:05:15,050
when we launched it which kind of means

00:05:13,640 --> 00:05:16,640
you know I'm sure people will argue

00:05:15,050 --> 00:05:19,010
about this I'm sure there's lots lot

00:05:16,640 --> 00:05:20,210
larger users in this room but we're kind

00:05:19,010 --> 00:05:22,340
of aiming for tens of millions of

00:05:20,210 --> 00:05:24,320
samples a second for this service you

00:05:22,340 --> 00:05:27,050
know hopefully more but but that's kind

00:05:24,320 --> 00:05:28,850
of the initial aim the other thing in

00:05:27,050 --> 00:05:30,530
the market for monitoring I don't know

00:05:28,850 --> 00:05:31,580
how much you know about it but like you

00:05:30,530 --> 00:05:32,930
know there's a lot of players it's a

00:05:31,580 --> 00:05:34,849
very crowded market and it's a race to

00:05:32,930 --> 00:05:36,740
the bottom so

00:05:34,849 --> 00:05:38,839
the kind of margins and overhead in this

00:05:36,740 --> 00:05:39,919
month really really low and so this has

00:05:38,839 --> 00:05:41,360
to be cheap for us to round it has to be

00:05:39,919 --> 00:05:42,770
really cost-effective for us to run and

00:05:41,360 --> 00:05:44,539
this really actually is probably the key

00:05:42,770 --> 00:05:46,369
motivator in the design that we

00:05:44,539 --> 00:05:47,869
eventually went with yeah and I've

00:05:46,369 --> 00:05:51,529
covered reusing as much as pre cious as

00:05:47,869 --> 00:05:52,849
possible so so 2 and 3 kind of implied

00:05:51,529 --> 00:05:55,550
to us this needs to be horizontally

00:05:52,849 --> 00:05:57,349
scalable there are alternatives to doing

00:05:55,550 --> 00:05:59,629
it you can you can sharp based on user

00:05:57,349 --> 00:06:01,879
you could do other things but we really

00:05:59,629 --> 00:06:03,289
wanted this to be nice and horizontally

00:06:01,879 --> 00:06:04,369
scalable and also scalable without

00:06:03,289 --> 00:06:06,949
intervention you know we didn't want to

00:06:04,369 --> 00:06:08,389
be manually configuring you know which

00:06:06,949 --> 00:06:10,519
user went to which replica and stuff

00:06:08,389 --> 00:06:11,990
like that we also wanted to make it

00:06:10,519 --> 00:06:13,849
completely stateless because it's much

00:06:11,990 --> 00:06:15,019
much easier to run a stapler service

00:06:13,849 --> 00:06:17,330
than it is to understate full service

00:06:15,019 --> 00:06:19,849
now a lot of you are probably thinking

00:06:17,330 --> 00:06:22,749
how the hell do you run a stateless time

00:06:19,849 --> 00:06:25,279
series database well i'll show you how

00:06:22,749 --> 00:06:26,869
we use third-party Services was really

00:06:25,279 --> 00:06:29,509
what we did to solve that problem so we

00:06:26,869 --> 00:06:32,240
reused as much of Amazon is where we run

00:06:29,509 --> 00:06:33,740
like probably most people so we reused

00:06:32,240 --> 00:06:35,990
as many of their services as we could in

00:06:33,740 --> 00:06:37,039
a cost-effective ways we could so

00:06:35,990 --> 00:06:39,079
actually meant we have to be really

00:06:37,039 --> 00:06:41,469
careful about how much of the amazon

00:06:39,079 --> 00:06:43,819
services we consumed we really easy to

00:06:41,469 --> 00:06:46,579
run up a huge bill if we weren't careful

00:06:43,819 --> 00:06:48,019
and I just wanted to add out there cuz

00:06:46,579 --> 00:06:49,909
I'm seeing a lot of kind of people her a

00:06:48,019 --> 00:06:52,610
bit worried or daunted by this prospect

00:06:49,909 --> 00:06:53,569
this is all open source this is you go

00:06:52,610 --> 00:06:55,849
to my github there's a link at the end

00:06:53,569 --> 00:06:57,619
of it we weave works a very committed to

00:06:55,849 --> 00:06:59,539
open source project everything we do is

00:06:57,619 --> 00:07:01,579
open source and we've cloud you can go

00:06:59,539 --> 00:07:02,899
and download the source code and the

00:07:01,579 --> 00:07:04,459
binaries that we run them we've cloud

00:07:02,899 --> 00:07:06,319
and run it yourself and there's a few

00:07:04,459 --> 00:07:07,999
bits missing like the user management

00:07:06,319 --> 00:07:08,719
bit is something we're we've not open

00:07:07,999 --> 00:07:10,869
sourced because there are better

00:07:08,719 --> 00:07:13,369
alternatives but realistically

00:07:10,869 --> 00:07:15,860
everything we do is add filters so don't

00:07:13,369 --> 00:07:17,959
fit so here's the time scale we went

00:07:15,860 --> 00:07:21,050
with we started this project just over

00:07:17,959 --> 00:07:22,399
two months ago we started by circulating

00:07:21,050 --> 00:07:24,259
this designed up which hopefully some of

00:07:22,399 --> 00:07:25,550
you might have seen we send it to the to

00:07:24,259 --> 00:07:28,369
the Prometheus developers mailing list

00:07:25,550 --> 00:07:29,749
and we discussed it with most of the

00:07:28,369 --> 00:07:32,209
core developers and a whole bunch of

00:07:29,749 --> 00:07:34,219
other people and really we started

00:07:32,209 --> 00:07:37,009
hacking on this you know in earnest

00:07:34,219 --> 00:07:39,829
about two months ago we launched our

00:07:37,009 --> 00:07:42,490
first jobs into our development cluster

00:07:39,829 --> 00:07:45,889
a month ago and I'm giving this talk now

00:07:42,490 --> 00:07:48,229
allegedly so how what does this look

00:07:45,889 --> 00:07:48,680
like how does this work so we we've

00:07:48,229 --> 00:07:50,570
tried

00:07:48,680 --> 00:07:52,910
to take the principle of decomposing

00:07:50,570 --> 00:07:55,490
prometheus into a set of micro sensors

00:07:52,910 --> 00:07:56,509
and actually like in hindsight you can

00:07:55,490 --> 00:07:57,680
debate whether that was a good idea or

00:07:56,509 --> 00:08:01,370
not I don't think it was actually that

00:07:57,680 --> 00:08:03,560
important what we've done we've broken

00:08:01,370 --> 00:08:05,509
it down into three bits a retriever a

00:08:03,560 --> 00:08:07,370
distributor and an inner gesture and

00:08:05,509 --> 00:08:09,020
I'll describe what they all do but it's

00:08:07,370 --> 00:08:11,860
actually just easier if we trace through

00:08:09,020 --> 00:08:14,870
the life of a sample and how the sample

00:08:11,860 --> 00:08:16,580
propagates through the system so firstly

00:08:14,870 --> 00:08:18,530
you've got some jobs running in your

00:08:16,580 --> 00:08:20,900
data center and you also have to run a

00:08:18,530 --> 00:08:22,970
retriever in your data center and the

00:08:20,900 --> 00:08:25,039
retriever is just a vanilla prometheus

00:08:22,970 --> 00:08:28,310
there's very little modifications we've

00:08:25,039 --> 00:08:30,410
added to it the retriever scrapes like

00:08:28,310 --> 00:08:32,390
normal prometheus does gathers up all

00:08:30,410 --> 00:08:34,039
the samples and then sends them over to

00:08:32,390 --> 00:08:35,750
our service and there's some front-end

00:08:34,039 --> 00:08:37,909
authentication that goes on but that's

00:08:35,750 --> 00:08:40,550
not particularly interesting so then it

00:08:37,909 --> 00:08:42,200
will randomly hit a distributor so there

00:08:40,550 --> 00:08:43,610
we just use cue benetti's load balancing

00:08:42,200 --> 00:08:44,810
here you can use whatever you flavor

00:08:43,610 --> 00:08:46,940
load balancing you like but we hit a

00:08:44,810 --> 00:08:48,410
random distributor and the distributor

00:08:46,940 --> 00:08:49,940
now starts to understand the

00:08:48,410 --> 00:08:52,220
architecture of the system it

00:08:49,940 --> 00:08:54,290
understands which in jester should

00:08:52,220 --> 00:08:55,880
should own which sample effectively so

00:08:54,290 --> 00:08:59,180
the distributor will then distribute the

00:08:55,880 --> 00:09:00,770
samples amongst the injustice some and

00:08:59,180 --> 00:09:02,000
jesters might get more sample some just

00:09:00,770 --> 00:09:03,980
as might get less we'll cover that in a

00:09:02,000 --> 00:09:05,690
minute and then eventually this will

00:09:03,980 --> 00:09:06,950
happen over and over again and these and

00:09:05,690 --> 00:09:09,680
jesters are buffering it in memory and

00:09:06,950 --> 00:09:12,529
eventually they've got enough samples to

00:09:09,680 --> 00:09:15,080
write out a chunk 2 s 3 and then they'll

00:09:12,529 --> 00:09:16,580
also write an index into dynamo dB so

00:09:15,080 --> 00:09:18,200
the the key thing to note here is

00:09:16,580 --> 00:09:20,120
actually the chunk format is Bjorn's

00:09:18,200 --> 00:09:23,330
trunk former this is just the Prometheus

00:09:20,120 --> 00:09:25,160
chunk format the in jesters do keep the

00:09:23,330 --> 00:09:26,570
samples in memory so there's a

00:09:25,160 --> 00:09:29,060
durability issue that we should you know

00:09:26,570 --> 00:09:32,839
we need to fix before we before we you

00:09:29,060 --> 00:09:34,160
know start charging people for this but

00:09:32,839 --> 00:09:35,839
yeah that's basically the other thing

00:09:34,160 --> 00:09:37,190
that's quite interesting here you know

00:09:35,839 --> 00:09:41,079
you hear a lot in Prometheus community

00:09:37,190 --> 00:09:41,079
about push versus pull we have both

00:09:42,029 --> 00:09:48,610
so there you go so retriever I'll just

00:09:46,930 --> 00:09:50,019
quickly go over these components that

00:09:48,610 --> 00:09:52,540
we've got this is a vanilla prometheus

00:09:50,019 --> 00:09:55,089
we use brians PR for generic rights

00:09:52,540 --> 00:09:57,129
which basically means that you can get

00:09:55,089 --> 00:09:59,800
your data out of the retrievers or

00:09:57,129 --> 00:10:02,679
prometheus and send them to our two HTTP

00:09:59,800 --> 00:10:03,850
endpoints super simple this is still in

00:10:02,679 --> 00:10:05,410
PR but we're hoping you know we're

00:10:03,850 --> 00:10:07,569
hoping to kind of you know help that

00:10:05,410 --> 00:10:10,300
dark stream hopefully we've also added a

00:10:07,569 --> 00:10:11,889
small modification to prevent your local

00:10:10,300 --> 00:10:14,470
retriever from doing any indexing or

00:10:11,889 --> 00:10:16,240
doing any storage but this is optional

00:10:14,470 --> 00:10:17,559
you know if you wanted to use that as a

00:10:16,240 --> 00:10:19,779
primary system and use our systems a

00:10:17,559 --> 00:10:22,209
backup system that's of course something

00:10:19,779 --> 00:10:24,160
you can do and then yeah you won bin

00:10:22,209 --> 00:10:26,610
Prometheus retrieval only storage remote

00:10:24,160 --> 00:10:28,420
generic URL which is a bit of a mouthful

00:10:26,610 --> 00:10:30,069
this obviously uses the same

00:10:28,420 --> 00:10:32,050
configuration for scraping the same

00:10:30,069 --> 00:10:33,279
service discovery this does you know a

00:10:32,050 --> 00:10:37,059
lot of the work for us which is

00:10:33,279 --> 00:10:38,860
fantastic so one of the things that's

00:10:37,059 --> 00:10:41,019
that's kind of interesting here I

00:10:38,860 --> 00:10:44,589
thought we because the model for our

00:10:41,019 --> 00:10:46,360
scale out is lots and lots of users and

00:10:44,589 --> 00:10:47,860
those users are probably going to be you

00:10:46,360 --> 00:10:49,059
know not huge they're not going to be

00:10:47,860 --> 00:10:51,009
the biggest users they're probably going

00:10:49,059 --> 00:10:52,360
to be of some average size we didn't

00:10:51,009 --> 00:10:54,910
actually consider how to scale out the

00:10:52,360 --> 00:10:56,529
retrievers so the volcom project which

00:10:54,910 --> 00:10:57,670
was open source a couple days ago I saw

00:10:56,529 --> 00:10:59,050
the source code a couple of days ago has

00:10:57,670 --> 00:11:01,209
has actually done a lot more work there

00:10:59,050 --> 00:11:05,769
in in how to horizontally scale the

00:11:01,209 --> 00:11:08,519
retrieval step so distributor this uses

00:11:05,769 --> 00:11:11,639
a consistent hashing algorithm to assign

00:11:08,519 --> 00:11:14,019
ranges around a hash ring to in jesters

00:11:11,639 --> 00:11:15,490
the input to the hash we use is the user

00:11:14,019 --> 00:11:17,949
ID and the metric name and this is

00:11:15,490 --> 00:11:20,079
actually probably the key design

00:11:17,949 --> 00:11:22,809
trade-off that we had to make and we'll

00:11:20,079 --> 00:11:24,879
discuss that a bit more in a minute the

00:11:22,809 --> 00:11:28,120
tokens for the hash so that the hashes

00:11:24,879 --> 00:11:31,360
are 32 bit unsigned integer and so 0 all

00:11:28,120 --> 00:11:35,679
the way through 22 32 and the tokens are

00:11:31,360 --> 00:11:38,679
stored in console this is just for you

00:11:35,679 --> 00:11:39,970
know ease of use but we've we've we've

00:11:38,679 --> 00:11:42,100
got this library called weave mesh which

00:11:39,970 --> 00:11:43,929
is a gossip kind of eventual consistency

00:11:42,100 --> 00:11:45,869
library that's actually used by a lot

00:11:43,929 --> 00:11:49,929
manager sorry to steal your thunder

00:11:45,869 --> 00:11:51,069
fabian but yes so we're considering i

00:11:49,929 --> 00:11:52,670
was just talking to john are about maybe

00:11:51,069 --> 00:11:55,550
moving this over to you we've met

00:11:52,670 --> 00:11:56,690
should make it easier to use the

00:11:55,550 --> 00:11:58,670
distributor is also worth noting

00:11:56,690 --> 00:12:00,560
distributors where the prom ql queries

00:11:58,670 --> 00:12:03,530
are parsed using Prometheus's prom ql

00:12:00,560 --> 00:12:06,920
passes and you know where the queries

00:12:03,530 --> 00:12:08,420
are executed so let's talk about the

00:12:06,920 --> 00:12:12,770
input hash because this is kind of the

00:12:08,420 --> 00:12:15,410
key bye-bye not only signing a sharding

00:12:12,770 --> 00:12:16,880
on user ID and metric name sorry by not

00:12:15,410 --> 00:12:17,870
only shouting on user ID but by

00:12:16,880 --> 00:12:19,850
including metric name would get

00:12:17,870 --> 00:12:20,930
significantly better load balancing but

00:12:19,850 --> 00:12:22,400
it's still not perfect right because

00:12:20,930 --> 00:12:23,870
this is assuming lots of users are of

00:12:22,400 --> 00:12:26,570
different sizes if you just shot it on

00:12:23,870 --> 00:12:28,400
user name on user ID you would get quite

00:12:26,570 --> 00:12:29,750
uneven charting and so by adding more

00:12:28,400 --> 00:12:31,640
and more information into the hash you

00:12:29,750 --> 00:12:34,040
get better and better shouting ideally

00:12:31,640 --> 00:12:35,690
you'd want to add all of your your

00:12:34,040 --> 00:12:37,910
entire label set for every time series

00:12:35,690 --> 00:12:39,470
for example into the hash but then it

00:12:37,910 --> 00:12:42,200
comes quite difficult to root the

00:12:39,470 --> 00:12:44,240
queries so when you root query you need

00:12:42,200 --> 00:12:45,980
to know which in gesture to send the

00:12:44,240 --> 00:12:49,250
query for the last hours worth of data

00:12:45,980 --> 00:12:50,870
for so this compromise allows you to do

00:12:49,250 --> 00:12:53,510
pretty much most queries that Prometheus

00:12:50,870 --> 00:12:55,610
can do but also gives you pretty good

00:12:53,510 --> 00:12:57,770
load balancing and it's not perfect you

00:12:55,610 --> 00:12:58,970
know is a compromise for instance one of

00:12:57,770 --> 00:13:00,560
the things you can't do with the systems

00:12:58,970 --> 00:13:03,590
you can't do queries that don't specify

00:13:00,560 --> 00:13:05,330
metric name and when I started doing

00:13:03,590 --> 00:13:06,560
this I didn't know of any useful queries

00:13:05,330 --> 00:13:09,530
that didn't specify metric name and I've

00:13:06,560 --> 00:13:10,850
learned a lot so now you here so one

00:13:09,530 --> 00:13:12,140
more thing just one more shout out if

00:13:10,850 --> 00:13:14,840
you follow this link there's a

00:13:12,140 --> 00:13:17,600
presentation about Cassandra's virtual

00:13:14,840 --> 00:13:19,220
node scheme which actually myself and a

00:13:17,600 --> 00:13:21,830
couple of my colleagues developed what

00:13:19,220 --> 00:13:23,750
year is it four years ago a different

00:13:21,830 --> 00:13:25,490
company and it's exactly the same scheme

00:13:23,750 --> 00:13:27,080
we've used in Prometheus and it is quite

00:13:25,490 --> 00:13:28,700
powerful I'm not going to explain here

00:13:27,080 --> 00:13:29,960
there's a whole half an hour talk just

00:13:28,700 --> 00:13:33,800
about how you do the consistent hashing

00:13:29,960 --> 00:13:35,330
oh and it's worth mentioning we intend

00:13:33,800 --> 00:13:37,250
to do replication as well so we can

00:13:35,330 --> 00:13:39,080
survive in just the failure but we don't

00:13:37,250 --> 00:13:40,520
do that yet and then just the reputation

00:13:39,080 --> 00:13:42,260
will probably follow a kind of dynamo

00:13:40,520 --> 00:13:45,170
style if you will should have read the

00:13:42,260 --> 00:13:47,210
paper by now listing so the ingest the

00:13:45,170 --> 00:13:49,400
next next section this is actually a

00:13:47,210 --> 00:13:51,020
copy of the memory series storage from

00:13:49,400 --> 00:13:53,510
from Prometheus that I've kind of

00:13:51,020 --> 00:13:54,980
simplified a little bit it uses the same

00:13:53,510 --> 00:13:56,600
chunk format it uses a lot of the same

00:13:54,980 --> 00:13:58,430
data structures as Prometheus's

00:13:56,600 --> 00:14:01,760
internals it's packaged in a separate

00:13:58,430 --> 00:14:03,410
binary but but it needn't be it keeps

00:14:01,760 --> 00:14:05,210
everything memory for a configurable

00:14:03,410 --> 00:14:05,590
amount of time and currently that set no

00:14:05,210 --> 00:14:07,180
actually

00:14:05,590 --> 00:14:09,160
it sets like 10 minutes isn't it I

00:14:07,180 --> 00:14:11,170
forget it currently you know its

00:14:09,160 --> 00:14:12,370
intention it'll be about an hour it also

00:14:11,170 --> 00:14:14,050
stores and this is crucial it also

00:14:12,370 --> 00:14:18,280
stores an inverted in memory index to

00:14:14,050 --> 00:14:19,690
enable it to do the queries it's key to

00:14:18,280 --> 00:14:21,970
note this is all in memory so if an in

00:14:19,690 --> 00:14:23,590
jester dies or if the whole service dies

00:14:21,970 --> 00:14:27,130
you'll lose your hours worth of data so

00:14:23,590 --> 00:14:28,990
the intention here is to keep a commit

00:14:27,130 --> 00:14:31,270
log again in a kind of cassandra style

00:14:28,990 --> 00:14:33,430
commit log of all the chunks come of all

00:14:31,270 --> 00:14:35,110
these samples coming in so that if an in

00:14:33,430 --> 00:14:36,850
jester does die when it comes back you

00:14:35,110 --> 00:14:38,830
can replay its commit log rebuild its

00:14:36,850 --> 00:14:40,570
indexes rebuild its chunks and then

00:14:38,830 --> 00:14:42,760
continue where it left off but it's also

00:14:40,570 --> 00:14:45,550
key to note this only has an hour's

00:14:42,760 --> 00:14:47,890
worth of data in it so there's really

00:14:45,550 --> 00:14:49,180
very little kind of they're very non

00:14:47,890 --> 00:14:51,130
performance sensitive so it's like a

00:14:49,180 --> 00:14:56,860
really really slimmed down prometheus

00:14:51,130 --> 00:14:59,590
server here so next final bit DynamoDB

00:14:56,860 --> 00:15:00,850
an s3 we chose these as as I said we're

00:14:59,590 --> 00:15:03,400
a small company we don't have a lot of

00:15:00,850 --> 00:15:04,690
people working for them and we really

00:15:03,400 --> 00:15:06,310
you know having run quite a few

00:15:04,690 --> 00:15:09,670
Cassandra databases I did not want to

00:15:06,310 --> 00:15:10,930
run another one so um so DynamoDB is

00:15:09,670 --> 00:15:12,820
expensive but it's actually a really

00:15:10,930 --> 00:15:15,790
good service low latency it's fantastic

00:15:12,820 --> 00:15:19,930
so we just we just bought that it's

00:15:15,790 --> 00:15:21,580
quite easy to swap it out we haven't we

00:15:19,930 --> 00:15:23,710
haven't built the interfaces yet but we

00:15:21,580 --> 00:15:25,270
really want to have kind of the backends

00:15:23,710 --> 00:15:27,100
you know where this stuff gets stored be

00:15:25,270 --> 00:15:28,090
swappable and pluggable and Cassandra

00:15:27,100 --> 00:15:30,340
would be you know if you're not running

00:15:28,090 --> 00:15:31,690
an AWS or say you're running in GC and

00:15:30,340 --> 00:15:33,430
you want to use big table or say you're

00:15:31,690 --> 00:15:34,660
running on your own hardware in you

00:15:33,430 --> 00:15:35,710
wanna use HP a sox under that's

00:15:34,660 --> 00:15:40,360
something we really want to you know

00:15:35,710 --> 00:15:42,820
achieve this is how long ago running out

00:15:40,360 --> 00:15:45,550
of time the key format we've used is

00:15:42,820 --> 00:15:47,560
actually really crucial to how we enable

00:15:45,550 --> 00:15:49,840
queries here so we're maintaining an

00:15:47,560 --> 00:15:52,320
external inverted index and external

00:15:49,840 --> 00:15:54,580
memory inverted index here so it's quite

00:15:52,320 --> 00:15:56,230
it's quite important how you model the

00:15:54,580 --> 00:15:57,340
data right so if you don't know an

00:15:56,230 --> 00:15:58,840
inverted index would be the same

00:15:57,340 --> 00:16:03,160
technology that Google search users

00:15:58,840 --> 00:16:06,160
right to to match queries now what we do

00:16:03,160 --> 00:16:07,660
our inverted index is it's quite hard to

00:16:06,160 --> 00:16:09,640
explain DynamoDB has two concepts as a

00:16:07,660 --> 00:16:11,200
hash key and you can basically just do

00:16:09,640 --> 00:16:13,210
fetch it on the hash key and then it has

00:16:11,200 --> 00:16:14,920
a range key and a range prearranged Key

00:16:13,210 --> 00:16:16,570
allows you to do range queries now the

00:16:14,920 --> 00:16:18,520
combination of the two allows you to get

00:16:16,570 --> 00:16:19,130
good load good even balancing in your

00:16:18,520 --> 00:16:21,260
DynamoDB

00:16:19,130 --> 00:16:22,850
Buster and enables range queries which

00:16:21,260 --> 00:16:25,970
are super important only for time series

00:16:22,850 --> 00:16:27,620
data bases but also for doing member for

00:16:25,970 --> 00:16:29,600
doing label matches that aren't quality

00:16:27,620 --> 00:16:31,190
so i say i wanted a reg ex leg label

00:16:29,600 --> 00:16:34,070
match i have to do a range query over

00:16:31,190 --> 00:16:36,730
all the values of that label to find the

00:16:34,070 --> 00:16:39,380
chunks that that use that uh that match

00:16:36,730 --> 00:16:41,420
so again probably spend 20 minutes

00:16:39,380 --> 00:16:43,220
talking just about the dynamo DB schema

00:16:41,420 --> 00:16:44,750
and it's something Julius nice a long

00:16:43,220 --> 00:16:46,700
time working on and a long time going

00:16:44,750 --> 00:16:49,510
over and over and there's you know a lot

00:16:46,700 --> 00:16:51,620
of tweaks and a lot of subtle tea there

00:16:49,510 --> 00:16:52,910
but again this is it's kind of

00:16:51,620 --> 00:16:54,500
interesting and the chunks finally the

00:16:52,910 --> 00:16:55,850
chunks are stored necessary and in the

00:16:54,500 --> 00:16:58,820
future what we'd really like to do is

00:16:55,850 --> 00:17:00,260
build jobs that take the small one

00:16:58,820 --> 00:17:01,670
kilobyte chunks and build them up into

00:17:00,260 --> 00:17:04,850
larger and larger chunks to make the

00:17:01,670 --> 00:17:06,380
kind of efficient historical queries

00:17:04,850 --> 00:17:09,880
that I've been talking about you know

00:17:06,380 --> 00:17:12,050
really possible so evaluation you know

00:17:09,880 --> 00:17:13,790
we set out two months ago we thought it

00:17:12,050 --> 00:17:15,740
was pretty ambitious what what did we do

00:17:13,790 --> 00:17:19,189
well it works I should have really said

00:17:15,740 --> 00:17:21,199
it's alive but I didn't and we got it

00:17:19,189 --> 00:17:22,850
done in two months and this fantastic it

00:17:21,199 --> 00:17:23,780
actually we've been I don't know about

00:17:22,850 --> 00:17:25,100
you do list but I've been pretty

00:17:23,780 --> 00:17:27,439
impressed with its performance and its

00:17:25,100 --> 00:17:28,910
reliability it mean it is a prototype

00:17:27,439 --> 00:17:30,770
this is you know message I want to get

00:17:28,910 --> 00:17:32,510
out there like it is a prototype but it

00:17:30,770 --> 00:17:34,490
actually works and it seems to be pretty

00:17:32,510 --> 00:17:37,550
reliable it does all the queries you

00:17:34,490 --> 00:17:38,720
would expect it to do and this chap in

00:17:37,550 --> 00:17:41,600
the corner is because I'm going to do

00:17:38,720 --> 00:17:42,800
the good bad hashing this scheme means

00:17:41,600 --> 00:17:43,820
we can't do queries that don't involve

00:17:42,800 --> 00:17:45,860
metronomes I touched on that earlier

00:17:43,820 --> 00:17:47,060
this is probably a limitation that I

00:17:45,860 --> 00:17:48,170
can't live without it's probably

00:17:47,060 --> 00:17:50,060
something we going to have to fix and

00:17:48,170 --> 00:17:54,410
Julius night started talking about how

00:17:50,060 --> 00:17:55,790
we're going to fix that then it's also

00:17:54,410 --> 00:17:57,140
because of the hashing scheme and

00:17:55,790 --> 00:17:58,880
actually the real crux of this whole

00:17:57,140 --> 00:18:01,130
design is this compromise we've made in

00:17:58,880 --> 00:18:02,780
the hashing skin it's also possible to

00:18:01,130 --> 00:18:04,580
top spot and in jester especially if you

00:18:02,780 --> 00:18:05,870
want to be like an adversarial user and

00:18:04,580 --> 00:18:08,410
as we're going to be running this as a

00:18:05,870 --> 00:18:10,820
service we should expect a few of those

00:18:08,410 --> 00:18:11,930
so again this point to the fact that

00:18:10,820 --> 00:18:14,090
were going to have to come and revisit

00:18:11,930 --> 00:18:16,670
this this this scheme we've come up with

00:18:14,090 --> 00:18:18,440
and finally if you have a code no

00:18:16,670 --> 00:18:20,660
offense do this I think you'd be the

00:18:18,440 --> 00:18:22,430
first to say we did a bit of a rush job

00:18:20,660 --> 00:18:24,020
it's fine it works and it's getting

00:18:22,430 --> 00:18:25,490
better and we're plan is to

00:18:24,020 --> 00:18:27,410
incrementally improve this over the over

00:18:25,490 --> 00:18:29,510
the next few months to to have the kind

00:18:27,410 --> 00:18:32,540
of co quality that we really are you

00:18:29,510 --> 00:18:35,690
know desire yeah so

00:18:32,540 --> 00:18:40,430
next demo let's see if we can get this

00:18:35,690 --> 00:18:47,450
to work so do do do so first things

00:18:40,430 --> 00:18:48,890
first it's over here thank you so first

00:18:47,450 --> 00:18:51,140
things first I'm going to go to our dev

00:18:48,890 --> 00:18:52,760
cluster and hopefully it logs me in yeah

00:18:51,140 --> 00:18:55,760
i got i'm looking at this screen up here

00:18:52,760 --> 00:18:58,160
so that's better so this is where this

00:18:55,760 --> 00:18:59,210
will work this is a scope which I touch

00:18:58,160 --> 00:19:00,680
them the very beginning and I'm just

00:18:59,210 --> 00:19:03,290
going to use this to show you what the

00:19:00,680 --> 00:19:06,350
service looks like so that's not that's

00:19:03,290 --> 00:19:08,780
not the service huh let's go to this one

00:19:06,350 --> 00:19:10,340
up here and then let's go to this one

00:19:08,780 --> 00:19:12,170
down here so this is a Cuba Nettie's

00:19:10,340 --> 00:19:14,150
cluster blah blah blah Frankenstein

00:19:12,170 --> 00:19:15,770
namespace this is what it looks like and

00:19:14,150 --> 00:19:18,020
what you can see here we're only running

00:19:15,770 --> 00:19:19,310
single distributor right now it actually

00:19:18,020 --> 00:19:20,630
I don't know why we're not running more

00:19:19,310 --> 00:19:22,550
it would be super easy but it's not a

00:19:20,630 --> 00:19:24,700
bottleneck we're running foreign jesters

00:19:22,550 --> 00:19:27,350
one two three four five and jesters

00:19:24,700 --> 00:19:28,730
there's a retriever over here there's

00:19:27,350 --> 00:19:30,410
also running in our cluster because we

00:19:28,730 --> 00:19:31,460
like to dog for your own product and the

00:19:30,410 --> 00:19:33,230
retriever is straighten all the

00:19:31,460 --> 00:19:36,230
components which is why sees myriad of

00:19:33,230 --> 00:19:37,760
lines and then we've got console sitting

00:19:36,230 --> 00:19:39,020
kind of around the place and some

00:19:37,760 --> 00:19:40,700
memcached clusters as there are some n

00:19:39,020 --> 00:19:44,930
cash modes as well it's actually much

00:19:40,700 --> 00:19:46,640
easier if you my cursor isms much is if

00:19:44,930 --> 00:19:47,930
you kind of aggregate together all the

00:19:46,640 --> 00:19:50,870
pods in the same service and you get a

00:19:47,930 --> 00:19:51,920
much clearer picture of what the

00:19:50,870 --> 00:19:54,410
architecture of the system looks like

00:19:51,920 --> 00:19:58,660
and now the next thing is I can go to

00:19:54,410 --> 00:20:01,430
the multi-tenant Prometheus interface so

00:19:58,660 --> 00:20:02,930
there you go and this should all be the

00:20:01,430 --> 00:20:05,270
Prometheus user interface that you know

00:20:02,930 --> 00:20:06,440
and love and so I've got the same screen

00:20:05,270 --> 00:20:10,130
up here so I'm going to minimize its

00:20:06,440 --> 00:20:11,840
it's really confusing me now if we do

00:20:10,130 --> 00:20:13,160
let's err I talked about this hash

00:20:11,840 --> 00:20:14,750
scheme and if you're going to read the

00:20:13,160 --> 00:20:16,970
slide deck I link to you'll see how the

00:20:14,750 --> 00:20:18,620
hash scheme guarantees are pretty even

00:20:16,970 --> 00:20:19,580
load balancing but I've also talked

00:20:18,620 --> 00:20:20,840
about how there's some problems with

00:20:19,580 --> 00:20:23,360
that so let's see what the load

00:20:20,840 --> 00:20:24,920
balancing is let's get percent and you

00:20:23,360 --> 00:20:27,500
get tab completion like you doing all

00:20:24,920 --> 00:20:29,690
previous percent ownership and then

00:20:27,500 --> 00:20:32,360
press ENTER and there we go so you can

00:20:29,690 --> 00:20:33,560
see that some of those five nodes so you

00:20:32,360 --> 00:20:34,580
would expect each one to have 20 cent

00:20:33,560 --> 00:20:38,120
I'm sorry you can't read this let me

00:20:34,580 --> 00:20:39,230
here they go you'd expect you'd expect

00:20:38,120 --> 00:20:41,150
everyone to have 20 and some of them

00:20:39,230 --> 00:20:43,340
have 19 and some of them have 20 11 to

00:20:41,150 --> 00:20:45,200
18 so it's not perfect it needs a bit

00:20:43,340 --> 00:20:46,310
more work but this is interesting well

00:20:45,200 --> 00:20:47,720
I'm in the dev

00:20:46,310 --> 00:20:48,860
environment here let's switch to a

00:20:47,720 --> 00:20:50,300
different environment and really

00:20:48,860 --> 00:20:52,400
demonstrate that this is multi-tenancy

00:20:50,300 --> 00:20:56,810
all right so if i switch to the prod

00:20:52,400 --> 00:21:02,320
environment to do and i'm not going to

00:20:56,810 --> 00:21:02,320
atheists and then i type person

00:21:03,170 --> 00:21:12,100
they look the same was I Thank You

00:21:08,090 --> 00:21:12,100
Johnny let's switch the dev environment

00:21:13,810 --> 00:21:18,970
and type sent

00:21:21,410 --> 00:21:24,230
and they look different now I'll look

00:21:22,850 --> 00:21:26,060
there's only four of them so this just

00:21:24,230 --> 00:21:27,950
you know quickly obviously proves to

00:21:26,060 --> 00:21:31,550
everyone unequivocally that this is

00:21:27,950 --> 00:21:33,050
really a multi-talented system you can

00:21:31,550 --> 00:21:34,940
do other queries obviously this is just

00:21:33,050 --> 00:21:40,930
a normal prometheus does anyone have a

00:21:34,940 --> 00:21:45,830
favorite query no good then I'll move on

00:21:40,930 --> 00:21:48,470
so what's next you know what are we

00:21:45,830 --> 00:21:50,420
going to do next well we very much

00:21:48,470 --> 00:21:51,920
intend to continue working on this we

00:21:50,420 --> 00:21:53,540
currently don't support recording rules

00:21:51,920 --> 00:21:55,400
and we don't be alerting so that's very

00:21:53,540 --> 00:21:56,930
much the next thing I'm going to do and

00:21:55,400 --> 00:21:58,910
that's I think once you've got recording

00:21:56,930 --> 00:22:00,860
walls and alerting then we're going to

00:21:58,910 --> 00:22:02,810
start using this as our main monitoring

00:22:00,860 --> 00:22:04,550
system and really dogfooding it then

00:22:02,810 --> 00:22:05,750
reliability you know there's a long list

00:22:04,550 --> 00:22:07,430
of things we need to do we need to

00:22:05,750 --> 00:22:09,700
implement replication we need to get the

00:22:07,430 --> 00:22:11,630
injector lifecycle code working properly

00:22:09,700 --> 00:22:13,310
consider separating out the query

00:22:11,630 --> 00:22:14,870
service so we can scale up independently

00:22:13,310 --> 00:22:16,430
I don't know that something we keep

00:22:14,870 --> 00:22:17,570
going round on or maybe just condense it

00:22:16,430 --> 00:22:19,640
all down into a single service to make

00:22:17,570 --> 00:22:22,250
it easier to easier to use query

00:22:19,640 --> 00:22:23,990
parallelization I touched on background

00:22:22,250 --> 00:22:26,150
chunk coalescing I touched on co clean

00:22:23,990 --> 00:22:29,360
up the other thing I'm super keen on

00:22:26,150 --> 00:22:31,550
this is currently my load my personal

00:22:29,360 --> 00:22:33,950
Fork of Prometheus and I just worked on

00:22:31,550 --> 00:22:35,150
there in that repo but I hopefully some

00:22:33,950 --> 00:22:37,160
of the changes we make we can work with

00:22:35,150 --> 00:22:38,720
the upstream team to with the with the

00:22:37,160 --> 00:22:40,610
upstream maintain errs to upstream and

00:22:38,720 --> 00:22:41,990
and then really figure out exactly kind

00:22:40,610 --> 00:22:44,030
of you know whether the whole thing

00:22:41,990 --> 00:22:45,710
should go upstream or whether different

00:22:44,030 --> 00:22:49,790
bits of it or exactly what you mean it's

00:22:45,710 --> 00:22:53,870
it's at my hands but but yeah so that's

00:22:49,790 --> 00:22:56,860
kind of it this lines don't work there

00:22:53,870 --> 00:22:56,860
we go questions

00:22:56,909 --> 00:22:58,970
I

00:23:13,770 --> 00:23:18,700
you mentioned that you were keeping some

00:23:16,600 --> 00:23:20,440
of the the time series and memory in the

00:23:18,700 --> 00:23:22,270
in jester for an hour that's right um

00:23:20,440 --> 00:23:23,800
when you actually do queries do the

00:23:22,270 --> 00:23:26,710
queries have to go to a combination of

00:23:23,800 --> 00:23:28,300
s3 and the ingestion okay that's why the

00:23:26,710 --> 00:23:30,040
query code lives in the distributor

00:23:28,300 --> 00:23:33,010
because it's jus be the distributor has

00:23:30,040 --> 00:23:34,180
the knowledge of which ingest restoring

00:23:33,010 --> 00:23:36,850
that our in memory for that particular

00:23:34,180 --> 00:23:40,900
time series so yeah it goes and hits

00:23:36,850 --> 00:23:42,460
that it goes in hits s3 and Bjorn was

00:23:40,900 --> 00:23:43,810
alluding to these iterators so all we've

00:23:42,460 --> 00:23:46,030
done is built a couple more iterators

00:23:43,810 --> 00:23:48,250
emerging iterator and some iterators for

00:23:46,030 --> 00:23:49,810
the chunk store and that basically meant

00:23:48,250 --> 00:23:57,900
us all up that was actually Judas that

00:23:49,810 --> 00:23:57,900
did that the blame in any more questions

00:23:58,290 --> 00:24:02,340
well yep O'Connor

00:24:05,000 --> 00:24:09,090
so this is going to stay open source you

00:24:07,920 --> 00:24:10,200
move your than I mean if you are

00:24:09,090 --> 00:24:11,610
finished at some point and have a

00:24:10,200 --> 00:24:13,440
product and sell it but would still be

00:24:11,610 --> 00:24:15,210
open source at that point in every I've

00:24:13,440 --> 00:24:16,470
miss not my company but everything we do

00:24:15,210 --> 00:24:18,000
is open source and everything we've done

00:24:16,470 --> 00:24:19,440
has always been open source so I think

00:24:18,000 --> 00:24:21,360
you know something would really have to

00:24:19,440 --> 00:24:23,610
go bad plus we've done it under the same

00:24:21,360 --> 00:24:26,160
license that prometheus is under so you

00:24:23,610 --> 00:24:27,870
know there's no way to unopen source it

00:24:26,160 --> 00:24:29,100
really is there in that we're we're yet

00:24:27,870 --> 00:24:30,540
everything we do is open source is going

00:24:29,100 --> 00:24:32,250
to stay off the sauce I'm pretty

00:24:30,540 --> 00:24:34,200
committed to that we get lots of

00:24:32,250 --> 00:24:35,700
questions you know an IRC and whatnot

00:24:34,200 --> 00:24:37,830
it's like can someone please run it for

00:24:35,700 --> 00:24:39,960
me so I don't think open sourcing it is

00:24:37,830 --> 00:24:41,520
necessarily a business problem yeah i

00:24:39,960 --> 00:24:43,260
mean that's that's the whole company's

00:24:41,520 --> 00:24:44,940
business model is you know build open

00:24:43,260 --> 00:24:49,140
source software I've run it for you you

00:24:44,940 --> 00:24:51,000
know I'm super curious about the

00:24:49,140 --> 00:24:52,800
performance of using s3s your back-end

00:24:51,000 --> 00:24:54,450
because I would have thought that you

00:24:52,800 --> 00:24:56,640
would have used like an EBS or something

00:24:54,450 --> 00:24:58,950
like that can you talk about what kind

00:24:56,640 --> 00:25:00,900
of performance you're seeing loading it

00:24:58,950 --> 00:25:03,570
off the best three yeah so that's that's

00:25:00,900 --> 00:25:05,520
an interesting question we we were

00:25:03,570 --> 00:25:07,560
suspect as well and for the multi-tenant

00:25:05,520 --> 00:25:09,600
scope that I briefly showed you we've

00:25:07,560 --> 00:25:11,990
done exactly the same thing so in index

00:25:09,600 --> 00:25:17,130
and dynamo DB and we store reports in s3

00:25:11,990 --> 00:25:19,230
the performance is ok but it depends how

00:25:17,130 --> 00:25:20,550
much you use it I'm assuming I mean some

00:25:19,230 --> 00:25:21,930
maybe somebody else knows more about s3

00:25:20,550 --> 00:25:24,540
than I do but I'm assuming behind the

00:25:21,930 --> 00:25:26,970
scenes in s3 they're scaling in response

00:25:24,540 --> 00:25:28,470
to the demand in response to load so if

00:25:26,970 --> 00:25:30,030
you don't use it very much your first

00:25:28,470 --> 00:25:31,500
query might be a few seconds but then

00:25:30,030 --> 00:25:32,940
suddenly are ones after that will be

00:25:31,500 --> 00:25:35,490
subset inquiries you know will be a few

00:25:32,940 --> 00:25:37,050
hundred milliseconds we you I briefly

00:25:35,490 --> 00:25:39,120
alluded to it as well we've got loads

00:25:37,050 --> 00:25:41,280
and loads of memcache in front of s3 so

00:25:39,120 --> 00:25:44,100
the reality is you shouldn't really be

00:25:41,280 --> 00:25:46,860
hitting s3 I think we do right ahead to

00:25:44,100 --> 00:25:48,330
a ps3 as well in the trunk yeah so two

00:25:46,860 --> 00:25:50,160
men pressure yeah so we right ahead cash

00:25:48,330 --> 00:25:52,290
as well so every time we flush a chunk 2

00:25:50,160 --> 00:25:53,790
s 3 we also put it into the memcache and

00:25:52,290 --> 00:25:56,250
then we leave memcache to tell oh you

00:25:53,790 --> 00:25:58,380
think so yeah the reality is I don't

00:25:56,250 --> 00:26:01,440
have a graph of it but your query

00:25:58,380 --> 00:26:02,940
shouldn't hit s3 you know s3 is really

00:26:01,440 --> 00:26:04,440
for those long term queries where we can

00:26:02,940 --> 00:26:08,210
massively paralyze them over a set of

00:26:04,440 --> 00:26:08,210
workers and we expect it to take seconds

00:26:09,679 --> 00:26:15,570
so we do have no of course you do um but

00:26:14,070 --> 00:26:20,490
Julie's built them so I don't know what

00:26:15,570 --> 00:26:26,220
they do any instrument here we go this

00:26:20,490 --> 00:26:28,380
is the do you mind if I show yeah we've

00:26:26,220 --> 00:26:30,240
got so i renamed the retriever job and

00:26:28,380 --> 00:26:33,780
because i brought this up in production

00:26:30,240 --> 00:26:35,130
this morning which you know why not so

00:26:33,780 --> 00:26:37,140
that's why that grass are broken but

00:26:35,130 --> 00:26:38,159
that the rest of the grass show stuff

00:26:37,140 --> 00:26:40,230
you know we had some problems of

00:26:38,159 --> 00:26:42,360
DynamoDB earlier because i saw

00:26:40,230 --> 00:26:43,590
everyone's I saw everyone's giving their

00:26:42,360 --> 00:26:45,150
presentations about the massive

00:26:43,590 --> 00:26:47,580
thousands millions of QPS they were

00:26:45,150 --> 00:26:48,690
doing and we're doing like five so I

00:26:47,580 --> 00:26:50,309
thought well I'll just turn all my

00:26:48,690 --> 00:26:53,280
retrieval periods down from like 15

00:26:50,309 --> 00:26:54,809
seconds to one second and just 15 X the

00:26:53,280 --> 00:26:57,830
load on the system so that's what that's

00:26:54,809 --> 00:27:00,450
why cause I then fixed the problem but

00:26:57,830 --> 00:27:02,159
it caused didn't and it's fine and then

00:27:00,450 --> 00:27:04,049
yeah so we do have we do have quite a

00:27:02,159 --> 00:27:05,429
lot metrics and this is actually this is

00:27:04,049 --> 00:27:07,500
one of the favored ones if anyone here

00:27:05,429 --> 00:27:09,120
Lee uses DynamoDB it has this its

00:27:07,500 --> 00:27:11,159
thickly provisioned unlike all the other

00:27:09,120 --> 00:27:14,340
amazon products so you have to tell them

00:27:11,159 --> 00:27:16,020
I want 800 QPS from a dynamo dB and if

00:27:14,340 --> 00:27:18,270
you go over that they throttle you and

00:27:16,020 --> 00:27:20,250
so one of the things when using DynamoDB

00:27:18,270 --> 00:27:21,809
you really really have to monitor is how

00:27:20,250 --> 00:27:23,909
much of this capacity the Stickley

00:27:21,809 --> 00:27:26,730
provisioned capacity you're using so

00:27:23,909 --> 00:27:28,620
that's that's that graph there but yeah

00:27:26,730 --> 00:27:29,730
we've got load to monitoring for it the

00:27:28,620 --> 00:27:34,559
problem right is knowing which bits

00:27:29,730 --> 00:27:36,570
useful yeah any more questions so one of

00:27:34,559 --> 00:27:38,900
the things I should have said whilst you

00:27:36,570 --> 00:27:42,000
think of your insightful questions is

00:27:38,900 --> 00:27:43,590
this is now like running and we're

00:27:42,000 --> 00:27:46,440
looking for people to try it out and

00:27:43,590 --> 00:27:48,600
tell us if they like it you know we we

00:27:46,440 --> 00:27:50,400
as I just discussed can only generate a

00:27:48,600 --> 00:27:51,840
limited amount of load ourselves and we

00:27:50,400 --> 00:27:53,909
want real world users to tell us whether

00:27:51,840 --> 00:27:56,970
this is useful whether this is so come

00:27:53,909 --> 00:27:58,860
along and try it out and yeah email help

00:27:56,970 --> 00:28:00,150
that we've got works apparently and i'll

00:27:58,860 --> 00:28:01,530
send you some instructions and add you

00:28:00,150 --> 00:28:03,860
to this little white list so you can

00:28:01,530 --> 00:28:05,920
have access to it

00:28:03,860 --> 00:28:05,920
Oh

00:28:11,450 --> 00:28:17,010
I was curious how you were doing testing

00:28:15,390 --> 00:28:19,770
so one of the problems we had was

00:28:17,010 --> 00:28:22,260
generating good test sets we do all kind

00:28:19,770 --> 00:28:23,580
of crazy like sine waves and stuff like

00:28:22,260 --> 00:28:26,400
this I don't know if you've come up with

00:28:23,580 --> 00:28:29,040
a good solution for that no no we

00:28:26,400 --> 00:28:31,410
haven't we we take the view of

00:28:29,040 --> 00:28:32,730
dogfooding basically so that's that's

00:28:31,410 --> 00:28:35,040
really our testavar we have a

00:28:32,730 --> 00:28:37,020
development environment in a separate

00:28:35,040 --> 00:28:38,550
Cuba Nettie's cluster that we continue

00:28:37,020 --> 00:28:40,560
every change that gets deployed there

00:28:38,550 --> 00:28:42,330
basically instantly and it monitors

00:28:40,560 --> 00:28:45,090
itself it's also monitoring production

00:28:42,330 --> 00:28:47,880
and so yeah dog food and unit tests and

00:28:45,090 --> 00:29:01,200
that's about it thank you Tom thank you

00:28:47,880 --> 00:29:04,110
very much oh I've been forced to give a

00:29:01,200 --> 00:29:05,850
sponsor pitch not for us lightly ours I

00:29:04,110 --> 00:29:08,670
don't normally do this how are they make

00:29:05,850 --> 00:29:10,050
it really quick we don't just do

00:29:08,670 --> 00:29:11,580
Prometheus as a service we have this

00:29:10,050 --> 00:29:13,920
thing called we've met it's an SDN for

00:29:11,580 --> 00:29:15,240
your containers you should use it it's

00:29:13,920 --> 00:29:17,240
quite cool actually i wrote we've dns

00:29:15,240 --> 00:29:20,670
and it's this gossip based dns library

00:29:17,240 --> 00:29:22,320
dns service that's pretty cool also the

00:29:20,670 --> 00:29:23,790
thing behind we've met is this weave

00:29:22,320 --> 00:29:26,720
mesh this gossip based library that

00:29:23,790 --> 00:29:29,910
Peter in the back Peter works on and

00:29:26,720 --> 00:29:31,230
that is the thing that Fabian hopefully

00:29:29,910 --> 00:29:33,000
will mention in one of the lightning

00:29:31,230 --> 00:29:34,320
talks this works with me sauce Cuba

00:29:33,000 --> 00:29:37,320
lefties in Dhaka the other thing you

00:29:34,320 --> 00:29:39,030
have seen is we've scope I that was

00:29:37,320 --> 00:29:40,590
ending again another thing we've worked

00:29:39,030 --> 00:29:43,050
on for quite a while it's pretty cool it

00:29:40,590 --> 00:29:45,510
visualizes stuff for you we've got a new

00:29:43,050 --> 00:29:47,400
office in Berlin and and for people

00:29:45,510 --> 00:29:49,020
don't know i think i'm not sure and yeah

00:29:47,400 --> 00:29:50,730
it's awesome it's I've never seen it but

00:29:49,020 --> 00:29:53,280
apparently looks really nice I'm going

00:29:50,730 --> 00:29:54,840
to see on monday and we have three

00:29:53,280 --> 00:29:56,370
offices with two other offices three

00:29:54,840 --> 00:29:57,960
officers are San Francisco in London and

00:29:56,370 --> 00:29:59,280
we're massively hiring we just got a big

00:29:57,960 --> 00:30:01,590
load of investment from google ventures

00:29:59,280 --> 00:30:03,800
so come work for us and that's the end

00:30:01,590 --> 00:30:03,800

YouTube URL: https://www.youtube.com/watch?v=3Tb4Wc0kfCM


