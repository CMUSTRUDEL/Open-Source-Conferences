Title: PromCon 2018: Autoscaling All Things Kubernetes with Prometheus
Publication date: 2018-11-10
Playlist: PromCon 2018
Description: 
	Speakers: Michael Hausenblas, Frederic Branczyk

Autoscaling in Kubernetes used to be an inconsistent concept, however using the new metrics APIs defined by Kubernetes SIG Instrumentation the monitoring system of choice can be used. As it turns out, Prometheus is a popular system being used alongside Kubernetes, and the community has already developed a custom-metrics-api adapter to be used for Prometheus. This means, we can now perform autoscaling on the cluster and application level with metrics collected by Prometheus.

While for some use cases single values are enough, for others more sophisticated historic metrics are necessary. In the context of the SIG Autoscaling, we're working on a Vertical Pod Autoscaler (VPA), allowing for vertical autoscaling of pods (that is, adapting resource limits and requests) based on metrics from Prometheus (see https://github.com/kubernetes/community/blob/master/contributors/design-proposals/autoscaling/vertical-pod-autoscaler.md).

Frederic and Michael will review the history of metrics in Kubernetes, discuss the current state of metrics and autoscaling on Kubernetes using Prometheus with a focus on VPAs as well as show it in action.
Captions: 
	00:00:12,370 --> 00:00:19,910
hi I'm Michael I'm a promise addict I

00:00:16,340 --> 00:00:24,829
work in the cloud platform team at Red

00:00:19,910 --> 00:00:27,439
Hat and hi Michael so I'm I'm Frederick

00:00:24,829 --> 00:00:30,469
I lead the monitoring team at Red Hat

00:00:27,439 --> 00:00:31,329
previously Korres and I basically work

00:00:30,469 --> 00:00:34,640
on anything

00:00:31,329 --> 00:00:37,640
Karuna DS and Prometheus and anything in

00:00:34,640 --> 00:00:39,170
between which is also why I work on the

00:00:37,640 --> 00:00:41,450
prometheus operator for example because

00:00:39,170 --> 00:00:43,420
that connects the two worlds so but

00:00:41,450 --> 00:00:46,579
today concretely we want to talk about

00:00:43,420 --> 00:00:51,140
auto-scaling everything on kubernetes

00:00:46,579 --> 00:00:53,960
with Prometheus and before we really

00:00:51,140 --> 00:00:57,019
dive into how all of that works let's

00:00:53,960 --> 00:00:59,059
make sure we all understand what we mean

00:00:57,019 --> 00:01:02,269
when we talk about auto-scaling so let's

00:00:59,059 --> 00:01:04,729
make sure we're all on the same page so

00:01:02,269 --> 00:01:08,360
on an abstract level auto-scaling is

00:01:04,729 --> 00:01:10,790
really about calculating the resources

00:01:08,360 --> 00:01:14,210
that we are going to need to cover the

00:01:10,790 --> 00:01:16,790
demand so really what why all of us are

00:01:14,210 --> 00:01:18,950
working on infrastructure is because our

00:01:16,790 --> 00:01:22,399
companies need to make money somehow

00:01:18,950 --> 00:01:24,369
right and we are the ones that provide

00:01:22,399 --> 00:01:27,700
resources to the rest of our companies

00:01:24,369 --> 00:01:32,659
to do this and to satisfy all of that

00:01:27,700 --> 00:01:34,850
and really how demand is measured is by

00:01:32,659 --> 00:01:36,770
metrics and obviously we do that with

00:01:34,850 --> 00:01:39,649
Prometheus which is why why we're here

00:01:36,770 --> 00:01:41,180
and in order to actually make use of all

00:01:39,649 --> 00:01:44,180
these things we need to collect them we

00:01:41,180 --> 00:01:47,390
need to make them queryable and we need

00:01:44,180 --> 00:01:50,630
to store them to query over time and

00:01:47,390 --> 00:01:53,149
what I started with is we ultimately do

00:01:50,630 --> 00:01:56,329
this in order to fulfill our service

00:01:53,149 --> 00:01:59,289
level objectives of our service level

00:01:56,329 --> 00:02:03,079
agreements so that basically means that

00:01:59,289 --> 00:02:06,700
we've promised our users some sort of

00:02:03,079 --> 00:02:10,160
availability or really whatever your

00:02:06,700 --> 00:02:12,530
service level objective is if you want

00:02:10,160 --> 00:02:14,390
to know more about these things I highly

00:02:12,530 --> 00:02:17,360
recommend reading the Google SR eBook

00:02:14,390 --> 00:02:20,569
there's loads of examples and even in

00:02:17,360 --> 00:02:23,270
the new workbook there are examples

00:02:20,569 --> 00:02:26,540
based on Prometheus and even prometheus

00:02:23,270 --> 00:02:29,300
direct queries and alerting rules and

00:02:26,540 --> 00:02:33,640
all of that but yeah basically the

00:02:29,300 --> 00:02:38,420
objective means I am promising my user

00:02:33,640 --> 00:02:42,010
99.99% uptime or availability and that's

00:02:38,420 --> 00:02:45,440
the SLO of the SLA and the indicator

00:02:42,010 --> 00:02:48,380
could be the combination of multiple

00:02:45,440 --> 00:02:54,110
services availability or really anything

00:02:48,380 --> 00:02:56,480
that influences that objective so now we

00:02:54,110 --> 00:02:59,960
know on a on an abstract level what

00:02:56,480 --> 00:03:01,760
auto-scaling is and why we do it now in

00:02:59,960 --> 00:03:03,680
kubernetes how does that what does that

00:03:01,760 --> 00:03:06,800
look like in kubernetes we really have

00:03:03,680 --> 00:03:08,300
two high-level kinds of auto scaling and

00:03:06,800 --> 00:03:09,980
one of them is not really auto scaling

00:03:08,300 --> 00:03:12,320
which is why I'm going to start start

00:03:09,980 --> 00:03:14,780
with that which is cluster level auto

00:03:12,320 --> 00:03:17,720
scaling so sizing our cluster based on

00:03:14,780 --> 00:03:20,060
the man and why I'm saying that is

00:03:17,720 --> 00:03:23,900
because in kubernetes this is not really

00:03:20,060 --> 00:03:26,600
based on any metrics in kubernetes

00:03:23,900 --> 00:03:28,940
cluster auto scaling works by the

00:03:26,600 --> 00:03:31,460
scheduler attempting to schedule a new

00:03:28,940 --> 00:03:33,350
pod so some workload and it doesn't fit

00:03:31,460 --> 00:03:37,900
into our kubernetes cluster anymore and

00:03:33,350 --> 00:03:40,370
if that's the case and adding a new node

00:03:37,900 --> 00:03:44,140
would solve that problem then we would

00:03:40,370 --> 00:03:47,000
increase the size of the cluster whereas

00:03:44,140 --> 00:03:49,610
the application or the workload auto

00:03:47,000 --> 00:03:52,760
scaling within kubernetes actually does

00:03:49,610 --> 00:03:54,530
work with metrics over time and that's

00:03:52,760 --> 00:03:58,130
what we're going to be taking a closer

00:03:54,530 --> 00:04:00,650
look at and the two like sub categories

00:03:58,130 --> 00:04:04,220
within that that we focus on in

00:04:00,650 --> 00:04:06,680
kubernetes is horizontal so how do we

00:04:04,220 --> 00:04:08,990
horizontal horizontally scale our

00:04:06,680 --> 00:04:11,990
workloads and how do we vertically see

00:04:08,990 --> 00:04:14,510
how those and I'm gonna talk about that

00:04:11,990 --> 00:04:16,040
a little bit more so and in tributaries

00:04:14,510 --> 00:04:19,310
we call this the horizontal pod

00:04:16,040 --> 00:04:21,380
autoscaler and really the resources that

00:04:19,310 --> 00:04:24,350
we are scaling here because remember we

00:04:21,380 --> 00:04:26,570
said auto scaling as sizing resources

00:04:24,350 --> 00:04:29,000
based on demand the resources were

00:04:26,570 --> 00:04:32,240
increasing or decreasing here are the

00:04:29,000 --> 00:04:34,010
replicas of our workload but that means

00:04:32,240 --> 00:04:36,569
that our applications actually need to

00:04:34,010 --> 00:04:40,020
be designed in order to be able to scale

00:04:36,569 --> 00:04:42,229
this way and sometimes in the kubernetes

00:04:40,020 --> 00:04:45,389
world people call this cloud native

00:04:42,229 --> 00:04:47,009
applications when applications or

00:04:45,389 --> 00:04:49,619
stateful applications are able to do

00:04:47,009 --> 00:04:52,559
this and really what all that means is

00:04:49,619 --> 00:04:56,610
that we can increase or change the

00:04:52,559 --> 00:04:59,669
amount of replicas based on demand yeah

00:04:56,610 --> 00:05:02,309
but our applications need to be designed

00:04:59,669 --> 00:05:06,719
that way but there's a lot of useful

00:05:02,309 --> 00:05:08,639
applications that are not cloud native

00:05:06,719 --> 00:05:12,029
or do not work in the way that we can

00:05:08,639 --> 00:05:14,360
just add more replicas and we decrease

00:05:12,029 --> 00:05:16,469
our query latency or something like that

00:05:14,360 --> 00:05:18,629
and that's fine

00:05:16,469 --> 00:05:22,409
there's loads of perfectly good

00:05:18,629 --> 00:05:24,809
databases out there that do not scale to

00:05:22,409 --> 00:05:28,709
plan at scale and that are still

00:05:24,809 --> 00:05:31,349
extremely useful but but those kinds of

00:05:28,709 --> 00:05:33,389
loop workloads still need to be sized

00:05:31,349 --> 00:05:35,069
appropriately and that's where vertical

00:05:33,389 --> 00:05:37,949
auto scaling comes in and we call this

00:05:35,069 --> 00:05:43,289
the vertical pot out of scale and so

00:05:37,949 --> 00:05:44,999
here we size resources really the CPU

00:05:43,289 --> 00:05:50,129
and memory that is available to that

00:05:44,999 --> 00:05:52,649
Todd or workload and as I said this is

00:05:50,129 --> 00:05:55,319
less complicated for the actual workload

00:05:52,649 --> 00:05:57,899
to be designed because it will just

00:05:55,319 --> 00:06:00,389
really use whatever it will use right

00:05:57,899 --> 00:06:03,419
and we with vertical pot auto scaling

00:06:00,389 --> 00:06:06,269
make sure that it will get the resources

00:06:03,419 --> 00:06:09,149
that it needs however that does mean

00:06:06,269 --> 00:06:13,079
that the auto scaling mechanism itself

00:06:09,149 --> 00:06:15,719
is a lot more complicated and we'll see

00:06:13,079 --> 00:06:18,829
we'll see later why that is now let's

00:06:15,719 --> 00:06:21,240
just take a look at how all of this

00:06:18,829 --> 00:06:23,039
historically worked in kubernetes and

00:06:21,240 --> 00:06:26,430
how it works today because it has

00:06:23,039 --> 00:06:29,129
evolved over time so within kubernetes

00:06:26,430 --> 00:06:32,189
there used to be this or still is this

00:06:29,129 --> 00:06:33,809
component called heap stir it is as of

00:06:32,189 --> 00:06:38,999
this rasili cycle for kubernetes

00:06:33,809 --> 00:06:41,909
deprecated and heap stir went to all

00:06:38,999 --> 00:06:44,939
kubernetes nodes and collected metrics

00:06:41,909 --> 00:06:47,669
from c advisor so every 60 seconds it

00:06:44,939 --> 00:06:50,550
went to all the kubernetes nodes and

00:06:47,669 --> 00:06:53,789
collected all of those metrics and then

00:06:50,550 --> 00:06:58,020
wrote it into some time series database

00:06:53,789 --> 00:07:01,770
and this was how every single system had

00:06:58,020 --> 00:07:04,560
to work and I think by default in flux

00:07:01,770 --> 00:07:08,220
TB was the was like the metric sync that

00:07:04,560 --> 00:07:10,849
was being used here and yeah that that

00:07:08,220 --> 00:07:14,280
worked fine in this kind of architecture

00:07:10,849 --> 00:07:19,289
and we could auto scale whoo

00:07:14,280 --> 00:07:21,479
but if you noticed we because of this

00:07:19,289 --> 00:07:23,659
architecture we really fundamentally

00:07:21,479 --> 00:07:27,949
were not able to use Prometheus for this

00:07:23,659 --> 00:07:30,599
which obviously makes us sad so enter

00:07:27,949 --> 00:07:32,099
resource and custom metrics API so we

00:07:30,599 --> 00:07:34,409
realized that this architecture won't

00:07:32,099 --> 00:07:36,780
really work and there were a variety of

00:07:34,409 --> 00:07:39,659
reasons why this architecture in the

00:07:36,780 --> 00:07:42,300
first place was suboptimal

00:07:39,659 --> 00:07:45,630
primarily for maintenance reasons so all

00:07:42,300 --> 00:07:48,000
the metrics things available in heap sir

00:07:45,630 --> 00:07:51,719
were actually in the heap sir code base

00:07:48,000 --> 00:07:53,639
and so what happened a lot was that some

00:07:51,719 --> 00:07:56,699
vendor who really wanted kubernetes

00:07:53,639 --> 00:07:59,190
support came came along and implemented

00:07:56,699 --> 00:08:01,110
that support and just abandoned it

00:07:59,190 --> 00:08:03,710
afterwards there's no that intent or

00:08:01,110 --> 00:08:06,659
anything but this happens and we said

00:08:03,710 --> 00:08:07,979
when we get to redesign something like

00:08:06,659 --> 00:08:10,349
this we need to make sure that this

00:08:07,979 --> 00:08:11,880
doesn't happen again and so when we

00:08:10,349 --> 00:08:15,949
designed the resource and custom metrics

00:08:11,880 --> 00:08:18,719
API we really wanted to make sure that

00:08:15,949 --> 00:08:20,690
we are not owning the implementation of

00:08:18,719 --> 00:08:24,120
these api's we are really just

00:08:20,690 --> 00:08:27,659
specifying well-known api's so that when

00:08:24,120 --> 00:08:31,949
someone wants support they will own that

00:08:27,659 --> 00:08:33,240
part and so that's what we did and we

00:08:31,949 --> 00:08:35,579
have these api's and they're well

00:08:33,240 --> 00:08:39,630
specified and there is a Prometheus

00:08:35,579 --> 00:08:43,380
adapter so we can now scale auto scale

00:08:39,630 --> 00:08:44,910
on Prometheus metrics and just just a

00:08:43,380 --> 00:08:47,100
fact about these api's as they work

00:08:44,910 --> 00:08:49,770
today they just return a single value so

00:08:47,100 --> 00:08:52,800
basically like the an instant result

00:08:49,770 --> 00:08:55,680
like the last entry in the time series

00:08:52,800 --> 00:08:57,779
and that's basically down here you can

00:08:55,680 --> 00:09:01,649
see the architecture of how that works

00:08:57,779 --> 00:09:04,410
so the kubernetes api has an a mechanism

00:09:01,649 --> 00:09:07,470
called api aggregation so we register

00:09:04,410 --> 00:09:10,110
the Prometheus adapter at the kubernetes

00:09:07,470 --> 00:09:12,390
api and whenever anyone asks the

00:09:10,110 --> 00:09:14,940
kubernetes api for metrics it will

00:09:12,390 --> 00:09:16,140
forward it to the Prometheus adapter

00:09:14,940 --> 00:09:18,930
which converts it to an actual

00:09:16,140 --> 00:09:21,540
Prometheus query and then returns the

00:09:18,930 --> 00:09:26,430
result for that so now we can actually

00:09:21,540 --> 00:09:29,790
use this to horizontally scale our

00:09:26,430 --> 00:09:33,570
workloads but it turns out this is not

00:09:29,790 --> 00:09:36,540
quite enough for also vertically auto

00:09:33,570 --> 00:09:42,920
scaling and this is where Michael comes

00:09:36,540 --> 00:09:50,330
in and tells us why a vertical thank you

00:09:42,920 --> 00:09:53,340
yeah okay alright so we learned so far

00:09:50,330 --> 00:09:55,370
how the basic metric stuff goes down

00:09:53,340 --> 00:09:58,110
especially how it goes down with

00:09:55,370 --> 00:10:00,030
horizontal part auto scaling and now

00:09:58,110 --> 00:10:03,890
we're gonna have a closer look including

00:10:00,030 --> 00:10:07,080
a demo around vertical pod auto scaling

00:10:03,890 --> 00:10:09,990
so first off we're gonna do the demo and

00:10:07,080 --> 00:10:12,630
whoever has been around last year in

00:10:09,990 --> 00:10:15,330
this place okay so you might have seen

00:10:12,630 --> 00:10:19,830
me failing in life and talk so play it

00:10:15,330 --> 00:10:26,850
safe this time I'm gonna play video not

00:10:19,830 --> 00:10:33,230
that's stupid now all I need to learn is

00:10:26,850 --> 00:10:33,230
how to operate VLC yeah I know right

00:10:35,330 --> 00:10:45,360
shouldn't there be a full screen all

00:10:41,850 --> 00:10:47,160
right okay so we decided that we're

00:10:45,360 --> 00:10:51,380
gonna first show you the demo and then

00:10:47,160 --> 00:10:55,110
explain what happened so we have here a

00:10:51,380 --> 00:10:58,470
as the kalidahs distribution we're using

00:10:55,110 --> 00:11:00,000
obviously open shift we have the VP a

00:10:58,470 --> 00:11:02,430
set up comes with a couple of components

00:11:00,000 --> 00:11:04,920
I'm gonna talk about that in a moment

00:11:02,430 --> 00:11:07,830
and then we have obviously Prometheus

00:11:04,920 --> 00:11:10,170
delivering us the metrics here and we

00:11:07,830 --> 00:11:13,890
have a very simple set up a deployment

00:11:10,170 --> 00:11:17,420
that initially has a certain request set

00:11:13,890 --> 00:11:19,730
for CPU and memory and then the

00:11:17,420 --> 00:11:21,110
APA kicks in and you know comes up with

00:11:19,730 --> 00:11:25,520
a recommendation so let's have a look at

00:11:21,110 --> 00:11:28,580
that so we have in the VP a namespace we

00:11:25,520 --> 00:11:30,170
have a couple of components here we have

00:11:28,580 --> 00:11:33,740
committees here and another namespace

00:11:30,170 --> 00:11:37,520
running and the actual target of the VP

00:11:33,740 --> 00:11:41,270
a that we want to scale vertically so we

00:11:37,520 --> 00:11:44,210
have a look at that you see in the upper

00:11:41,270 --> 00:11:46,880
part you see the actual VP a definition

00:11:44,210 --> 00:11:49,220
that essentially all you really need to

00:11:46,880 --> 00:11:53,060
have a look at is that label so every

00:11:49,220 --> 00:11:54,700
part no matter what the owner is that

00:11:53,060 --> 00:11:59,350
has that label in this case a p--

00:11:54,700 --> 00:12:02,210
hamster will be vertically scaled by

00:11:59,350 --> 00:12:04,660
this VP here and then we have the

00:12:02,210 --> 00:12:08,330
deployment initially two replicas and

00:12:04,660 --> 00:12:12,800
you look down their resources requests

00:12:08,330 --> 00:12:15,950
we have CPU and memory right one of the

00:12:12,800 --> 00:12:18,320
issues you might wonder how do I arrive

00:12:15,950 --> 00:12:20,240
at these initial values come back to it

00:12:18,320 --> 00:12:25,520
in a moment but that's the starting

00:12:20,240 --> 00:12:27,710
point right and now we kind of tell

00:12:25,520 --> 00:12:31,040
qualities go ahead and and apply that

00:12:27,710 --> 00:12:33,110
this is the desired State go off and in

00:12:31,040 --> 00:12:34,790
this case cube color create color apply

00:12:33,110 --> 00:12:36,710
would have probably been better it

00:12:34,790 --> 00:12:39,470
creates these resources the deployment

00:12:36,710 --> 00:12:41,750
kicks in the two pots spinning up and if

00:12:39,470 --> 00:12:44,960
we look into the parts then we will see

00:12:41,750 --> 00:12:46,730
that we have initially as requested the

00:12:44,960 --> 00:12:52,330
one hundred million cars and the 50

00:12:46,730 --> 00:12:54,860
megabytes right okay so now VPSs well

00:12:52,330 --> 00:12:57,040
I'm supposed to look after these parts

00:12:54,860 --> 00:12:59,980
because they are labeled with AB hamster

00:12:57,040 --> 00:13:02,810
so let's have a look at what the VPSs

00:12:59,980 --> 00:13:04,730
not much so far right just says yeah I'm

00:13:02,810 --> 00:13:06,470
aware of that but I don't really know

00:13:04,730 --> 00:13:08,720
anything about that without being able

00:13:06,470 --> 00:13:11,720
to observe these parts and what they are

00:13:08,720 --> 00:13:13,910
consuming VPA can't do anything about it

00:13:11,720 --> 00:13:18,320
right it just needs to learn first you

00:13:13,910 --> 00:13:20,150
know what is the actual usage and after

00:13:18,320 --> 00:13:22,160
some time and now we could be talking

00:13:20,150 --> 00:13:26,210
about cats or we have a little bit cat

00:13:22,160 --> 00:13:28,100
and the cat pattern today right it takes

00:13:26,210 --> 00:13:31,710
a little and maybe this is this is one

00:13:28,100 --> 00:13:34,830
thing to note here I'm using the the up

00:13:31,710 --> 00:13:39,240
mode auto essentially telling vpa hey

00:13:34,830 --> 00:13:40,920
you decide how to update these parts you

00:13:39,240 --> 00:13:43,230
can use other modes but initially this

00:13:40,920 --> 00:13:47,280
is the default and probably in this

00:13:43,230 --> 00:13:50,160
stage wise to use and at some point in

00:13:47,280 --> 00:13:53,670
time if we look at the parts there we

00:13:50,160 --> 00:13:56,070
will see that vpa comes up with a

00:13:53,670 --> 00:13:57,660
recommendation and says hey I noticed

00:13:56,070 --> 00:13:59,820
you're actually using that amount of

00:13:57,660 --> 00:14:02,490
memory that amount of CPU and if we look

00:13:59,820 --> 00:14:05,160
at the BPA again you see it down there

00:14:02,490 --> 00:14:07,260
and recommendation part hey I figured

00:14:05,160 --> 00:14:09,030
out this is the lower bound this is the

00:14:07,260 --> 00:14:12,030
upper bound and this is the target here

00:14:09,030 --> 00:14:14,880
right and this is the part we then can

00:14:12,030 --> 00:14:16,410
say well you know thanks for his

00:14:14,880 --> 00:14:21,030
recommendation I actually want to apply

00:14:16,410 --> 00:14:23,670
it right and that's what's what's gonna

00:14:21,030 --> 00:14:26,850
happen so the updater will based on the

00:14:23,670 --> 00:14:30,540
policy here take that and actually

00:14:26,850 --> 00:14:35,610
update the pot and again you know after

00:14:30,540 --> 00:14:40,470
some seconds the the pots will have the

00:14:35,610 --> 00:14:42,600
new requests set so in terms of the UX I

00:14:40,470 --> 00:14:45,210
guess we try to keep it as simple as

00:14:42,600 --> 00:14:49,470
possible the very well-known established

00:14:45,210 --> 00:14:51,510
pattern of opt-in pie by labels and you

00:14:49,470 --> 00:14:53,160
know creating this BPA yes you do need

00:14:51,510 --> 00:14:55,350
infrastructure but who doesn't love to

00:14:53,160 --> 00:14:59,280
deploy and run parameters there so

00:14:55,350 --> 00:15:01,320
should be rather straightforward I think

00:14:59,280 --> 00:15:04,350
I've reached the time that the pots are

00:15:01,320 --> 00:15:07,850
actually yes now we have it here at if

00:15:04,350 --> 00:15:11,210
you now look here you see that a new

00:15:07,850 --> 00:15:17,970
request for CPU and memory are in place

00:15:11,210 --> 00:15:23,070
right and that's pretty much it now the

00:15:17,970 --> 00:15:26,420
question is why are we doing that and

00:15:23,070 --> 00:15:26,420
how does it actually work

00:15:27,710 --> 00:15:34,320
so we had a look at that already right

00:15:30,830 --> 00:15:37,170
yamo we we are in Cadiz land we're ml

00:15:34,320 --> 00:15:39,270
engineers first and foremost the more

00:15:37,170 --> 00:15:41,270
advanced ones already you see chasing it

00:15:39,270 --> 00:15:44,490
but most of us are still doing you know

00:15:41,270 --> 00:15:48,360
and that little innocent

00:15:44,490 --> 00:15:50,459
heart of the Yemma file is really it's

00:15:48,360 --> 00:15:52,800
it's you know it's an optional thing and

00:15:50,459 --> 00:15:55,500
people often go like yeah I I don't know

00:15:52,800 --> 00:15:57,089
I I have no idea how much you know CPU

00:15:55,500 --> 00:16:01,350
or memory I should put there I just

00:15:57,089 --> 00:16:03,600
leave it out why not before we get into

00:16:01,350 --> 00:16:05,850
the the why and and how a little bit of

00:16:03,600 --> 00:16:07,529
terminology and and background who a

00:16:05,850 --> 00:16:11,190
viewer asked at last year as well who of

00:16:07,529 --> 00:16:14,000
you is familiar to a certain extent with

00:16:11,190 --> 00:16:16,560
communities show of hands please

00:16:14,000 --> 00:16:21,260
that's less than last year okay so it's

00:16:16,560 --> 00:16:25,200
like 30 percent ish okay let's more so

00:16:21,260 --> 00:16:29,520
what does in a certain level what does

00:16:25,200 --> 00:16:31,200
kun it is really do well the nodes

00:16:29,520 --> 00:16:35,010
they're offering resources right like

00:16:31,200 --> 00:16:37,500
CPU memory and so on the pots consume

00:16:35,010 --> 00:16:40,260
these resources they burn the cycles in

00:16:37,500 --> 00:16:42,899
the scheduler matches the needs of the

00:16:40,260 --> 00:16:45,029
pot placing them on certain notes that

00:16:42,899 --> 00:16:47,040
fulfill these needs CPU memory just

00:16:45,029 --> 00:16:49,410
being two kinds of resources there

00:16:47,040 --> 00:16:51,570
obviously others in terms of machine

00:16:49,410 --> 00:16:53,399
learning GPUs are important and they're

00:16:51,570 --> 00:16:56,910
you know discs and many many more but

00:16:53,399 --> 00:17:00,660
for our considerations only CPU time and

00:16:56,910 --> 00:17:03,120
memories is of relevance there are

00:17:00,660 --> 00:17:05,280
different types of resources some of

00:17:03,120 --> 00:17:07,860
them are compressible for example CPU

00:17:05,280 --> 00:17:10,500
time if I take away CPU time it would

00:17:07,860 --> 00:17:12,179
just run slower if I take it away memory

00:17:10,500 --> 00:17:13,740
well I might be taking away the wrong

00:17:12,179 --> 00:17:18,929
page and you might be dead

00:17:13,740 --> 00:17:20,870
so that's incompressible and Cletus has

00:17:18,929 --> 00:17:23,670
the notion of equality for service and

00:17:20,870 --> 00:17:24,929
he might start to see that it's kind of

00:17:23,670 --> 00:17:27,929
important that you're actually setting

00:17:24,929 --> 00:17:31,110
these requests so request is what the

00:17:27,929 --> 00:17:34,170
scheduler uses to determine which node I

00:17:31,110 --> 00:17:35,790
can order scheduler can use to place the

00:17:34,170 --> 00:17:39,510
pot to run the pot and tell the qubit

00:17:35,790 --> 00:17:41,970
locally hey run that pot for me limit

00:17:39,510 --> 00:17:44,820
means this is the absolute minute limit

00:17:41,970 --> 00:17:49,170
that this pot or actually under the

00:17:44,820 --> 00:17:52,530
container level can can can use if you

00:17:49,170 --> 00:17:54,360
set limit and request equally it says as

00:17:52,530 --> 00:17:56,640
I said limit is not used by the scalar

00:17:54,360 --> 00:17:57,400
scheduler only the request if you set it

00:17:56,640 --> 00:17:59,920
equal

00:17:57,400 --> 00:18:04,780
then you have guarantee a so-called

00:17:59,920 --> 00:18:06,670
guaranteed pot if you have a request set

00:18:04,780 --> 00:18:09,010
which is must be greater than zero and

00:18:06,670 --> 00:18:10,720
you have a limit which is greater than a

00:18:09,010 --> 00:18:14,800
request then you have so-called

00:18:10,720 --> 00:18:16,480
burstable resources there and best

00:18:14,800 --> 00:18:19,690
effort essentially means well you're

00:18:16,480 --> 00:18:21,160
lazy you're not setting either you're

00:18:19,690 --> 00:18:22,960
not setting the limit not setting the

00:18:21,160 --> 00:18:25,870
request and that is where when I

00:18:22,960 --> 00:18:28,960
initially said this is optional well

00:18:25,870 --> 00:18:31,030
kind of right if you don't care about

00:18:28,960 --> 00:18:34,750
the quality of service then yeah it's

00:18:31,030 --> 00:18:37,030
optional most of us kind of do so I

00:18:34,750 --> 00:18:39,340
would say you're probably better off in

00:18:37,030 --> 00:18:41,910
the burstable or at the guaranteed case

00:18:39,340 --> 00:18:46,030
depending on the use case

00:18:41,910 --> 00:18:49,000
all right motivation this is not

00:18:46,030 --> 00:18:51,190
something that we came up with we as in

00:18:49,000 --> 00:18:53,200
the kwid is you know contributors a

00:18:51,190 --> 00:18:55,090
community or whatever but this is really

00:18:53,200 --> 00:18:58,150
driven by users yes

00:18:55,090 --> 00:19:00,870
as with many many other things Google

00:18:58,150 --> 00:19:03,490
had that for for forever I guess

00:19:00,870 --> 00:19:06,190
autopilot there and the design is

00:19:03,490 --> 00:19:08,110
certainly improved by that but these are

00:19:06,190 --> 00:19:09,640
real-world uses out there that we're

00:19:08,110 --> 00:19:13,000
literally you know complaining about

00:19:09,640 --> 00:19:15,640
that and rightly so right they they said

00:19:13,000 --> 00:19:18,910
well you got to do that you got a set to

00:19:15,640 --> 00:19:21,190
request there but how you're going about

00:19:18,910 --> 00:19:24,160
that do you do that manually can you do

00:19:21,190 --> 00:19:25,780
that manually does that make sense and I

00:19:24,160 --> 00:19:27,700
do encourage you to read these these

00:19:25,780 --> 00:19:29,980
blog posts that just pulled out some

00:19:27,700 --> 00:19:32,530
specific parts that that are relevant

00:19:29,980 --> 00:19:35,500
here but this is really the motivation

00:19:32,530 --> 00:19:36,880
to to say well obviously we need to

00:19:35,500 --> 00:19:39,520
provide something there and luckily we

00:19:36,880 --> 00:19:42,010
have with with the auto pilot work of

00:19:39,520 --> 00:19:45,660
Google we have a blueprint there a way

00:19:42,010 --> 00:19:48,730
how to go about that so the goals why

00:19:45,660 --> 00:19:51,670
what are we aiming for obviously it's

00:19:48,730 --> 00:19:54,840
about automation so you want to free you

00:19:51,670 --> 00:19:58,240
know people to not do that manually

00:19:54,840 --> 00:20:01,780
because manually settings they are it's

00:19:58,240 --> 00:20:04,000
like well I think it should be to geeks

00:20:01,780 --> 00:20:05,470
and whatever so you either provisioning

00:20:04,000 --> 00:20:08,110
for the worst case which means a lot of

00:20:05,470 --> 00:20:09,750
waste because you're expecting you know

00:20:08,110 --> 00:20:10,960
this amount of traffic and you know

00:20:09,750 --> 00:20:13,260
that's

00:20:10,960 --> 00:20:16,330
you're putting there and in the request

00:20:13,260 --> 00:20:18,970
so this is brittle and and hearts people

00:20:16,330 --> 00:20:22,000
either as I said because it's it's it's

00:20:18,970 --> 00:20:26,320
optional don't care or because it's hard

00:20:22,000 --> 00:20:29,169
they don't do it but as we've learned

00:20:26,320 --> 00:20:33,659
if there are no requests set for the

00:20:29,169 --> 00:20:36,610
container then you end up invested right

00:20:33,659 --> 00:20:39,330
on the other hand you also want to

00:20:36,610 --> 00:20:41,620
improve utilization I'm not arguing for

00:20:39,330 --> 00:20:43,690
100% no one should be doing that because

00:20:41,620 --> 00:20:45,640
that means that in case of failure if

00:20:43,690 --> 00:20:47,470
you're up upgrading a node or whatever

00:20:45,640 --> 00:20:50,740
that you don't have any wiggle room left

00:20:47,470 --> 00:20:52,390
to to compensate for that but you might

00:20:50,740 --> 00:20:56,980
want to aim for I don't know

00:20:52,390 --> 00:21:00,850
60% 70% whatever so you allow the

00:20:56,980 --> 00:21:02,799
scheduler to better beam back and there

00:21:00,850 --> 00:21:04,960
is also an impact on other functionality

00:21:02,799 --> 00:21:07,840
within equalities for example the out of

00:21:04,960 --> 00:21:10,539
resource handling because if a node

00:21:07,840 --> 00:21:12,610
starts to you know running out of

00:21:10,539 --> 00:21:15,220
resources there is a certain defined

00:21:12,610 --> 00:21:17,700
protocol in there saying like well first

00:21:15,220 --> 00:21:22,659
I think it's priority and then it's

00:21:17,700 --> 00:21:24,610
around the Q s classes there so and

00:21:22,659 --> 00:21:26,110
maybe in future I'm not really sure if

00:21:24,610 --> 00:21:27,760
anyone knows about that where we are

00:21:26,110 --> 00:21:30,760
with the optimizing scheduler that they

00:21:27,760 --> 00:21:34,270
can actually reach shuffle proactively

00:21:30,760 --> 00:21:37,240
reshuffle pods that that might also be a

00:21:34,270 --> 00:21:39,250
case but for now it's mainly you want to

00:21:37,240 --> 00:21:41,649
automate that you don't want people to

00:21:39,250 --> 00:21:43,840
manually guess more or less guesstimates

00:21:41,649 --> 00:21:48,779
put in there and you want to improve

00:21:43,840 --> 00:21:52,649
utilization in a nutshell use cases

00:21:48,779 --> 00:21:55,770
where as we early hurt the HP aid

00:21:52,649 --> 00:21:58,440
horizontal pod autoscaler it's probably

00:21:55,770 --> 00:22:02,020
best suited for stateless apps

00:21:58,440 --> 00:22:04,809
especially the ones that you know if you

00:22:02,020 --> 00:22:07,419
horizontal can can scale them you get a

00:22:04,809 --> 00:22:09,970
better performance out there if you have

00:22:07,419 --> 00:22:12,220
a state full app for example you have

00:22:09,970 --> 00:22:14,740
your WordPress that you know encapsulate

00:22:12,220 --> 00:22:15,850
my cycle I believe in by default or you

00:22:14,740 --> 00:22:19,470
have something like a single node

00:22:15,850 --> 00:22:23,470
database or whatever that benefits from

00:22:19,470 --> 00:22:34,900
adding one minute

00:22:23,470 --> 00:22:37,340
what Wow okay what okay yeah legacy apps

00:22:34,900 --> 00:22:39,350
this is the API server we are talking

00:22:37,340 --> 00:22:45,140
about the API server this is the part

00:22:39,350 --> 00:22:49,190
where the Jesus Christ one we have 40

00:22:45,140 --> 00:22:51,650
minutes where the request comes in and

00:22:49,190 --> 00:22:54,320
you can actually mutate the request

00:22:51,650 --> 00:22:56,270
there and the basic idea of the EPA is

00:22:54,320 --> 00:22:57,770
you observe the resource consumption you

00:22:56,270 --> 00:22:59,990
build up that historic profile through

00:22:57,770 --> 00:23:01,910
the recommender you apply that to pots

00:22:59,990 --> 00:23:04,850
through the labels which is the updater

00:23:01,910 --> 00:23:06,800
and that is how it looks like probably

00:23:04,850 --> 00:23:09,410
not the easiest thing to digest most

00:23:06,800 --> 00:23:11,090
important thing here his storage history

00:23:09,410 --> 00:23:13,460
storage that is where parameters comes

00:23:11,090 --> 00:23:15,710
in because as we've learned early on the

00:23:13,460 --> 00:23:17,450
the API server there are metric serve

00:23:15,710 --> 00:23:19,100
really gives us just at the point in

00:23:17,450 --> 00:23:21,590
time there and then you have the VP a

00:23:19,100 --> 00:23:23,570
controller that looks after the VP a

00:23:21,590 --> 00:23:25,070
objects and the admission control that

00:23:23,570 --> 00:23:27,710
was in the previous bit where it

00:23:25,070 --> 00:23:32,900
interjects in the API server and updates

00:23:27,710 --> 00:23:34,550
the spec accordingly limitations this is

00:23:32,900 --> 00:23:38,030
really early days it's pre-alpha

00:23:34,550 --> 00:23:41,750
so my ask to you are our ask to you is

00:23:38,030 --> 00:23:43,910
really pleased with the crab up out of

00:23:41,750 --> 00:23:46,100
it tested T sub edge cases we really

00:23:43,910 --> 00:23:47,750
need feedback there is one part which is

00:23:46,100 --> 00:23:49,490
currently due to different reasons

00:23:47,750 --> 00:23:51,650
container runtime support and so on and

00:23:49,490 --> 00:23:53,960
so forth you can't in place update so it

00:23:51,650 --> 00:23:57,080
will create a new pod which sucks

00:23:53,960 --> 00:23:59,690
obviously also around you should Spike's

00:23:57,080 --> 00:24:03,050
unclear how to deal with that best so

00:23:59,690 --> 00:24:06,110
far okay I have a look at the issue of

00:24:03,050 --> 00:24:07,100
the design please provide feedback choy

00:24:06,110 --> 00:24:09,770
and sig auto-scaling

00:24:07,100 --> 00:24:11,090
we had lovely people there and sig

00:24:09,770 --> 00:24:12,760
instrumentation and sig auto-scaling

00:24:11,090 --> 00:24:15,470
work together towards a unified

00:24:12,760 --> 00:24:16,490
historical matrix API big shout out to

00:24:15,470 --> 00:24:19,010
Solly

00:24:16,490 --> 00:24:21,559
sorry Russ who put together the the

00:24:19,010 --> 00:24:24,790
screencast there of the demo and I think

00:24:21,559 --> 00:24:24,790
with that that's it

00:24:26,370 --> 00:24:29,580
[Applause]

00:24:30,080 --> 00:24:36,270
well tell you we have four minutes for

00:24:32,910 --> 00:24:38,190
questions but your hand up that light's

00:24:36,270 --> 00:24:43,620
very bright as well okay can actually

00:24:38,190 --> 00:24:47,010
seen your hands see you're nice to me

00:24:43,620 --> 00:24:49,890
nice to meet having all the same side hi

00:24:47,010 --> 00:24:52,200
thank you so much for the talk so I

00:24:49,890 --> 00:24:54,420
think you roughly address this real

00:24:52,200 --> 00:24:56,550
quick there at the end but maybe I was

00:24:54,420 --> 00:24:57,990
gonna ask you to maybe explain a little

00:24:56,550 --> 00:25:00,780
bit more about how you're thinking about

00:24:57,990 --> 00:25:02,670
hysteresis in terms of scaling up and

00:25:00,780 --> 00:25:05,850
down rapidly especially if we can't do

00:25:02,670 --> 00:25:09,420
in place updates really good question

00:25:05,850 --> 00:25:12,450
and I guess the the continent in a

00:25:09,420 --> 00:25:15,540
nutshell is there is the design document

00:25:12,450 --> 00:25:18,120
that specifies how things you know

00:25:15,540 --> 00:25:22,170
should work there is the current code

00:25:18,120 --> 00:25:24,570
but it's so early days that unless we

00:25:22,170 --> 00:25:27,540
get testing and feedback from outside

00:25:24,570 --> 00:25:29,850
the the usual suspects we're doing that

00:25:27,540 --> 00:25:32,700
and and you know we're probably blind to

00:25:29,850 --> 00:25:36,690
that or bias very heavily biased to to

00:25:32,700 --> 00:25:38,640
our workloads our use cases so for now

00:25:36,690 --> 00:25:39,810
this is certainly not anything that you

00:25:38,640 --> 00:25:40,440
can use in production again this is

00:25:39,810 --> 00:25:42,810
pre-alpha

00:25:40,440 --> 00:25:45,510
right it's not even part of that release

00:25:42,810 --> 00:25:46,500
cycle yet but what what should be

00:25:45,510 --> 00:25:49,470
happening

00:25:46,500 --> 00:25:51,870
once the in place updates coming in

00:25:49,470 --> 00:25:53,400
place that you essentially also in the

00:25:51,870 --> 00:25:55,260
belief also Google does that essentially

00:25:53,400 --> 00:25:57,180
project you say you're in a certain

00:25:55,260 --> 00:25:59,130
trajectory you're it looks like you're

00:25:57,180 --> 00:26:00,240
going up like that so I proactively give

00:25:59,130 --> 00:26:03,290
you a recommendation where you then

00:26:00,240 --> 00:26:07,410
again can opt in say yeah apply that or

00:26:03,290 --> 00:26:09,480
ignore that apparent spike but again

00:26:07,410 --> 00:26:12,300
this is really really it very much

00:26:09,480 --> 00:26:13,890
depends on what you can do what you can

00:26:12,300 --> 00:26:15,930
provide us and in terms of feedback

00:26:13,890 --> 00:26:18,270
right and anything you know and if you

00:26:15,930 --> 00:26:21,660
say hey this is crap right this is crap

00:26:18,270 --> 00:26:24,360
design also good feedback right because

00:26:21,660 --> 00:26:26,910
we really really really don't know yet

00:26:24,360 --> 00:26:29,190
other than the code that you can try out

00:26:26,910 --> 00:26:30,840
yourself would that really work for the

00:26:29,190 --> 00:26:32,490
majority of people it is a definitely a

00:26:30,840 --> 00:26:34,080
problem and there's that motivation a

00:26:32,490 --> 00:26:36,330
strong motivation there but is it the

00:26:34,080 --> 00:26:40,070
right solution is the right UX or

00:26:36,330 --> 00:26:40,070
whatever please tell us

00:26:40,879 --> 00:26:45,859
just what part of the question that you

00:26:43,489 --> 00:26:47,809
asked like basically how to prevent over

00:26:45,859 --> 00:26:51,139
Auto scanning like Auto scanning too

00:26:47,809 --> 00:26:54,979
often this is already solved in the HP a

00:26:51,139 --> 00:26:59,179
kind of at least so at least for scaling

00:26:54,979 --> 00:27:02,539
down there is a like a cool-down which

00:26:59,179 --> 00:27:05,450
is larger than the one going up so you

00:27:02,539 --> 00:27:07,220
can always more easily scale up then you

00:27:05,450 --> 00:27:09,590
can scale down and it's just a safety

00:27:07,220 --> 00:27:11,419
mechanism I can imagine maybe something

00:27:09,590 --> 00:27:17,299
like that could be possible for the VPI

00:27:11,419 --> 00:27:19,009
as well okay again it really depends on

00:27:17,299 --> 00:27:20,509
on the kind of reasons you look at if

00:27:19,009 --> 00:27:23,779
you have non-compressed reasons like

00:27:20,509 --> 00:27:25,820
memory you kind of like you know stuck

00:27:23,779 --> 00:27:31,369
there sorry okay and how its integrated

00:27:25,820 --> 00:27:34,340
with HP and prometheus so currently I

00:27:31,369 --> 00:27:36,349
would say don't necessarily use HP AAS

00:27:34,340 --> 00:27:38,479
in VPS together and premiere those

00:27:36,349 --> 00:27:41,929
integrations exactly that here right so

00:27:38,479 --> 00:27:43,489
the metric server you know always gives

00:27:41,929 --> 00:27:45,830
you just this one point in time there's

00:27:43,489 --> 00:27:47,570
one reading so you need something at I'm

00:27:45,830 --> 00:27:49,849
serious database sounds familiar that

00:27:47,570 --> 00:27:52,149
you know actually allows you confirming

00:27:49,849 --> 00:27:55,820
this data API there which would be

00:27:52,149 --> 00:27:58,369
historic what's the official term like

00:27:55,820 --> 00:28:00,679
there there is no historical metrics api

00:27:58,369 --> 00:28:02,720
defined yet and that is why sic

00:28:00,679 --> 00:28:05,419
instrumentation and cicadas guys are

00:28:02,720 --> 00:28:08,269
going to be working together to do that

00:28:05,419 --> 00:28:13,249
because really only cicadas scaling as a

00:28:08,269 --> 00:28:15,889
as a use case for that right now there's

00:28:13,249 --> 00:28:18,049
just we know that we need this and we

00:28:15,889 --> 00:28:22,519
want to design it we just haven't gotten

00:28:18,049 --> 00:28:24,990
to yet okay and we are out of time there

00:28:22,519 --> 00:28:30,170
so thank you very much

00:28:24,990 --> 00:28:30,170

YouTube URL: https://www.youtube.com/watch?v=yaB8I6_qR_k


