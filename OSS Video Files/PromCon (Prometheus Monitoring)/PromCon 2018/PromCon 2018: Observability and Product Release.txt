Title: PromCon 2018: Observability and Product Release
Publication date: 2018-11-10
Playlist: PromCon 2018
Description: 
	Speaker: Sneha Inguva

The pillars of observability have long been accepted as key components of any microservice-in-production. But what about those new products - those new features - that have yet to be released? Properly instrumenting and leveraging metrics at this stage is perhaps even more crucial; when a product is yet to be released, identifying and addressing early bugs is critical.

Within this talk, I will discuss how we have leveraged Prometheus to properly instrument and test features within the software-defined networking pillar at Digital Ocean. I will highlight instrumentation, key visualizations, as well as takeaways from our experience. Perhaps even more importantly, I will touch upon areas that we can certainly improve upon. Listeners - especially those on product teams - will be able to utilize these learnings and hopefully apply them before their releases as well.
Captions: 
	00:00:12,469 --> 00:00:18,029
great hi my name is Sneha and Guba I'm

00:00:16,080 --> 00:00:20,520
here to talk about Prometheus today as

00:00:18,029 --> 00:00:23,070
you can guess given that we are at prom

00:00:20,520 --> 00:00:24,800
con Soho my talk today is about

00:00:23,070 --> 00:00:27,689
observability and product release

00:00:24,800 --> 00:00:29,519
basically leveraging Prometheus to build

00:00:27,689 --> 00:00:31,289
and test new products it's a little bit

00:00:29,519 --> 00:00:33,210
about myself I'm a software engineer at

00:00:31,289 --> 00:00:35,460
digitalocean I'm currently on the

00:00:33,210 --> 00:00:37,170
networking services team and here's a

00:00:35,460 --> 00:00:39,269
photo of my cat in honor of

00:00:37,170 --> 00:00:43,860
international cat day which was actually

00:00:39,269 --> 00:00:47,969
yesterday so some stats um we use

00:00:43,860 --> 00:00:50,550
Prometheus at my company obvious at this

00:00:47,969 --> 00:00:52,920
point but we use it for you know both

00:00:50,550 --> 00:00:55,320
monitoring particular services as well

00:00:52,920 --> 00:00:57,750
as for data center wide like hypervisor

00:00:55,320 --> 00:01:00,180
and virtual machine monitoring so some

00:00:57,750 --> 00:01:03,660
stats we have about 90 million or more

00:01:00,180 --> 00:01:07,640
time series about 85 instances of

00:01:03,660 --> 00:01:11,940
Prometheus and we scrape at a rate about

00:01:07,640 --> 00:01:14,580
1.7 million samples per second but that

00:01:11,940 --> 00:01:18,990
was not always the case in the olden

00:01:14,580 --> 00:01:21,060
dark days we didn't use Prometheus we

00:01:18,990 --> 00:01:24,119
were using Nagios on different plugins

00:01:21,060 --> 00:01:26,240
like van RP plug-in to do blackbox

00:01:24,119 --> 00:01:28,320
monitoring this was hard you know

00:01:26,240 --> 00:01:29,970
operationally and then we also didn't

00:01:28,320 --> 00:01:33,090
get a lot of introspection into our

00:01:29,970 --> 00:01:36,390
services we then at some point moved to

00:01:33,090 --> 00:01:38,520
using collecti stats t graphite this was

00:01:36,390 --> 00:01:40,500
okay but you know we didn't have great

00:01:38,520 --> 00:01:42,180
querying abilities and we didn't

00:01:40,500 --> 00:01:44,880
necessarily have enough dimensionality

00:01:42,180 --> 00:01:47,670
of our metrics there's also very briefly

00:01:44,880 --> 00:01:50,700
are actually not briefly enough an open

00:01:47,670 --> 00:01:52,380
TS DB cluster and the issue with that

00:01:50,700 --> 00:01:53,689
was it was also difficult to

00:01:52,380 --> 00:01:57,030
operationally maintain

00:01:53,689 --> 00:01:58,790
thankfully some kind soul discovered

00:01:57,030 --> 00:02:01,409
prometheus and we moved to that

00:01:58,790 --> 00:02:02,790
prometheus is great offers us you know

00:02:01,409 --> 00:02:04,950
white box monitoring

00:02:02,790 --> 00:02:07,590
there's multi-dimensional data a lot of

00:02:04,950 --> 00:02:09,840
labels there's a great querying language

00:02:07,590 --> 00:02:12,090
it just helps us for like analytical

00:02:09,840 --> 00:02:14,730
purposes and for just general alerting

00:02:12,090 --> 00:02:17,519
around the same time we also moved to

00:02:14,730 --> 00:02:19,829
kubernetes so pre kubernetes we were

00:02:17,519 --> 00:02:22,800
using chef and bash scripts and other

00:02:19,829 --> 00:02:25,470
unholy things to deploy services

00:02:22,800 --> 00:02:27,480
this was difficult once again for

00:02:25,470 --> 00:02:29,820
various reasons but then luckily moving

00:02:27,480 --> 00:02:33,540
to kubernetes allowed us to easily

00:02:29,820 --> 00:02:35,520
update services made it easier to scale

00:02:33,540 --> 00:02:38,640
up or scale down services and then also

00:02:35,520 --> 00:02:41,190
given the API service discovery method

00:02:38,640 --> 00:02:44,250
was very easy to set up Prometheus and

00:02:41,190 --> 00:02:46,590
alert manager and set up alerting so

00:02:44,250 --> 00:02:48,420
this is what I discussed last year over

00:02:46,590 --> 00:02:51,270
at the beginning of this year I joined

00:02:48,420 --> 00:02:53,550
the network services team at my company

00:02:51,270 --> 00:02:55,740
which was interesting because whereas in

00:02:53,550 --> 00:02:58,110
the past I focused on how we were using

00:02:55,740 --> 00:03:00,240
Prometheus for services that were

00:02:58,110 --> 00:03:02,700
already in production this year I

00:03:00,240 --> 00:03:04,410
started to see how we use Prometheus

00:03:02,700 --> 00:03:06,660
before we even release a product when

00:03:04,410 --> 00:03:08,790
we're just building and testing a

00:03:06,660 --> 00:03:09,780
product so on that note that's what I'm

00:03:08,790 --> 00:03:12,420
going to talk about today

00:03:09,780 --> 00:03:14,970
I'm going to talk about how we have been

00:03:12,420 --> 00:03:18,600
using Prometheus for both the B PC

00:03:14,970 --> 00:03:20,580
product as well as the DHCP product so

00:03:18,600 --> 00:03:22,470
the plan for today I'm going to first

00:03:20,580 --> 00:03:24,420
talk about observability at digitalocean

00:03:22,470 --> 00:03:26,670
kind of what we measure I'm going to

00:03:24,420 --> 00:03:28,590
talk about our build instrument test and

00:03:26,670 --> 00:03:31,080
iterate loop and finally I'm going to

00:03:28,590 --> 00:03:34,290
end with the two examples I had on the

00:03:31,080 --> 00:03:35,880
previous slide so what exactly do we

00:03:34,290 --> 00:03:38,300
measure these are the three pillars of

00:03:35,880 --> 00:03:41,459
observability someone came up with that

00:03:38,300 --> 00:03:43,500
but we definitely still use them at

00:03:41,459 --> 00:03:45,810
digitalocean you know metrics being a

00:03:43,500 --> 00:03:47,850
time series of sample data very useful

00:03:45,810 --> 00:03:48,680
for both down sampling and collecting

00:03:47,850 --> 00:03:50,790
more data

00:03:48,680 --> 00:03:53,700
tracing very helpful in determining

00:03:50,790 --> 00:03:55,230
causality and then logging very useful

00:03:53,700 --> 00:03:57,959
when looking at needle in the haystack

00:03:55,230 --> 00:03:59,700
type events in terms of what we measure

00:03:57,959 --> 00:04:02,550
last year I mentioned the four golden

00:03:59,700 --> 00:04:05,160
signals still holds true we loved to

00:04:02,550 --> 00:04:08,340
measure latency traffic and saturation

00:04:05,160 --> 00:04:11,010
for request based services we also

00:04:08,340 --> 00:04:14,390
measure the use metrics utilization

00:04:11,010 --> 00:04:17,180
saturation and error rate for resources

00:04:14,390 --> 00:04:20,609
so you know the quote often is that

00:04:17,180 --> 00:04:23,190
using the use metrics often allow users

00:04:20,609 --> 00:04:24,930
to solve 80% of server issues with 5% of

00:04:23,190 --> 00:04:28,230
the effort and we've discovered that

00:04:24,930 --> 00:04:30,780
this is definitely true so that's

00:04:28,230 --> 00:04:33,510
exactly what we measure at digitalocean

00:04:30,780 --> 00:04:35,139
but how do we measure it and so that's

00:04:33,510 --> 00:04:37,810
why I'm going to talk about our

00:04:35,139 --> 00:04:40,389
building process typically we you know

00:04:37,810 --> 00:04:42,550
design the service submit an RFC get

00:04:40,389 --> 00:04:44,979
some feedback maybe iterate some more

00:04:42,550 --> 00:04:47,949
then we write the service and go we do

00:04:44,979 --> 00:04:49,840
love go integer lotion we often use

00:04:47,949 --> 00:04:51,879
internally shared libraries in our mono

00:04:49,840 --> 00:04:53,199
repo which is known as Cthulhu I think

00:04:51,879 --> 00:04:55,629
there's a couple of blog posts out there

00:04:53,199 --> 00:04:57,219
about it so here's an example of one of

00:04:55,629 --> 00:05:00,279
the internally shared libraries we use

00:04:57,219 --> 00:05:02,400
it's a wrapper around G RPC it's super

00:05:00,279 --> 00:05:04,569
helpful because it has a variety of

00:05:02,400 --> 00:05:07,569
observability interceptors which allow

00:05:04,569 --> 00:05:10,270
us to automatically get metrics such as

00:05:07,569 --> 00:05:14,159
you know request latency just request

00:05:10,270 --> 00:05:16,659
rate error rate and just automatically

00:05:14,159 --> 00:05:19,389
once we actually build the service we

00:05:16,659 --> 00:05:21,250
then instrument so we typically send you

00:05:19,389 --> 00:05:23,590
know logs to centralized logging we

00:05:21,250 --> 00:05:25,719
spend send spans or you know time to

00:05:23,590 --> 00:05:27,909
operations of processes to trace

00:05:25,719 --> 00:05:30,099
collectors and then use a tracing

00:05:27,909 --> 00:05:32,439
back-end such as a light step we then

00:05:30,099 --> 00:05:34,389
also set up Prometheus metrics so the

00:05:32,439 --> 00:05:35,979
three-way we typically instrument an

00:05:34,389 --> 00:05:38,349
application is you know simply by using

00:05:35,979 --> 00:05:41,020
the go client here's an example of us

00:05:38,349 --> 00:05:42,909
just using gauge metrics configuring

00:05:41,020 --> 00:05:46,029
them and in Alysha initialize metrics

00:05:42,909 --> 00:05:48,069
method and then setting them we also

00:05:46,029 --> 00:05:49,360
frequently make use of the collector

00:05:48,069 --> 00:05:50,830
interface and if you're familiar with

00:05:49,360 --> 00:05:52,779
Prometheus you are probably familiar

00:05:50,830 --> 00:05:55,389
with this it's simply an interface that

00:05:52,779 --> 00:05:58,149
is often implemented by Prometheus

00:05:55,389 --> 00:06:00,789
exporters so that's one way that we

00:05:58,149 --> 00:06:03,159
commonly use it we add we occasionally

00:06:00,789 --> 00:06:05,110
build our own exporters such as the OVS

00:06:03,159 --> 00:06:08,050
exporter that we use on the networking

00:06:05,110 --> 00:06:10,839
team or we just use external exporters

00:06:08,050 --> 00:06:12,849
for third-party software it's an example

00:06:10,839 --> 00:06:14,800
of that it's a my sequel exporter rabbit

00:06:12,849 --> 00:06:17,259
um rabbitmq exporter node exporter

00:06:14,800 --> 00:06:19,960
there's a ton and we use we make use of

00:06:17,259 --> 00:06:21,939
a lot of them the other thing that we've

00:06:19,960 --> 00:06:25,319
especially been doing recently is having

00:06:21,939 --> 00:06:28,389
internal structures within our code also

00:06:25,319 --> 00:06:29,860
implement the collector interface this

00:06:28,389 --> 00:06:31,810
is particularly useful because

00:06:29,860 --> 00:06:34,659
especially within two of the examples

00:06:31,810 --> 00:06:37,659
I'll discuss later we have a lot of in

00:06:34,659 --> 00:06:39,849
memory metrics that we're storing so

00:06:37,659 --> 00:06:42,789
here's an example of how we are storing

00:06:39,849 --> 00:06:45,159
like rate in memory for a rate limiter

00:06:42,789 --> 00:06:47,380
and by using the collector interface

00:06:45,159 --> 00:06:51,250
we're actually kind of providing a snap

00:06:47,380 --> 00:06:53,020
of requests straight across clients so

00:06:51,250 --> 00:06:54,820
once we instrument our applications we

00:06:53,020 --> 00:06:57,220
then make dashboards here's an example

00:06:54,820 --> 00:06:58,870
of a dashboard for our V PC product so

00:06:57,220 --> 00:07:01,780
we typically have a master dashboard

00:06:58,870 --> 00:07:03,460
which links to a bunch of the components

00:07:01,780 --> 00:07:05,170
in the system and our master dashboard

00:07:03,460 --> 00:07:08,260
often has state metrics you know

00:07:05,170 --> 00:07:10,090
basically up or down metrics or is our

00:07:08,260 --> 00:07:12,310
like north geo service up

00:07:10,090 --> 00:07:16,930
same thing with RabbitMQ console and my

00:07:12,310 --> 00:07:19,390
sequel for Argos services we then have -

00:07:16,930 --> 00:07:21,340
public sub component dashboards which

00:07:19,390 --> 00:07:24,370
have report like request based metrics

00:07:21,340 --> 00:07:27,460
such as latency request rate here we're

00:07:24,370 --> 00:07:29,230
actually taking like introspecting by

00:07:27,460 --> 00:07:30,790
the type of request but quite often

00:07:29,230 --> 00:07:32,890
there's additional graphs which also

00:07:30,790 --> 00:07:36,460
break it down by successful and

00:07:32,890 --> 00:07:38,710
unsuccessful requests then we have a

00:07:36,460 --> 00:07:39,970
dashboard for utilization metrics you

00:07:38,710 --> 00:07:42,910
know you're helpful number of go

00:07:39,970 --> 00:07:45,250
routines for go services things like CPU

00:07:42,910 --> 00:07:47,950
memory and disk usage also very

00:07:45,250 --> 00:07:49,660
important we also use a lot of

00:07:47,950 --> 00:07:52,150
third-party software as I mentioned so

00:07:49,660 --> 00:07:53,620
for example for my sequel we have the my

00:07:52,150 --> 00:07:56,020
sequel dashboard which i think is

00:07:53,620 --> 00:07:59,020
actually available online if you ever

00:07:56,020 --> 00:07:59,620
want one of these which has queries per

00:07:59,020 --> 00:08:02,920
second

00:07:59,620 --> 00:08:05,530
things like utilization also RabbitMQ

00:08:02,920 --> 00:08:07,630
dashboards we particularly pay attention

00:08:05,530 --> 00:08:09,610
to saturation for rabbitmq looking at

00:08:07,630 --> 00:08:12,610
things like queue depth or message

00:08:09,610 --> 00:08:16,120
status unacknowledged versus ready

00:08:12,610 --> 00:08:17,320
messages so once we have our dashboards

00:08:16,120 --> 00:08:19,210
ready to go

00:08:17,320 --> 00:08:20,590
we then typically start testing so the

00:08:19,210 --> 00:08:22,450
most common thing when we've been using

00:08:20,590 --> 00:08:25,420
for a lot of products is load testing

00:08:22,450 --> 00:08:27,070
given that we use G RPC internally for

00:08:25,420 --> 00:08:29,800
our ghost services this is fairly easy

00:08:27,070 --> 00:08:31,840
to do we just use a lot of go routines

00:08:29,800 --> 00:08:33,669
and G RPC clients to just send

00:08:31,840 --> 00:08:35,740
concurrent requests to our services and

00:08:33,669 --> 00:08:37,599
this has been pretty just you know

00:08:35,740 --> 00:08:40,419
enough to load test the system but there

00:08:37,599 --> 00:08:42,280
have been efforts to build internal load

00:08:40,419 --> 00:08:44,050
testers with like controllers and

00:08:42,280 --> 00:08:46,840
workers to load tests even more but for

00:08:44,050 --> 00:08:49,000
the most part this works we also do a

00:08:46,840 --> 00:08:51,070
lot of chaos testing taking down certain

00:08:49,000 --> 00:08:53,050
components of the system given that most

00:08:51,070 --> 00:08:55,540
of our products consist of a lot of

00:08:53,050 --> 00:08:57,190
components then integration testing also

00:08:55,540 --> 00:08:59,530
important given that we are a cloud

00:08:57,190 --> 00:09:00,870
provider with a variety of features it's

00:08:59,530 --> 00:09:02,730
really important to see how in

00:09:00,870 --> 00:09:04,800
producing a new feature and a new

00:09:02,730 --> 00:09:08,310
service kind of integrates with the rest

00:09:04,800 --> 00:09:10,290
of the cloud so what is testing actually

00:09:08,310 --> 00:09:12,720
accomplish so one of the the helpful

00:09:10,290 --> 00:09:14,820
things especially while using metrics

00:09:12,720 --> 00:09:16,770
for testing is you know we can look into

00:09:14,820 --> 00:09:18,510
our latency for example if there's

00:09:16,770 --> 00:09:20,279
issues with latency we can often use

00:09:18,510 --> 00:09:22,800
another observability primitives such as

00:09:20,279 --> 00:09:24,720
tracing to dig down we can look into

00:09:22,800 --> 00:09:26,880
resource usage such as you know go

00:09:24,720 --> 00:09:28,710
routines is there a go routine leak we

00:09:26,880 --> 00:09:32,130
can make code adjustments we can also

00:09:28,710 --> 00:09:34,200
look into CPU and memory usage and then

00:09:32,130 --> 00:09:36,510
use profiling if needed to analyze that

00:09:34,200 --> 00:09:38,910
further we can also look at things like

00:09:36,510 --> 00:09:42,180
error rate and then use logs to check

00:09:38,910 --> 00:09:45,120
for a particular type of error something

00:09:42,180 --> 00:09:47,160
else that testing helps us do is a tuner

00:09:45,120 --> 00:09:48,990
metrics further so we can often look

00:09:47,160 --> 00:09:51,450
into if the metric that we're using as a

00:09:48,990 --> 00:09:53,160
measure is actually useful we can also

00:09:51,450 --> 00:09:55,740
see if we need to collect more data and

00:09:53,160 --> 00:09:56,910
instrument our apps further then finally

00:09:55,740 --> 00:09:59,010
we can see if we need more

00:09:56,910 --> 00:10:03,240
dimensionality to our data for analysis

00:09:59,010 --> 00:10:05,700
maybe we need to add more labels another

00:10:03,240 --> 00:10:08,010
thing which you know is is also fairly

00:10:05,700 --> 00:10:10,110
obvious is that we can use testing to

00:10:08,010 --> 00:10:13,110
tune our alerts so especially with you

00:10:10,110 --> 00:10:14,550
know chaos testing we can definitely

00:10:13,110 --> 00:10:16,320
figure out the sort of state based

00:10:14,550 --> 00:10:18,900
alerts we might need such as if our

00:10:16,320 --> 00:10:20,520
service is up or down with load testing

00:10:18,900 --> 00:10:22,830
and chaos testing it's very useful to

00:10:20,520 --> 00:10:24,720
also set up threshold alerts such as at

00:10:22,830 --> 00:10:27,180
what request rate does our service fail

00:10:24,720 --> 00:10:31,860
or like what sort of threshold alerts

00:10:27,180 --> 00:10:34,170
should we set for memory or CPU such and

00:10:31,860 --> 00:10:35,370
then finally documentation you know a

00:10:34,170 --> 00:10:38,190
lot of people don't really like

00:10:35,370 --> 00:10:39,600
documentation but I will say that this

00:10:38,190 --> 00:10:42,120
is something that testing has especially

00:10:39,600 --> 00:10:43,709
been useful for for a lot of products

00:10:42,120 --> 00:10:46,680
such as VPC that we released recently

00:10:43,709 --> 00:10:48,990
we've been able to determine how exactly

00:10:46,680 --> 00:10:51,450
to recover certain components of the

00:10:48,990 --> 00:10:55,100
system which is very useful for you know

00:10:51,450 --> 00:10:57,600
both ops teams and on-call engineers so

00:10:55,100 --> 00:11:00,150
once we do all of this we typically

00:10:57,600 --> 00:11:02,070
iterate but I think the the easiest way

00:11:00,150 --> 00:11:03,690
to kind of talk about how we iterate is

00:11:02,070 --> 00:11:09,240
just by looking at some real-life

00:11:03,690 --> 00:11:10,950
examples so that's that's a plan yeh so

00:11:09,240 --> 00:11:13,500
the first product I'm going to talk

00:11:10,950 --> 00:11:14,840
about is DHCP something that all of your

00:11:13,500 --> 00:11:16,580
router

00:11:14,840 --> 00:11:18,110
something that we're also building at

00:11:16,580 --> 00:11:19,460
her company as part of our

00:11:18,110 --> 00:11:23,870
software-defined Network

00:11:19,460 --> 00:11:25,640
so DHCP the actual go service is called

00:11:23,870 --> 00:11:27,440
HP at orgy so if you see that in

00:11:25,640 --> 00:11:31,010
diagrams now you know

00:11:27,440 --> 00:11:33,770
so here's DHCP we basically have the HP

00:11:31,010 --> 00:11:36,830
a dirty daemon running on a hypervisor

00:11:33,770 --> 00:11:38,360
so it consists of a G RPC service that

00:11:36,830 --> 00:11:41,120
receives requests from something known

00:11:38,360 --> 00:11:44,270
as HP flow T which communicates with OBS

00:11:41,120 --> 00:11:47,810
or open V switch it also consists of a

00:11:44,270 --> 00:11:51,470
DHCP v4 and v6 server which receive DHCP

00:11:47,810 --> 00:11:54,560
traffic from the droplet or or virtual

00:11:51,470 --> 00:11:56,870
machine and so given that the DHCP

00:11:54,560 --> 00:11:59,450
component of the HP a dirty daemon

00:11:56,870 --> 00:12:02,990
actually receives date Lake DHCP client

00:11:59,450 --> 00:12:04,010
traffic from the virtual machines we

00:12:02,990 --> 00:12:05,900
felt like that was something very

00:12:04,010 --> 00:12:08,300
important a load test given that there

00:12:05,900 --> 00:12:11,000
could be bad actors on people's virtual

00:12:08,300 --> 00:12:13,400
machines that's what we did as you can

00:12:11,000 --> 00:12:15,020
see we are increasing the number of DHCP

00:12:13,400 --> 00:12:18,290
discovery requests that are being sent

00:12:15,020 --> 00:12:18,950
to our G HTTP server I'll be very

00:12:18,290 --> 00:12:20,600
quickly

00:12:18,950 --> 00:12:24,650
early on discovered that we could

00:12:20,600 --> 00:12:26,930
actually the daemon and then we're like

00:12:24,650 --> 00:12:29,300
oh that's terrible so he decided that we

00:12:26,930 --> 00:12:32,390
needed to do three things one we decided

00:12:29,300 --> 00:12:33,740
to build a custom collector or we

00:12:32,390 --> 00:12:35,180
decided to implement the collector

00:12:33,740 --> 00:12:37,550
interface with one of the custom

00:12:35,180 --> 00:12:40,460
Struck's that was within the code so

00:12:37,550 --> 00:12:45,050
given our like kind of the way that

00:12:40,460 --> 00:12:46,880
we're using our Sdn and OVS we actually

00:12:45,050 --> 00:12:49,730
had to drop down to the Ethernet level

00:12:46,880 --> 00:12:51,620
to process some of the pad like the DHCP

00:12:49,730 --> 00:12:55,220
traffic so the package that was doing

00:12:51,620 --> 00:12:57,260
that was DHCP for con we decided to

00:12:55,220 --> 00:12:58,910
actually instrument this such that we

00:12:57,260 --> 00:13:01,700
were exposing the number of bytes that

00:12:58,910 --> 00:13:04,430
were read and written and you can see

00:13:01,700 --> 00:13:07,040
that once we instrumented we put that on

00:13:04,430 --> 00:13:08,540
our dashboard as well we then also

00:13:07,040 --> 00:13:10,880
realized that part of the reason that

00:13:08,540 --> 00:13:12,680
the service was booming was because we

00:13:10,880 --> 00:13:14,420
were allocating go routines to process

00:13:12,680 --> 00:13:16,370
every request and that could just get

00:13:14,420 --> 00:13:18,440
out of control so then we decided to use

00:13:16,370 --> 00:13:20,660
worker pooling basically a buffered

00:13:18,440 --> 00:13:22,940
channel and limiting the number of go

00:13:20,660 --> 00:13:25,400
routines that we could have and that

00:13:22,940 --> 00:13:26,870
also tackled some of the issues but at

00:13:25,400 --> 00:13:28,850
the same time even though that tackled

00:13:26,870 --> 00:13:30,920
some of the out of memory errors we

00:13:28,850 --> 00:13:33,410
realized that given that we do get

00:13:30,920 --> 00:13:36,530
customer traffic we wanted to actually

00:13:33,410 --> 00:13:38,240
rate limit bad actors potentially so we

00:13:36,530 --> 00:13:40,460
ended up building a rate limiter which

00:13:38,240 --> 00:13:43,730
also implemented the collector interface

00:13:40,460 --> 00:13:45,380
the rate limiter basically rate limited

00:13:43,730 --> 00:13:47,660
on a per client basis and had an

00:13:45,380 --> 00:13:51,200
internal map that was storing request

00:13:47,660 --> 00:13:55,490
rates so that's what we exposed with the

00:13:51,200 --> 00:13:56,960
new constant metric the other useful

00:13:55,490 --> 00:13:59,180
thing with the rate limiter is we were

00:13:56,960 --> 00:14:02,180
actually able to build rate alerts and

00:13:59,180 --> 00:14:03,830
get information about abusive actors we

00:14:02,180 --> 00:14:05,660
did this by having it in middle log line

00:14:03,830 --> 00:14:07,490
when something was actually rate limited

00:14:05,660 --> 00:14:10,070
having that sent to centralized logging

00:14:07,490 --> 00:14:12,410
and then using a last alert which works

00:14:10,070 --> 00:14:15,110
with elasticsearch to actually send a

00:14:12,410 --> 00:14:17,990
message to slack so the final result

00:14:15,110 --> 00:14:20,210
when we load test it again something the

00:14:17,990 --> 00:14:22,940
actual bad actor was very quickly rate

00:14:20,210 --> 00:14:27,380
limited and our request rate didn't jump

00:14:22,940 --> 00:14:28,610
up so thank you for me theas so our

00:14:27,380 --> 00:14:30,740
second product I'm going to talk about

00:14:28,610 --> 00:14:32,840
is VP C with it which is I think

00:14:30,740 --> 00:14:34,670
something we released maybe actually

00:14:32,840 --> 00:14:38,450
this month or last month so super

00:14:34,670 --> 00:14:40,310
exciting so with VP C it also consists

00:14:38,450 --> 00:14:42,800
of a number of components we have the

00:14:40,310 --> 00:14:45,790
North T go service we have a South T

00:14:42,800 --> 00:14:49,310
Workers which read from the rabbitmq

00:14:45,790 --> 00:14:52,190
queues and then we also have a my sequel

00:14:49,310 --> 00:14:54,710
cluster so a lot of components so we

00:14:52,190 --> 00:14:57,170
immediately started by a load testing VP

00:14:54,710 --> 00:14:58,850
see earlier this year and you can see

00:14:57,170 --> 00:15:00,470
that right here on the chart all the way

00:14:58,850 --> 00:15:02,270
on the left the load tester is

00:15:00,470 --> 00:15:04,610
repeatedly making some sync initial

00:15:02,270 --> 00:15:06,290
chassis calls this also corresponds to

00:15:04,610 --> 00:15:08,270
an increase in the number of RabbitMQ

00:15:06,290 --> 00:15:11,060
messages and the increase in the number

00:15:08,270 --> 00:15:12,770
of South team messages but immediately

00:15:11,060 --> 00:15:15,590
early on we saw that we were running

00:15:12,770 --> 00:15:17,780
into latency issues we realized we

00:15:15,590 --> 00:15:19,400
needed to dig into that a little more we

00:15:17,780 --> 00:15:20,840
decided to use tracing for that so

00:15:19,400 --> 00:15:23,150
here's one of our tracing dashboards

00:15:20,840 --> 00:15:25,790
which also indicated latency issues with

00:15:23,150 --> 00:15:28,910
the sync national chassis call so we

00:15:25,790 --> 00:15:29,900
decided to look at a entire trace but

00:15:28,910 --> 00:15:32,570
then we noticed that we were actually

00:15:29,900 --> 00:15:35,480
load testing at such a fast rate that

00:15:32,570 --> 00:15:36,320
some of these spans or the traces were

00:15:35,480 --> 00:15:38,540
being dropped

00:15:36,320 --> 00:15:41,930
so we slowed down our load testing we're

00:15:38,540 --> 00:15:44,030
able to actually have at collect all the

00:15:41,930 --> 00:15:46,250
time to operation information and see

00:15:44,030 --> 00:15:48,040
the full trace quickly discovered that

00:15:46,250 --> 00:15:50,780
we actually had a my sequel issue

00:15:48,040 --> 00:15:52,490
basically super inefficient query that

00:15:50,780 --> 00:15:54,590
was making three calls where we needed

00:15:52,490 --> 00:15:56,960
to make one we fixed that and we

00:15:54,590 --> 00:15:58,820
addressed some latency issues something

00:15:56,960 --> 00:16:00,380
else that's very interesting to note is

00:15:58,820 --> 00:16:02,210
that we saw even when we were load

00:16:00,380 --> 00:16:05,150
testing our queue depth didn't actually

00:16:02,210 --> 00:16:06,980
increase that much for rabbitmq so we

00:16:05,150 --> 00:16:09,260
realized it was kind of just being used

00:16:06,980 --> 00:16:11,870
as a very slow load balancer so do we

00:16:09,260 --> 00:16:14,540
even need a queue so this is something

00:16:11,870 --> 00:16:18,320
that we're also looking at probably

00:16:14,540 --> 00:16:21,380
gonna remove it so the last thing we did

00:16:18,320 --> 00:16:23,300
was chaos testing VPC it was also very

00:16:21,380 --> 00:16:25,040
important given that we have a number of

00:16:23,300 --> 00:16:27,470
components some of the things we tried

00:16:25,040 --> 00:16:30,050
to do induce fail fail over there

00:16:27,470 --> 00:16:31,910
failure in North deve the go service to

00:16:30,050 --> 00:16:35,180
see if the failover was properly working

00:16:31,910 --> 00:16:37,760
we tried to induce South D fail failure

00:16:35,180 --> 00:16:39,620
we tried to mess around with RabbitMQ we

00:16:37,760 --> 00:16:42,410
tried to drop a primary my sequel

00:16:39,620 --> 00:16:44,240
database and we realized we definitely

00:16:42,410 --> 00:16:46,820
needed to add some alerts so you know

00:16:44,240 --> 00:16:48,260
basically being state-based alerts right

00:16:46,820 --> 00:16:49,640
now it's going to slack but when the

00:16:48,260 --> 00:16:51,890
product was actually released we

00:16:49,640 --> 00:16:55,280
promoted the alert so that people would

00:16:51,890 --> 00:16:57,500
get paged as well we also of course

00:16:55,280 --> 00:16:59,930
added some key threshold alerts as a

00:16:57,500 --> 00:17:02,630
result of both chaos and load testing so

00:16:59,930 --> 00:17:04,400
for example we wanted to see if our my

00:17:02,630 --> 00:17:06,440
sequel replicas were ultimately delayed

00:17:04,400 --> 00:17:08,570
but then we also realize that we want to

00:17:06,440 --> 00:17:12,260
see if there's a high error rate as well

00:17:08,570 --> 00:17:13,430
for the North to ego service so that's

00:17:12,260 --> 00:17:15,800
pretty much what I have for you today

00:17:13,430 --> 00:17:18,320
and hopefully what you've realized from

00:17:15,800 --> 00:17:20,360
this is that it's very important to

00:17:18,320 --> 00:17:22,130
instrument your applications as early as

00:17:20,360 --> 00:17:24,080
possible even when you're just building

00:17:22,130 --> 00:17:25,640
them and you have an actual eerily

00:17:24,080 --> 00:17:27,620
similar production the things that are

00:17:25,640 --> 00:17:28,730
important to measure pretty much same as

00:17:27,620 --> 00:17:30,620
last year and I think what a lot of

00:17:28,730 --> 00:17:33,400
people in the industry agree with the

00:17:30,620 --> 00:17:36,440
four golden signals for request based

00:17:33,400 --> 00:17:38,510
systems as well as the use metrics which

00:17:36,440 --> 00:17:40,220
concerned resources and the other thing

00:17:38,510 --> 00:17:42,440
that's pretty important and that we

00:17:40,220 --> 00:17:45,680
found is that you know Prometheus often

00:17:42,440 --> 00:17:48,000
helps identify what's actually happening

00:17:45,680 --> 00:17:49,380
but then you can combine from

00:17:48,000 --> 00:17:51,660
atheists with other observability

00:17:49,380 --> 00:17:53,850
metrics to kind of dig further and try

00:17:51,660 --> 00:17:55,980
to like analyze what's happening look

00:17:53,850 --> 00:17:57,990
into causality and then finally perhaps

00:17:55,980 --> 00:18:01,110
have improvement or in performance

00:17:57,990 --> 00:18:10,800
improvements as well so thank you

00:18:01,110 --> 00:18:11,970
another photo of my cat IQ smack so we

00:18:10,800 --> 00:18:14,250
have I think 10 Minister questions

00:18:11,970 --> 00:18:15,750
because we actually started early

00:18:14,250 --> 00:18:21,390
so put your up your hand if you have a

00:18:15,750 --> 00:18:25,410
question so just ask the question and

00:18:21,390 --> 00:18:27,810
hand it back hi great talk by the way I

00:18:25,410 --> 00:18:29,790
was just curious what was your

00:18:27,810 --> 00:18:34,230
production experience like when dealing

00:18:29,790 --> 00:18:36,420
with 85 permittees instances and also

00:18:34,230 --> 00:18:37,650
like if you can elaborate on the fact if

00:18:36,420 --> 00:18:39,900
you're running out of multiple data

00:18:37,650 --> 00:18:42,930
centers or how how much valuable data

00:18:39,900 --> 00:18:44,460
loss is for your company yeah we I mean

00:18:42,930 --> 00:18:46,650
our I think our like production

00:18:44,460 --> 00:18:48,270
experience and maintenance experience

00:18:46,650 --> 00:18:50,820
with that is actually pretty good I

00:18:48,270 --> 00:18:53,130
think we've been paged not that often

00:18:50,820 --> 00:18:56,580
and then we also have two instances of

00:18:53,130 --> 00:18:58,080
Prometheus for for every like time

00:18:56,580 --> 00:19:00,000
series that we're monitoring so we

00:18:58,080 --> 00:19:01,380
definitely we haven't really had too

00:19:00,000 --> 00:19:03,240
many issues with data loss as a result

00:19:01,380 --> 00:19:05,040
of that and then we have like meta

00:19:03,240 --> 00:19:13,620
instances that are sort of monitoring

00:19:05,040 --> 00:19:16,800
the other instances hi just happy to see

00:19:13,620 --> 00:19:18,900
you up there as well so for your 85

00:19:16,800 --> 00:19:21,390
instances for Prometheus I'm very brand

00:19:18,900 --> 00:19:24,930
new at it what was your infrastructure

00:19:21,390 --> 00:19:27,090
like look like to handle all that and my

00:19:24,930 --> 00:19:29,090
second question is for your deck will it

00:19:27,090 --> 00:19:31,050
be available for us to view afterwards

00:19:29,090 --> 00:19:33,030
okay so I'll start with the second

00:19:31,050 --> 00:19:34,830
question definitely it's already on

00:19:33,030 --> 00:19:37,170
SlideShare but I'll tweet it afterward

00:19:34,830 --> 00:19:40,380
and in terms of the infrastructure for

00:19:37,170 --> 00:19:43,590
the 85 instances so we have about two

00:19:40,380 --> 00:19:45,750
instances per type of we kind of like we

00:19:43,590 --> 00:19:48,680
have functional sharding in a way so for

00:19:45,750 --> 00:19:51,570
every single data center we have a

00:19:48,680 --> 00:19:53,760
couple of demons on each hypervisor that

00:19:51,570 --> 00:19:56,100
are measuring certain key like virtual

00:19:53,760 --> 00:19:59,640
machine metrics and node metrics so we

00:19:56,100 --> 00:20:00,810
have that for each data center and

00:19:59,640 --> 00:20:03,360
we have something that's monitoring

00:20:00,810 --> 00:20:05,100
those and then we also like I think for

00:20:03,360 --> 00:20:06,990
certain services that have a

00:20:05,100 --> 00:20:09,120
particularly high number of time series

00:20:06,990 --> 00:20:10,560
we also have a specialized Prometheus

00:20:09,120 --> 00:20:12,300
instance for that and then we have men

00:20:10,560 --> 00:20:14,400
monitoring for each of those Prometheus

00:20:12,300 --> 00:20:16,080
instances and those meta monitoring

00:20:14,400 --> 00:20:20,000
instances are also monitoring each other

00:20:16,080 --> 00:20:20,000
so just very meta

00:20:22,190 --> 00:20:25,470
yeah just mention as well all the

00:20:24,090 --> 00:20:30,950
recordings and slides will be online

00:20:25,470 --> 00:20:30,950
afterwards so next question look around

00:20:35,780 --> 00:20:43,560
in average how many alerts or pages do

00:20:40,410 --> 00:20:46,680
you receive in a day across the company

00:20:43,560 --> 00:20:49,140
for my team for for my team

00:20:46,680 --> 00:20:51,450
ah that's hard to say I would say that

00:20:49,140 --> 00:20:52,920
we have a pretty stable system and I

00:20:51,450 --> 00:20:54,720
think the other thing is you know

00:20:52,920 --> 00:20:57,000
there's there's different types of

00:20:54,720 --> 00:20:58,770
alerts that we can have there's pages

00:20:57,000 --> 00:21:01,380
for actual things being down and I would

00:20:58,770 --> 00:21:03,300
say that both OVS a lot of Argos

00:21:01,380 --> 00:21:05,330
services are pretty stable but then we

00:21:03,300 --> 00:21:07,920
do often have support requests for

00:21:05,330 --> 00:21:09,840
networking related things often because

00:21:07,920 --> 00:21:22,070
as people are doing stuff on their

00:21:09,840 --> 00:21:26,530
droplets hey

00:21:22,070 --> 00:21:30,530
thank you so I wanted to ask you about

00:21:26,530 --> 00:21:33,230
you you used elasticsearch to log line

00:21:30,530 --> 00:21:36,830
for like abusive clients right and then

00:21:33,230 --> 00:21:39,440
you used elastic alert detection right

00:21:36,830 --> 00:21:42,860
that's like why not use premier fuse

00:21:39,440 --> 00:21:45,890
like counter or dig gouge and when do

00:21:42,860 --> 00:21:48,020
you use one and then the use yeah so the

00:21:45,890 --> 00:21:49,700
reason we actually did that for the the

00:21:48,020 --> 00:21:51,680
rate limiter is because we felt like we

00:21:49,700 --> 00:21:55,100
could have a cardinality explosion if we

00:21:51,680 --> 00:21:58,790
were exposing the metrics for just every

00:21:55,100 --> 00:22:01,040
single DHCP client so instead of doing

00:21:58,790 --> 00:22:03,260
that and then having a Prometheus I'm

00:22:01,040 --> 00:22:05,450
having Prometheus scrape those metrics

00:22:03,260 --> 00:22:07,790
we decided to have like a snapshot of

00:22:05,450 --> 00:22:09,500
request distribution which kind of

00:22:07,790 --> 00:22:12,680
provided I guess just like an overall

00:22:09,500 --> 00:22:14,900
view but didn't like results in

00:22:12,680 --> 00:22:16,430
cardinality issues and because of that

00:22:14,900 --> 00:22:19,130
we decided to actually just have a log

00:22:16,430 --> 00:22:28,550
line that's emitted like when there are

00:22:19,130 --> 00:22:42,020
abusive actors doctor and it's like

00:22:28,550 --> 00:22:44,300
primary school I what was your approach

00:22:42,020 --> 00:22:47,540
to choosing alerting routes so you

00:22:44,300 --> 00:22:49,730
mentioned slack and also pages and do

00:22:47,540 --> 00:22:51,380
you also differentiate between pages and

00:22:49,730 --> 00:22:54,770
kind of tickets in a sort of Google

00:22:51,380 --> 00:22:57,110
approach or and I guess this is also

00:22:54,770 --> 00:22:58,910
about how do you reduce the potential of

00:22:57,110 --> 00:23:01,490
noisiness in your alerts and make sure

00:22:58,910 --> 00:23:03,650
that engineers can prioritize of course

00:23:01,490 --> 00:23:05,360
so I think before the product is

00:23:03,650 --> 00:23:08,180
actually even released pretty much

00:23:05,360 --> 00:23:09,500
everything is just sent to slack and I

00:23:08,180 --> 00:23:11,630
think once something is actually

00:23:09,500 --> 00:23:14,450
released we prioritize by if it can

00:23:11,630 --> 00:23:16,250
actually be addressed and fixed then we

00:23:14,450 --> 00:23:18,560
get paged for it and if it's something

00:23:16,250 --> 00:23:20,300
that's just nice to know we will just

00:23:18,560 --> 00:23:22,040
get a slack message or maybe a log line

00:23:20,300 --> 00:23:24,110
or it'll show up on our dashboard and

00:23:22,040 --> 00:23:26,030
well will notice that later and then we

00:23:24,110 --> 00:23:27,710
do have support tickets which are kind

00:23:26,030 --> 00:23:29,930
of I think things that are just not

00:23:27,710 --> 00:23:31,160
particularly urgent or you know customer

00:23:29,930 --> 00:23:33,140
focused and this is something that I've

00:23:31,160 --> 00:23:35,230
noticed is you know slightly different

00:23:33,140 --> 00:23:51,400
based on the product that you're working

00:23:35,230 --> 00:23:53,350
at the company next question hey I just

00:23:51,400 --> 00:23:55,450
noticed that your slack alerts were much

00:23:53,350 --> 00:23:57,610
cleaner than the ones that we get and

00:23:55,450 --> 00:23:59,110
I'm wondering if I was already thought

00:23:57,610 --> 00:24:03,120
put into the templates being put out to

00:23:59,110 --> 00:24:06,280
reduce that just line noise yeah I think

00:24:03,120 --> 00:24:08,230
so I think one thing that we we did at

00:24:06,280 --> 00:24:11,169
least as we started adding information

00:24:08,230 --> 00:24:14,350
about the particular instance given that

00:24:11,169 --> 00:24:17,140
we have so for example for the DHCP

00:24:14,350 --> 00:24:19,540
thing we have like a DHCP daemon on

00:24:17,140 --> 00:24:20,950
every hypervisor so we try to try to add

00:24:19,540 --> 00:24:22,900
information such as the relevant

00:24:20,950 --> 00:24:25,270
dashboard that we could get information

00:24:22,900 --> 00:24:27,070
for that hypervisor was in the actual

00:24:25,270 --> 00:24:28,870
slack message as well as the actual

00:24:27,070 --> 00:24:31,260
instance so that's I think one thing

00:24:28,870 --> 00:24:33,760
that we did and then we also do have a

00:24:31,260 --> 00:24:36,730
observability team who kind of just

00:24:33,760 --> 00:24:38,049
reviews a lot of the alerts for every

00:24:36,730 --> 00:25:01,150
other team like product team at the

00:24:38,049 --> 00:25:03,820
company great talk

00:25:01,150 --> 00:25:07,330
I was wondering if you do inhibition so

00:25:03,820 --> 00:25:09,580
you have two alerting as far as I get to

00:25:07,330 --> 00:25:13,299
alerting systems alert manger and the

00:25:09,580 --> 00:25:15,700
last rule how do you deal when one other

00:25:13,299 --> 00:25:17,559
triggers another so uh I think

00:25:15,700 --> 00:25:19,570
specifically for a last alert an alert

00:25:17,559 --> 00:25:21,760
manager they're monitoring two different

00:25:19,570 --> 00:25:23,590
things so we don't really have to deal

00:25:21,760 --> 00:25:25,000
with inhibitions for that we do deal

00:25:23,590 --> 00:25:26,980
with that just for like two different

00:25:25,000 --> 00:25:30,870
Prometheus alerts like warning and

00:25:26,980 --> 00:25:34,390
actual alerts and we do kind of inhibit

00:25:30,870 --> 00:25:35,620
alerts based on labeling for for that so

00:25:34,390 --> 00:25:37,450
for example I think if the number of

00:25:35,620 --> 00:25:39,010
instances of something dropped down to

00:25:37,450 --> 00:25:41,140
one that would be a warning if it

00:25:39,010 --> 00:25:43,270
dropped down to zero that would be like

00:25:41,140 --> 00:25:45,630
this is serious and we inhibit for that

00:25:43,270 --> 00:25:45,630
of course

00:25:47,120 --> 00:25:56,240
anymore okay thank you very much thank

00:25:54,900 --> 00:26:01,099
you

00:25:56,240 --> 00:26:01,099

YouTube URL: https://www.youtube.com/watch?v=gNk7Y0AW9HI


