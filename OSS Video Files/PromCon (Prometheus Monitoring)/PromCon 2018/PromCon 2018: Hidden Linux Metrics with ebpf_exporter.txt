Title: PromCon 2018: Hidden Linux Metrics with ebpf_exporter
Publication date: 2018-11-10
Playlist: PromCon 2018
Description: 
	Speaker: Ivan Babrou

While there are plenty of readily available metrics for monitoring the Linux kernel, many gems remain hidden. With the help of recent developments in eBPF, it is now possible to run safe programs in the kernel to collect arbitrary information with little to no overhead. A few examples include:

Disk latency and IO size histograms
Run queue (scheduler) latency
Page cache efficiency
Directory cache efficiency
LLC (aka L3 cache) efficiency
Kernel timer counters
System-wide TCP retransmits
Practically any event from "perf list" output and any kernel function can be traced, analyzed, and turned into a metric with almost arbitrary labels attached to it.

If you are already familiar with BCC tools, you may think of ebpf_exporter as BCC tools turned into Prometheus metrics.

In this talk we'll go over eBPF basics, how to write programs and get insights into a running system.

https://github.com/cloudflare/ebpf_exporter
Captions: 
	00:00:09,810 --> 00:00:16,330
so I work for a company called

00:00:11,920 --> 00:00:18,160
CloudFlare so where I focus on

00:00:16,330 --> 00:00:21,220
performance and efficiency of our

00:00:18,160 --> 00:00:24,180
products I don't usually do public

00:00:21,220 --> 00:00:27,099
speaking and I have terrible memory so

00:00:24,180 --> 00:00:31,150
please forgive me if I look at the notes

00:00:27,099 --> 00:00:33,930
too much so if you never heard about

00:00:31,150 --> 00:00:37,810
CloudFlare it means one of two things

00:00:33,930 --> 00:00:39,579
either you own a website and we fail to

00:00:37,810 --> 00:00:43,420
inform you that you should totally use

00:00:39,579 --> 00:00:45,129
us or you don't own a website and we're

00:00:43,420 --> 00:00:49,090
working well enough for you to dun know

00:00:45,129 --> 00:00:49,780
that we're we exist we're a CDN with

00:00:49,090 --> 00:00:52,989
benefits

00:00:49,780 --> 00:00:56,649
so in additional to traditional caching

00:00:52,989 --> 00:00:57,129
and DDoS protection and like bringing

00:00:56,649 --> 00:01:00,039
content

00:00:57,129 --> 00:01:01,329
closer to the user we try to be the

00:01:00,039 --> 00:01:05,050
forefront of innovation with

00:01:01,329 --> 00:01:08,440
technologies like TLS 1.3 quick server

00:01:05,050 --> 00:01:10,270
push edge workers and the idea is

00:01:08,440 --> 00:01:13,150
generally to make Internet the more

00:01:10,270 --> 00:01:15,610
secure and up-to-date for everyone even

00:01:13,150 --> 00:01:18,400
if your origin server does not support

00:01:15,610 --> 00:01:22,210
these technologies we're also the

00:01:18,400 --> 00:01:24,220
fastest authority of DNS and we provide

00:01:22,210 --> 00:01:26,829
privacy oriented one that one that one

00:01:24,220 --> 00:01:29,530
dot one resolver so you should totally

00:01:26,829 --> 00:01:32,649
use that and you should totally use that

00:01:29,530 --> 00:01:34,240
with DNS over TLS so you're I speak

00:01:32,649 --> 00:01:37,840
don't see what you're browsing on the

00:01:34,240 --> 00:01:42,070
internet and Prometheus and prom Khan

00:01:37,840 --> 00:01:43,689
website also uses apparently last year

00:01:42,070 --> 00:01:46,240
my colleague Matt Bostick was here he

00:01:43,689 --> 00:01:49,380
was talking how we do planet-scale edge

00:01:46,240 --> 00:01:51,670
not network monitoring with promises and

00:01:49,380 --> 00:01:54,219
you can check the slides in the video

00:01:51,670 --> 00:01:59,049
it's still useful I'm just gonna give a

00:01:54,219 --> 00:02:01,049
quick refresher on on the numbers so the

00:01:59,049 --> 00:02:04,659
only thing that didn't change here is

00:02:01,049 --> 00:02:07,119
the 10% number and it's a it's not

00:02:04,659 --> 00:02:10,530
exactly scientific number it depends on

00:02:07,119 --> 00:02:13,870
how you count the internet requests

00:02:10,530 --> 00:02:16,990
otherwise we doubled in terms of HTTP

00:02:13,870 --> 00:02:18,879
and DNS requests and DNS does not

00:02:16,990 --> 00:02:22,180
include the public one dot one dot one

00:02:18,879 --> 00:02:23,170
dot one resolver so that did grow by a

00:02:22,180 --> 00:02:28,599
factor of

00:02:23,170 --> 00:02:31,330
infinity we're at 150 data centers and

00:02:28,599 --> 00:02:36,940
instead of a hundred just a year ago and

00:02:31,330 --> 00:02:39,580
we plan to cross 200 this year and what

00:02:36,940 --> 00:02:43,989
each of those needs monitoring and

00:02:39,580 --> 00:02:46,510
that's what we use operators for and our

00:02:43,989 --> 00:02:51,370
global Prometheus deployment did grow

00:02:46,510 --> 00:02:52,690
with with our H Network the numbers here

00:02:51,370 --> 00:02:59,110
are per Prometheus

00:02:52,690 --> 00:03:02,110
so they not for all 267 Prometheus

00:02:59,110 --> 00:03:04,810
services and we're very much looking

00:03:02,110 --> 00:03:09,100
forward to having tennis to manage this

00:03:04,810 --> 00:03:11,769
and keep this in check so that's all

00:03:09,100 --> 00:03:13,209
great but that's not what I was here to

00:03:11,769 --> 00:03:15,610
talk about so I want to tell about an

00:03:13,209 --> 00:03:18,700
exporter so let's first have a look at

00:03:15,610 --> 00:03:22,180
what do we have now so there are two

00:03:18,700 --> 00:03:26,290
main exporters one can use for system

00:03:22,180 --> 00:03:27,940
metrics with promises on Linux so the

00:03:26,290 --> 00:03:31,150
first one is not exported gives you

00:03:27,940 --> 00:03:33,340
information about basics like CPU usage

00:03:31,150 --> 00:03:36,519
breakdown by type memory usage disk IO

00:03:33,340 --> 00:03:39,220
file system and network the second one

00:03:36,519 --> 00:03:41,590
is C advisor it gives you similar

00:03:39,220 --> 00:03:44,200
metrics but instead of having them for

00:03:41,590 --> 00:03:46,870
the whole system you get them with the

00:03:44,200 --> 00:03:49,959
container level and system D units are

00:03:46,870 --> 00:03:54,430
also containers for C advisor so you get

00:03:49,959 --> 00:03:56,829
those as well and you can with that you

00:03:54,430 --> 00:04:00,760
can see like how what's the breakdown of

00:03:56,829 --> 00:04:03,299
total CPU usage by actual and services

00:04:00,760 --> 00:04:06,370
and this is absolute meaning minimum of

00:04:03,299 --> 00:04:09,790
what of what you should have for your

00:04:06,370 --> 00:04:13,690
systems and if you don't have those you

00:04:09,790 --> 00:04:15,730
should probably start there and I should

00:04:13,690 --> 00:04:17,650
mention that every screenshot on this

00:04:15,730 --> 00:04:21,750
talk is from actual production machine

00:04:17,650 --> 00:04:23,650
doing something maybe remotely useful

00:04:21,750 --> 00:04:25,570
but we have different machine

00:04:23,650 --> 00:04:28,780
generations so don't assume that's every

00:04:25,570 --> 00:04:30,430
conformation is just like that so here

00:04:28,780 --> 00:04:32,470
you can see the basics you get from node

00:04:30,430 --> 00:04:35,530
export in terms of CPU memory you can

00:04:32,470 --> 00:04:36,940
see like how much is used how much slack

00:04:35,530 --> 00:04:41,440
resources you have

00:04:36,940 --> 00:04:46,030
to grow sama metrics from node exporters

00:04:41,440 --> 00:04:49,570
well now for disk i/o and there are

00:04:46,030 --> 00:04:51,070
similar metrics for network and at the

00:04:49,570 --> 00:04:53,680
basic level you can do some correlation

00:04:51,070 --> 00:04:56,110
like you if you see that CPU went up you

00:04:53,680 --> 00:05:01,810
can see did Network go up to this guy or

00:04:56,110 --> 00:05:03,490
go up that kind of stuff so with

00:05:01,810 --> 00:05:06,250
advisors gets more interesting because

00:05:03,490 --> 00:05:09,220
you can see how global resources like

00:05:06,250 --> 00:05:13,060
CPU gets sliced down between individual

00:05:09,220 --> 00:05:16,270
services so if he CPU or memory growth

00:05:13,060 --> 00:05:18,850
for for the whole machine you can see

00:05:16,270 --> 00:05:21,190
which exact service is responsible and

00:05:18,850 --> 00:05:23,050
if it doesn't grow for the whole machine

00:05:21,190 --> 00:05:27,280
you can still see how it changes between

00:05:23,050 --> 00:05:29,050
the services that run to the machine and

00:05:27,280 --> 00:05:31,180
all of this information comes from the

00:05:29,050 --> 00:05:33,090
simplest counters you just take right

00:05:31,180 --> 00:05:35,940
over them and you're done

00:05:33,090 --> 00:05:40,300
so counters are great but they lack

00:05:35,940 --> 00:05:43,660
details so for example in IE o we get

00:05:40,300 --> 00:05:47,169
device time from node exporter and the

00:05:43,660 --> 00:05:48,940
rate of that cannot go beyond one second

00:05:47,169 --> 00:05:52,660
per one second of real time so you can

00:05:48,940 --> 00:05:55,240
get like you get a red bold red line at

00:05:52,660 --> 00:05:57,010
one second mark and you can't go beyond

00:05:55,240 --> 00:06:01,180
that so you know like you know you limit

00:05:57,010 --> 00:06:05,680
and you know where you are and that's

00:06:01,180 --> 00:06:08,220
basically the utilization you get we get

00:06:05,680 --> 00:06:11,500
this one number but that doesn't really

00:06:08,220 --> 00:06:13,600
characterizes our workload because it's

00:06:11,500 --> 00:06:15,640
simply not enough like you don't know if

00:06:13,600 --> 00:06:17,740
you're doing many fast i/o operations if

00:06:15,640 --> 00:06:21,520
you don't few slow ones if there's kind

00:06:17,740 --> 00:06:24,430
some kind of mix so this questions back

00:06:21,520 --> 00:06:26,020
for a histogram and like the counters

00:06:24,430 --> 00:06:29,500
what we have above and the histogram is

00:06:26,020 --> 00:06:32,080
what what's below and keep in mind that

00:06:29,500 --> 00:06:37,300
parameters histograms are inclusive so

00:06:32,080 --> 00:06:43,150
I'll a label means all events below that

00:06:37,300 --> 00:06:45,729
mark so imagine we have that a histogram

00:06:43,150 --> 00:06:47,890
for disk i/o and this is just to

00:06:45,729 --> 00:06:50,740
visualize the difference in in details

00:06:47,890 --> 00:06:53,590
you get this is the same event

00:06:50,740 --> 00:06:55,330
in two views on the left you have like

00:06:53,590 --> 00:06:57,130
just a simple counter and on the right

00:06:55,330 --> 00:06:59,050
you have a histogram

00:06:57,130 --> 00:07:01,660
the event is SSD replacement in

00:06:59,050 --> 00:07:06,789
production and you can see that the

00:07:01,660 --> 00:07:09,520
different shifts you get from that any

00:07:06,789 --> 00:07:13,030
new curve 105 you can see you can get

00:07:09,520 --> 00:07:15,130
hit Maps yeah you can get the histograms

00:07:13,030 --> 00:07:17,500
visualized to hate Maps and so that's

00:07:15,130 --> 00:07:19,330
pretty nice so the next slide has a

00:07:17,500 --> 00:07:22,090
bigger screen shot here you just see

00:07:19,330 --> 00:07:28,289
like difference in the amount of detail

00:07:22,090 --> 00:07:31,500
you get so here you see like color-coded

00:07:28,289 --> 00:07:33,759
buckets for each time slowed and

00:07:31,500 --> 00:07:36,849
distribution in a tooltip so you can see

00:07:33,759 --> 00:07:39,520
like you get that many i/o operations in

00:07:36,849 --> 00:07:42,849
this bucket it can definitely be better

00:07:39,520 --> 00:07:45,490
but it's it's already a lot better than

00:07:42,849 --> 00:07:49,180
just one simple line you get from from

00:07:45,490 --> 00:07:52,030
the counter so in additional two nice

00:07:49,180 --> 00:07:53,949
visualizations you can also plot a

00:07:52,030 --> 00:07:57,639
number of events below or above some

00:07:53,949 --> 00:07:59,830
line and you can have SLO or allergen in

00:07:57,639 --> 00:08:02,770
that so you can like I cannot absolutely

00:07:59,830 --> 00:08:08,169
can I have I owe s beyond one second and

00:08:02,770 --> 00:08:10,509
you can get alert and if you looking

00:08:08,169 --> 00:08:13,300
closely at the histograms you may have

00:08:10,509 --> 00:08:18,009
noticed that vanish and y-axis are kinda

00:08:13,300 --> 00:08:20,110
high and that was SSD so before the

00:08:18,009 --> 00:08:23,319
replacement you can see like the bucket

00:08:20,110 --> 00:08:26,979
from 0.5 second to one second and the

00:08:23,319 --> 00:08:31,240
text specs give you that the typical eye

00:08:26,979 --> 00:08:35,079
operations is 50 microseconds so that

00:08:31,240 --> 00:08:39,880
doesn't really add so it just doesn't

00:08:35,079 --> 00:08:42,010
live up to the spec and this is again

00:08:39,880 --> 00:08:45,579
that's the kind of detail you get from

00:08:42,010 --> 00:08:46,620
histograms so why now he should be

00:08:45,579 --> 00:08:54,670
convinced that you should have

00:08:46,620 --> 00:08:57,579
histograms for events like that okay so

00:08:54,670 --> 00:08:59,260
if you wanted to switch your all of your

00:08:57,579 --> 00:09:01,300
stored latency measurements to

00:08:59,260 --> 00:09:03,759
histograms I have bad news for you

00:09:01,300 --> 00:09:05,679
because you can't

00:09:03,759 --> 00:09:07,600
with no to exporter because linux only

00:09:05,679 --> 00:09:09,549
provides you with just one counter for

00:09:07,600 --> 00:09:12,549
each device but still it's just one

00:09:09,549 --> 00:09:17,169
counter you can try to mess with blk

00:09:12,549 --> 00:09:20,769
trace but that's not gonna be fast and

00:09:17,169 --> 00:09:22,980
it's not gonna be practical I think so

00:09:20,769 --> 00:09:26,679
let's switch gears a little and see how

00:09:22,980 --> 00:09:29,799
summary stats for counters can be more

00:09:26,679 --> 00:09:32,439
deceiving so this is the research from

00:09:29,799 --> 00:09:36,509
outer desk the creature on the Left

00:09:32,439 --> 00:09:40,049
called betas hours then there are

00:09:36,509 --> 00:09:42,970
figures in the middle there are target

00:09:40,049 --> 00:09:45,639
distributions and on the right you have

00:09:42,970 --> 00:09:48,249
animations going from the data zeroes to

00:09:45,639 --> 00:09:54,609
the targets and the amazing thing here

00:09:48,249 --> 00:09:57,669
is all all animations each frame on the

00:09:54,609 --> 00:10:02,379
right has the same mean and standard

00:09:57,669 --> 00:10:05,410
deviation for every frame so if you

00:10:02,379 --> 00:10:10,480
don't plot your data you you just don't

00:10:05,410 --> 00:10:12,639
know what's happening so yeah so from

00:10:10,480 --> 00:10:17,049
summary point summers to this point of

00:10:12,639 --> 00:10:20,139
view it's all the same this is from the

00:10:17,049 --> 00:10:24,129
same research you can clearly see how

00:10:20,139 --> 00:10:26,709
block how box plots in the middle don't

00:10:24,129 --> 00:10:29,259
give you the amount of detail in one and

00:10:26,709 --> 00:10:33,039
on the top you see histograms which is

00:10:29,259 --> 00:10:34,720
what you basically want so we

00:10:33,039 --> 00:10:38,109
established that histograms is what we

00:10:34,720 --> 00:10:41,559
need so but you need individual events

00:10:38,109 --> 00:10:43,660
to make those histograms so what advice

00:10:41,559 --> 00:10:49,029
would you need for a system that gives

00:10:43,660 --> 00:10:51,279
you that handles that assuming that you

00:10:49,029 --> 00:10:54,339
wanna more more stuff than just eye

00:10:51,279 --> 00:10:55,869
operations or links it has to be low

00:10:54,339 --> 00:10:58,389
overhead otherwise you can't run it in

00:10:55,869 --> 00:11:01,329
production and that's what you wanna run

00:10:58,389 --> 00:11:05,829
it it has to be universal so you not

00:11:01,329 --> 00:11:07,119
like lock down into just IO tracing like

00:11:05,829 --> 00:11:09,939
there are plenty of things you can

00:11:07,119 --> 00:11:13,059
measure on Linux kernel and it has to be

00:11:09,939 --> 00:11:17,420
supported out of the box so third-party

00:11:13,059 --> 00:11:21,110
modules are not very practical for that

00:11:17,420 --> 00:11:25,700
and finally it has to be safe it's not

00:11:21,110 --> 00:11:28,010
really fun if you crash a kernel across

00:11:25,700 --> 00:11:30,110
a large chunk of the internet many

00:11:28,010 --> 00:11:34,310
people won't be happy probably you won't

00:11:30,110 --> 00:11:37,970
be so in turns out and there's a

00:11:34,310 --> 00:11:40,280
solution is called EB PF it's a low

00:11:37,970 --> 00:11:42,110
overhead sandboxed user-defined bytecode

00:11:40,280 --> 00:11:44,300
running in the kernel it can have a

00:11:42,110 --> 00:11:47,210
crash hand or interfere with the kernel

00:11:44,300 --> 00:11:51,650
negatively so that's kind of vague

00:11:47,210 --> 00:11:54,200
but here two links for for you to dive

00:11:51,650 --> 00:11:57,800
into the details and explained how it

00:11:54,200 --> 00:11:59,810
works so the main part here is that it's

00:11:57,800 --> 00:12:02,750
already included in the kernel you

00:11:59,810 --> 00:12:05,770
already have it it's using networking

00:12:02,750 --> 00:12:10,610
subsystems and stuff like SEC comm for

00:12:05,770 --> 00:12:12,890
security and stuff but the general idea

00:12:10,610 --> 00:12:18,230
is that you can run safe code in the

00:12:12,890 --> 00:12:21,320
kernel is it's way beyond just just that

00:12:18,230 --> 00:12:25,850
and I think this dick is gonna have that

00:12:21,320 --> 00:12:28,160
as as a back-end for day 2 as well so

00:12:25,850 --> 00:12:29,000
you won't need a kernel module to run

00:12:28,160 --> 00:12:31,760
sis deck

00:12:29,000 --> 00:12:35,570
so the previous line said is it's a byte

00:12:31,760 --> 00:12:37,340
code this is how it looks so the good

00:12:35,570 --> 00:12:40,010
part is that you'll never have to write

00:12:37,340 --> 00:12:44,540
it by hand and maybe never even see this

00:12:40,010 --> 00:12:47,180
scary stuff so to use EB bf you write a

00:12:44,540 --> 00:12:49,370
small C programs that attach to kernel

00:12:47,180 --> 00:12:52,670
functions and run before or after them

00:12:49,370 --> 00:12:55,340
and your C code is then compiled into

00:12:52,670 --> 00:12:58,610
byte code verified and load it into

00:12:55,340 --> 00:13:01,340
kernel where it runs with JIT compiler

00:12:58,610 --> 00:13:04,280
and translate it into negative op codes

00:13:01,340 --> 00:13:07,490
for your machine the constraints of the

00:13:04,280 --> 00:13:09,400
code enforced by verifier so you're not

00:13:07,490 --> 00:13:12,740
you're guaranteed not to be able to

00:13:09,400 --> 00:13:15,860
write an infinite loop or allocate a lot

00:13:12,740 --> 00:13:17,930
of memory which is great because people

00:13:15,860 --> 00:13:22,540
are generally not good at writing code

00:13:17,930 --> 00:13:25,310
and to share neither with EB PF programs

00:13:22,540 --> 00:13:27,680
you can use maps so in terms of metrics

00:13:25,310 --> 00:13:30,440
collections you update maps in the

00:13:27,680 --> 00:13:31,130
kernel and then you fetch the values

00:13:30,440 --> 00:13:33,500
from this map

00:13:31,130 --> 00:13:35,840
in the user space when you scrape

00:13:33,500 --> 00:13:39,560
metrics and it's critical for

00:13:35,840 --> 00:13:42,050
performance that EBP fvm runs in the

00:13:39,560 --> 00:13:43,700
kernel and it never does any Cisco's

00:13:42,050 --> 00:13:46,850
because well you're already in the

00:13:43,700 --> 00:13:51,740
kernel and you can see the workflow on

00:13:46,850 --> 00:13:53,750
the image down here so having to write C

00:13:51,740 --> 00:13:55,790
code is not a EBP F constrained you can

00:13:53,750 --> 00:13:57,920
produce byte code in any way you want

00:13:55,790 --> 00:14:02,000
like you can write it by hand you can

00:13:57,920 --> 00:14:04,250
use another language like ply or Lua or

00:14:02,000 --> 00:14:06,950
just like C's dick and good can do it

00:14:04,250 --> 00:14:10,670
now and maybe someday people will be

00:14:06,950 --> 00:14:13,220
able to like write save JavaScript in

00:14:10,670 --> 00:14:17,990
the kernel that sounds scary but just

00:14:13,220 --> 00:14:23,120
think about it so just like GCC compiler

00:14:17,990 --> 00:14:25,790
C code into machine codes BCC compile C

00:14:23,120 --> 00:14:29,600
code into EBP F help codes for load in

00:14:25,790 --> 00:14:33,980
the kernel so BPA BCC is a real writer

00:14:29,600 --> 00:14:36,550
plus LLVM compiler in a library so you

00:14:33,980 --> 00:14:40,010
can use it in your code

00:14:36,550 --> 00:14:42,410
there are bindings for C C++ Python and

00:14:40,010 --> 00:14:44,480
go and in this example we see like

00:14:42,410 --> 00:14:47,650
simple function that runs after dlookup

00:14:44,480 --> 00:14:52,460
kernel function that's responsible for

00:14:47,650 --> 00:14:53,870
directory cache lookups and it doesn't

00:14:52,460 --> 00:14:58,580
look that complicated

00:14:53,870 --> 00:15:00,650
if you look at it some time especially

00:14:58,580 --> 00:15:04,010
if you like understand how C languages

00:15:00,650 --> 00:15:05,750
worked in addition to compiler BC C

00:15:04,010 --> 00:15:10,280
includes some tools that can be used for

00:15:05,750 --> 00:15:13,580
debugging production with low overhead

00:15:10,280 --> 00:15:17,540
ABI right so let's quickly go over a few

00:15:13,580 --> 00:15:19,940
of them so this is by latency the buyer

00:15:17,540 --> 00:15:23,480
here not doesn't mean biology it means

00:15:19,940 --> 00:15:26,210
block IO and show you a histogram of

00:15:23,480 --> 00:15:29,060
disk IO operations and this exactly what

00:15:26,210 --> 00:15:31,160
we started with and that's is just in a

00:15:29,060 --> 00:15:35,990
different formats it's a script instead

00:15:31,160 --> 00:15:37,730
of exporter this is exact

00:15:35,990 --> 00:15:40,400
it allows you to see which commands are

00:15:37,730 --> 00:15:41,900
being latched in the system it's often

00:15:40,400 --> 00:15:44,340
useful if you want to catch something

00:15:41,900 --> 00:15:50,880
that is quickly exiting

00:15:44,340 --> 00:15:55,920
so you can see it in PS output this is X

00:15:50,880 --> 00:15:58,140
for slower which is showing slow file

00:15:55,920 --> 00:16:01,500
system operations instead of i/o

00:15:58,140 --> 00:16:05,550
operations so you may think that this to

00:16:01,500 --> 00:16:07,320
map together fairly closely but one file

00:16:05,550 --> 00:16:10,500
system operation does not necessarily

00:16:07,320 --> 00:16:12,570
mean one eye operation for example it

00:16:10,500 --> 00:16:17,089
writes can do can go into write back

00:16:12,570 --> 00:16:19,589
cache and not touch disk until later

00:16:17,089 --> 00:16:21,540
which can involve multiple i/o

00:16:19,589 --> 00:16:25,470
operations to physical device if you

00:16:21,540 --> 00:16:28,050
read a large buffer and Ritz can also

00:16:25,470 --> 00:16:30,690
get blocked behind async writes that you

00:16:28,050 --> 00:16:33,000
issued earlier so the more you know

00:16:30,690 --> 00:16:35,130
about this guy home the more you wanna

00:16:33,000 --> 00:16:39,210
run stateless really so it's just

00:16:35,130 --> 00:16:42,870
depressing okay so now its main idea of

00:16:39,210 --> 00:16:44,700
the talk we have all this primitives so

00:16:42,870 --> 00:16:46,500
we now should be able to tie them all

00:16:44,700 --> 00:16:49,820
together and get an exporter on our

00:16:46,500 --> 00:16:53,810
hands to get metrics into Prometheus and

00:16:49,820 --> 00:16:57,420
like graph them and we'll alert on them

00:16:53,810 --> 00:17:01,530
so many BCC tools already have kernel

00:16:57,420 --> 00:17:03,930
side ready like by a snoop or by latency

00:17:01,530 --> 00:17:06,660
and they are reviewed by smart people

00:17:03,930 --> 00:17:09,839
that understand what they doing not not

00:17:06,660 --> 00:17:12,360
people like me so the hardest part here

00:17:09,839 --> 00:17:14,420
is covered so the idea is simple we

00:17:12,360 --> 00:17:18,150
compile BCC programs and load them with

00:17:14,420 --> 00:17:20,970
the exporter the programs then run in

00:17:18,150 --> 00:17:23,880
the kernel and collect metrics we want

00:17:20,970 --> 00:17:28,590
them to collect and on scrape time we

00:17:23,880 --> 00:17:32,580
just pull sorry oh I'll be quick we

00:17:28,590 --> 00:17:35,100
pulled them entrance from keys into

00:17:32,580 --> 00:17:37,290
labels and values just convert them to

00:17:35,100 --> 00:17:42,210
float 64 because kernel doesn't know

00:17:37,290 --> 00:17:45,900
floats exist so let's look at the simple

00:17:42,210 --> 00:17:48,480
example to get counters for timers file

00:17:45,900 --> 00:17:50,520
in the kernel an example is from EBP if

00:17:48,480 --> 00:17:53,250
exporter repo we can when you can find

00:17:50,520 --> 00:17:56,610
more examples more complexes than this

00:17:53,250 --> 00:17:58,260
one on the beside VCC side you can

00:17:56,610 --> 00:18:00,870
define a hash and attached to

00:17:58,260 --> 00:18:03,870
tracepoint and when a trace point fires

00:18:00,870 --> 00:18:06,299
we increment the key we can increment

00:18:03,870 --> 00:18:09,000
the map where the key is address of the

00:18:06,299 --> 00:18:11,850
function and when we scrape we convert

00:18:09,000 --> 00:18:16,169
this address into function name with the

00:18:11,850 --> 00:18:17,580
map provided by the kernel so this is

00:18:16,169 --> 00:18:21,919
the example graph you can get from it

00:18:17,580 --> 00:18:26,190
and you can see like how it changes with

00:18:21,919 --> 00:18:28,049
with a daily load patterns why can this

00:18:26,190 --> 00:18:30,480
be useful you can check out the

00:18:28,049 --> 00:18:35,130
CloudFlare blog post about how we traced

00:18:30,480 --> 00:18:38,040
issues with timers when we did the OS

00:18:35,130 --> 00:18:40,320
upgrade the TLDR is that system we broke

00:18:38,040 --> 00:18:44,870
tcp segmentation offload on VLAN

00:18:40,320 --> 00:18:48,600
interface and that increased cpu 5x and

00:18:44,870 --> 00:18:50,190
introduced interesting stuff like page

00:18:48,600 --> 00:18:52,919
allocation stones when you try to

00:18:50,190 --> 00:18:55,110
allocate memory it stalls for like a

00:18:52,919 --> 00:18:56,910
minute and that's not fun so if we had

00:18:55,110 --> 00:19:01,770
metrics for this enabled we would see it

00:18:56,910 --> 00:19:05,360
sooner another bundle example gives you

00:19:01,770 --> 00:19:08,460
I P C which is instructions per cycle

00:19:05,360 --> 00:19:10,590
for each CPU so in production you can

00:19:08,460 --> 00:19:13,380
see in this production system you can

00:19:10,590 --> 00:19:17,250
see a few interesting things like not

00:19:13,380 --> 00:19:18,960
all cores I equal so core the Green Line

00:19:17,250 --> 00:19:21,630
is course your own and the yellow is

00:19:18,960 --> 00:19:24,090
core one so you can see like the green

00:19:21,630 --> 00:19:25,950
is on top and yellow on the bottom and

00:19:24,090 --> 00:19:28,890
the rest of the cores are in the middle

00:19:25,950 --> 00:19:32,429
and that's because how we pin services

00:19:28,890 --> 00:19:34,890
to the course and there was some event

00:19:32,429 --> 00:19:37,350
that affected what looks like upper

00:19:34,890 --> 00:19:40,890
course so that looks like something that

00:19:37,350 --> 00:19:42,660
can be investigated so why can't this be

00:19:40,890 --> 00:19:44,970
useful you can check out the brain in

00:19:42,660 --> 00:19:49,530
Greg's blog post about CPU utilization

00:19:44,970 --> 00:19:53,070
and ta are here is that a CPU percentage

00:19:49,530 --> 00:19:55,200
means not what you may think it means so

00:19:53,070 --> 00:19:58,799
in the picture you can see if your CPU

00:19:55,200 --> 00:20:01,110
is that some percentage busy it can mean

00:19:58,799 --> 00:20:03,929
that it just stalls on memory access

00:20:01,110 --> 00:20:06,360
instead of doing actual work and you can

00:20:03,929 --> 00:20:11,429
see with IPC metrics what is gradually

00:20:06,360 --> 00:20:11,910
happening another panel example is LLC

00:20:11,429 --> 00:20:14,970
or

00:20:11,910 --> 00:20:17,910
three CPU cache hit rate this is from

00:20:14,970 --> 00:20:20,160
the same machine as IPC metrics so you

00:20:17,910 --> 00:20:22,170
can see you like the same event

00:20:20,160 --> 00:20:24,180
happening and you can still see like

00:20:22,170 --> 00:20:27,810
different cores have different parents

00:20:24,180 --> 00:20:30,720
and if you understand this you can

00:20:27,810 --> 00:20:32,190
probably investigate but if not like you

00:20:30,720 --> 00:20:34,920
do internal operate and you see it

00:20:32,190 --> 00:20:39,180
changes maybe go ask somebody that's

00:20:34,920 --> 00:20:41,310
what I did so why can this be useful ah

00:20:39,180 --> 00:20:43,470
you can see how your CPU cache is doing

00:20:41,310 --> 00:20:45,930
and you can see how it may be affected

00:20:43,470 --> 00:20:48,990
by bigger cache or more cores sharing

00:20:45,930 --> 00:20:51,570
the same cache and that can be used as

00:20:48,990 --> 00:20:56,070
like as a purchasing decision do you

00:20:51,570 --> 00:20:56,490
want to like buy bigger CPU now to the

00:20:56,070 --> 00:20:59,160
fun part

00:20:56,490 --> 00:21:01,590
we started with IO latency histograms

00:20:59,160 --> 00:21:03,720
and they bundled this is also a

00:21:01,590 --> 00:21:06,090
histogram but this is for Ronquillo

00:21:03,720 --> 00:21:09,780
latency so when the process wakes up in

00:21:06,090 --> 00:21:12,360
the kernel it's ready to run and if it

00:21:09,780 --> 00:21:13,950
if we have a CPU ready it can run

00:21:12,360 --> 00:21:16,500
immediately but if not it has to wait

00:21:13,950 --> 00:21:19,500
and this is measuring that schedule in

00:21:16,500 --> 00:21:21,300
the lane and you can see like this this

00:21:19,500 --> 00:21:24,810
wasn't expected for me that you can wait

00:21:21,300 --> 00:21:29,070
for like 66 milli milliseconds to wait

00:21:24,810 --> 00:21:31,410
and is true you can get the same metric

00:21:29,070 --> 00:21:33,840
from C advisor but again as a counter

00:21:31,410 --> 00:21:40,650
instead of histogram so you wouldn't see

00:21:33,840 --> 00:21:45,170
your actual distribution I'm gonna skip

00:21:40,650 --> 00:21:47,640
over but this is this is actually useful

00:21:45,170 --> 00:21:50,550
this is not there's are not only

00:21:47,640 --> 00:21:53,070
possible examples you can have like 500

00:21:50,550 --> 00:21:55,320
hardware events encounters on uses on a

00:21:53,070 --> 00:21:57,660
typical system there are 2,000

00:21:55,320 --> 00:22:01,470
tracepoint and like a billion kernel

00:21:57,660 --> 00:22:05,040
functions you can trace and this is from

00:22:01,470 --> 00:22:06,900
Brendan Greg's BCC tools a much and it

00:22:05,040 --> 00:22:09,480
shows you what you can measure with what

00:22:06,900 --> 00:22:11,910
tools and each of those tools can be

00:22:09,480 --> 00:22:15,380
converted into exporter to give you

00:22:11,910 --> 00:22:19,110
metrics for a long time investigation

00:22:15,380 --> 00:22:21,270
but nothing this life is free and this

00:22:19,110 --> 00:22:22,740
even lower hat means some overhead so

00:22:21,270 --> 00:22:24,300
you should be measure on your own

00:22:22,740 --> 00:22:25,149
environment this is this is what we've

00:22:24,300 --> 00:22:28,119
got

00:22:25,149 --> 00:22:32,700
for fast get beat Cisco you get like 34

00:22:28,119 --> 00:22:36,969
percent overhead which sounds like a lot

00:22:32,700 --> 00:22:38,589
but it's a hundred nanoseconds and I

00:22:36,969 --> 00:22:41,109
googled what a hundred nanoseconds is

00:22:38,589 --> 00:22:42,999
it's one memory reference and which

00:22:41,109 --> 00:22:45,399
means to like just that get beat is very

00:22:42,999 --> 00:22:47,979
fast for a complex example where you

00:22:45,399 --> 00:22:50,200
like make some randomness read some

00:22:47,979 --> 00:22:52,899
memory it gets to a hundred percent

00:22:50,200 --> 00:22:55,179
overhead but you can still do you 1.5

00:22:52,899 --> 00:22:57,309
million cease calls per second on one

00:22:55,179 --> 00:22:59,589
core instead of three million so if you

00:22:57,309 --> 00:23:02,019
measure stuff like this guy up that you

00:22:59,589 --> 00:23:05,219
don't do three million times per second

00:23:02,019 --> 00:23:09,909
per core then to probably it'll be fine

00:23:05,219 --> 00:23:14,320
and for rank Yelena's see with so 300

00:23:09,909 --> 00:23:16,839
millisecond cpu increase just one and on

00:23:14,320 --> 00:23:19,509
machine with 40 logical course in 250

00:23:16,839 --> 00:23:22,779
scheduled events per second so that

00:23:19,509 --> 00:23:24,549
that's visible but still low so we

00:23:22,779 --> 00:23:27,309
should do run the exporter anywhere you

00:23:24,549 --> 00:23:29,979
feel comfortable but complex programs

00:23:27,309 --> 00:23:32,589
can be gated to can or machines iCloud

00:23:29,979 --> 00:23:36,070
for who have can read data centers where

00:23:32,589 --> 00:23:37,419
we have actual live users but does it

00:23:36,070 --> 00:23:39,909
that's not as bad as it sounds because

00:23:37,419 --> 00:23:43,089
we just on ourselves before we roll out

00:23:39,909 --> 00:23:45,219
to this test users and for a great

00:23:43,089 --> 00:23:48,940
events you can roll it out on machines

00:23:45,219 --> 00:23:51,549
that experience that great events that's

00:23:48,940 --> 00:23:54,249
all the time I had I even ran over thank

00:23:51,549 --> 00:23:57,670
you you can use it do it there are more

00:23:54,249 --> 00:24:01,539
links that's it

00:23:57,670 --> 00:24:01,539
[Applause]

00:24:06,249 --> 00:24:10,690
time for four minutes

00:24:08,070 --> 00:24:19,509
hey thank you for your talk

00:24:10,690 --> 00:24:23,080
I was wondering if I was wondering if

00:24:19,509 --> 00:24:26,409
the the EB PF program only writes in two

00:24:23,080 --> 00:24:28,599
maps in the kernel and then the user

00:24:26,409 --> 00:24:30,129
space program exposes that metric to

00:24:28,599 --> 00:24:33,129
Prometheus how do you make sure that

00:24:30,129 --> 00:24:34,690
when the thing that you're actually

00:24:33,129 --> 00:24:37,749
monitoring goes away that that's

00:24:34,690 --> 00:24:40,359
actually cleared from the kernel Maps so

00:24:37,749 --> 00:24:42,909
that we're like not creating a memory

00:24:40,359 --> 00:24:45,549
leak in the kernel Oh so EVP of problems

00:24:42,909 --> 00:24:47,679
are attached to the process like they

00:24:45,549 --> 00:24:51,570
you attach them from a process and I'm

00:24:47,679 --> 00:24:54,369
gonna process the ice they removed and

00:24:51,570 --> 00:24:56,499
the data unless you pin it you can pin

00:24:54,369 --> 00:24:59,919
it and use it later but we don't do it

00:24:56,499 --> 00:25:02,139
for Explorer because we don't need it

00:24:59,919 --> 00:25:04,089
so you mentioned see advisor at some

00:25:02,139 --> 00:25:05,469
point earlier does this work like can

00:25:04,089 --> 00:25:08,049
you export the label that's the C group

00:25:05,469 --> 00:25:11,440
you are touching the probe to that is

00:25:08,049 --> 00:25:14,950
not something that's possible now but in

00:25:11,440 --> 00:25:19,269
the future there are proposals to attach

00:25:14,950 --> 00:25:21,279
container ID to C group because like you

00:25:19,269 --> 00:25:23,769
have C groups you have other things that

00:25:21,279 --> 00:25:27,609
like constitute container and they don't

00:25:23,769 --> 00:25:30,039
always are visible for stuff outside of

00:25:27,609 --> 00:25:31,659
say advisor so it's not possible now but

00:25:30,039 --> 00:25:38,859
they're a work in progress

00:25:31,659 --> 00:25:42,399
okay thanks two quick questions one does

00:25:38,859 --> 00:25:46,330
this require route yes well you have to

00:25:42,399 --> 00:25:50,259
attach in the kernel so yeah I guess you

00:25:46,330 --> 00:25:54,009
can do it without route if you have

00:25:50,259 --> 00:25:56,409
sufficient capabilities but I haven't

00:25:54,009 --> 00:25:58,509
tried it because like we go deep in the

00:25:56,409 --> 00:26:00,460
kernel that kind of invalidates a second

00:25:58,509 --> 00:26:02,619
question it was if there was any idea of

00:26:00,460 --> 00:26:04,269
maybe putting some parts of these are

00:26:02,619 --> 00:26:08,019
safe and very well test it into the

00:26:04,269 --> 00:26:10,539
notice border so I don't know it's like

00:26:08,019 --> 00:26:13,330
it can be later in node exposure but

00:26:10,539 --> 00:26:15,909
like I don't know if it should be

00:26:13,330 --> 00:26:17,760
because it's a free form when node

00:26:15,909 --> 00:26:21,480
exporters like rigid metrics

00:26:17,760 --> 00:26:26,690
yeah ask the colonel people to expose

00:26:21,480 --> 00:26:31,070
more metrics yes good luck with that

00:26:26,690 --> 00:26:34,470
hey there great talk so can we run

00:26:31,070 --> 00:26:36,980
different trace points per one node

00:26:34,470 --> 00:26:41,840
exporter or do we need to run several

00:26:36,980 --> 00:26:45,480
you can have multiple programs in one

00:26:41,840 --> 00:26:48,540
PPF exporter so we run a block il we're

00:26:45,480 --> 00:26:50,730
on schedule in latency or on timers all

00:26:48,540 --> 00:26:56,400
in the same Explorer then can you mix

00:26:50,730 --> 00:26:58,080
like approves and other like micro can

00:26:56,400 --> 00:27:01,320
you repeat louder please and you mix the

00:26:58,080 --> 00:27:04,020
probes that you're using in the same ABF

00:27:01,320 --> 00:27:06,210
exporter yes yes you can mix them in the

00:27:04,020 --> 00:27:08,610
same you can have different metrics from

00:27:06,210 --> 00:27:10,500
same appfx port and you can have one

00:27:08,610 --> 00:27:23,700
programs attached into multiple tries

00:27:10,500 --> 00:27:29,340
points okay thank you for the top you

00:27:23,700 --> 00:27:31,670
said that the exporter composer it does

00:27:29,340 --> 00:27:31,670
that mean

00:27:34,020 --> 00:27:44,430
so the the LLVM comes in in terms of a

00:27:39,420 --> 00:27:46,830
library so the two we use called go BPF

00:27:44,430 --> 00:27:48,780
and we when you compile it as a

00:27:46,830 --> 00:27:51,270
dependency it compounds into you go

00:27:48,780 --> 00:28:02,570
binary as like it just compiled in you

00:27:51,270 --> 00:28:06,960
don't need to BM you can do it with the

00:28:02,570 --> 00:28:09,630
tool bundled with the kernel but well

00:28:06,960 --> 00:28:15,510
it's it's for different reasons okay

00:28:09,630 --> 00:28:18,260
thank you you're really giving me

00:28:15,510 --> 00:28:18,260
exercise here

00:28:19,020 --> 00:28:25,720
thank you have you run into any gotchas

00:28:23,500 --> 00:28:28,930
or limitations on the Prometheus

00:28:25,720 --> 00:28:31,900
histogram format as far as bucket limits

00:28:28,930 --> 00:28:34,540
or granularity in any capacity just

00:28:31,900 --> 00:28:38,110
because these tend to vary in scope it

00:28:34,540 --> 00:28:42,250
seems yeah so you have to provide the

00:28:38,110 --> 00:28:45,460
buckets you want to see and for us if

00:28:42,250 --> 00:28:49,030
you do power of two up to a minute that

00:28:45,460 --> 00:28:50,740
should cover everything if like it many

00:28:49,030 --> 00:28:53,410
things won't go over second in the

00:28:50,740 --> 00:29:03,360
kernel but like I always surprisingly

00:28:53,410 --> 00:29:03,360
bad any other questions

00:29:03,480 --> 00:29:12,529
nope well thank you very much

00:29:06,630 --> 00:29:12,529
[Applause]

00:29:15,140 --> 00:29:17,200

YouTube URL: https://www.youtube.com/watch?v=dRdJ75tKZak


