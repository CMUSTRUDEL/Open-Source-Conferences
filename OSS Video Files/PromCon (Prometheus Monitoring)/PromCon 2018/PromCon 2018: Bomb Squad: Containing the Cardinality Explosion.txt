Title: PromCon 2018: Bomb Squad: Containing the Cardinality Explosion
Publication date: 2018-11-10
Playlist: PromCon 2018
Description: 
	Speaker: Cody Boggs

Series cardinality can be a large foot-gun that can take out a Prometheus instance quickly. It can have similar impacts to downstream systems when using remote write. Finding the offending series and recovering from such an event can cause a large downtime for your Prometheus installation. In this talk I will show an implementation of a sidecar to Prometheus that automatically detects and squelches series whose cardinality is growing quickly. Using a combination of recording rules, dynamic metric relabel configs, and alerting, we can detect, throttle, and alert on any explosion in progress.
Captions: 
	00:00:12,540 --> 00:00:16,090
all right thank you can you hear me okay

00:00:14,910 --> 00:00:17,770
excellent

00:00:16,090 --> 00:00:20,529
all right so actually to the last

00:00:17,770 --> 00:00:22,630
question for Julian's this is hopefully

00:00:20,529 --> 00:00:24,400
very very relevant so we'll see so yeah

00:00:22,630 --> 00:00:25,960
this is about bomb squad it's a project

00:00:24,400 --> 00:00:28,329
that we're working on to hopefully help

00:00:25,960 --> 00:00:29,500
tone down some bad events that we'll

00:00:28,329 --> 00:00:32,710
talk about in a moment

00:00:29,500 --> 00:00:35,470
first up Who am I so I'm Cody blogs

00:00:32,710 --> 00:00:38,010
nicknamed Choni if we have time we may

00:00:35,470 --> 00:00:40,270
tell that story I don't know it depends

00:00:38,010 --> 00:00:42,399
I'm an ops nerd for about the last eight

00:00:40,270 --> 00:00:44,109
years been lead of ops for fresh tracks

00:00:42,399 --> 00:00:46,629
for about the last year a little bit

00:00:44,109 --> 00:00:48,460
more I'm obsessed with metrics and most

00:00:46,629 --> 00:00:50,980
other things observability related I

00:00:48,460 --> 00:00:52,570
like to pretend to write code my team is

00:00:50,980 --> 00:00:55,179
helping me to be less pretend II and

00:00:52,570 --> 00:00:57,219
more actual you know writing and I break

00:00:55,179 --> 00:00:59,649
all the things because that's apparently

00:00:57,219 --> 00:01:01,570
how I learn best so I'll go from there

00:00:59,649 --> 00:01:03,010
so what's on deck let's talk about what

00:01:01,570 --> 00:01:04,540
is cardinality we've heard it several

00:01:03,010 --> 00:01:06,820
times today but we should get a better

00:01:04,540 --> 00:01:09,640
feel for it what does it mean when it

00:01:06,820 --> 00:01:12,400
explodes it sounds bad and spoiler alert

00:01:09,640 --> 00:01:13,870
it is who cares we need charts and

00:01:12,400 --> 00:01:15,940
graphs so we can kind of back up some of

00:01:13,870 --> 00:01:19,150
my claims and then we'll see how well we

00:01:15,940 --> 00:01:21,760
get through a live demo of bomb squad so

00:01:19,150 --> 00:01:22,900
first what is cardinality we talk a lot

00:01:21,760 --> 00:01:24,550
about high cardinality and low

00:01:22,900 --> 00:01:26,680
cardinality it's it's simpler than it

00:01:24,550 --> 00:01:28,360
seems sometimes your birds are too high

00:01:26,680 --> 00:01:31,090
and sometimes they're too low so that's

00:01:28,360 --> 00:01:32,770
my talk thank you so much I hope that's

00:01:31,090 --> 00:01:34,120
the only bad joke in the talk we'll see

00:01:32,770 --> 00:01:36,190
how it goes

00:01:34,120 --> 00:01:38,260
more generally and more seriously it's

00:01:36,190 --> 00:01:41,440
just the count of a number of things in

00:01:38,260 --> 00:01:43,540
a group so in this little fake set of

00:01:41,440 --> 00:01:46,660
things of B 42 and tree we have a

00:01:43,540 --> 00:01:48,940
cardinality of 3 a little bit more scope

00:01:46,660 --> 00:01:51,250
to this talk we're gonna talk more

00:01:48,940 --> 00:01:54,910
specifically about how many discrete

00:01:51,250 --> 00:01:57,310
series there are for a given metric I

00:01:54,910 --> 00:01:58,870
know that as to Julian's point it is

00:01:57,310 --> 00:02:00,400
indeed series all the way down we're

00:01:58,870 --> 00:02:02,500
gonna flip that script a little bit and

00:02:00,400 --> 00:02:04,600
think about it more semantically as a

00:02:02,500 --> 00:02:05,820
metric is a thing that contains series

00:02:04,600 --> 00:02:08,709
so we'll talk more about that in a bit

00:02:05,820 --> 00:02:11,200
but for my little fake series here

00:02:08,709 --> 00:02:13,540
called CPU if I had one label called

00:02:11,200 --> 00:02:17,410
host and 3 distinct values for it I have

00:02:13,540 --> 00:02:19,330
a cardinality of 3 for my metric CPU so

00:02:17,410 --> 00:02:21,610
let's settle on terms back to the

00:02:19,330 --> 00:02:22,960
comment that I jumped ahead on so a

00:02:21,610 --> 00:02:23,410
series for anybody who's forgotten

00:02:22,960 --> 00:02:25,960
already

00:02:23,410 --> 00:02:28,510
is just a discrete pairing of label

00:02:25,960 --> 00:02:31,090
names and their values for the sake of

00:02:28,510 --> 00:02:34,990
this talk at least a metric is a set of

00:02:31,090 --> 00:02:37,390
series whose name special label match so

00:02:34,990 --> 00:02:40,180
for instance API requests CPU on the

00:02:37,390 --> 00:02:42,430
last slide so forth a cardinality

00:02:40,180 --> 00:02:44,320
explosion or a high cardinality event as

00:02:42,430 --> 00:02:46,510
we'll talk about a lot it's a really

00:02:44,320 --> 00:02:48,490
sharp increase it's a really sudden blow

00:02:46,510 --> 00:02:50,290
up basically of the amount of series

00:02:48,490 --> 00:02:53,380
that are associated with a particular

00:02:50,290 --> 00:02:55,570
metric and an exploding label as fun as

00:02:53,380 --> 00:02:57,940
that sounds it really is it's just a

00:02:55,570 --> 00:03:01,360
label who's getting these values being

00:02:57,940 --> 00:03:03,820
set to very very high cardinality values

00:03:01,360 --> 00:03:05,440
so and it's disproportionately high

00:03:03,820 --> 00:03:07,570
compared to others and therefore makes

00:03:05,440 --> 00:03:10,240
the metric under which all those series

00:03:07,570 --> 00:03:12,670
exist explode we'll see more of this as

00:03:10,240 --> 00:03:15,580
we go through so let's unpack this idea

00:03:12,670 --> 00:03:17,380
of explosions a little bit better first

00:03:15,580 --> 00:03:18,850
off I didn't have exploding birds in the

00:03:17,380 --> 00:03:21,220
first slide because it seemed a little

00:03:18,850 --> 00:03:23,140
violent so we won't go there but so

00:03:21,220 --> 00:03:25,480
again this is a really rapid inflation

00:03:23,140 --> 00:03:27,850
of I have a metric and suddenly it got a

00:03:25,480 --> 00:03:30,370
ton of series created for it and

00:03:27,850 --> 00:03:32,320
generally it's a sustained issue some of

00:03:30,370 --> 00:03:33,790
the things that can cause that the first

00:03:32,320 --> 00:03:35,110
two aren't necessarily bad in and of

00:03:33,790 --> 00:03:38,110
themselves and they tend to not be a

00:03:35,110 --> 00:03:39,640
prolonged sustained explosion but if you

00:03:38,110 --> 00:03:41,980
have really prolonged extreme pod

00:03:39,640 --> 00:03:44,320
turnover rates some of you just keeps

00:03:41,980 --> 00:03:45,400
spinning things up and back down more

00:03:44,320 --> 00:03:47,260
often than you'd expect

00:03:45,400 --> 00:03:50,290
this could cause at least a short-lived

00:03:47,260 --> 00:03:51,970
and less severe version of this a highly

00:03:50,290 --> 00:03:53,680
elastic workload where suddenly it's

00:03:51,970 --> 00:03:55,630
churning through tons and tons of work

00:03:53,680 --> 00:03:57,610
that you didn't expect or potentially

00:03:55,630 --> 00:03:59,650
out of character for it you could end up

00:03:57,610 --> 00:04:01,810
with again potentially hopefully a

00:03:59,650 --> 00:04:04,930
short-lived version of this but most

00:04:01,810 --> 00:04:06,850
commonly and the most devastatingly that

00:04:04,930 --> 00:04:09,880
we see are actually the bad code deploy

00:04:06,850 --> 00:04:11,920
we're in a well-intentioned engineer

00:04:09,880 --> 00:04:14,739
either knowingly or unknowingly deploys

00:04:11,920 --> 00:04:16,450
a code change where one or more metrics

00:04:14,739 --> 00:04:18,640
ends up having some really high

00:04:16,450 --> 00:04:21,580
cardinality data highly unique data like

00:04:18,640 --> 00:04:24,520
request IDs time stamps which we'll use

00:04:21,580 --> 00:04:26,230
for the demo into a label or more than

00:04:24,520 --> 00:04:28,270
one label and so suddenly that metric

00:04:26,230 --> 00:04:29,500
starts to starts to explode

00:04:28,270 --> 00:04:31,960
like I said that seems to be the most

00:04:29,500 --> 00:04:34,270
common and it's the most most painful

00:04:31,960 --> 00:04:35,620
the magnitude of these tends to be very

00:04:34,270 --> 00:04:37,570
huge and they tend to be really hard to

00:04:35,620 --> 00:04:39,430
track down too

00:04:37,570 --> 00:04:41,200
one of the questions earlier it's not

00:04:39,430 --> 00:04:43,360
always easy to figure out what metric

00:04:41,200 --> 00:04:45,790
who's doing it where it's coming from so

00:04:43,360 --> 00:04:50,260
we've tried to ease some of that but

00:04:45,790 --> 00:04:52,270
first why do you care so basically you

00:04:50,260 --> 00:04:54,130
can catch everything on fire the first

00:04:52,270 --> 00:04:55,450
one up is kind of actually the lowest

00:04:54,130 --> 00:04:57,370
priority one so we'll get it out of the

00:04:55,450 --> 00:04:59,080
way is that during and after these kinds

00:04:57,370 --> 00:05:02,110
of events the data that was affected by

00:04:59,080 --> 00:05:04,090
this explosion is borders on meaningless

00:05:02,110 --> 00:05:05,950
because typically you get a single data

00:05:04,090 --> 00:05:07,510
point that was valid at some point and

00:05:05,950 --> 00:05:09,040
then it just sticks because it keeps

00:05:07,510 --> 00:05:11,500
getting exposed by the exporter that

00:05:09,040 --> 00:05:13,930
pushed in the first place but more

00:05:11,500 --> 00:05:16,090
importantly it will burn your Prometheus

00:05:13,930 --> 00:05:18,220
down in a hurry as we'll see in a moment

00:05:16,090 --> 00:05:19,390
so query times scrape times all of the

00:05:18,220 --> 00:05:21,100
metrics that we care about in a

00:05:19,390 --> 00:05:23,800
Prometheus server will start to look

00:05:21,100 --> 00:05:25,510
terrible and you'll end up losing your

00:05:23,800 --> 00:05:27,340
ability to investigate what took it down

00:05:25,510 --> 00:05:30,310
because you would typically use

00:05:27,340 --> 00:05:31,900
Prometheus to find that out so and then

00:05:30,310 --> 00:05:34,360
also downstream services right so if

00:05:31,900 --> 00:05:35,950
fresh tracks we push our agent tree is

00:05:34,360 --> 00:05:38,380
effectively just a Prometheus instance

00:05:35,950 --> 00:05:40,270
with a very bespoke configuration that

00:05:38,380 --> 00:05:42,460
ships data to cortex which is our

00:05:40,270 --> 00:05:44,470
backing store that's backed by BigTable

00:05:42,460 --> 00:05:45,940
and neither of those pieces really likes

00:05:44,470 --> 00:05:47,920
these explosions at all I say neither

00:05:45,940 --> 00:05:51,450
all three of those pieces if I can learn

00:05:47,920 --> 00:05:53,830
to count really dislike that behavior so

00:05:51,450 --> 00:05:55,780
obviously pics or it didn't happen

00:05:53,830 --> 00:05:57,160
so this is where standalone Prometheus

00:05:55,780 --> 00:05:59,980
instance that I used for testing this

00:05:57,160 --> 00:06:02,020
and building some graphs this was simply

00:05:59,980 --> 00:06:04,540
scraping one workload which was 20 pods

00:06:02,020 --> 00:06:06,790
of my little toy app that emits around a

00:06:04,540 --> 00:06:09,070
hundred new series per second

00:06:06,790 --> 00:06:11,980
so generally simulating approximately

00:06:09,070 --> 00:06:14,320
2,000 requests a second on some

00:06:11,980 --> 00:06:15,850
application and I don't know we won't go

00:06:14,320 --> 00:06:17,500
too deep into these particular graphs

00:06:15,850 --> 00:06:19,510
but if you've ever seen these metrics or

00:06:17,500 --> 00:06:20,590
are at all familiar with them you'll

00:06:19,510 --> 00:06:22,900
know this is the last thing your

00:06:20,590 --> 00:06:26,650
Prometheus sees before it dies a fiery

00:06:22,900 --> 00:06:28,480
death so this is a bad place to be more

00:06:26,650 --> 00:06:31,270
fun data this is actually real data from

00:06:28,480 --> 00:06:33,490
one of our incidents in production so as

00:06:31,270 --> 00:06:35,650
you can see we kind of had a nice steady

00:06:33,490 --> 00:06:37,330
state for ingestion for a while until

00:06:35,650 --> 00:06:39,220
one of our customers had a code deploy

00:06:37,330 --> 00:06:41,320
go out there agent started shipping us

00:06:39,220 --> 00:06:42,970
extremely high cardinality data because

00:06:41,320 --> 00:06:45,220
there were request IDs being stuck into

00:06:42,970 --> 00:06:47,860
a label and so our ingestion path

00:06:45,220 --> 00:06:50,080
started to churn total time from front

00:06:47,860 --> 00:06:50,710
door to the back end started to get to

00:06:50,080 --> 00:06:52,540
unreason

00:06:50,710 --> 00:06:54,760
levels from nanoseconds to almost a

00:06:52,540 --> 00:06:57,250
minute and if you look at the big table

00:06:54,760 --> 00:06:58,960
graph that one was really fun simply

00:06:57,250 --> 00:07:00,940
because not only did the request rate

00:06:58,960 --> 00:07:02,680
climb dramatically for the backing store

00:07:00,940 --> 00:07:05,320
itself but these were very very tiny

00:07:02,680 --> 00:07:08,050
rights as it turns out over time and it

00:07:05,320 --> 00:07:10,780
did not like that at all so you can

00:07:08,050 --> 00:07:12,580
really turn bad when these things happen

00:07:10,780 --> 00:07:14,080
this was actually this event actually is

00:07:12,580 --> 00:07:17,170
what prompted me to start working on

00:07:14,080 --> 00:07:18,910
bomb squad and prompted this talk so on

00:07:17,170 --> 00:07:20,800
that note what in the world is bomb

00:07:18,910 --> 00:07:23,860
squad other than a really awesome name

00:07:20,800 --> 00:07:25,150
that I didn't come up with deadly so

00:07:23,860 --> 00:07:27,130
it's basically just a small little

00:07:25,150 --> 00:07:29,200
binary that's intended to be run as a

00:07:27,130 --> 00:07:32,080
sidecar to your kubernetes hosted

00:07:29,200 --> 00:07:33,820
prometheus at least at this point it

00:07:32,080 --> 00:07:35,830
bootstraps itself into that Prometheus

00:07:33,820 --> 00:07:39,070
with a couple with a recording rule that

00:07:35,830 --> 00:07:40,660
it uses for doing the detection it

00:07:39,070 --> 00:07:42,730
starts to watch for exploding metrics

00:07:40,660 --> 00:07:44,590
when it finds one its figures out okay

00:07:42,730 --> 00:07:47,320
which label is contributing the most to

00:07:44,590 --> 00:07:48,910
this explosion within that metric and

00:07:47,320 --> 00:07:50,920
then it will insert a silencing rule

00:07:48,910 --> 00:07:54,130
which is just a generated relabeled

00:07:50,920 --> 00:07:55,450
config after some indeterminate number

00:07:54,130 --> 00:07:57,280
of steps where this has been remediated

00:07:55,450 --> 00:07:58,810
and the new code has been deployed and

00:07:57,280 --> 00:08:01,030
the explosion is no longer taking place

00:07:58,810 --> 00:08:01,690
there are some CLI commands that look at

00:08:01,030 --> 00:08:05,560
here in a moment

00:08:01,690 --> 00:08:06,250
so let's see how well we can do this can

00:08:05,560 --> 00:08:10,540
you still hear me

00:08:06,250 --> 00:08:16,060
good thank you alright so drop out of

00:08:10,540 --> 00:08:20,320
here there we go all right um make this

00:08:16,060 --> 00:08:23,200
bigger say when that mostly legible for

00:08:20,320 --> 00:08:24,730
everybody great excellent bear with me I

00:08:23,200 --> 00:08:26,320
use team hooks so I'll be splitting this

00:08:24,730 --> 00:08:28,600
into a couple of pains here in a moment

00:08:26,320 --> 00:08:31,570
but if you get lost tell me to stop and

00:08:28,600 --> 00:08:35,170
I'll rewind so I have running here is a

00:08:31,570 --> 00:08:37,360
mini cube instance so I have two pods

00:08:35,170 --> 00:08:39,100
one is called stat spitter that's the

00:08:37,360 --> 00:08:40,810
toy app that I mentioned it just when I

00:08:39,100 --> 00:08:42,580
ask it to will start to generate

00:08:40,810 --> 00:08:44,920
approximately a hundred new series per

00:08:42,580 --> 00:08:46,810
second and then we have Prometheus where

00:08:44,920 --> 00:08:48,970
its Prometheus proper there's the

00:08:46,810 --> 00:08:51,490
ubiquitous Jimmy Dyson config nap

00:08:48,970 --> 00:08:53,320
preload sidecar which is kind of vital

00:08:51,490 --> 00:08:56,100
to this whole process and then there's

00:08:53,320 --> 00:08:59,560
bomb squad running as the other sidecar

00:08:56,100 --> 00:09:02,830
excuse me so on that note let's go take

00:08:59,560 --> 00:09:04,420
a look at the Prometheus UI and we can

00:09:02,830 --> 00:09:06,610
kind of point out a couple of the thing

00:09:04,420 --> 00:09:09,160
that I mentioned earlier that seems

00:09:06,610 --> 00:09:10,899
fairly large so if we go look at the

00:09:09,160 --> 00:09:12,339
existing rules in this Prometheus these

00:09:10,899 --> 00:09:15,820
are the this is the recording rule that

00:09:12,339 --> 00:09:18,790
that bomb-squad bootstraps into the

00:09:15,820 --> 00:09:20,320
Prometheus instance it's not a terribly

00:09:18,790 --> 00:09:22,510
complicated one now this is actually

00:09:20,320 --> 00:09:25,180
using the label replace function that

00:09:22,510 --> 00:09:27,010
Julian mentioned in his Q&A and it

00:09:25,180 --> 00:09:30,519
builds up this card count metric that we

00:09:27,010 --> 00:09:32,170
use which has thankfully one label which

00:09:30,519 --> 00:09:34,180
is the original metric name that it was

00:09:32,170 --> 00:09:35,889
calculating and it just tracks what is

00:09:34,180 --> 00:09:38,589
the cardinality of any given metric in

00:09:35,889 --> 00:09:39,490
the system at that point in time if we

00:09:38,589 --> 00:09:41,290
want to look at a slightly more

00:09:39,490 --> 00:09:44,529
interesting example let's at least

00:09:41,290 --> 00:09:45,910
pertinent to the demo we can go look at

00:09:44,529 --> 00:09:47,709
it for our test metric that we're gonna

00:09:45,910 --> 00:09:49,329
be watching and destroying here in a

00:09:47,709 --> 00:09:51,880
moment so we just have this little test

00:09:49,329 --> 00:09:53,920
gauge from stat spitter and currently

00:09:51,880 --> 00:09:56,230
it's cardinality is 11 we can look at

00:09:53,920 --> 00:09:57,730
that over a little bit of time and see

00:09:56,230 --> 00:10:00,010
it's been nice and stable which is great

00:09:57,730 --> 00:10:01,470
I just have some really contrived label

00:10:00,010 --> 00:10:04,600
values that I've pushed in there for now

00:10:01,470 --> 00:10:07,480
what we can do is make this explode

00:10:04,600 --> 00:10:08,800
watch bomb-squad silence it remediate

00:10:07,480 --> 00:10:11,470
the problem and then watch it all come

00:10:08,800 --> 00:10:14,440
back after we've explicitly told it stop

00:10:11,470 --> 00:10:16,510
silencing this so we will go back over

00:10:14,440 --> 00:10:19,149
here if I can remember remember all my

00:10:16,510 --> 00:10:21,120
steps let's go ahead and open up the

00:10:19,149 --> 00:10:23,680
logs bomb-squad

00:10:21,120 --> 00:10:26,800
by the way if you are not using stern

00:10:23,680 --> 00:10:28,480
you're missing out it's great you'll

00:10:26,800 --> 00:10:30,279
never watch a single log from one

00:10:28,480 --> 00:10:31,779
container ever again it's good stuff

00:10:30,279 --> 00:10:34,360
so we've got the bomb squad logs on the

00:10:31,779 --> 00:10:37,570
left on the right let's go ahead and

00:10:34,360 --> 00:10:40,720
toggle this so that we will have stats

00:10:37,570 --> 00:10:43,510
bitter start to spit out new series the

00:10:40,720 --> 00:10:45,670
rate of approximately 100 per second we

00:10:43,510 --> 00:10:48,279
should see that take place pretty

00:10:45,670 --> 00:10:50,589
quickly yeah

00:10:48,279 --> 00:10:54,250
so as you can see this is only a hundred

00:10:50,589 --> 00:10:54,640
new series per second it's going fairly

00:10:54,250 --> 00:10:56,760
quickly

00:10:54,640 --> 00:11:00,160
we're gonna look at some data if we can

00:10:56,760 --> 00:11:02,949
in a moment let's take this guy and just

00:11:00,160 --> 00:11:05,949
look at it on its own before it gets too

00:11:02,949 --> 00:11:08,079
slow to query it because it will very

00:11:05,949 --> 00:11:09,310
quickly see how it's already turning

00:11:08,079 --> 00:11:12,339
when there's nothing else happening it's

00:11:09,310 --> 00:11:14,350
good times so if we look through here I

00:11:12,339 --> 00:11:18,250
meant to take some of these labels out

00:11:14,350 --> 00:11:20,950
but fortunately we have a we had

00:11:18,250 --> 00:11:22,960
they find function in the browser cool

00:11:20,950 --> 00:11:24,850
so this is the label that we're stuffing

00:11:22,960 --> 00:11:26,830
all of these unique values into you can

00:11:24,850 --> 00:11:28,600
see I had nice static values until at

00:11:26,830 --> 00:11:31,650
some point I started putting in boom

00:11:28,600 --> 00:11:34,090
underscore and a timestamp boom just

00:11:31,650 --> 00:11:37,090
seemed important for the bomb squad and

00:11:34,090 --> 00:11:39,760
Hames good times so these are bad very

00:11:37,090 --> 00:11:41,470
very very bad don't ever do this but

00:11:39,760 --> 00:11:44,410
when you do do this hopefully we can

00:11:41,470 --> 00:11:46,120
help so as you can see that's starting

00:11:44,410 --> 00:11:50,710
to blow up fairly quickly we're actually

00:11:46,120 --> 00:11:52,180
going to just go back to this page

00:11:50,710 --> 00:11:53,830
because all the data that's on that

00:11:52,180 --> 00:11:56,110
screen is not rendering terribly fun

00:11:53,830 --> 00:11:59,560
well let's go back to our cardinality

00:11:56,110 --> 00:12:02,830
watch and you see we're at a pretty high

00:11:59,560 --> 00:12:04,960
value already if we shrink this back

00:12:02,830 --> 00:12:06,970
down to a more useable time frame you

00:12:04,960 --> 00:12:10,420
see how quickly this is going if we jump

00:12:06,970 --> 00:12:11,890
back over here however we see wow that

00:12:10,420 --> 00:12:14,320
was touchy here we go

00:12:11,890 --> 00:12:16,480
we can actually see that bomb squad has

00:12:14,320 --> 00:12:17,590
already detected that and if I had

00:12:16,480 --> 00:12:18,820
thought about it I would have switched

00:12:17,590 --> 00:12:20,230
back sooner and you would have seen that

00:12:18,820 --> 00:12:22,690
it only took a few seconds to find it

00:12:20,230 --> 00:12:24,790
because I have my eval intervals turned

00:12:22,690 --> 00:12:25,990
down really low but basically it finds

00:12:24,790 --> 00:12:28,270
it and says hey there's a label

00:12:25,990 --> 00:12:30,160
exploding it's called high card and it's

00:12:28,270 --> 00:12:32,830
exploding on this metric name currently

00:12:30,160 --> 00:12:34,630
I didn't find the silencing rules that I

00:12:32,830 --> 00:12:36,820
expected to find in order for this to

00:12:34,630 --> 00:12:38,680
not happen which kind of makes sense so

00:12:36,820 --> 00:12:40,870
I inserted it into every scrape config

00:12:38,680 --> 00:12:42,010
because I can't guarantee that

00:12:40,870 --> 00:12:44,200
necessarily

00:12:42,010 --> 00:12:45,339
I can't necessarily guarantee yet that

00:12:44,200 --> 00:12:46,690
you don't have multiple scrape jobs

00:12:45,339 --> 00:12:48,700
grabbing the same metrics from the same

00:12:46,690 --> 00:12:51,310
target it would be silly to do that but

00:12:48,700 --> 00:12:53,140
it does happen so we just go across the

00:12:51,310 --> 00:12:55,410
blower at the board we still detect it

00:12:53,140 --> 00:12:57,910
for a while because we have to wait for

00:12:55,410 --> 00:12:59,380
and eventually consistent mechanism of

00:12:57,910 --> 00:13:01,780
kubernetes which is the config map

00:12:59,380 --> 00:13:03,730
reload inside the pod to take place but

00:13:01,780 --> 00:13:05,560
then that way they can fig map reload

00:13:03,730 --> 00:13:08,110
container will hot reload the Prometheus

00:13:05,560 --> 00:13:11,280
config to make our silence and rules

00:13:08,110 --> 00:13:13,980
take effect so if we go back over here

00:13:11,280 --> 00:13:16,210
and we look at this again you can see

00:13:13,980 --> 00:13:18,910
sure enough we actually cranked our

00:13:16,210 --> 00:13:20,260
cardinality way down interestingly we

00:13:18,910 --> 00:13:22,270
dropped below what the original

00:13:20,260 --> 00:13:25,030
cardinality was and we'll see why here

00:13:22,270 --> 00:13:26,770
in just a moment but you can see how

00:13:25,030 --> 00:13:28,540
quickly this escalated right in this

00:13:26,770 --> 00:13:30,459
course of less than a minute and a half

00:13:28,540 --> 00:13:32,290
or so right around that we got up to

00:13:30,459 --> 00:13:33,910
00:13:32,290 --> 00:13:35,379
distinct series underneath this metric

00:13:33,910 --> 00:13:36,550
almost all of which are useless and

00:13:35,379 --> 00:13:38,459
we're probably starting to put

00:13:36,550 --> 00:13:40,779
everything under considerable strain

00:13:38,459 --> 00:13:42,850
what we've done is we've taken a high

00:13:40,779 --> 00:13:46,379
card label and the values for all of

00:13:42,850 --> 00:13:49,930
those now are simply the words the word

00:13:46,379 --> 00:13:53,199
make sure I don't mess it up this is

00:13:49,930 --> 00:13:54,639
gonna hurt anyways we basically replaced

00:13:53,199 --> 00:13:56,980
all of those time stamps that are still

00:13:54,639 --> 00:13:59,769
being emitted from step spitter with the

00:13:56,980 --> 00:14:01,720
string I believe be s underscore silence

00:13:59,769 --> 00:14:05,079
by the way bomb squad shrinks down to a

00:14:01,720 --> 00:14:09,279
really good name it's great um yeah so

00:14:05,079 --> 00:14:10,839
if we come down here like it is yeah so

00:14:09,279 --> 00:14:12,189
here's the static label that we've got

00:14:10,839 --> 00:14:13,600
and so this is where we're getting we

00:14:12,189 --> 00:14:15,310
have some other label that's distinct

00:14:13,600 --> 00:14:16,779
between these two but the one that's

00:14:15,310 --> 00:14:18,420
exploding we've replaced with a static

00:14:16,779 --> 00:14:20,709
string you can call it whatever you like

00:14:18,420 --> 00:14:23,470
but that cranks the cardinality down

00:14:20,709 --> 00:14:25,990
needless to say this is going to really

00:14:23,470 --> 00:14:27,759
really really mess up any dashboards

00:14:25,990 --> 00:14:29,189
that are monitoring or watching this

00:14:27,759 --> 00:14:31,689
metric if you're displaying it

00:14:29,189 --> 00:14:34,209
particularly by way of using any of the

00:14:31,689 --> 00:14:35,589
exploding labels things are going to

00:14:34,209 --> 00:14:37,000
look really bad and this is a super

00:14:35,589 --> 00:14:39,370
subtle way to have to go find out that

00:14:37,000 --> 00:14:40,660
it got silenced so instead of making you

00:14:39,370 --> 00:14:41,589
just dig around and hope that you can

00:14:40,660 --> 00:14:43,600
figure out what happened

00:14:41,589 --> 00:14:46,720
bomb squad does expose a label or a

00:14:43,600 --> 00:14:48,459
metric to indicate what label is

00:14:46,720 --> 00:14:50,019
exploding and what the distinct values

00:14:48,459 --> 00:14:52,180
have been at any given time so ideally

00:14:50,019 --> 00:14:53,410
you would probably alert on this to say

00:14:52,180 --> 00:14:55,269
hey don't do that anymore

00:14:53,410 --> 00:14:56,889
or in our case we would at least have

00:14:55,269 --> 00:14:58,120
this emitted back to us so that when

00:14:56,889 --> 00:15:01,449
this happens and we see somebody's

00:14:58,120 --> 00:15:02,670
agents die off we can find out why and

00:15:01,449 --> 00:15:05,439
who it was

00:15:02,670 --> 00:15:07,389
so all it does is track label and

00:15:05,439 --> 00:15:08,860
metrics that are exploding tells you

00:15:07,389 --> 00:15:10,990
what the cardinality count was at the

00:15:08,860 --> 00:15:12,819
time that it detected it and before the

00:15:10,990 --> 00:15:14,649
silence actually took effect so these

00:15:12,819 --> 00:15:17,230
should generally line up pretty closely

00:15:14,649 --> 00:15:19,149
with that card count metric so we've

00:15:17,230 --> 00:15:21,309
done this we've talked to the developers

00:15:19,149 --> 00:15:23,079
and said hey you did a thing I'm sure

00:15:21,309 --> 00:15:27,069
you meant well but you broke all of the

00:15:23,079 --> 00:15:28,750
things it's probably me I need you to go

00:15:27,069 --> 00:15:30,850
fix this and redeploy the redeploy is

00:15:28,750 --> 00:15:32,319
crucial because otherwise the old

00:15:30,850 --> 00:15:34,569
instance is if you just toggle it the

00:15:32,319 --> 00:15:37,059
way that I did to turn it on then and

00:15:34,569 --> 00:15:38,350
then unsilenced the metric then you're

00:15:37,059 --> 00:15:41,019
going to still have all of the old

00:15:38,350 --> 00:15:42,610
exploding series in the existing

00:15:41,019 --> 00:15:44,380
registry from the current and from the

00:15:42,610 --> 00:15:45,610
old running instances they'll get Rhys

00:15:44,380 --> 00:15:47,019
creped immediately bombs go

00:15:45,610 --> 00:15:49,329
we'll see it the silence everything

00:15:47,019 --> 00:15:50,500
you'll say bad things about me and open

00:15:49,329 --> 00:15:52,060
all kinds of github issues which is

00:15:50,500 --> 00:15:53,500
probably not what we want so the

00:15:52,060 --> 00:15:55,029
redeploy is really crucial it has to

00:15:53,500 --> 00:15:56,740
stop and start again we have to clear

00:15:55,029 --> 00:16:00,399
out that registry so we'll simulate that

00:15:56,740 --> 00:16:02,529
by pardon my shortenings of words I

00:16:00,399 --> 00:16:04,450
don't type well so we will go ahead and

00:16:02,529 --> 00:16:07,000
just delete our status bitter pod which

00:16:04,450 --> 00:16:09,519
will simulate a nice little redeploy

00:16:07,000 --> 00:16:12,250
it'll start back up defaulting to not an

00:16:09,519 --> 00:16:13,870
exploding state we'll come back over

00:16:12,250 --> 00:16:15,490
here and say cool that's great but now

00:16:13,870 --> 00:16:17,230
how in the world do I go through and fix

00:16:15,490 --> 00:16:19,360
this so that I can get my labels back

00:16:17,230 --> 00:16:21,899
and my dashboards aren't awful anymore

00:16:19,360 --> 00:16:26,320
so in that case we've added a nice

00:16:21,899 --> 00:16:29,350
fairly handy a hack it worked a little

00:16:26,320 --> 00:16:32,160
CLI that you can use so you can run bs

00:16:29,350 --> 00:16:34,420
list and you'll get a very graphite ish

00:16:32,160 --> 00:16:35,829
representation of what metric and label

00:16:34,420 --> 00:16:39,370
name are exploding and have been

00:16:35,829 --> 00:16:40,899
subsequently silenced so the format's

00:16:39,370 --> 00:16:43,390
funky but it makes for a nice copy-paste

00:16:40,899 --> 00:16:45,100
so there it is once you've confirmed

00:16:43,390 --> 00:16:46,570
that everything should be better and

00:16:45,100 --> 00:16:49,600
you're confident that it won't explode

00:16:46,570 --> 00:16:53,380
again you can save jumped the gun

00:16:49,600 --> 00:16:55,690
alright you say BS on silence paste in

00:16:53,380 --> 00:16:57,790
that value it will delete delete the

00:16:55,690 --> 00:17:00,310
silencing rules from every script config

00:16:57,790 --> 00:17:01,990
that it found we can look over here in

00:17:00,310 --> 00:17:03,699
the bomb squad logs on the left again

00:17:01,990 --> 00:17:06,130
and see that it's resetting the metrics

00:17:03,699 --> 00:17:09,370
for itself let's go back to Prometheus

00:17:06,130 --> 00:17:11,260
and just look at that momentarily get

00:17:09,370 --> 00:17:12,360
this down again so you can see we went

00:17:11,260 --> 00:17:14,260
ahead and just reset it back to zero

00:17:12,360 --> 00:17:15,520
ideally to give you the benefit of the

00:17:14,260 --> 00:17:17,620
doubt that you're devs really did fix

00:17:15,520 --> 00:17:20,589
the problem and that your alerts can

00:17:17,620 --> 00:17:22,720
hopefully resolve seems like a good plan

00:17:20,589 --> 00:17:25,270
at least so there's that we should also

00:17:22,720 --> 00:17:28,960
then be able to see our cardinality come

00:17:25,270 --> 00:17:31,270
back to a nice stable value of 11 which

00:17:28,960 --> 00:17:32,770
we do so there's that you can see it

00:17:31,270 --> 00:17:34,780
went from two eventually dropped to one

00:17:32,770 --> 00:17:36,790
I haven't figured that part out yet but

00:17:34,780 --> 00:17:39,429
and we went back to our 11 stable values

00:17:36,790 --> 00:17:41,650
so we've effectively kept the original

00:17:39,429 --> 00:17:43,419
Prometheus instance from just dying in a

00:17:41,650 --> 00:17:45,520
fire and we've been able to

00:17:43,419 --> 00:17:47,830
automatically suppress that particularly

00:17:45,520 --> 00:17:49,419
bad behavior and still give you some

00:17:47,830 --> 00:17:51,460
kind of visibility into exactly what

00:17:49,419 --> 00:17:53,380
happened with what metric you can add

00:17:51,460 --> 00:17:54,490
whatever labels you want to this there's

00:17:53,380 --> 00:17:55,929
probably some things that need to be

00:17:54,490 --> 00:17:58,240
white listed I assume we haven't quite

00:17:55,929 --> 00:17:59,280
implemented that yet but there's a lot

00:17:58,240 --> 00:18:03,300
of potential here I think

00:17:59,280 --> 00:18:03,690
so let's see I think that's all there

00:18:03,300 --> 00:18:06,360
was

00:18:03,690 --> 00:18:08,100
so on that note that's bomb squad and

00:18:06,360 --> 00:18:10,890
that's me hopefully I didn't rush that

00:18:08,100 --> 00:18:12,530
too terribly bad this is open source by

00:18:10,890 --> 00:18:15,210
the way you can go here and go grab it

00:18:12,530 --> 00:18:15,540
it's a bit proof of concept to you right

00:18:15,210 --> 00:18:38,190
now

00:18:15,540 --> 00:18:40,530
so PRS are welcome but so questions yeah

00:18:38,190 --> 00:18:42,360
so great hook and actually thank you for

00:18:40,530 --> 00:18:46,260
that because probably you saved all the

00:18:42,360 --> 00:18:48,570
last and that I think us for from

00:18:46,260 --> 00:18:50,730
writing the same thing because I think

00:18:48,570 --> 00:18:53,640
there's a must-have and speaking of that

00:18:50,730 --> 00:18:55,350
when we can see all of this inside from

00:18:53,640 --> 00:18:57,300
adduce without an inside card that's my

00:18:55,350 --> 00:19:00,240
question that is a fabulous question for

00:18:57,300 --> 00:19:01,530
the maintainer 'he's no absolutely so no

00:19:00,240 --> 00:19:03,180
we're actually pretty excited because we

00:19:01,530 --> 00:19:04,410
know we we do try to contribute back to

00:19:03,180 --> 00:19:06,270
prometheus whenever we can this is

00:19:04,410 --> 00:19:08,100
something that seems pretty cool and

00:19:06,270 --> 00:19:10,200
yeah if we could actually potentially

00:19:08,100 --> 00:19:12,720
insert somewhere in this great path this

00:19:10,200 --> 00:19:14,850
may become more efficient or at least a

00:19:12,720 --> 00:19:19,200
quicker trigger so yeah it would be a

00:19:14,850 --> 00:19:23,760
neat project to take on for sure any

00:19:19,200 --> 00:19:26,850
other questions sorry oh thank you for

00:19:23,760 --> 00:19:28,710
the talk so what if the Guardian at the

00:19:26,850 --> 00:19:31,170
high cardinality comes from the

00:19:28,710 --> 00:19:33,630
different label names not from the paper

00:19:31,170 --> 00:19:36,480
values is it an issue and it can bomb

00:19:33,630 --> 00:19:37,980
squad deal with this that is a fantastic

00:19:36,480 --> 00:19:40,290
question that is not something we've

00:19:37,980 --> 00:19:42,810
tackled quite yet it is definitely a

00:19:40,290 --> 00:19:45,480
possible scenario we've not seen it in

00:19:42,810 --> 00:19:47,340
production yet but it is one that we we

00:19:45,480 --> 00:19:50,070
thought about just haven't quite figured

00:19:47,340 --> 00:19:51,870
out an appropriate approach yet for that

00:19:50,070 --> 00:19:53,910
I don't think we can access label names

00:19:51,870 --> 00:19:56,130
the same way that we do act label values

00:19:53,910 --> 00:19:58,710
in the real Abel config so we may have

00:19:56,130 --> 00:19:59,760
to get creative with that yeah different

00:19:58,710 --> 00:20:01,650
metric names is actually one that

00:19:59,760 --> 00:20:03,540
happens every now and then yeah true

00:20:01,650 --> 00:20:05,070
unfortunately it's just a label but then

00:20:03,540 --> 00:20:10,170
it kind of does break this semantic

00:20:05,070 --> 00:20:12,600
picture that I just drew so yeah hi

00:20:10,170 --> 00:20:15,539
thank you for the talk

00:20:12,600 --> 00:20:17,130
seems really useful but what about for

00:20:15,539 --> 00:20:18,870
those of us who maybe aren't using

00:20:17,130 --> 00:20:21,120
kubernetes just yet

00:20:18,870 --> 00:20:24,480
maybe nomads or in the case of

00:20:21,120 --> 00:20:26,759
monitoring static deploy sure so the

00:20:24,480 --> 00:20:29,039
current implementation is very much only

00:20:26,759 --> 00:20:30,389
in kubernetes just because that's our

00:20:29,039 --> 00:20:31,380
stack and we needed to make it work for

00:20:30,389 --> 00:20:33,720
us

00:20:31,380 --> 00:20:34,740
that said in conceptually there's no

00:20:33,720 --> 00:20:36,299
reason it can't work on other

00:20:34,740 --> 00:20:39,000
orchestration systems or potentially

00:20:36,299 --> 00:20:40,980
even in a bare metal environment you

00:20:39,000 --> 00:20:43,500
would just need to figure out the auto

00:20:40,980 --> 00:20:45,779
reload of the config in prometheus bit I

00:20:43,500 --> 00:20:47,549
think is the key thing so I'd love to

00:20:45,779 --> 00:20:48,779
see it expand out though and be much

00:20:47,549 --> 00:20:50,730
more broadly applicable than just

00:20:48,779 --> 00:21:01,470
kubernetes cuz it's definitely a

00:20:50,730 --> 00:21:03,929
situation hi thanks for the talk again

00:21:01,470 --> 00:21:05,700
it seems that you're modifying the

00:21:03,929 --> 00:21:08,549
configuration of Prometheus on the fly

00:21:05,700 --> 00:21:10,200
right yes cool do you have any issues

00:21:08,549 --> 00:21:12,149
when you've done this that people might

00:21:10,200 --> 00:21:14,879
deploy Prometheus or like make

00:21:12,149 --> 00:21:17,220
modifications to the rules yeah so I

00:21:14,879 --> 00:21:18,870
believe we did manage to implement

00:21:17,220 --> 00:21:20,940
before this talk I'll have to go back

00:21:18,870 --> 00:21:23,070
and check but that's so bomb-squad

00:21:20,940 --> 00:21:25,110
actually keeps its own config as well

00:21:23,070 --> 00:21:26,190
oh so sad you can't see my pretty slide

00:21:25,110 --> 00:21:28,230
anyways

00:21:26,190 --> 00:21:29,789
bomb squad has its own key in the same

00:21:28,230 --> 00:21:31,909
config map as you Prometheus config

00:21:29,789 --> 00:21:35,879
where it tracks those metrics able name

00:21:31,909 --> 00:21:38,970
bits and it actually has a base64

00:21:35,879 --> 00:21:41,340
encoded copy of the rule itself so that

00:21:38,970 --> 00:21:42,779
can actually rescan the config on

00:21:41,340 --> 00:21:45,210
startup and say I know I should be

00:21:42,779 --> 00:21:47,490
silencing this am i and then it will

00:21:45,210 --> 00:21:56,970
reinsert the rule if it needs to perfect

00:21:47,490 --> 00:21:59,730
thanks thanks for the talk

00:21:56,970 --> 00:22:02,190
can you catch matrix that disappear like

00:21:59,730 --> 00:22:06,840
if on which scrape I get a new set of

00:22:02,190 --> 00:22:08,519
labels and dude one is gone like I get

00:22:06,840 --> 00:22:12,179
the current timestamp in my label for

00:22:08,519 --> 00:22:14,669
example yeah that so is this similar to

00:22:12,179 --> 00:22:17,970
the unique high cardinality label names

00:22:14,669 --> 00:22:20,370
question oh yeah about if you have in

00:22:17,970 --> 00:22:22,470
the label the current date time of the

00:22:20,370 --> 00:22:24,149
screen for example so that the next time

00:22:22,470 --> 00:22:26,190
you scrape it then you have a new set of

00:22:24,149 --> 00:22:28,530
labels a new set of metrics but

00:22:26,190 --> 00:22:29,220
the number of is the same but just like

00:22:28,530 --> 00:22:31,380
Oh

00:22:29,220 --> 00:22:33,120
different I see so yeah the cardinality

00:22:31,380 --> 00:22:35,310
has shifted dramatically but it hasn't

00:22:33,120 --> 00:22:37,470
necessarily continued to climb it stays

00:22:35,310 --> 00:22:38,730
static on the the card count yeah I

00:22:37,470 --> 00:22:39,930
haven't haven't accounted for that one

00:22:38,730 --> 00:22:42,510
yet that is an interesting case though

00:22:39,930 --> 00:22:46,550
that would be very painful I think to

00:22:42,510 --> 00:22:46,550
diagnose without something automated so

00:22:47,840 --> 00:22:56,730
who else any more questions so you want

00:22:53,550 --> 00:22:58,530
a little to go to lunch early yeah thank

00:22:56,730 --> 00:23:05,549
you so much

00:22:58,530 --> 00:23:05,549

YouTube URL: https://www.youtube.com/watch?v=MHiXP5QW4_U


