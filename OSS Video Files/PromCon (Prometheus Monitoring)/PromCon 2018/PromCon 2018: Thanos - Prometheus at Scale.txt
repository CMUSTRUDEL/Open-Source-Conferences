Title: PromCon 2018: Thanos - Prometheus at Scale
Publication date: 2018-11-10
Playlist: PromCon 2018
Description: 
	Speakers: Bartłomiej Płotka, Fabian Reinartz

The Prometheus Monitoring system has been thriving for several years. Along with its powerful data model, operational simplicity and reliability have been a key factor in its success. However, some questions were still largely unaddressed to this day. How can we store historical data at the order of petabytes in a reliable and cost-efficient way? Can we do so without sacrificing responsive query times? And what about a global view of all our metrics and transparent handling of HA setups?

Thanos takes Prometheus' strong foundations and extends it into a clustered, yet coordination free, globally scalable metric system. It retains Prometheus's simple operational model and even simplifies deployments further. Under the hood, Thanos uses highly cost-efficient object storage that's available in virtually all environments today. By building directly on top of the storage format introduced with Prometheus 2.0, Thanos achieves near real-time responsiveness even for cold queries against historical data. All while having virtually no cost overhead beyond that of the underlying object storage.

We will show the theoretical concepts behind Thanos and demonstrate how it seamlessly integrates into existing Prometheus setups.
Captions: 
	00:00:10,740 --> 00:00:21,029
[Applause]

00:00:15,170 --> 00:00:22,950
it's mine okay hello

00:00:21,029 --> 00:00:27,330
I think everyone I guess everyone is

00:00:22,950 --> 00:00:30,150
ready for some after lunch nap right but

00:00:27,330 --> 00:00:32,189
hopefully not so hello everyone I'm my

00:00:30,150 --> 00:00:35,010
name is Bartek and I work for the

00:00:32,189 --> 00:00:37,890
company called improbable and this is

00:00:35,010 --> 00:00:40,860
Fabian who worked with us for a short

00:00:37,890 --> 00:00:43,050
time between his role at the septic

00:00:40,860 --> 00:00:45,390
licked and caress and in the current

00:00:43,050 --> 00:00:48,539
role at Google and we are really really

00:00:45,390 --> 00:00:50,760
excited to introduce you to the tano's

00:00:48,539 --> 00:00:54,629
our open source project that we started

00:00:50,760 --> 00:00:56,850
in the by the end of the last year we

00:00:54,629 --> 00:00:59,070
address this talk to those who are not

00:00:56,850 --> 00:01:01,350
yet familiar with the tunnels but also

00:00:59,070 --> 00:01:03,329
for those that maybe have seen our blog

00:01:01,350 --> 00:01:05,759
post improbable blog post about tunnels

00:01:03,329 --> 00:01:08,130
maybe experimented a little bit with

00:01:05,759 --> 00:01:10,649
tunnels already or even deployed that on

00:01:08,130 --> 00:01:12,720
their environments so by the end of this

00:01:10,649 --> 00:01:17,009
talk I would like you to know what a

00:01:12,720 --> 00:01:19,140
nose is what problem it solves and how

00:01:17,009 --> 00:01:21,780
to use it and how to deploy it so next

00:01:19,140 --> 00:01:25,079
20 minutes we look as false first we

00:01:21,780 --> 00:01:26,970
will talk what problems are you it's

00:01:25,079 --> 00:01:29,970
worth to consider when running from to

00:01:26,970 --> 00:01:33,390
use at scale secondly we will cover how

00:01:29,970 --> 00:01:36,619
proton O's helps in those areas and by

00:01:33,390 --> 00:01:40,079
the end we will talk shortly about

00:01:36,619 --> 00:01:43,409
explained example deployment models so I

00:01:40,079 --> 00:01:45,630
need a clicker yep so everything starts

00:01:43,409 --> 00:01:48,720
with the single prompted server and I

00:01:45,630 --> 00:01:51,329
think it is safe to say that single prom

00:01:48,720 --> 00:01:54,030
to use server is extremely powerful it

00:01:51,329 --> 00:01:56,399
has variable it was it has proven to be

00:01:54,030 --> 00:01:59,880
reliable it has flexible query language

00:01:56,399 --> 00:02:02,520
and it is capable to scrape 5 million

00:01:59,880 --> 00:02:04,140
series time series at the same time we

00:02:02,520 --> 00:02:06,869
were out using too many resources so

00:02:04,140 --> 00:02:09,720
it's great and the primitives to is even

00:02:06,869 --> 00:02:11,610
more improved thanks of the hard work of

00:02:09,720 --> 00:02:15,329
the Proteus community for example

00:02:11,610 --> 00:02:19,199
retention can be a lot of longer even

00:02:15,329 --> 00:02:23,280
with just a local storage but how all of

00:02:19,199 --> 00:02:25,050
those things are features

00:02:23,280 --> 00:02:28,560
together when we are talking about scale

00:02:25,050 --> 00:02:30,240
and in fact when we talk when we are

00:02:28,560 --> 00:02:32,250
talking about the scale that is out of

00:02:30,240 --> 00:02:34,170
the scope for the single prompt used

00:02:32,250 --> 00:02:37,530
server we are not necessarily talking

00:02:34,170 --> 00:02:39,600
about the performance limits usually the

00:02:37,530 --> 00:02:42,900
usual reason why people scale out from

00:02:39,600 --> 00:02:43,800
to use servers are because the service

00:02:42,900 --> 00:02:46,230
that you want to monitor

00:02:43,800 --> 00:02:49,140
so monitor that monitoring targets are

00:02:46,230 --> 00:02:52,260
grouped within multiple of isolated

00:02:49,140 --> 00:02:54,739
clusters our data centers that are

00:02:52,260 --> 00:02:59,370
spread across the world within different

00:02:54,739 --> 00:03:01,530
geographical regions and if the

00:02:59,370 --> 00:03:03,810
recommended way to run Prometheus is

00:03:01,530 --> 00:03:06,690
within the same failure domain so the

00:03:03,810 --> 00:03:09,390
same network the same region we have no

00:03:06,690 --> 00:03:12,260
other choice than actually running one

00:03:09,390 --> 00:03:16,470
on more primitive server in each cluster

00:03:12,260 --> 00:03:19,319
so let's maybe focus on on the issues we

00:03:16,470 --> 00:03:21,630
can spot while using this approach so

00:03:19,319 --> 00:03:23,459
first first issue is quite obvious so

00:03:21,630 --> 00:03:25,860
how to use this all of this data from a

00:03:23,459 --> 00:03:28,620
single place usually you have a separate

00:03:25,860 --> 00:03:31,380
cluster that holds your dashboard engine

00:03:28,620 --> 00:03:33,660
your audit managers and you basically

00:03:31,380 --> 00:03:36,930
connect this cluster to all of the

00:03:33,660 --> 00:03:40,650
scrapers so that usually works but it's

00:03:36,930 --> 00:03:42,600
kind of limited because you when you

00:03:40,650 --> 00:03:45,570
want to do a query that aggregates the

00:03:42,600 --> 00:03:48,090
data from the various cluster well you

00:03:45,570 --> 00:03:49,620
cannot easily do that right now it's

00:03:48,090 --> 00:03:52,530
also the query that aggregates you know

00:03:49,620 --> 00:03:55,440
all environments on all clusters and one

00:03:52,530 --> 00:04:00,209
could say that well it is relatively

00:03:55,440 --> 00:04:01,380
easy to solve using Prommy to use keyaki

00:04:00,209 --> 00:04:03,840
whole Federation right

00:04:01,380 --> 00:04:06,420
so you basically spin up another global

00:04:03,840 --> 00:04:10,170
point use that scrapes all the Leafs

00:04:06,420 --> 00:04:14,519
prompt uses and exposes enables that

00:04:10,170 --> 00:04:16,979
query but yeah it has its limits first

00:04:14,519 --> 00:04:18,900
of all there is a double scrape there so

00:04:16,979 --> 00:04:21,780
it's yet another place to care for

00:04:18,900 --> 00:04:25,200
missed scripts but more importantly

00:04:21,780 --> 00:04:27,750
usually you are not able to fit all of

00:04:25,200 --> 00:04:31,200
the data from the Leafs promises in your

00:04:27,750 --> 00:04:33,300
federated one that means that you need

00:04:31,200 --> 00:04:36,419
we are forced to use pre-prepared

00:04:33,300 --> 00:04:37,140
recording rules and actually expose only

00:04:36,419 --> 00:04:41,580
of those

00:04:37,140 --> 00:04:43,650
it's not easy and and and might be

00:04:41,580 --> 00:04:45,720
problematic in some cases and the whole

00:04:43,650 --> 00:04:50,820
problem we are talking about we can call

00:04:45,720 --> 00:04:53,460
a global view problem next one is how to

00:04:50,820 --> 00:04:57,930
maintain the high availability for your

00:04:53,460 --> 00:05:00,630
monitoring data 100% time it's not

00:04:57,930 --> 00:05:03,630
strictly related to scalability but it

00:05:00,630 --> 00:05:04,140
is essential for for example for us for

00:05:03,630 --> 00:05:05,970
improvable

00:05:04,140 --> 00:05:08,670
because we care about every minute every

00:05:05,970 --> 00:05:11,850
sample of the monitoring data so it's

00:05:08,670 --> 00:05:14,280
kind of essential and the usual way to

00:05:11,850 --> 00:05:16,680
do it is to basically a yet another

00:05:14,280 --> 00:05:18,720
replica of the promptitude server to

00:05:16,680 --> 00:05:20,760
your cluster in disaster so you have

00:05:18,720 --> 00:05:24,630
multiple replicas that's create the same

00:05:20,760 --> 00:05:27,330
targets and well you have an IJ but how

00:05:24,630 --> 00:05:29,880
to use this data again so we have yet

00:05:27,330 --> 00:05:33,480
another primitive server to support for

00:05:29,880 --> 00:05:36,270
global view problem with the exception

00:05:33,480 --> 00:05:39,200
that this data is special duplicated and

00:05:36,270 --> 00:05:40,860
there is no native handling of those on

00:05:39,200 --> 00:05:45,300
natively on gravano

00:05:40,860 --> 00:05:47,220
from ua the last problem I think is the

00:05:45,300 --> 00:05:52,530
major trouble for the typical primitives

00:05:47,220 --> 00:05:56,370
user so basically how to make available

00:05:52,530 --> 00:05:58,800
all the data that is super super old and

00:05:56,370 --> 00:06:00,570
we are talking about a month war for the

00:05:58,800 --> 00:06:05,390
data or or even a couple of years and

00:06:00,570 --> 00:06:08,880
this screenshot actually shows the

00:06:05,390 --> 00:06:11,580
screenshot from Tanis query so the

00:06:08,880 --> 00:06:13,830
memory consumption for five months so

00:06:11,580 --> 00:06:16,680
it's kind of perfect but it is not

00:06:13,830 --> 00:06:18,980
always like that without tunnels you are

00:06:16,680 --> 00:06:21,210
I mean usually pretty limited and

00:06:18,980 --> 00:06:24,990
sometimes in the worst cases too few

00:06:21,210 --> 00:06:26,730
days so how to enable a longer attention

00:06:24,990 --> 00:06:29,340
without tunnels

00:06:26,730 --> 00:06:32,760
well I think there are like two major

00:06:29,340 --> 00:06:36,990
options first you are basically deployed

00:06:32,760 --> 00:06:39,710
Prometheus with huge SSE disk or you can

00:06:36,990 --> 00:06:42,810
use remote write feature with some

00:06:39,710 --> 00:06:45,750
hosted or on-premise solution available

00:06:42,810 --> 00:06:48,960
on the market but both of these options

00:06:45,750 --> 00:06:50,580
are not perfect the the former one the

00:06:48,960 --> 00:06:52,349
huge disk

00:06:50,580 --> 00:06:54,360
basically has some cost and

00:06:52,349 --> 00:06:56,729
manageability issues and I'm talking

00:06:54,360 --> 00:06:59,789
about operations like resize like back

00:06:56,729 --> 00:07:03,900
up the latter one the remote right

00:06:59,789 --> 00:07:05,759
feature is it's not perfect as well

00:07:03,900 --> 00:07:08,550
because of the very delicate and

00:07:05,759 --> 00:07:11,129
complicated path for sending for pushing

00:07:08,550 --> 00:07:15,060
basically gigabytes of sample on the fly

00:07:11,129 --> 00:07:17,639
so it's not easy to implement and that's

00:07:15,060 --> 00:07:20,819
why we created tons basically that were

00:07:17,639 --> 00:07:23,699
the three reason we we had in our mind

00:07:20,819 --> 00:07:26,759
while creating that were quite stifled

00:07:23,699 --> 00:07:29,520
while starting that at the November 2017

00:07:26,759 --> 00:07:32,669
so let's take a closer look how Donald

00:07:29,520 --> 00:07:35,159
solves those and we start with the

00:07:32,669 --> 00:07:37,759
global view so the ability to see all

00:07:35,159 --> 00:07:40,470
the data from the very single place

00:07:37,759 --> 00:07:43,259
everything starts again with Prometheus

00:07:40,470 --> 00:07:45,030
vanilla primitive use that is running

00:07:43,259 --> 00:07:48,060
somewhere with some local storage and

00:07:45,030 --> 00:07:51,229
scraping some targets to enable tunnels

00:07:48,060 --> 00:07:54,900
you basically want to deploy another

00:07:51,229 --> 00:07:57,360
binary called tunnel sidecar next to the

00:07:54,900 --> 00:07:59,310
prompt to use binary so for example

00:07:57,360 --> 00:08:02,340
within the say pod if you are running on

00:07:59,310 --> 00:08:05,659
communities and by default this sidecar

00:08:02,340 --> 00:08:08,520
exposes a G RPC API called store API

00:08:05,659 --> 00:08:11,400
that allows to fetch a local data from

00:08:08,520 --> 00:08:14,849
prom to use and in fact the same store

00:08:11,400 --> 00:08:18,300
API is implemented the same interface is

00:08:14,849 --> 00:08:22,110
used on every tunnels component so it's

00:08:18,300 --> 00:08:23,880
the same everywhere and but in terms of

00:08:22,110 --> 00:08:26,639
like sidecar implementation it's

00:08:23,880 --> 00:08:29,279
relatively easy relatively simple it is

00:08:26,639 --> 00:08:34,760
a simple proxy that fetches compressed

00:08:29,279 --> 00:08:38,969
serious series matched by level measures

00:08:34,760 --> 00:08:40,979
and why this is helpful so basically we

00:08:38,969 --> 00:08:44,070
can now add yet another honest component

00:08:40,979 --> 00:08:46,170
called tunnel Square that understand the

00:08:44,070 --> 00:08:48,660
story API and is able to fetch the data

00:08:46,170 --> 00:08:50,970
from the local prom to use and actually

00:08:48,660 --> 00:08:53,910
well evaluate the query on the Tannis

00:08:50,970 --> 00:08:56,070
query level and what is really nice

00:08:53,910 --> 00:09:00,060
about it it exposed the same query API

00:08:56,070 --> 00:09:02,820
HTTP query API as parameters does so if

00:09:00,060 --> 00:09:04,170
you're using any external system that

00:09:02,820 --> 00:09:06,930
you integrate with the prom to use

00:09:04,170 --> 00:09:08,550
they move from the tunnels from the

00:09:06,930 --> 00:09:12,690
Proms use to Thanos it should be

00:09:08,550 --> 00:09:15,000
seamless and thanks to the fact that the

00:09:12,690 --> 00:09:16,740
tunnels query is a separate component to

00:09:15,000 --> 00:09:19,160
the tunnel side car we can actually

00:09:16,740 --> 00:09:21,449
point to multiple off-site cars and

00:09:19,160 --> 00:09:24,540
access the data from the very single

00:09:21,449 --> 00:09:28,670
place so it's pretty so basically that

00:09:24,540 --> 00:09:31,740
allows us to have unified global view

00:09:28,670 --> 00:09:35,240
but additionally to that choir also

00:09:31,740 --> 00:09:38,779
implements the duplication logic which

00:09:35,240 --> 00:09:41,399
allows you to point to the multiple

00:09:38,779 --> 00:09:43,889
replicas of Ramat youths from juices

00:09:41,399 --> 00:09:45,690
that scrapes the same target and you can

00:09:43,889 --> 00:09:47,760
distinguish those by a special label

00:09:45,690 --> 00:09:50,910
that you configure it could be replicas

00:09:47,760 --> 00:09:53,699
like in our case and you configure that

00:09:50,910 --> 00:09:57,000
label on the channel square side and a

00:09:53,699 --> 00:09:59,639
queer then is able to duplicate the data

00:09:57,000 --> 00:10:03,000
based on that label and it trims that

00:09:59,639 --> 00:10:05,490
special label from the expose results as

00:10:03,000 --> 00:10:08,610
an effect from the to let's say

00:10:05,490 --> 00:10:11,430
duplicated series we have one so the H a

00:10:08,610 --> 00:10:17,670
is totally transparent for users so it's

00:10:11,430 --> 00:10:20,100
go and yeah in fact Thomas sidecar and

00:10:17,670 --> 00:10:21,480
tunnels query are just enough these two

00:10:20,100 --> 00:10:24,089
components are just enough to fulfill

00:10:21,480 --> 00:10:27,240
two first goals so the global view and

00:10:24,089 --> 00:10:29,070
availability and in fact you can stop

00:10:27,240 --> 00:10:32,220
right now if that satisfies your

00:10:29,070 --> 00:10:34,440
requirements and you just want this all

00:10:32,220 --> 00:10:37,380
these two goals you can just grab it on

00:10:34,440 --> 00:10:41,370
square point to the multiple scrapers

00:10:37,380 --> 00:10:44,310
and maybe specify which prompt to use is

00:10:41,370 --> 00:10:46,199
in we load in which H a group and you

00:10:44,310 --> 00:10:48,899
can use square from there yeah you can

00:10:46,199 --> 00:10:50,760
use you can have a global view and

00:10:48,899 --> 00:10:53,610
actually access everything from the

00:10:50,760 --> 00:10:55,769
single place so now I'm going to pass

00:10:53,610 --> 00:10:58,680
the microphone to Fabian who will talk

00:10:55,769 --> 00:11:06,120
more about more advanced features and

00:10:58,680 --> 00:11:09,029
capabilities of dance all right so the

00:11:06,120 --> 00:11:10,829
third code we had was historical data so

00:11:09,029 --> 00:11:13,470
we would like to have basically infinite

00:11:10,829 --> 00:11:16,199
retention and before we look into that

00:11:13,470 --> 00:11:17,970
we look back at Prometheus as local he

00:11:16,199 --> 00:11:20,550
has to be storage pond

00:11:17,970 --> 00:11:22,230
and promises does it accumulates data in

00:11:20,550 --> 00:11:23,879
memory for about two hours and every two

00:11:22,230 --> 00:11:27,120
hours it dumps all current memory data

00:11:23,879 --> 00:11:29,939
onto disk in a single compact block in a

00:11:27,120 --> 00:11:32,009
very compressed form and instead of

00:11:29,939 --> 00:11:35,220
these blocks are a few larger files that

00:11:32,009 --> 00:11:36,300
contain compressed series data so these

00:11:35,220 --> 00:11:38,189
can be back to the fights about the N

00:11:36,300 --> 00:11:40,410
size about like two hundred three

00:11:38,189 --> 00:11:41,939
hundred megabytes I think and they

00:11:40,410 --> 00:11:44,279
appeared with one index file and the

00:11:41,939 --> 00:11:45,629
index file like she knows what we as

00:11:44,279 --> 00:11:47,550
which samples are where and how to find

00:11:45,629 --> 00:11:49,379
them so the index file is required to

00:11:47,550 --> 00:11:52,889
basically resolve a query and then fight

00:11:49,379 --> 00:11:55,680
to find the right data points and as we

00:11:52,889 --> 00:11:57,120
read northeast to our blocks we at some

00:11:55,680 --> 00:11:58,500
point compact and together which means

00:11:57,120 --> 00:12:00,420
we can improve the compression

00:11:58,500 --> 00:12:02,279
efficiency even more and also we

00:12:00,420 --> 00:12:04,410
basically don't have to do as many index

00:12:02,279 --> 00:12:07,500
lookups for a curvy that spans along a

00:12:04,410 --> 00:12:10,019
time range so that's how promises works

00:12:07,500 --> 00:12:12,389
locally today and actually gonna use

00:12:10,019 --> 00:12:14,459
this so the first step to actually doing

00:12:12,389 --> 00:12:15,839
a lock time so it is getting the data to

00:12:14,459 --> 00:12:18,420
a place where it can actually store them

00:12:15,839 --> 00:12:20,699
in a scalable way you don't have to

00:12:18,420 --> 00:12:23,069
worry about it anymore and we found that

00:12:20,699 --> 00:12:24,779
one option is quite convenient and that

00:12:23,069 --> 00:12:27,300
is object storage because it's available

00:12:24,779 --> 00:12:29,309
everywhere and no matter which cloud

00:12:27,300 --> 00:12:30,899
you're in or which sort of reasonably

00:12:29,309 --> 00:12:32,970
sized on-prem installation you're in

00:12:30,899 --> 00:12:34,920
there's always going to be some kind of

00:12:32,970 --> 00:12:37,819
object storage and they're usually

00:12:34,920 --> 00:12:41,399
really really cheap and really durable

00:12:37,819 --> 00:12:42,959
I'm just kind of slow so what we now

00:12:41,399 --> 00:12:44,579
make this like I do is we watch the data

00:12:42,959 --> 00:12:45,899
directory of Prometheus and whenever we

00:12:44,579 --> 00:12:49,290
see a new block appear which is uploaded

00:12:45,899 --> 00:12:51,809
to our object so each bucket and that's

00:12:49,290 --> 00:12:53,250
pretty cheap and pretty simple and the

00:12:51,809 --> 00:12:54,930
good thing is that Prometheus basically

00:12:53,250 --> 00:12:56,399
keep these blocks around for longer so

00:12:54,930 --> 00:12:57,930
even if you had have some intermittent

00:12:56,399 --> 00:13:00,420
failure between our object storage and

00:12:57,930 --> 00:13:01,829
the Prometheus server and we are have no

00:13:00,420 --> 00:13:03,360
connectivity for like five hours after

00:13:01,829 --> 00:13:05,399
these five hours we just come back and

00:13:03,360 --> 00:13:07,259
we can just upload these blocks and

00:13:05,399 --> 00:13:09,110
since they are very well compressed it's

00:13:07,259 --> 00:13:13,740
not gonna sort of consume all our

00:13:09,110 --> 00:13:15,540
network link we only have to make two

00:13:13,740 --> 00:13:17,250
really small changes we have to

00:13:15,540 --> 00:13:18,839
basically disable the local compaction

00:13:17,250 --> 00:13:20,939
in Prometheus so that the sidecar

00:13:18,839 --> 00:13:22,230
uploading blocks and Prometheus trying

00:13:20,939 --> 00:13:24,149
to compact them don't interfere with

00:13:22,230 --> 00:13:25,470
each other and in return we just make

00:13:24,149 --> 00:13:26,970
the retention much shorter because our

00:13:25,470 --> 00:13:28,800
long term data now it's going to be in

00:13:26,970 --> 00:13:31,079
the object storage and we just don't

00:13:28,800 --> 00:13:31,770
need more than 12 to 48 hours and

00:13:31,079 --> 00:13:33,900
premies yes

00:13:31,770 --> 00:13:35,310
except for this set of safety margin so

00:13:33,900 --> 00:13:41,580
we don't lose data if there's some

00:13:35,310 --> 00:13:43,800
temporary paid on that's fairly easy but

00:13:41,580 --> 00:13:46,410
now with the create and as I said object

00:13:43,800 --> 00:13:47,940
so it is kind of slow especially because

00:13:46,410 --> 00:13:49,620
these files in our blocks are pretty

00:13:47,940 --> 00:13:52,380
large and one option would be okay

00:13:49,620 --> 00:13:54,240
whenever the crow yeah get screwy we

00:13:52,380 --> 00:13:56,760
just download our blocks that kind of

00:13:54,240 --> 00:13:58,830
match what we want to use which would

00:13:56,760 --> 00:14:02,670
probably take a few minutes or even

00:13:58,830 --> 00:14:04,830
hours for service for bigquery so we

00:14:02,670 --> 00:14:06,720
come to that um instead we have a store

00:14:04,830 --> 00:14:08,430
gateway component and which basically

00:14:06,720 --> 00:14:10,200
what is the object storage and whatever

00:14:08,430 --> 00:14:12,500
new block appears it sort of gets a

00:14:10,200 --> 00:14:14,790
really most basic metadata out of it and

00:14:12,500 --> 00:14:16,860
only it's the smallest entry point into

00:14:14,790 --> 00:14:19,830
the index so that is roughly knows where

00:14:16,860 --> 00:14:22,830
to start resolving the index in the

00:14:19,830 --> 00:14:24,720
query in the files and what we then do

00:14:22,830 --> 00:14:26,850
is whenever really query we look up in

00:14:24,720 --> 00:14:29,580
our cache which blocks what we

00:14:26,850 --> 00:14:31,200
applicable and then we do range queries

00:14:29,580 --> 00:14:32,790
you can see objects so much so we don't

00:14:31,200 --> 00:14:34,560
download into our files we just download

00:14:32,790 --> 00:14:36,390
where instead of like a few kilobytes to

00:14:34,560 --> 00:14:39,060
a few megabytes with only the data we

00:14:36,390 --> 00:14:39,360
need and this is actually really really

00:14:39,060 --> 00:14:42,000
fast

00:14:39,360 --> 00:14:43,680
surprisingly so we developed this Ensco

00:14:42,000 --> 00:14:44,970
big cloud storage initially and i think

00:14:43,680 --> 00:14:46,890
the average latency for these three

00:14:44,970 --> 00:14:49,020
quizzes like 20 milliseconds and that's

00:14:46,890 --> 00:14:51,060
for practical purposes for monitoring

00:14:49,020 --> 00:14:53,070
data almost for your time so it's in

00:14:51,060 --> 00:14:54,930
some cases really have to notice whether

00:14:53,070 --> 00:14:59,340
you're hitting Prometheus directly are

00:14:54,930 --> 00:15:01,770
actually getting object storage yeah

00:14:59,340 --> 00:15:04,770
that's basically it

00:15:01,770 --> 00:15:06,240
we basically get infinite retention

00:15:04,770 --> 00:15:08,760
right object so it is insanely cheap

00:15:06,240 --> 00:15:10,020
even if you cumulate like dozens of

00:15:08,760 --> 00:15:12,240
terabytes of data we probably don't

00:15:10,020 --> 00:15:14,100
really care and if we don't care about

00:15:12,240 --> 00:15:15,930
some data anymore we can easily delete

00:15:14,100 --> 00:15:17,430
it because these blocks are fairly by

00:15:15,930 --> 00:15:18,660
specified in terms of which time whence

00:15:17,430 --> 00:15:21,240
the cover and from which from easier

00:15:18,660 --> 00:15:22,740
server they came from so you can really

00:15:21,240 --> 00:15:24,120
just like write your own best commands

00:15:22,740 --> 00:15:29,100
and read data you don't care about any

00:15:24,120 --> 00:15:32,040
longer so just sort of summarizes how we

00:15:29,100 --> 00:15:33,330
go from Prometheus to Thanos the Korea

00:15:32,040 --> 00:15:35,190
and Prometheus the crabby engine is

00:15:33,330 --> 00:15:36,720
basically replaced by the stateless and

00:15:35,190 --> 00:15:38,910
horizontally scalable query layer ten

00:15:36,720 --> 00:15:40,680
assess which is nice because now if you

00:15:38,910 --> 00:15:42,450
have sort of queries of death you're not

00:15:40,680 --> 00:15:44,280
going to wipe out your square in logic

00:15:42,450 --> 00:15:45,040
anymore you just take out one replica

00:15:44,280 --> 00:15:47,320
and

00:15:45,040 --> 00:15:52,149
probably mostly be fine that's a really

00:15:47,320 --> 00:15:53,889
big benefit the scrape engine is simply

00:15:52,149 --> 00:15:56,230
replaced by a bunch of promises there

00:15:53,889 --> 00:15:58,060
was their main job today is we now to

00:15:56,230 --> 00:15:59,829
just scrape data and persist it locally

00:15:58,060 --> 00:16:02,709
at least until the sidecar put it into

00:15:59,829 --> 00:16:08,470
the long-term storage and service at

00:16:02,709 --> 00:16:09,610
least for some time range yeah and you

00:16:08,470 --> 00:16:11,139
still want to keep most of your

00:16:09,610 --> 00:16:12,579
recording routes and alerts on the local

00:16:11,139 --> 00:16:14,459
promises service just because you want

00:16:12,579 --> 00:16:17,440
to introduce another few hops in between

00:16:14,459 --> 00:16:18,699
and you can basically serve these alerts

00:16:17,440 --> 00:16:20,829
and recording routes from memory in

00:16:18,699 --> 00:16:23,019
Prometheus which is also sort of more

00:16:20,829 --> 00:16:24,430
fault tolerant and but in some cases you

00:16:23,019 --> 00:16:26,079
might want to have alerts across your

00:16:24,430 --> 00:16:27,610
entire fleet because all your data send

00:16:26,079 --> 00:16:29,050
us and for that the Tanners ruler

00:16:27,610 --> 00:16:30,610
component is quite nice because it can

00:16:29,050 --> 00:16:33,639
actually hit the courier and then

00:16:30,610 --> 00:16:34,810
evaluate against the global state and

00:16:33,639 --> 00:16:36,160
since we disable compaction on

00:16:34,810 --> 00:16:37,389
Prometheus we have to do it somewhere if

00:16:36,160 --> 00:16:40,029
you want to get the same sort of

00:16:37,389 --> 00:16:43,149
efficiencies out of it and that's all

00:16:40,029 --> 00:16:45,160
you have this very simple single batch

00:16:43,149 --> 00:16:49,690
job which essentially just looks at your

00:16:45,160 --> 00:16:51,310
bucket looks at your packet and yeah

00:16:49,690 --> 00:16:53,199
then just downloads blocks compact and

00:16:51,310 --> 00:16:55,389
together locally and we upload them and

00:16:53,199 --> 00:16:56,829
delete the old ones so that's very

00:16:55,389 --> 00:17:00,480
simple though that's sort of the most

00:16:56,829 --> 00:17:02,199
complex component probably yeah and

00:17:00,480 --> 00:17:03,790
additionally of course we have the

00:17:02,199 --> 00:17:04,990
object so it fits all our data and this

00:17:03,790 --> 00:17:07,689
door gateway now gives us easy access

00:17:04,990 --> 00:17:09,400
and from the career perspective general

00:17:07,689 --> 00:17:11,260
Euler the sidecar the store gateway they

00:17:09,400 --> 00:17:13,390
all are the same they just provide us of

00:17:11,260 --> 00:17:15,220
data and the career link and some smiles

00:17:13,390 --> 00:17:17,319
you figure out which of these it

00:17:15,220 --> 00:17:18,819
actually has to hit for certain queries

00:17:17,319 --> 00:17:23,500
based on the label measures or time

00:17:18,819 --> 00:17:26,829
ranges yeah so how do you deploy this

00:17:23,500 --> 00:17:29,409
and it's actually quite flexible so from

00:17:26,829 --> 00:17:31,570
the in general the architectures project

00:17:29,409 --> 00:17:33,820
this we have a cluster and UVC servers

00:17:31,570 --> 00:17:35,049
in there and then just a sidecar to it

00:17:33,820 --> 00:17:36,340
and for each cluster we have a query

00:17:35,049 --> 00:17:38,650
layer that knows how to talk to all

00:17:36,340 --> 00:17:39,880
these promises servers and you would

00:17:38,650 --> 00:17:42,850
probably and that was like one bucket

00:17:39,880 --> 00:17:45,190
and per cluster and then since Korea's

00:17:42,850 --> 00:17:46,600
also implement the store API they can

00:17:45,190 --> 00:17:48,130
appear as just another data source to

00:17:46,600 --> 00:17:49,840
any other courier which is quite

00:17:48,130 --> 00:17:51,940
convenient because that's essentially

00:17:49,840 --> 00:17:53,530
Federation so if you have some sort of

00:17:51,940 --> 00:17:56,620
master monitoring cluster we actually

00:17:53,530 --> 00:17:58,400
they actually hit to get all data and

00:17:56,620 --> 00:18:02,690
you can just set up this curve

00:17:58,400 --> 00:18:04,790
yeah to hit all other couriers but if

00:18:02,690 --> 00:18:07,100
you are sort of bigger and have more

00:18:04,790 --> 00:18:08,690
complex and more customized structure

00:18:07,100 --> 00:18:09,890
like improbable and you might have

00:18:08,690 --> 00:18:11,630
something like this you have a bunch of

00:18:09,890 --> 00:18:13,310
glasses and they all have one pair of

00:18:11,630 --> 00:18:15,080
Prometheus service and then you have

00:18:13,310 --> 00:18:16,520
another sort of master cluster that

00:18:15,080 --> 00:18:18,800
hosts all the other panels components

00:18:16,520 --> 00:18:21,560
and careers you can fan out some other

00:18:18,800 --> 00:18:24,260
clusters and if you didn't further like

00:18:21,560 --> 00:18:26,450
in that case you have multiple sort of

00:18:24,260 --> 00:18:28,670
layers of these environments of these

00:18:26,450 --> 00:18:30,620
multiple clusters and you could just

00:18:28,670 --> 00:18:32,060
another layer on top and that you can

00:18:30,620 --> 00:18:34,370
then show you to create all the other

00:18:32,060 --> 00:18:37,670
ones yeah so in general you're really

00:18:34,370 --> 00:18:38,990
reflexive and in theory you could even

00:18:37,670 --> 00:18:41,060
just plug in your own data source and

00:18:38,990 --> 00:18:45,680
because the Korea is just looking for

00:18:41,060 --> 00:18:47,900
other story is yeah that's what the

00:18:45,680 --> 00:18:51,260
basic thing and now it's my bonus

00:18:47,900 --> 00:18:53,210
section on down sampling Prometheus has

00:18:51,260 --> 00:18:55,190
really good compression and so 16 byte

00:18:53,210 --> 00:18:57,440
sample ends up being just slightly over

00:18:55,190 --> 00:18:59,180
one byte of data and we're just really

00:18:57,440 --> 00:19:00,230
good in terms of storage efficiencies

00:18:59,180 --> 00:19:03,040
that's what we don't really care about

00:19:00,230 --> 00:19:05,060
how much so it were using object storage

00:19:03,040 --> 00:19:09,470
but that comes with the cost and that's

00:19:05,060 --> 00:19:11,150
of course computational so the

00:19:09,470 --> 00:19:13,670
compressing one sample takes about 10 to

00:19:11,150 --> 00:19:15,470
40 nanoseconds and if you do this across

00:19:13,670 --> 00:19:17,750
like 1,000 time series at a 30 second

00:19:15,470 --> 00:19:19,190
scrub interval over one year which seems

00:19:17,750 --> 00:19:22,070
like very reasonable now that we have

00:19:19,190 --> 00:19:24,110
all this kind of stuff going on and that

00:19:22,070 --> 00:19:26,840
means it's like 10 to 40 seconds just to

00:19:24,110 --> 00:19:28,130
decompress the sample values and there's

00:19:26,840 --> 00:19:29,630
sort of a overshadowing actually

00:19:28,130 --> 00:19:32,510
fetching the data and resolving the data

00:19:29,630 --> 00:19:35,000
so that's really like your main main

00:19:32,510 --> 00:19:37,130
time consumer and then of course if you

00:19:35,000 --> 00:19:39,160
create all these samples the processing

00:19:37,130 --> 00:19:41,090
over those will also be more expensive

00:19:39,160 --> 00:19:44,210
so down sampling doesn't really matter

00:19:41,090 --> 00:19:45,740
for us internal space or sort of network

00:19:44,210 --> 00:19:48,860
vendors we use but just in terms of

00:19:45,740 --> 00:19:51,710
query processing and that's why we

00:19:48,860 --> 00:19:54,460
decided to just add two basic levels of

00:19:51,710 --> 00:19:56,750
down sampling which is a five-minute

00:19:54,460 --> 00:19:59,630
resolution in a one-hour resolution on

00:19:56,750 --> 00:20:00,980
top of Eros a raw resolution and this

00:19:59,630 --> 00:20:03,350
basically done was in the compactor

00:20:00,980 --> 00:20:05,930
which then on top of compacting also

00:20:03,350 --> 00:20:08,180
does down sampling and it's a bit

00:20:05,930 --> 00:20:09,530
complicated let's say because we can't

00:20:08,180 --> 00:20:11,120
just sort of delete some samples in

00:20:09,530 --> 00:20:12,230
between now it could but that's not the

00:20:11,120 --> 00:20:14,480
best way to do it I guess

00:20:12,230 --> 00:20:16,910
and we're actually computing multiple

00:20:14,480 --> 00:20:18,320
time aggregates okay

00:20:16,910 --> 00:20:19,880
so that we can basically access

00:20:18,320 --> 00:20:21,530
different downsample aggregates

00:20:19,880 --> 00:20:25,250
depending on the career we are getting

00:20:21,530 --> 00:20:27,530
and that gives you over better results

00:20:25,250 --> 00:20:30,320
yeah so we reach our goals everything's

00:20:27,530 --> 00:20:33,500
great and we have hopefully time for

00:20:30,320 --> 00:21:00,860
questions or not yes have you thought of

00:20:33,500 --> 00:21:06,260
get up start okay so questions first him

00:21:00,860 --> 00:21:08,990
then yes hi thanks for a talk I was

00:21:06,260 --> 00:21:12,620
wondering how the deduplication work so

00:21:08,990 --> 00:21:17,270
you have two Prometheus and you just

00:21:12,620 --> 00:21:20,480
show like one time series how yeah if

00:21:17,270 --> 00:21:25,130
one has a gap and the other hasn't how

00:21:20,480 --> 00:21:26,690
do you select yeah so basically what we

00:21:25,130 --> 00:21:29,679
do is we follow one time series as long

00:21:26,690 --> 00:21:32,179
as it is providing data was in its usual

00:21:29,679 --> 00:21:34,160
resolution that we measuring and if you

00:21:32,179 --> 00:21:36,020
sort of notice that there are two skips

00:21:34,160 --> 00:21:38,059
in it or more we just flip over to the

00:21:36,020 --> 00:21:40,520
other one if possible that means that we

00:21:38,059 --> 00:21:42,140
don't sort of oscillate around if they

00:21:40,520 --> 00:21:44,270
are some data anomaly that could give

00:21:42,140 --> 00:21:45,530
you with results so yeah to sort of try

00:21:44,270 --> 00:21:51,230
to stick with one serious and only flip

00:21:45,530 --> 00:21:53,690
over if there's a significant gap can

00:21:51,230 --> 00:21:55,700
you give us a sense of the scale you've

00:21:53,690 --> 00:21:57,320
tried running they knows that and what

00:21:55,700 --> 00:21:59,720
kind of latency characteristics you've

00:21:57,320 --> 00:22:01,760
seen and my second question is like what

00:21:59,720 --> 00:22:09,679
kind of cost should we expect from like

00:22:01,760 --> 00:22:12,049
running the objects tour yeah so

00:22:09,679 --> 00:22:15,290
actually I don't have like actual

00:22:12,049 --> 00:22:18,169
numbers yet but what I can offer you is

00:22:15,290 --> 00:22:20,090
a benchmarking tool we wrote for Thanos

00:22:18,169 --> 00:22:22,640
and we have some in shell numbers I

00:22:20,090 --> 00:22:24,950
don't remember those now's but I can I

00:22:22,640 --> 00:22:26,120
can point you basically it is an Adonis

00:22:24,950 --> 00:22:29,300
repository

00:22:26,120 --> 00:22:30,920
asked question on the slack and and/or

00:22:29,300 --> 00:22:34,310
the issue and I can point you to the

00:22:30,920 --> 00:22:36,590
initial kind of results so that's for

00:22:34,310 --> 00:22:39,710
the first question for the last the cost

00:22:36,590 --> 00:22:43,910
for the object storage I think we have

00:22:39,710 --> 00:22:45,560
slide for that can we show okay we have

00:22:43,910 --> 00:22:47,240
we have a slide but it's like nobody

00:22:45,560 --> 00:22:48,770
else at the example so I will just try

00:22:47,240 --> 00:22:50,930
to do a different one it's gonna be

00:22:48,770 --> 00:22:52,070
cheaper and you know the query

00:22:50,930 --> 00:22:53,480
processing is gonna be the same cost

00:22:52,070 --> 00:22:54,140
right just like moving it to a different

00:22:53,480 --> 00:22:56,780
process

00:22:54,140 --> 00:22:59,390
I'm same for the scrubbing stuff that

00:22:56,780 --> 00:23:01,190
stays the same but mostly changes is

00:22:59,390 --> 00:23:02,630
that you need less disk space so we're

00:23:01,190 --> 00:23:05,180
saving as a STIs which can be quite

00:23:02,630 --> 00:23:06,980
expensive especially in cloud and true

00:23:05,180 --> 00:23:09,200
places with object storage which is much

00:23:06,980 --> 00:23:14,480
much cheaper so you should end up saving

00:23:09,200 --> 00:23:17,110
money overall how much down something is

00:23:14,480 --> 00:23:17,110
helping with her

00:23:46,030 --> 00:23:50,720
probably repeat real quick we have a new

00:23:48,380 --> 00:23:53,480
market so the question was how much down

00:23:50,720 --> 00:23:54,830
sampling actually have surgeries and as

00:23:53,480 --> 00:23:56,480
you've seen we are basic down sampling

00:23:54,830 --> 00:23:58,550
from the average like 30 seconds to 5

00:23:56,480 --> 00:24:00,590
minutes first 9 to 1 hour so if you're

00:23:58,550 --> 00:24:02,000
zoom out you basically get a lower

00:24:00,590 --> 00:24:03,230
resolution data and if you zoom into

00:24:02,000 --> 00:24:05,300
your graph you get high as a resolution

00:24:03,230 --> 00:24:06,470
data that happens automatically and and

00:24:05,300 --> 00:24:09,170
if you sort of pick the lowest

00:24:06,470 --> 00:24:11,120
resolution like one hour that's about 20

00:24:09,170 --> 00:24:12,980
X less samples you have to process so

00:24:11,120 --> 00:24:15,590
that means this particular step which is

00:24:12,980 --> 00:24:18,200
the most expensive one will improve

00:24:15,590 --> 00:24:20,510
significantly so you can go easily from

00:24:18,200 --> 00:24:24,410
next 60 seconds to like 2 seconds or

00:24:20,510 --> 00:24:27,410
less high for your name for your project

00:24:24,410 --> 00:24:29,750
I notice I'm curious was it named after

00:24:27,410 --> 00:24:33,530
Marvel we didn't quite think that's

00:24:29,750 --> 00:24:37,520
right to be honest okay for like long

00:24:33,530 --> 00:24:40,450
time so it's pretty bad idea to write

00:24:37,520 --> 00:24:45,890
down something that kind of fits somehow

00:24:40,450 --> 00:24:47,270
yeah so just two points of order a 10x

00:24:45,890 --> 00:24:49,010
speaker please move up and P also

00:24:47,270 --> 00:24:50,450
defragment and the people standing there

00:24:49,010 --> 00:24:52,250
there's more seats in the middle and

00:24:50,450 --> 00:25:01,310
here more questions

00:24:52,250 --> 00:25:05,240
ah hey grey talk so about tunnels floor

00:25:01,310 --> 00:25:08,030
I know that it was in development is it

00:25:05,240 --> 00:25:13,670
already now can we use it or should we

00:25:08,030 --> 00:25:16,010
still use about tunnels Ruhr that rose

00:25:13,670 --> 00:25:19,190
yeah can is it ready

00:25:16,010 --> 00:25:22,130
can we use it last time I checked it was

00:25:19,190 --> 00:25:25,040
in development or something now it

00:25:22,130 --> 00:25:30,460
should be usable it is kind of tested

00:25:25,040 --> 00:25:30,460
but yeah just use it I

00:25:31,380 --> 00:25:51,340
not worry about more questions yes for

00:25:49,210 --> 00:25:52,870
very large datasets is it possible to

00:25:51,340 --> 00:25:54,909
specify retention periods and

00:25:52,870 --> 00:25:57,549
specifically is it possible to specify

00:25:54,909 --> 00:25:59,380
differently by resolution so what lower

00:25:57,549 --> 00:25:59,830
resolution data is kept for a longer

00:25:59,380 --> 00:26:03,130
time

00:25:59,830 --> 00:26:05,679
yeah very good question I think this is

00:26:03,130 --> 00:26:08,409
something you can still do relatively

00:26:05,679 --> 00:26:10,390
quickly using some you know script you

00:26:08,409 --> 00:26:13,870
write on your own but there is

00:26:10,390 --> 00:26:15,580
definitely super super plenty of people

00:26:13,870 --> 00:26:17,549
who wants this feature so actually we

00:26:15,580 --> 00:26:21,789
create I think I created that last week

00:26:17,549 --> 00:26:24,039
and kind of people have issue that that

00:26:21,789 --> 00:26:26,440
states how we can do it so yeah this is

00:26:24,039 --> 00:26:28,659
totally possible and make sense it

00:26:26,440 --> 00:26:30,429
doesn't make sense for Micah is

00:26:28,659 --> 00:26:32,950
definitely for my company because it's

00:26:30,429 --> 00:26:35,140
kind of cheap so so we are not that at

00:26:32,950 --> 00:26:38,200
this point that we care about the size

00:26:35,140 --> 00:26:40,299
but for for other purposes that from

00:26:38,200 --> 00:26:41,620
some object storage you have a limited

00:26:40,299 --> 00:26:43,120
number of objects or something like that

00:26:41,620 --> 00:26:47,380
I totally get that

00:26:43,120 --> 00:26:49,929
so yeah Perce welcome but if not someday

00:26:47,380 --> 00:26:53,260
it will be implemented I guess but we

00:26:49,929 --> 00:26:55,210
have some I want to also add that feel

00:26:53,260 --> 00:26:57,039
free to join our slack community that

00:26:55,210 --> 00:27:00,010
the slack you have slack link invitation

00:26:57,039 --> 00:27:02,650
in the tunnels repository go go there

00:27:00,010 --> 00:27:07,600
ask questions and and basically you can

00:27:02,650 --> 00:27:09,490
help either excellent project I love it

00:27:07,600 --> 00:27:13,440
I know it's just a number but when do

00:27:09,490 --> 00:27:17,350
you think we can expect like 1.0 of it

00:27:13,440 --> 00:27:21,070
I'll do that one is a bit scary at least

00:27:17,350 --> 00:27:25,780
for my managers so yeah yeah it is yeah

00:27:21,070 --> 00:27:28,780
it's kind of on purpose because yeah we

00:27:25,780 --> 00:27:30,850
want to test it and can be we want to be

00:27:28,780 --> 00:27:33,340
perfectly sure that everything is stable

00:27:30,850 --> 00:27:35,799
and and the API is stable and we have

00:27:33,340 --> 00:27:38,320
all the features we have a cup of

00:27:35,799 --> 00:27:40,600
actually two proposal that proposed us

00:27:38,320 --> 00:27:44,080
that I will show maybe afterwards when I

00:27:40,600 --> 00:27:46,059
have some time after the talks so

00:27:44,080 --> 00:27:48,039
features are still missing so so before

00:27:46,059 --> 00:27:53,980
moving to the perfect one perfect plan

00:27:48,039 --> 00:27:57,159
really swung right oh hi Thanks

00:27:53,980 --> 00:27:59,230
one more question how for example in new

00:27:57,159 --> 00:28:01,539
features huh well something changed on

00:27:59,230 --> 00:28:04,360
storage level or layer on primitives or

00:28:01,539 --> 00:28:08,370
funk new function he said it how how do

00:28:04,360 --> 00:28:11,919
you plan to come to keep upstream or

00:28:08,370 --> 00:28:17,110
what is basically would be the project

00:28:11,919 --> 00:28:19,299
which you can after year and can can

00:28:17,110 --> 00:28:22,240
become obsolete it's community-driven

00:28:19,299 --> 00:28:25,960
only right at the moment exactly um so

00:28:22,240 --> 00:28:28,480
the main thing is that I imagine what

00:28:25,960 --> 00:28:31,360
what could I mean it's hard to imagine

00:28:28,480 --> 00:28:34,240
what can happen right but we really are

00:28:31,360 --> 00:28:37,149
using the vanilla components from from

00:28:34,240 --> 00:28:38,980
the prompt use itself we basically work

00:28:37,149 --> 00:28:41,950
depends on it we use the same code for

00:28:38,980 --> 00:28:42,519
the query for the storage layer blocks

00:28:41,950 --> 00:28:45,220
whatever

00:28:42,519 --> 00:28:47,590
so when from these changes when TCB

00:28:45,220 --> 00:28:49,480
changes we are changing as well so I

00:28:47,590 --> 00:28:50,190
think we are be able to adapt pretty

00:28:49,480 --> 00:28:53,610
quickly

00:28:50,190 --> 00:28:59,390
hopefully thank you very much

00:28:53,610 --> 00:28:59,390
[Applause]

00:29:00,760 --> 00:29:02,820

YouTube URL: https://www.youtube.com/watch?v=Fb_lYX01IX4


