Title: PromCon 2018: Migrating to Prometheus at Fastly
Publication date: 2018-11-10
Playlist: PromCon 2018
Description: 
	Speaker: Marcus Barczak

Monitoring at Scale: Migrating to Prometheus at Fastly


Over the past 6 months at Fastly we've migrated away from our legacy monitoring systems and have deployed Prometheus as our primary system for infrastructure and application monitoring.

The Prometheus approach posed some unique challenges over traditional monitoring systems, whilst at the same time enabling us to easily scale our monitoring infrastructure alongside our global network growth.

It hasn't been completely smooth sailing and deploying Prometheus across a globe spanning network serving over 10% of the world's internet traffic has raised its fair share of challenges both technical and cultural.

In this presentation you will learn how we addressed these issues in ways that deviate slightly from conventional wisdom, the mistakes we made along the way, and how the new system has been received by our teams. We hope that our experiences can help you succeed in deploying Prometheus within your organization.
Captions: 
	00:00:10,480 --> 00:00:14,559
[Applause]

00:00:11,790 --> 00:00:16,299
so yeah my name is Marcus I'm a

00:00:14,559 --> 00:00:18,040
principal engineer at fastly and at the

00:00:16,299 --> 00:00:21,010
moment I'm focusing on our sort of

00:00:18,040 --> 00:00:23,110
observability stack for those of you

00:00:21,010 --> 00:00:26,289
that aren't familiar with fastly and and

00:00:23,110 --> 00:00:27,910
I wouldn't be surprised if you aren't we

00:00:26,289 --> 00:00:30,580
power we're a content distribution

00:00:27,910 --> 00:00:32,710
network and edge cloud and we power but

00:00:30,580 --> 00:00:33,580
sort of hundreds of the largest web

00:00:32,710 --> 00:00:36,220
sites in the world

00:00:33,580 --> 00:00:37,360
so you probably use this every day but

00:00:36,220 --> 00:00:40,840
might not know exactly what the company

00:00:37,360 --> 00:00:42,820
is so our network spans the globe we

00:00:40,840 --> 00:00:44,260
have 51 points of presence which are

00:00:42,820 --> 00:00:46,900
individual data centers scattered all

00:00:44,260 --> 00:00:48,970
around the world of those 51 data

00:00:46,900 --> 00:00:51,430
centers we have 22 terabits of connected

00:00:48,970 --> 00:00:52,930
network transit to support all of that

00:00:51,430 --> 00:00:54,989
customer traffic and to get all of the

00:00:52,930 --> 00:00:57,760
content to you as quickly as possible so

00:00:54,989 --> 00:00:59,470
as you can imagine the importance of us

00:00:57,760 --> 00:01:00,790
being able to monitor the performance

00:00:59,470 --> 00:01:03,940
and reliability of our network is

00:01:00,790 --> 00:01:05,710
absolutely paramount both not only for

00:01:03,940 --> 00:01:07,210
our customers that are allowing us to

00:01:05,710 --> 00:01:09,460
deliver their content but to all of you

00:01:07,210 --> 00:01:13,540
who want to browse reddit when you're

00:01:09,460 --> 00:01:16,450
not working so to set the stage let's

00:01:13,540 --> 00:01:18,430
have a little bit of a chat about how we

00:01:16,450 --> 00:01:20,560
were monitoring fastly so fastly is a

00:01:18,430 --> 00:01:22,210
relatively young company we've been

00:01:20,560 --> 00:01:24,610
around for seven years and we've been

00:01:22,210 --> 00:01:26,710
monitoring since day one so the original

00:01:24,610 --> 00:01:29,590
monitoring stack at fastly was ganglia

00:01:26,710 --> 00:01:30,850
who is familiar with ganglia fewer a few

00:01:29,590 --> 00:01:33,880
people who still runs it is their

00:01:30,850 --> 00:01:35,170
primary monitoring okay there's got to

00:01:33,880 --> 00:01:38,400
be one or two for sure

00:01:35,170 --> 00:01:41,260
okay so ganglia is an our ID based

00:01:38,400 --> 00:01:43,720
monitoring solution it was originally

00:01:41,260 --> 00:01:45,820
designed to support monitoring of high

00:01:43,720 --> 00:01:47,290
performance computing clusters so a lot

00:01:45,820 --> 00:01:49,600
of its functionality is centered around

00:01:47,290 --> 00:01:51,220
sort of this concept of clustering and

00:01:49,600 --> 00:01:52,930
having groups of machines that are

00:01:51,220 --> 00:01:54,610
mostly homogeneous so for us it was a

00:01:52,930 --> 00:01:56,140
pretty good fit because we have all of

00:01:54,610 --> 00:01:58,240
these data centers that are relatively

00:01:56,140 --> 00:01:59,520
the same but it did have some

00:01:58,240 --> 00:02:03,010
limitations

00:01:59,520 --> 00:02:04,659
so as we scaled as a company so from you

00:02:03,010 --> 00:02:06,940
know seven years starting seven years

00:02:04,659 --> 00:02:09,090
ago we started to have some growing

00:02:06,940 --> 00:02:11,470
pains with our ganglia infrastructure

00:02:09,090 --> 00:02:13,930
there's a pretty heavy operational

00:02:11,470 --> 00:02:15,580
overhead so ganglia is designed in sort

00:02:13,930 --> 00:02:17,650
of more traditional approach where you

00:02:15,580 --> 00:02:19,299
have a central aggregator and all of

00:02:17,650 --> 00:02:21,609
your infrastructure pushes its metrics

00:02:19,299 --> 00:02:23,079
to that central aggregator so as our

00:02:21,609 --> 00:02:23,620
infrastructure was growing we're just

00:02:23,079 --> 00:02:25,390
ABB's

00:02:23,620 --> 00:02:27,790
hammering this aggregator with it with

00:02:25,390 --> 00:02:30,910
more and more traffic it's also quite a

00:02:27,790 --> 00:02:32,739
complicated system to run there's a lot

00:02:30,910 --> 00:02:34,569
of components it doesn't provide a lot

00:02:32,739 --> 00:02:36,580
of observability out of the box for how

00:02:34,569 --> 00:02:39,160
you can introspect how itself is running

00:02:36,580 --> 00:02:42,129
so you're in a tangley mess of debugging

00:02:39,160 --> 00:02:44,860
xml and XDR over UDP which is awesome

00:02:42,129 --> 00:02:46,569
fun as you can imagine given that it's

00:02:44,860 --> 00:02:48,610
based on our ID it has very limited

00:02:46,569 --> 00:02:49,810
graphing functions so for the most part

00:02:48,610 --> 00:02:51,790
most of the views will give you an

00:02:49,810 --> 00:02:54,489
average and depending on how you set

00:02:51,790 --> 00:02:56,140
your sort of retention schema you may

00:02:54,489 --> 00:02:57,310
sort of start to lose resolution over

00:02:56,140 --> 00:02:58,750
time and that's always a trade-off

00:02:57,310 --> 00:03:01,329
between how long you want to keep the

00:02:58,750 --> 00:03:02,709
metrics around for and and what your

00:03:01,329 --> 00:03:04,390
sort of functions are and there's no

00:03:02,709 --> 00:03:06,670
native alerting support so to actually

00:03:04,390 --> 00:03:08,980
bolt alerting into ganglia it required

00:03:06,670 --> 00:03:10,780
building a whole sort of menagerie of

00:03:08,980 --> 00:03:13,510
components that integrated with icinga

00:03:10,780 --> 00:03:15,010
to be able to query ganglia for metrics

00:03:13,510 --> 00:03:17,140
and thresholds and things like that and

00:03:15,010 --> 00:03:18,879
you would end up with this real

00:03:17,140 --> 00:03:20,590
disconnect between the alert that fired

00:03:18,879 --> 00:03:22,450
and getting back to the actual graph

00:03:20,590 --> 00:03:24,940
that was measuring that thing that

00:03:22,450 --> 00:03:26,920
alerted and there was no real API and

00:03:24,940 --> 00:03:29,709
this sort of became more of an issue in

00:03:26,920 --> 00:03:31,389
later life when our data science team

00:03:29,709 --> 00:03:33,069
and our capacity planning teams were

00:03:31,389 --> 00:03:35,319
wanting to sort of take all of this

00:03:33,069 --> 00:03:37,120
juicy system telemetry that we have and

00:03:35,319 --> 00:03:38,859
use those for their their research

00:03:37,120 --> 00:03:40,349
purposes or for capacity planning so

00:03:38,859 --> 00:03:44,290
we're kind of a little bit stuck there

00:03:40,349 --> 00:03:46,000
so moving along a few years ago there

00:03:44,290 --> 00:03:47,139
was an attempt at fastly to try and you

00:03:46,000 --> 00:03:48,670
know get out of this mess would

00:03:47,139 --> 00:03:51,280
identified these problems what could we

00:03:48,670 --> 00:03:54,910
do to solve it so we rub a bit of sass

00:03:51,280 --> 00:03:56,620
on it plenty of third-party SAS vendors

00:03:54,910 --> 00:03:58,569
that do metric collection for you so

00:03:56,620 --> 00:04:00,250
there was a project underway to kind of

00:03:58,569 --> 00:04:01,989
start to move a lot of the ganglia stuff

00:04:00,250 --> 00:04:03,579
into data dock but it wasn't necessarily

00:04:01,989 --> 00:04:07,000
a move it was more let's do it in

00:04:03,579 --> 00:04:08,109
parallel so the resulting situation

00:04:07,000 --> 00:04:10,450
there is now we've just doubled our

00:04:08,109 --> 00:04:12,430
panes we're now supporting two systems

00:04:10,450 --> 00:04:15,310
so we have metrics going to two separate

00:04:12,430 --> 00:04:16,299
systems there's the the perennial

00:04:15,310 --> 00:04:17,919
question around where do I put my

00:04:16,299 --> 00:04:19,299
metrics do I put them in ganglia do I

00:04:17,919 --> 00:04:21,340
put them in data dock do I put them in

00:04:19,299 --> 00:04:23,740
both where do people expect metrics to

00:04:21,340 --> 00:04:25,450
be so the consumers of the systems would

00:04:23,740 --> 00:04:26,590
be expecting certain metrics to be in

00:04:25,450 --> 00:04:29,470
one of the systems that wouldn't be

00:04:26,590 --> 00:04:30,729
there was just confusion and horror we

00:04:29,470 --> 00:04:32,740
were still having to ride a lot of

00:04:30,729 --> 00:04:34,960
blackbox monitoring sort of plugins and

00:04:32,740 --> 00:04:36,090
agents so both of these systems relied

00:04:34,960 --> 00:04:38,520
on

00:04:36,090 --> 00:04:41,520
sketchy shell scripts and Python code

00:04:38,520 --> 00:04:43,290
that we'd go and try and sort of infer

00:04:41,520 --> 00:04:44,790
the performance and the characteristics

00:04:43,290 --> 00:04:46,380
of the services they'll monitoring and

00:04:44,790 --> 00:04:47,580
then emit those metrics out and we

00:04:46,380 --> 00:04:49,110
really kind of would have preferred a

00:04:47,580 --> 00:04:50,850
sort of a more white boxy approach and

00:04:49,110 --> 00:04:53,310
it was very limiting with both these

00:04:50,850 --> 00:04:55,650
systems and monitoring was always

00:04:53,310 --> 00:04:58,680
treated as like a post release phase so

00:04:55,650 --> 00:05:00,000
folks would ship a new service and then

00:04:58,680 --> 00:05:01,560
once it would be thrown over the wall to

00:05:00,000 --> 00:05:02,970
ops it would be a case of oh now we have

00:05:01,560 --> 00:05:05,430
to add monitoring and there'd be a mad

00:05:02,970 --> 00:05:07,530
scramble trying to figure out a what

00:05:05,430 --> 00:05:09,060
should I be monitoring because it was

00:05:07,530 --> 00:05:10,470
that disconnect between the sort of the

00:05:09,060 --> 00:05:12,180
engineers that wrote the code and the

00:05:10,470 --> 00:05:14,250
operators who are who were running it

00:05:12,180 --> 00:05:15,270
and yeah it was essentially the

00:05:14,250 --> 00:05:16,410
equivalent of like will write the

00:05:15,270 --> 00:05:18,180
documentation later

00:05:16,410 --> 00:05:19,620
so we'd have missing monitors or we

00:05:18,180 --> 00:05:21,600
wouldn't have the right monitors and

00:05:19,620 --> 00:05:23,160
then the sort of underlying problem that

00:05:21,600 --> 00:05:25,080
we had was that as our infrastructure

00:05:23,160 --> 00:05:26,730
scaled horizontally so we're adding more

00:05:25,080 --> 00:05:29,310
and more pops and more and more servers

00:05:26,730 --> 00:05:31,110
both of these metrics collection systems

00:05:29,310 --> 00:05:33,840
had to be scaled vertically so in the

00:05:31,110 --> 00:05:35,760
ganglia case we need bigger net we need

00:05:33,840 --> 00:05:37,530
bigger NICs we need a faster network we

00:05:35,760 --> 00:05:39,360
need more disk i/o we need more memory

00:05:37,530 --> 00:05:40,890
and now we need a whole new machine and

00:05:39,360 --> 00:05:42,780
then migrate everything to a new machine

00:05:40,890 --> 00:05:44,610
and this was like a almost a monthly

00:05:42,780 --> 00:05:47,340
process at some point and then on the

00:05:44,610 --> 00:05:49,110
SAS side we had to continue scaling the

00:05:47,340 --> 00:05:50,940
pile of money that we were giving them

00:05:49,110 --> 00:05:53,220
every month to be able to support this

00:05:50,940 --> 00:05:55,950
extended use and both systems just sort

00:05:53,220 --> 00:05:57,390
of became untenable so you might be

00:05:55,950 --> 00:05:58,919
thinking okay well we've got to

00:05:57,390 --> 00:06:01,080
monitoring systems now should we throw a

00:05:58,919 --> 00:06:04,020
third in so maybe we'll get lucky this

00:06:01,080 --> 00:06:05,790
time around so we had a few goals in

00:06:04,020 --> 00:06:08,010
mind when we started thinking about if

00:06:05,790 --> 00:06:09,570
we were to to redo our sort of

00:06:08,010 --> 00:06:11,370
monitoring it fastly what would we do

00:06:09,570 --> 00:06:12,690
and what was sort of some of the

00:06:11,370 --> 00:06:15,090
high-level requirements that we would

00:06:12,690 --> 00:06:16,320
have so the most important one really

00:06:15,090 --> 00:06:18,180
was that we needed a system that would

00:06:16,320 --> 00:06:19,740
scale with our infrastructure growth so

00:06:18,180 --> 00:06:21,360
as we started rolling out more

00:06:19,740 --> 00:06:22,890
infrastructure we really needed the

00:06:21,360 --> 00:06:25,229
monitoring system to not get in the way

00:06:22,890 --> 00:06:27,600
of that we wanted a system that would be

00:06:25,229 --> 00:06:29,400
easy to deploy and operate so we don't

00:06:27,600 --> 00:06:31,229
want to have to be burning op cycles on

00:06:29,400 --> 00:06:33,390
trying to keep the system up and the

00:06:31,229 --> 00:06:36,000
system not being up is sort of losses as

00:06:33,390 --> 00:06:38,070
critical visibility we wanted a system

00:06:36,000 --> 00:06:40,020
that also was really friendly to

00:06:38,070 --> 00:06:42,960
engineers so that instrumenting your

00:06:40,020 --> 00:06:45,750
applications was not an onerous task we

00:06:42,960 --> 00:06:47,880
have in the ganglia days we did actually

00:06:45,750 --> 00:06:49,500
have a couple of applications core

00:06:47,880 --> 00:06:49,800
applications that had built in the G

00:06:49,500 --> 00:06:51,870
meter

00:06:49,800 --> 00:06:54,690
libraries themselves which is if you

00:06:51,870 --> 00:06:56,940
thought UDP and XDR and XML is bad like

00:06:54,690 --> 00:06:59,819
those libraries or even more terrifying

00:06:56,940 --> 00:07:01,650
to to integrate so we wanted it to make

00:06:59,819 --> 00:07:03,720
it really easy for for engineers to

00:07:01,650 --> 00:07:06,389
instrument their own applications and we

00:07:03,720 --> 00:07:08,159
wanted a first-class API support it just

00:07:06,389 --> 00:07:09,300
makes sense if you're putting data into

00:07:08,159 --> 00:07:11,960
a system you should be able to get it

00:07:09,300 --> 00:07:13,979
out in any way that you you see fit and

00:07:11,960 --> 00:07:16,139
the last thing that I'm just going to

00:07:13,979 --> 00:07:18,180
mention really briefly so Peter Bergen

00:07:16,139 --> 00:07:20,159
was my sort of partner on this project

00:07:18,180 --> 00:07:21,690
he and I sort of still working on and

00:07:20,159 --> 00:07:24,389
have been for the last sort of half a

00:07:21,690 --> 00:07:26,520
year um he did a presentation at monitor

00:07:24,389 --> 00:07:28,080
armour earlier this year because the

00:07:26,520 --> 00:07:29,699
other aspect of it was we really wanted

00:07:28,080 --> 00:07:32,250
to try and transform the monitoring

00:07:29,699 --> 00:07:34,830
culture at fastly to move it to be more

00:07:32,250 --> 00:07:36,509
of a everyone's involved kind of

00:07:34,830 --> 00:07:38,340
approach so Peters presentation is

00:07:36,509 --> 00:07:39,960
online there and that goes much more

00:07:38,340 --> 00:07:41,759
into the detail of the sort of the work

00:07:39,960 --> 00:07:44,430
we've done to change the monitoring

00:07:41,759 --> 00:07:47,599
culture adversely that is sort of like

00:07:44,430 --> 00:07:51,479
the companion piece to this presentation

00:07:47,599 --> 00:07:53,370
so now that we have identified yes let's

00:07:51,479 --> 00:07:54,779
try and do this project now we have to

00:07:53,370 --> 00:07:57,360
decide what system are we going to use

00:07:54,779 --> 00:07:59,039
so it had to be on Prem so it limited

00:07:57,360 --> 00:08:01,680
that the sort of solutions based there

00:07:59,039 --> 00:08:03,180
for us a bit most of the on Prem

00:08:01,680 --> 00:08:04,529
solutions that we looked at still

00:08:03,180 --> 00:08:06,719
suffered from a lot of the same problems

00:08:04,529 --> 00:08:08,310
we had this central aggregation point we

00:08:06,719 --> 00:08:10,529
had to be pushing all of our metrics to

00:08:08,310 --> 00:08:12,330
that single aggregation point the newer

00:08:10,529 --> 00:08:14,819
systems did offer things like clustering

00:08:12,330 --> 00:08:16,860
and sharding which would help the data

00:08:14,819 --> 00:08:18,629
volume scale and the sort of ingest

00:08:16,860 --> 00:08:21,779
capacity scale but again that sort of

00:08:18,629 --> 00:08:24,810
felt to be quite a significant

00:08:21,779 --> 00:08:26,550
operational burden to take on so what

00:08:24,810 --> 00:08:29,930
else was there out there that we could

00:08:26,550 --> 00:08:34,409
possibly use and no surprises Prometheus

00:08:29,930 --> 00:08:36,300
so Peter sorry Peter had worked at our

00:08:34,409 --> 00:08:38,820
cloud previously with Julius and one was

00:08:36,300 --> 00:08:40,020
one of the very early sort of was using

00:08:38,820 --> 00:08:41,579
some of the very early versions of

00:08:40,020 --> 00:08:44,760
Prometheus I think before any of us got

00:08:41,579 --> 00:08:46,170
to see it and he said to us that he was

00:08:44,760 --> 00:08:48,959
amazed at like this sort of more

00:08:46,170 --> 00:08:51,060
revolutionary approach to to monitoring

00:08:48,959 --> 00:08:53,910
so we figured well let's give it a go I

00:08:51,060 --> 00:08:55,529
did some initial testing and then just

00:08:53,910 --> 00:08:58,020
getting up and running in a few minutes

00:08:55,529 --> 00:08:59,279
was really simple and an awesome so we

00:08:58,020 --> 00:09:01,560
figured let's go with that and see how

00:08:59,279 --> 00:09:02,800
we go so what do we need to do to get

00:09:01,560 --> 00:09:03,880
started

00:09:02,800 --> 00:09:06,790
the first thing was to build our

00:09:03,880 --> 00:09:09,250
proof-of-concept infrastructure building

00:09:06,790 --> 00:09:10,899
a proof-of-concept we took traceable it

00:09:09,250 --> 00:09:12,730
sort of design methodology and that's

00:09:10,899 --> 00:09:15,040
sort of traceable it design is where you

00:09:12,730 --> 00:09:17,800
sort of pick how you want to build your

00:09:15,040 --> 00:09:19,420
architecture and build it with all of

00:09:17,800 --> 00:09:21,100
the all of the rigor that you would with

00:09:19,420 --> 00:09:22,750
a production system so given that we

00:09:21,100 --> 00:09:24,160
sort of had this large geographical set

00:09:22,750 --> 00:09:26,140
of pops it was let's build it in a

00:09:24,160 --> 00:09:27,700
couple of pops but with full production

00:09:26,140 --> 00:09:29,680
capacity so we don't have this sort of

00:09:27,700 --> 00:09:32,170
horrible production ization at the end

00:09:29,680 --> 00:09:35,140
stage should we decide to go ahead with

00:09:32,170 --> 00:09:36,850
things as part of the sort of cultural

00:09:35,140 --> 00:09:39,220
thing we really wanted engineers to sort

00:09:36,850 --> 00:09:41,110
of be fully empowered to have all the

00:09:39,220 --> 00:09:42,790
skills that necessary to instrument

00:09:41,110 --> 00:09:44,230
their own applications write their own -

00:09:42,790 --> 00:09:46,510
but build their own dashboards write

00:09:44,230 --> 00:09:49,360
their own alerts so part of this project

00:09:46,510 --> 00:09:51,310
was to pair with these teams and bring

00:09:49,360 --> 00:09:53,680
them in gently to the prometheus world

00:09:51,310 --> 00:09:55,959
so we would pair with a pilot team who

00:09:53,680 --> 00:09:57,850
one of our early adopters to instrument

00:09:55,959 --> 00:09:59,440
the services that they ran build helped

00:09:57,850 --> 00:10:02,800
them build their initial dashboards and

00:09:59,440 --> 00:10:04,899
alerts and use them as leverage for the

00:10:02,800 --> 00:10:07,200
other teams to kind of build the

00:10:04,899 --> 00:10:09,279
excitement and advocacy for Prometheus

00:10:07,200 --> 00:10:11,770
we then continued through the rest of

00:10:09,279 --> 00:10:13,300
the team's same kind of process run both

00:10:11,770 --> 00:10:14,800
systems in parallel make sure that

00:10:13,300 --> 00:10:17,709
they're all doing the same thing and

00:10:14,800 --> 00:10:19,360
then we'd decommission the SAS system in

00:10:17,709 --> 00:10:20,829
ganglia and we're kind of in the middle

00:10:19,360 --> 00:10:22,690
of decommissioning this asset off now

00:10:20,829 --> 00:10:25,750
ganglia will probably stick around for a

00:10:22,690 --> 00:10:27,100
little bit longer so now daunted that

00:10:25,750 --> 00:10:29,380
sort of technical side of things

00:10:27,100 --> 00:10:32,800
what does fastly as infrastructure look

00:10:29,380 --> 00:10:34,810
like for Prometheus this diagram

00:10:32,800 --> 00:10:35,170
probably unsurprising - to everyone in

00:10:34,810 --> 00:10:37,300
the room

00:10:35,170 --> 00:10:39,520
I think everyone's kind of elided

00:10:37,300 --> 00:10:41,980
towards this is the way that you run

00:10:39,520 --> 00:10:44,560
Prometheus in a high availability way so

00:10:41,980 --> 00:10:46,660
in each of our individual pops we run a

00:10:44,560 --> 00:10:48,130
pair of Prometheus instances it's

00:10:46,660 --> 00:10:49,360
fantastic that they have local storage

00:10:48,130 --> 00:10:51,370
and they're shared-nothing

00:10:49,360 --> 00:10:53,350
both of those instances scrape all of

00:10:51,370 --> 00:10:55,209
the devices that live in those pops so

00:10:53,350 --> 00:10:57,820
we have all of our servers and network

00:10:55,209 --> 00:11:00,850
switches in PDUs both of them scrape all

00:10:57,820 --> 00:11:03,190
the metrics that's awesome and so that's

00:11:00,850 --> 00:11:04,750
like out of San Jose pop we then take

00:11:03,190 --> 00:11:06,850
that pattern and stamp it out across our

00:11:04,750 --> 00:11:10,120
whole infrastructure so every pop gets

00:11:06,850 --> 00:11:11,770
exactly the same topology and so for us

00:11:10,120 --> 00:11:13,779
in terms of doing this tracer bullet

00:11:11,770 --> 00:11:15,279
design once we'd sort of built out the

00:11:13,779 --> 00:11:15,920
chef's code and everything to build out

00:11:15,279 --> 00:11:18,320
our premiere

00:11:15,920 --> 00:11:19,570
instances in one pop it was trivial to

00:11:18,320 --> 00:11:23,000
roll out to everywhere else

00:11:19,570 --> 00:11:24,740
the other component in our stack is what

00:11:23,000 --> 00:11:26,300
we call our front end so our front end

00:11:24,740 --> 00:11:29,450
runs in Google on Google cloud platform

00:11:26,300 --> 00:11:31,670
and it consists of a few components we

00:11:29,450 --> 00:11:35,660
have nginx operating as our sort of

00:11:31,670 --> 00:11:38,660
reverse proxy router nginx allows users

00:11:35,660 --> 00:11:40,490
to just pass a URL path which will route

00:11:38,660 --> 00:11:42,980
them to any of the Prometheus's in our

00:11:40,490 --> 00:11:44,570
global infrastructure we have graph

00:11:42,980 --> 00:11:46,519
honor obviously for all of our alerting

00:11:44,570 --> 00:11:49,639
and dashboarding in a very long list of

00:11:46,519 --> 00:11:51,500
sites as data sources and we have a

00:11:49,639 --> 00:11:52,699
couple of federated and also alert

00:11:51,500 --> 00:11:53,870
manager cluster but I'm not going to

00:11:52,699 --> 00:11:56,060
talk about those I think you know

00:11:53,870 --> 00:12:00,199
everyone's kind of familiar now one of

00:11:56,060 --> 00:12:02,029
the real important aspects of fastest

00:12:00,199 --> 00:12:04,910
infrastructure is unlike a lot of folks

00:12:02,029 --> 00:12:06,380
we don't really have a private corporate

00:12:04,910 --> 00:12:08,209
network like we don't have a back-end

00:12:06,380 --> 00:12:10,639
network our back-end network is the

00:12:08,209 --> 00:12:12,139
internet so for a request to go from the

00:12:10,639 --> 00:12:14,720
front end to all of our Prometheus

00:12:12,139 --> 00:12:16,430
instances it goes across the internet so

00:12:14,720 --> 00:12:18,980
what we needed to implement there was we

00:12:16,430 --> 00:12:20,510
needed full TLS not only TLS but TLS and

00:12:18,980 --> 00:12:23,000
authentication from the front end

00:12:20,510 --> 00:12:24,829
instances Promethea from gravano and the

00:12:23,000 --> 00:12:26,209
federated and everything needed to be

00:12:24,829 --> 00:12:29,630
able to reach every Prometheus around

00:12:26,209 --> 00:12:31,820
the world secured with TLS so let's talk

00:12:29,630 --> 00:12:35,600
about what our service stack looks like

00:12:31,820 --> 00:12:37,250
on an individual Prometheus server so

00:12:35,600 --> 00:12:40,399
the first component we have which is

00:12:37,250 --> 00:12:42,199
what gave us the TLS magic was a package

00:12:40,399 --> 00:12:44,980
called ghost tunnel which is released by

00:12:42,199 --> 00:12:49,040
Square up and saw and it's an awesome

00:12:44,980 --> 00:12:52,070
lightweight TLS reverse proxy that also

00:12:49,040 --> 00:12:53,600
does mutual authentication we're using

00:12:52,070 --> 00:12:55,660
that for almost everything now

00:12:53,600 --> 00:12:59,029
everywhere not just for Prometheus

00:12:55,660 --> 00:13:01,310
highly recommend we have a service

00:12:59,029 --> 00:13:03,260
discovery system that we've built one

00:13:01,310 --> 00:13:04,579
component of that is the sidecar and I'm

00:13:03,260 --> 00:13:06,529
going to go into more detail about this

00:13:04,579 --> 00:13:08,060
shortly but it's responsible for sort of

00:13:06,529 --> 00:13:10,910
preparing the entire prometheus

00:13:08,060 --> 00:13:13,910
configuration we have our rules loader

00:13:10,910 --> 00:13:16,639
sort of daemon process at fastly we use

00:13:13,910 --> 00:13:18,079
a sort of get ops approach to managing

00:13:16,639 --> 00:13:20,300
all of our loading and recording rules

00:13:18,079 --> 00:13:21,709
so any engineering fastly is is sort of

00:13:20,300 --> 00:13:23,470
empowered to go and write their own

00:13:21,709 --> 00:13:26,540
recording rules their own alerting rules

00:13:23,470 --> 00:13:28,100
open a PR have it reviewed it runs

00:13:26,540 --> 00:13:28,460
through our linting and validation and

00:13:28,100 --> 00:13:30,620
then when it

00:13:28,460 --> 00:13:33,950
merged to master that process is

00:13:30,620 --> 00:13:35,990
responsible responsible for updating

00:13:33,950 --> 00:13:37,640
those recording rules on every instance

00:13:35,990 --> 00:13:41,180
around the world and it's nothing more

00:13:37,640 --> 00:13:44,240
clever than like get pull in a cron and

00:13:41,180 --> 00:13:47,030
then of course Prometheus then on the

00:13:44,240 --> 00:13:49,580
server side so a typically instrumented

00:13:47,030 --> 00:13:52,340
server at fastly it's radically simple

00:13:49,580 --> 00:13:53,660
we have our exporters obviously they're

00:13:52,340 --> 00:13:55,700
either built into the services

00:13:53,660 --> 00:13:58,640
themselves or they operate as a sidecar

00:13:55,700 --> 00:14:01,790
for those horrible things like Ruby apps

00:13:58,640 --> 00:14:03,080
and whatnot and then we have the other

00:14:01,790 --> 00:14:04,940
component of our service discovery

00:14:03,080 --> 00:14:06,710
secretive system which is the services

00:14:04,940 --> 00:14:09,470
gallery proxy and we're going to sort of

00:14:06,710 --> 00:14:10,910
talk about those now so why would you

00:14:09,470 --> 00:14:13,100
want to build your own service discovery

00:14:10,910 --> 00:14:14,540
so Prometheus out-of-the-box supports

00:14:13,100 --> 00:14:16,100
somewhere I think between like either 12

00:14:14,540 --> 00:14:18,920
or 14 different service discovery

00:14:16,100 --> 00:14:20,840
mechanisms but unfortunately for us as

00:14:18,920 --> 00:14:23,420
well as having the Internet as our back

00:14:20,840 --> 00:14:26,480
know sort of as our back our corporate

00:14:23,420 --> 00:14:29,180
network all about faster infrastructure

00:14:26,480 --> 00:14:30,980
is bare metal it's old-school racks and

00:14:29,180 --> 00:14:32,480
stacks in in data centers and it's all

00:14:30,980 --> 00:14:34,040
bare metal so we don't have any of the

00:14:32,480 --> 00:14:36,460
conveniences they get in like a cloud

00:14:34,040 --> 00:14:39,740
native architecture so you don't have

00:14:36,460 --> 00:14:41,420
beautiful api's for doing everything we

00:14:39,740 --> 00:14:42,800
do have some of our own internal systems

00:14:41,420 --> 00:14:44,300
but obviously they're internal and

00:14:42,800 --> 00:14:47,360
bespoke so they don't sort of have a

00:14:44,300 --> 00:14:48,740
out-of-the-box solution and what we

00:14:47,360 --> 00:14:50,750
ultimately wanted was we need the

00:14:48,740 --> 00:14:51,980
ability for our Prometheus

00:14:50,750 --> 00:14:53,870
infrastructure as it grows to just

00:14:51,980 --> 00:14:57,790
configure itself for the most part and

00:14:53,870 --> 00:15:00,140
we did want to use chef because reasons

00:14:57,790 --> 00:15:01,910
service so what requirements do we have

00:15:00,140 --> 00:15:04,370
for building this like service discovery

00:15:01,910 --> 00:15:06,830
mechanism we wanted automatic discovery

00:15:04,370 --> 00:15:08,810
of targets so as we would roll out new

00:15:06,830 --> 00:15:10,670
machines or roll out new pops or roll

00:15:08,810 --> 00:15:13,340
out new services we wanted them to just

00:15:10,670 --> 00:15:14,750
magically be automatically instrumented

00:15:13,340 --> 00:15:17,720
without anyone having to do a lot of

00:15:14,750 --> 00:15:19,640
work we wanted self-service registration

00:15:17,720 --> 00:15:21,440
of exporter endpoints so while we're

00:15:19,640 --> 00:15:23,570
trying to empower all of our engineers

00:15:21,440 --> 00:15:26,060
to instrument their own applications

00:15:23,570 --> 00:15:28,160
they'll be exposing Prometheus metrics

00:15:26,060 --> 00:15:30,980
either on a special port or under a

00:15:28,160 --> 00:15:33,230
special path in their application so we

00:15:30,980 --> 00:15:34,850
wanted them to be able to register where

00:15:33,230 --> 00:15:36,710
their exporter will be exposing the

00:15:34,850 --> 00:15:38,550
metrics to our configuration systems

00:15:36,710 --> 00:15:40,770
that we could consume later on

00:15:38,550 --> 00:15:43,080
we obviously needed TLS encryption for

00:15:40,770 --> 00:15:45,900
all exporter traffic look non-negotiable

00:15:43,080 --> 00:15:48,480
fastly and we want a minimal exposure of

00:15:45,900 --> 00:15:50,880
TCP ports for exporters so on a typical

00:15:48,480 --> 00:15:53,040
system we have anywhere from 15 to 25

00:15:50,880 --> 00:15:54,780
different exporters running we didn't

00:15:53,040 --> 00:15:57,270
want to have to be exposing TCP ports

00:15:54,780 --> 00:15:59,520
for every one of those they kind of set

00:15:57,270 --> 00:16:02,970
the stage for what we needed to plumb

00:15:59,520 --> 00:16:05,130
this whole menagerie together so at a

00:16:02,970 --> 00:16:06,840
very high level going back to the

00:16:05,130 --> 00:16:08,490
original sort of diagram this is these

00:16:06,840 --> 00:16:11,400
are the sort of interactions of the two

00:16:08,490 --> 00:16:14,190
components the sidecar queries the proxy

00:16:11,400 --> 00:16:15,510
to say hey what hey server what targets

00:16:14,190 --> 00:16:18,030
do you have available for me to scrape

00:16:15,510 --> 00:16:19,770
the sidecar also generates the

00:16:18,030 --> 00:16:23,370
configurations of the Prometheus users

00:16:19,770 --> 00:16:25,560
and Prometheus then calls the proxied

00:16:23,370 --> 00:16:28,170
endpoints via the proxy so what we're

00:16:25,560 --> 00:16:29,820
exposing externally and then that takes

00:16:28,170 --> 00:16:32,370
care of it getting routed to the actual

00:16:29,820 --> 00:16:34,830
exporter running on the machine so let's

00:16:32,370 --> 00:16:38,760
go another level deeper what does the

00:16:34,830 --> 00:16:40,770
sidecar do so sidecar process runs on

00:16:38,760 --> 00:16:43,560
Prometheus runs on a Prometheus server

00:16:40,770 --> 00:16:45,420
tiny little go daemon first thing it

00:16:43,560 --> 00:16:47,100
does is our configure our sort of global

00:16:45,420 --> 00:16:48,540
configuration systems called config Lee

00:16:47,100 --> 00:16:51,330
because everything it fastly ends with

00:16:48,540 --> 00:16:54,030
an ly it's part of the part of the

00:16:51,330 --> 00:16:55,590
onboarding requirement so config Lee is

00:16:54,030 --> 00:16:57,300
the runtime configuration for every

00:16:55,590 --> 00:16:59,730
service that we run and every machine

00:16:57,300 --> 00:17:01,560
that we run so what promise to this side

00:16:59,730 --> 00:17:03,210
counts sorry promise D is the internal

00:17:01,560 --> 00:17:06,350
name of this this service discovery

00:17:03,210 --> 00:17:10,080
system very imaginative it queries

00:17:06,350 --> 00:17:12,089
config Lee it says ok I'm SJC tell me

00:17:10,080 --> 00:17:12,990
the IP addresses of every device in in

00:17:12,089 --> 00:17:16,350
SJC

00:17:12,990 --> 00:17:18,900
once it has that it then iterates over

00:17:16,350 --> 00:17:21,060
those and it asks the proxy that's

00:17:18,900 --> 00:17:23,400
running on each machine which is the

00:17:21,060 --> 00:17:25,320
other component of this tell me all the

00:17:23,400 --> 00:17:26,910
targets that you know about so we sort

00:17:25,320 --> 00:17:29,010
of have these two data sources now here

00:17:26,910 --> 00:17:30,570
all the IP addresses for each IP address

00:17:29,010 --> 00:17:31,890
we have the list of all of the targets

00:17:30,570 --> 00:17:35,190
that are available on that on that

00:17:31,890 --> 00:17:37,050
system from there it generates a

00:17:35,190 --> 00:17:39,480
standard Prometheus file service

00:17:37,050 --> 00:17:41,730
discovery JSON file with the with the

00:17:39,480 --> 00:17:44,040
sort of compiled configuration so in

00:17:41,730 --> 00:17:46,410
this case here are the targets here are

00:17:44,040 --> 00:17:49,530
the routes to the here are the routes to

00:17:46,410 --> 00:17:52,169
the various exporters that traverse via

00:17:49,530 --> 00:17:54,239
the proxy and

00:17:52,169 --> 00:17:55,559
and then from there Prometheus reads

00:17:54,239 --> 00:17:58,379
that file and that's our target

00:17:55,559 --> 00:18:01,499
configuration this whole process kind of

00:17:58,379 --> 00:18:02,909
runs polling every 30 seconds and then

00:18:01,499 --> 00:18:06,179
when that JSON file gets updated

00:18:02,909 --> 00:18:08,340
prometheus just updates with with the

00:18:06,179 --> 00:18:10,109
the new configuration so if we were to

00:18:08,340 --> 00:18:11,700
go and roll out a new new host in this

00:18:10,109 --> 00:18:13,980
environment it'll have a new IP address

00:18:11,700 --> 00:18:15,539
it'll get picked up and magically it's

00:18:13,980 --> 00:18:17,279
there without any sort of operator

00:18:15,539 --> 00:18:19,440
intervention aside from our standard

00:18:17,279 --> 00:18:24,600
sort of build and deploy process that

00:18:19,440 --> 00:18:25,919
adds that system into config Li so now

00:18:24,600 --> 00:18:28,230
that we've talked about the proxy let's

00:18:25,919 --> 00:18:30,059
see how the proxy works so this is the

00:18:28,230 --> 00:18:33,259
other component the proxy promise T

00:18:30,059 --> 00:18:36,239
proxy runs on every one of our servers

00:18:33,259 --> 00:18:37,679
so what it does first is we we're sort

00:18:36,239 --> 00:18:39,600
of using system D for everything you

00:18:37,679 --> 00:18:40,499
just got to embrace the overlords and be

00:18:39,600 --> 00:18:43,169
done with it

00:18:40,499 --> 00:18:45,179
so we make d-bus queries to system D and

00:18:43,169 --> 00:18:47,059
we ask system D tell me every service

00:18:45,179 --> 00:18:50,399
that you have running on the machine

00:18:47,059 --> 00:18:51,809
from that we then do a call to config Li

00:18:50,399 --> 00:18:53,970
to say what's the runtime configuration

00:18:51,809 --> 00:18:56,340
for every single one of those services

00:18:53,970 --> 00:18:58,049
that you have what we did in our

00:18:56,340 --> 00:19:00,149
configuration sort of schemer as we

00:18:58,049 --> 00:19:01,980
added an additional map called

00:19:00,149 --> 00:19:04,169
Prometheus properties and in Prometheus

00:19:01,980 --> 00:19:06,179
properties you can specify the target so

00:19:04,169 --> 00:19:07,919
that's the local address on the machine

00:19:06,179 --> 00:19:10,230
for web exporters running so if you use

00:19:07,919 --> 00:19:12,720
node exporter as an example here it's

00:19:10,230 --> 00:19:15,570
listening on localhost 9100 so that

00:19:12,720 --> 00:19:17,639
tells the system that's where where to

00:19:15,570 --> 00:19:19,259
expect to find it that could also be a

00:19:17,639 --> 00:19:20,639
UNIX domain socket which is something

00:19:19,259 --> 00:19:23,190
we're starting to look at and doing

00:19:20,639 --> 00:19:25,739
things like abstract kernel namespaces

00:19:23,190 --> 00:19:27,600
to even hide the socket if you want and

00:19:25,739 --> 00:19:29,549
as part of the prometheus properties you

00:19:27,600 --> 00:19:31,499
can also specify additional labels you

00:19:29,549 --> 00:19:33,600
might want to override the local metrics

00:19:31,499 --> 00:19:35,009
path and things like that so you've sort

00:19:33,600 --> 00:19:37,019
of got a lot of flexibility in how you

00:19:35,009 --> 00:19:38,519
do that but that sort of describes the

00:19:37,019 --> 00:19:41,369
relationship between the system D

00:19:38,519 --> 00:19:44,159
process and how its configured and so

00:19:41,369 --> 00:19:46,679
from that if a user deploys or an

00:19:44,159 --> 00:19:48,539
engineer deploys a new service it pops

00:19:46,679 --> 00:19:50,309
up in system D if it's got a Prometheus

00:19:48,539 --> 00:19:53,279
properties the promise T the proxy knows

00:19:50,309 --> 00:19:55,440
about it and then that exposes the API

00:19:53,279 --> 00:19:58,980
that's consumed by both the sidecar and

00:19:55,440 --> 00:20:01,830
also by Prometheus and from that API the

00:19:58,980 --> 00:20:04,320
target endpoint that sidecar requests

00:20:01,830 --> 00:20:06,020
will give the list of all of the targets

00:20:04,320 --> 00:20:08,930
that are available on that

00:20:06,020 --> 00:20:10,670
host and then from that prometheus will

00:20:08,930 --> 00:20:14,180
use the what we do is we have these

00:20:10,670 --> 00:20:16,760
computed paths so for instance slash

00:20:14,180 --> 00:20:21,680
node exporter underscored 9100 slash

00:20:16,760 --> 00:20:24,710
metrics is the proxy side that routes to

00:20:21,680 --> 00:20:26,300
the local the local export a running on

00:20:24,710 --> 00:20:28,400
the machine it's a little bit confusing

00:20:26,300 --> 00:20:31,850
but it's slightly simpler in practice

00:20:28,400 --> 00:20:34,250
but it's working pretty well so now that

00:20:31,850 --> 00:20:36,940
we have those two pieces in place did it

00:20:34,250 --> 00:20:39,830
actually achieve what we hoped so yes

00:20:36,940 --> 00:20:42,370
it's really easy to leverage the the

00:20:39,830 --> 00:20:44,840
file service discovery mechanism to

00:20:42,370 --> 00:20:46,550
whatever function that you need there's

00:20:44,840 --> 00:20:48,920
a blog post recently about using it to

00:20:46,550 --> 00:20:49,970
do some docker swarm stuff as long as

00:20:48,920 --> 00:20:52,070
you can generate the file in the right

00:20:49,970 --> 00:20:54,860
format it just works so that was really

00:20:52,070 --> 00:20:56,780
easy and really fun new targets for

00:20:54,860 --> 00:20:58,370
engineers all they need to do when they

00:20:56,780 --> 00:21:00,890
deploy their services add that single

00:20:58,370 --> 00:21:02,420
Prometheus properties line so if I spin

00:21:00,890 --> 00:21:05,030
up a new service and it's listening for

00:21:02,420 --> 00:21:06,620
metrics on one two three four we just

00:21:05,030 --> 00:21:09,320
add that line and it will roll out

00:21:06,620 --> 00:21:11,240
magically to the fleet we have TLS and

00:21:09,320 --> 00:21:12,560
authentication everywhere so every

00:21:11,240 --> 00:21:14,330
connection from every system that

00:21:12,560 --> 00:21:16,700
Prometheus touches either an X either

00:21:14,330 --> 00:21:18,950
Prometheus to an exporter or a query to

00:21:16,700 --> 00:21:21,740
Prometheus is mutually authenticated and

00:21:18,950 --> 00:21:23,660
over TLS and we have a single exporter

00:21:21,740 --> 00:21:26,060
port per host which is a bit of a

00:21:23,660 --> 00:21:28,550
catch-22 in some cases because now if

00:21:26,060 --> 00:21:31,790
promised T proxy goes down so do all

00:21:28,550 --> 00:21:34,180
your targets which unfortunately a few

00:21:31,790 --> 00:21:36,410
diverse areas because hello systemd

00:21:34,180 --> 00:21:37,850
they will occasionally hang but it's a

00:21:36,410 --> 00:21:41,680
really great proxy for the machine is

00:21:37,850 --> 00:21:44,390
down cool so now we've done that

00:21:41,680 --> 00:21:45,680
whirlwind tour of getting things out so

00:21:44,390 --> 00:21:47,840
what does Prometheus adoption look like

00:21:45,680 --> 00:21:50,420
it fastly so far so we started this

00:21:47,840 --> 00:21:52,700
project in November 2017 and sort of

00:21:50,420 --> 00:21:54,560
been working our way through we've got

00:21:52,700 --> 00:21:57,890
probably about 70% of our infrastructure

00:21:54,560 --> 00:21:59,870
fully Promethea ties so what are the

00:21:57,890 --> 00:22:01,340
sort of some of the stats so we have a

00:21:59,870 --> 00:22:03,980
hundred and fourteen servers globally

00:22:01,340 --> 00:22:05,690
that's growing there's many more to come

00:22:03,980 --> 00:22:07,340
we have around about twenty eight point

00:22:05,690 --> 00:22:09,680
four million time series we're still

00:22:07,340 --> 00:22:11,840
instrumenting a bunch of applications at

00:22:09,680 --> 00:22:14,330
this time as well we're doing about 2.2

00:22:11,840 --> 00:22:17,360
million samples per second on the

00:22:14,330 --> 00:22:18,860
developer side there was a little bit of

00:22:17,360 --> 00:22:22,010
hesitation

00:22:18,860 --> 00:22:24,110
so one of our data science folks did a

00:22:22,010 --> 00:22:26,809
lot of work with our SAS product to pull

00:22:24,110 --> 00:22:28,700
metrics out and I just introduced him to

00:22:26,809 --> 00:22:30,590
Prometheus and giving him his API keys

00:22:28,700 --> 00:22:32,390
and he was ready to go and he sent me

00:22:30,590 --> 00:22:33,769
this slack and he said I'm still looking

00:22:32,390 --> 00:22:35,090
at the functions available previous and

00:22:33,769 --> 00:22:38,299
it seems even more constrained than

00:22:35,090 --> 00:22:41,090
vendor he'd been burned by every system

00:22:38,299 --> 00:22:42,799
at this point and then I said here's the

00:22:41,090 --> 00:22:44,000
awesome documentation go and have a look

00:22:42,799 --> 00:22:47,029
if you're still having problems let me

00:22:44,000 --> 00:22:49,179
know and a few hours later he sends me a

00:22:47,029 --> 00:22:51,649
tweet it's much better than vendor and

00:22:49,179 --> 00:22:54,950
so much faster and he's been loving it

00:22:51,649 --> 00:22:56,539
he's one of our sort of biggest adopters

00:22:54,950 --> 00:22:58,880
I think he wrote something like 800

00:22:56,539 --> 00:23:04,399
recording rules the other day in one go

00:22:58,880 --> 00:23:05,690
so so PR yeah looks fine no problem what

00:23:04,399 --> 00:23:06,950
we were always saying well what was

00:23:05,690 --> 00:23:08,750
getting mentioned yesterday about

00:23:06,950 --> 00:23:09,850
sub-queries yeah that would be really

00:23:08,750 --> 00:23:14,360
helpful

00:23:09,850 --> 00:23:15,740
so overall engineers love it everyone

00:23:14,360 --> 00:23:17,090
that we've worked with has just loved

00:23:15,740 --> 00:23:19,850
instrumenting their applications they've

00:23:17,090 --> 00:23:21,320
found it super simple and we've covered

00:23:19,850 --> 00:23:23,299
all the languages we even have Perl

00:23:21,320 --> 00:23:25,880
applications that are that are in

00:23:23,299 --> 00:23:27,830
pushing metrics into Prometheus as well

00:23:25,880 --> 00:23:29,289
- Porton alert quality's increased and

00:23:27,830 --> 00:23:32,120
that was sort of one of the one of the

00:23:29,289 --> 00:23:33,950
goals of our cultural transformation is

00:23:32,120 --> 00:23:35,990
to be not be accepting of just alert

00:23:33,950 --> 00:23:37,220
noise and to spend the time because we

00:23:35,990 --> 00:23:37,909
were moving to this new system that

00:23:37,220 --> 00:23:40,059
required

00:23:37,909 --> 00:23:42,289
Riaan strim n ting applications and

00:23:40,059 --> 00:23:43,970
rebuilding a rebuilding dashboards and

00:23:42,289 --> 00:23:45,860
alerts to really think like what are the

00:23:43,970 --> 00:23:48,880
most meaningful metrics I need for my

00:23:45,860 --> 00:23:51,559
application so they've increased tenfold

00:23:48,880 --> 00:23:53,600
prom QL is enabled us to get some really

00:23:51,559 --> 00:23:54,889
deep insights like the expressiveness of

00:23:53,600 --> 00:23:58,100
the languages let us do some pretty

00:23:54,889 --> 00:24:00,169
crazy things I didn't sleep much last

00:23:58,100 --> 00:24:03,139
week with the linux tcp kernel

00:24:00,169 --> 00:24:06,409
vulnerability that got revealed and as

00:24:03,139 --> 00:24:08,120
we were testing the exploits and testing

00:24:06,409 --> 00:24:10,340
our mitigations we were able to build

00:24:08,120 --> 00:24:13,010
dashboards and alerts as we were going

00:24:10,340 --> 00:24:14,269
to validate what we were seeing to the

00:24:13,010 --> 00:24:16,370
point that within you know less than

00:24:14,269 --> 00:24:18,799
half a day we had alerting per our whole

00:24:16,370 --> 00:24:20,269
infrastructure had tested the exploit

00:24:18,799 --> 00:24:21,710
against our infrastructure to know that

00:24:20,269 --> 00:24:23,870
those alerts would fire if someone did

00:24:21,710 --> 00:24:25,519
come at us and then as the as the

00:24:23,870 --> 00:24:27,230
mitigations rolled across the fleet we

00:24:25,519 --> 00:24:29,960
could see the improvements and the

00:24:27,230 --> 00:24:31,610
progression of kernel rolls and cisco

00:24:29,960 --> 00:24:32,570
tool changes and some custom modules

00:24:31,610 --> 00:24:36,349
that would written

00:24:32,570 --> 00:24:38,570
and ultimately what it had scaled

00:24:36,349 --> 00:24:40,639
Lenalee without infrastructure growth so

00:24:38,570 --> 00:24:42,289
having those building blocks of those

00:24:40,639 --> 00:24:44,059
two prometheus instances in every pop

00:24:42,289 --> 00:24:45,169
they're really simple to roll out as we

00:24:44,059 --> 00:24:47,419
roll out more pops and more

00:24:45,169 --> 00:24:49,070
infrastructure it just grows with us so

00:24:47,419 --> 00:24:50,659
we're not running into the situation of

00:24:49,070 --> 00:24:52,699
having this single aggregation point

00:24:50,659 --> 00:24:54,979
that's just getting slammed as we add

00:24:52,699 --> 00:24:56,419
more and more infrastructure and the

00:24:54,979 --> 00:24:58,190
nice part about it is our pops are

00:24:56,419 --> 00:24:59,839
actually relatively small so we're

00:24:58,190 --> 00:25:01,399
getting nowhere near five million

00:24:59,839 --> 00:25:04,309
metrics per second on a single machine

00:25:01,399 --> 00:25:05,359
so we have a Headroom for days so our

00:25:04,309 --> 00:25:08,929
machines don't need to be that big

00:25:05,359 --> 00:25:11,059
crooning Prometheus but there are still

00:25:08,929 --> 00:25:13,039
some rough edges and the first one is

00:25:11,059 --> 00:25:14,749
one that I kind of I guess proposed to

00:25:13,039 --> 00:25:18,679
the community it'd be good to discuss at

00:25:14,749 --> 00:25:20,539
some point is if you want to do ad hoc

00:25:18,679 --> 00:25:23,419
metric exploration so we've kind of

00:25:20,539 --> 00:25:25,339
termed it bread bread the first search

00:25:23,419 --> 00:25:27,559
for for issues

00:25:25,339 --> 00:25:30,049
so in ganglia what we would have quite

00:25:27,559 --> 00:25:31,729
commonly is something's gone wrong with

00:25:30,049 --> 00:25:35,509
a machine it's not immediately clear

00:25:31,729 --> 00:25:37,609
what it is I can tell Grant ganglia to

00:25:35,509 --> 00:25:39,739
render all 2,000 grafts on one page for

00:25:37,609 --> 00:25:41,989
me and then uses some mouse scroll wheel

00:25:39,739 --> 00:25:45,049
debugging ability to look for which

00:25:41,989 --> 00:25:46,429
things lined up at the same time and

00:25:45,049 --> 00:25:48,440
that's actually really easy and that's

00:25:46,429 --> 00:25:50,539
one of the workflows that happens a lot

00:25:48,440 --> 00:25:52,159
of fastly because we have a lot of sort

00:25:50,539 --> 00:25:54,259
of complexity and crazy stuff going on

00:25:52,159 --> 00:25:56,809
so if you've ever tried to plot 2,000

00:25:54,259 --> 00:25:59,809
graphs in graph on whether like a

00:25:56,809 --> 00:26:01,519
hundred Meg of days on if you want to

00:25:59,809 --> 00:26:03,529
crash chrome real quick that's a great

00:26:01,519 --> 00:26:05,359
thing so if you don't have that sort of

00:26:03,529 --> 00:26:07,729
a priori knowledge knowledge of what you

00:26:05,359 --> 00:26:09,469
want it would be really great to talk to

00:26:07,729 --> 00:26:11,449
anyone if they've sort of had this same

00:26:09,469 --> 00:26:12,949
problem or have any ideas of how you can

00:26:11,449 --> 00:26:15,259
sort of say just show me all these

00:26:12,949 --> 00:26:16,669
metrics at once and and let's kind of

00:26:15,259 --> 00:26:18,589
figure it out we've we've got all sort

00:26:16,669 --> 00:26:20,419
of a rough prototype of just a rendering

00:26:18,589 --> 00:26:22,369
tool that does client-side rendering

00:26:20,419 --> 00:26:24,499
you're sort of bringing the 90s back

00:26:22,369 --> 00:26:26,719
which is kind of cool like if those

00:26:24,499 --> 00:26:29,529
clients are a server-side rendering for

00:26:26,719 --> 00:26:32,119
graph are now that would be sort of cool

00:26:29,529 --> 00:26:32,929
yeah so that that that's a bit of an

00:26:32,119 --> 00:26:35,179
interesting one

00:26:32,929 --> 00:26:36,799
alert managers flexibility I don't know

00:26:35,179 --> 00:26:39,769
if it's possible for a system to be too

00:26:36,799 --> 00:26:41,749
flexible but when we have hundreds of

00:26:39,769 --> 00:26:43,639
Engineers and dozens of teams all

00:26:41,749 --> 00:26:45,529
writing alerts though routing trees get

00:26:43,639 --> 00:26:46,410
terrifying and your ability to

00:26:45,529 --> 00:26:49,410
completely

00:26:46,410 --> 00:26:52,290
over another team becomes really really

00:26:49,410 --> 00:26:53,790
a reality so that's a little bit of a

00:26:52,290 --> 00:26:55,350
challenge and we've sort of abstracted a

00:26:53,790 --> 00:26:57,780
little bit around that to try and avoid

00:26:55,350 --> 00:26:59,880
things which sadly has meant we've had

00:26:57,780 --> 00:27:03,300
to kind of like chop some features off

00:26:59,880 --> 00:27:05,040
from people because we can't risk the

00:27:03,300 --> 00:27:07,800
impact that their changes might have and

00:27:05,040 --> 00:27:09,690
secondarily to that the alert manager

00:27:07,800 --> 00:27:11,820
default search interface is fantastic

00:27:09,690 --> 00:27:13,440
but a lot of users just want to see

00:27:11,820 --> 00:27:15,450
something sort of similar to nag Lite

00:27:13,440 --> 00:27:17,310
which is just show me a list of red and

00:27:15,450 --> 00:27:19,950
green of all the things that are firing

00:27:17,310 --> 00:27:21,990
or not firing and maybe some filters to

00:27:19,950 --> 00:27:24,990
be able to do that so I think with a

00:27:21,990 --> 00:27:26,520
with with a sort of a v1 API for alert

00:27:24,990 --> 00:27:31,170
manager we could maybe fix a lot of that

00:27:26,520 --> 00:27:34,200
stuff Federation global views huge topic

00:27:31,170 --> 00:27:36,810
typically Federation comes up when

00:27:34,200 --> 00:27:40,230
someone someone at fastly will come and

00:27:36,810 --> 00:27:43,680
say hey I can't find this metric on the

00:27:40,230 --> 00:27:45,750
global instance and it's like yeah it's

00:27:43,680 --> 00:27:46,980
not federated and usually when they ask

00:27:45,750 --> 00:27:49,830
that question they wanted to see it

00:27:46,980 --> 00:27:51,570
federated an hour ago so the opt-in

00:27:49,830 --> 00:27:55,320
nature of it kind of gets a little bit

00:27:51,570 --> 00:27:56,700
tricky and having that global view it's

00:27:55,320 --> 00:27:58,380
sort of like a bit of a trade off like

00:27:56,700 --> 00:27:59,850
when we're querying hundreds of

00:27:58,380 --> 00:28:01,440
Prometheus instances and want to have

00:27:59,850 --> 00:28:02,790
that aggregated view you sort of got a

00:28:01,440 --> 00:28:05,700
little bit of pre thought up front which

00:28:02,790 --> 00:28:07,080
is in itself not that bad but but yeah

00:28:05,700 --> 00:28:09,030
if first for some users it's a bit of a

00:28:07,080 --> 00:28:10,680
struggle and long-term storage is a bit

00:28:09,030 --> 00:28:12,240
of an open question so when we started

00:28:10,680 --> 00:28:14,430
the project we knew this was the case

00:28:12,240 --> 00:28:15,690
and we made a sort of a service level

00:28:14,430 --> 00:28:16,950
agreement with the company that said

00:28:15,690 --> 00:28:19,230
we're going to cut keep somewhere

00:28:16,950 --> 00:28:20,730
between 60 and 90 days online and after

00:28:19,230 --> 00:28:23,100
that you know it's all about real-time

00:28:20,730 --> 00:28:25,110
operational telemetry not long-term

00:28:23,100 --> 00:28:26,940
trending but now people have got a taste

00:28:25,110 --> 00:28:29,070
they go oh I'd love to see this in a

00:28:26,940 --> 00:28:31,290
year view or whatever so we're currently

00:28:29,070 --> 00:28:34,220
evaluating thinnest and super excited

00:28:31,290 --> 00:28:37,290
about that thank you for building it so

00:28:34,220 --> 00:28:41,010
ultimately fastly loves Prometheus

00:28:37,290 --> 00:28:42,510
Prometheus also loves fastly and the

00:28:41,010 --> 00:28:44,940
whole project has been like a raging

00:28:42,510 --> 00:28:47,820
success so far I couldn't have asked for

00:28:44,940 --> 00:28:49,410
anything better so huge thanks to the

00:28:47,820 --> 00:28:51,030
whole Prometheus community and the

00:28:49,410 --> 00:28:53,100
developers and everyone and the graph on

00:28:51,030 --> 00:28:54,600
a team that have really given us

00:28:53,100 --> 00:28:55,770
something that's let us solve some of

00:28:54,600 --> 00:28:57,900
these problems that we've just had to

00:28:55,770 --> 00:28:58,330
like struggle through for the last 10 or

00:28:57,900 --> 00:29:08,950
00:28:58,330 --> 00:29:11,760
years so thanks thank you Marcus we have

00:29:08,950 --> 00:29:16,360
a couple of minutes left for questions

00:29:11,760 --> 00:29:19,809
hi hi question where do you add labels

00:29:16,360 --> 00:29:21,940
to your things where we had labels so

00:29:19,809 --> 00:29:23,380
from which dimension so if your

00:29:21,940 --> 00:29:25,570
instrumenting your application you can

00:29:23,380 --> 00:29:28,240
add labels there we don't do any label

00:29:25,570 --> 00:29:30,340
addition specifically okay because you

00:29:28,240 --> 00:29:32,919
add the target and then how you know

00:29:30,340 --> 00:29:35,889
where the target is in which all those

00:29:32,919 --> 00:29:37,360
labels pass through so as far as the

00:29:35,889 --> 00:29:39,130
exporters concerned it's just being

00:29:37,360 --> 00:29:40,929
called locally we sort of do a bit of

00:29:39,130 --> 00:29:43,000
mutation with the metrics paths and

00:29:40,929 --> 00:29:44,919
things too to be able to get around that

00:29:43,000 --> 00:29:46,240
so for the most part it feels very much

00:29:44,919 --> 00:29:48,399
like you're just talking on a native

00:29:46,240 --> 00:29:52,740
system and you don't know that that sort

00:29:48,399 --> 00:29:52,740
of sidecar and proxy layer exists at all

00:29:54,000 --> 00:29:59,830
hey you mentioned that with a lot of

00:29:58,480 --> 00:30:02,139
people using alert managers

00:29:59,830 --> 00:30:03,460
configurations it's possible to mess

00:30:02,139 --> 00:30:06,429
things up for other people without

00:30:03,460 --> 00:30:08,860
realizing it or anticipating it can you

00:30:06,429 --> 00:30:10,179
give a slightly more concrete example

00:30:08,860 --> 00:30:12,159
I'm just kind of curious what kind of

00:30:10,179 --> 00:30:13,690
things might might come up yeah so it's

00:30:12,159 --> 00:30:15,490
kind of like that full through behavior

00:30:13,690 --> 00:30:16,990
so at the moment we have a sort of a

00:30:15,490 --> 00:30:18,970
structure where teams are kind of

00:30:16,990 --> 00:30:20,740
allocated a special name and that map's

00:30:18,970 --> 00:30:23,169
to their slack channels and it maps to

00:30:20,740 --> 00:30:24,639
their page duties and what we've had is

00:30:23,169 --> 00:30:26,740
we've had engineers who have said well I

00:30:24,639 --> 00:30:29,679
want this alert to go to more than one

00:30:26,740 --> 00:30:30,940
destination so if you're listing a slack

00:30:29,679 --> 00:30:32,500
channel if you want to go to multiple

00:30:30,940 --> 00:30:36,309
slack channels it needs to be multiple

00:30:32,500 --> 00:30:37,840
sort of routes for it to get there so at

00:30:36,309 --> 00:30:39,820
the moment we have like a single sort of

00:30:37,840 --> 00:30:41,590
a single set of global routes and once

00:30:39,820 --> 00:30:42,850
it once it matches one's one it bails

00:30:41,590 --> 00:30:45,669
out but if you want to fall through

00:30:42,850 --> 00:30:47,889
there's the potential that you may have

00:30:45,669 --> 00:30:49,450
other alerts it's yeah I mean it's a

00:30:47,889 --> 00:30:51,429
yeah that's I mean one of the immediate

00:30:49,450 --> 00:30:53,169
ones that we've run into but even trying

00:30:51,429 --> 00:30:55,269
to kind of understand and grok how it

00:30:53,169 --> 00:31:01,269
all hangs together is you know it takes

00:30:55,269 --> 00:31:03,970
a little bit of work okay yes as a happy

00:31:01,269 --> 00:31:08,559
you vastly customer is there a plan to

00:31:03,970 --> 00:31:11,200
export metrics love to would love to one

00:31:08,559 --> 00:31:12,190
of our our core varnish engineers which

00:31:11,200 --> 00:31:14,409
is like that sort of

00:31:12,190 --> 00:31:17,740
underlying technology that we have he's

00:31:14,409 --> 00:31:18,909
one of our number one customers so we're

00:31:17,740 --> 00:31:21,669
looking at building for ourselves

00:31:18,909 --> 00:31:23,409
building in native prometheus exposition

00:31:21,669 --> 00:31:24,850
but yeah it would be pretty awesome too

00:31:23,409 --> 00:31:37,960
we're definitely thinking about it no

00:31:24,850 --> 00:31:40,029
promises thank you because it's yeah

00:31:37,960 --> 00:31:42,070
totally totally there's there's a bunch

00:31:40,029 --> 00:31:43,629
of exporter exporter type projects that

00:31:42,070 --> 00:31:45,250
do allow you to do the TLS and things

00:31:43,629 --> 00:31:46,539
but the key for us as we needed to be

00:31:45,250 --> 00:31:49,029
able to tie into our configuration

00:31:46,539 --> 00:31:50,679
system and so like the the actual

00:31:49,029 --> 00:31:53,019
service discovery promise D stuff we

00:31:50,679 --> 00:31:55,120
wrote it's like 150 lines ago it's not

00:31:53,019 --> 00:31:56,440
particularly complicated but we are

00:31:55,120 --> 00:32:02,200
having to make a few adaptions

00:31:56,440 --> 00:32:05,919
specifically to our environment my

00:32:02,200 --> 00:32:14,320
question is I could see the two servers

00:32:05,919 --> 00:32:16,629
yeah so what we do with the a and B is

00:32:14,320 --> 00:32:18,850
that we have nginx configured with the

00:32:16,629 --> 00:32:21,009
nginx upstream with two hosts and it's

00:32:18,850 --> 00:32:23,950
just failover so if a disappears it'll

00:32:21,009 --> 00:32:25,149
fall over failover to B because what we

00:32:23,950 --> 00:32:26,710
didn't want to do with the load

00:32:25,149 --> 00:32:28,120
balancing is because you have that sort

00:32:26,710 --> 00:32:29,320
of the scrape intervals can get slightly

00:32:28,120 --> 00:32:30,730
out of whack you know on those graphs

00:32:29,320 --> 00:32:33,429
are like oscillate if you're getting

00:32:30,730 --> 00:32:35,470
slightly out of time metrics so yeah we

00:32:33,429 --> 00:32:37,600
biased the a we've got plenty of CPU and

00:32:35,470 --> 00:32:41,639
disk and Headroom to do that and then

00:32:37,600 --> 00:32:41,639
should it fail nginx fails over to the B

00:32:42,779 --> 00:32:46,269
comment and question so first of all

00:32:44,889 --> 00:32:47,740
great talk really enjoyed it so in

00:32:46,269 --> 00:32:49,389
comment is I just wanted to thank you

00:32:47,740 --> 00:32:51,850
guys for powering all the downloads for

00:32:49,389 --> 00:32:54,190
the graph on open source project and the

00:32:51,850 --> 00:32:56,320
question is you know you mentioned after

00:32:54,190 --> 00:32:57,789
going through sort of your on-prem sort

00:32:56,320 --> 00:33:00,730
of monitoring solution one on pram

00:32:57,789 --> 00:33:02,379
monitoring solution to SAS that it was

00:33:00,730 --> 00:33:04,210
sort of a foregone conclusion that you

00:33:02,379 --> 00:33:06,279
wanted to go back to on-prem just

00:33:04,210 --> 00:33:08,049
curious was that a philosophical

00:33:06,279 --> 00:33:10,389
decision or was it more driven by just

00:33:08,049 --> 00:33:12,519
the irrationality of the SAS pricing oh

00:33:10,389 --> 00:33:16,450
the long question or the long answer the

00:33:12,519 --> 00:33:18,639
short answer all of the above so there

00:33:16,450 --> 00:33:20,080
were I mean once you've used prom QL

00:33:18,639 --> 00:33:21,990
it's really hard to use any other SAS

00:33:20,080 --> 00:33:25,269
product like that's that's a bit of a

00:33:21,990 --> 00:33:25,600
starter at a scale and the amount of

00:33:25,269 --> 00:33:27,760
cost

00:33:25,600 --> 00:33:29,530
metrics and things that we export the

00:33:27,760 --> 00:33:32,559
price does become like a realistic

00:33:29,530 --> 00:33:34,030
factor and the other factor was and we

00:33:32,559 --> 00:33:36,549
sort of ran into this a few times is

00:33:34,030 --> 00:33:40,419
that there's the who owns your

00:33:36,549 --> 00:33:43,030
availability comm and if you are relying

00:33:40,419 --> 00:33:45,280
on a third party as your eyes into your

00:33:43,030 --> 00:33:47,140
infrastructure and you're powering the

00:33:45,280 --> 00:33:51,010
infrastructure that that's SS vendor

00:33:47,140 --> 00:33:52,929
might possibly use you end up in a

00:33:51,010 --> 00:33:54,610
situation where they go down you can't

00:33:52,929 --> 00:33:56,559
see it you don't know it you can't bring

00:33:54,610 --> 00:33:58,600
them back up so you kind of end up in

00:33:56,559 --> 00:34:00,880
this crazy loop so it's a little bit of

00:33:58,600 --> 00:34:02,380
all and it's some this the SAS products

00:34:00,880 --> 00:34:04,210
are amazing at what they're able to do

00:34:02,380 --> 00:34:06,640
now but for us it was just a better fit

00:34:04,210 --> 00:34:08,080
to bring it back in-house and it's so

00:34:06,640 --> 00:34:11,020
cheap to run now like given how

00:34:08,080 --> 00:34:14,429
lightweight Prometheus's thank you

00:34:11,020 --> 00:34:19,399
Marcus thank you thanks Aaron

00:34:14,429 --> 00:34:19,399

YouTube URL: https://www.youtube.com/watch?v=ouLY973Ld24


