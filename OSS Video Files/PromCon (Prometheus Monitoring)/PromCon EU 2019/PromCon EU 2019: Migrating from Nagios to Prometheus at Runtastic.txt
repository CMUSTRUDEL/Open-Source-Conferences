Title: PromCon EU 2019: Migrating from Nagios to Prometheus at Runtastic
Publication date: 2019-12-29
Playlist: PromCon EU 2019
Description: 
	Speaker: Niko Dominkowitsch

Thinking of replacing your existing monitoring solution with Prometheus? In this talk we will show you how Runtastic moved its monitoring and alerting stack from Nagios to Prometheus. We'll show you how we managed to do this, how monitoring and alerting works with Prometheus and what challenges we were facing.

We will cover these topics:

    How our journey started
    Using Consul for Service Discovery
    Using Terraform for managing health checks and silences for new micro services
    Building a pipeline to test and deploy alerting and recording rules automatically
    Using OpsGenie for Alert management
    Examples of common checks in Nagios and how we implemented them with Prometheus
    Examples of custom written Exporters
    Lessons Learned



Slides: https://promcon.io/2019-munich/slides/migrating-from-nagios-to-prometheus-at-runtastic.pdf
Captions: 
	00:00:00,650 --> 00:00:08,910
[Music]

00:00:09,650 --> 00:00:14,540
so hello good morning everyone after all

00:00:12,170 --> 00:00:18,170
not to present you the first talkie at

00:00:14,540 --> 00:00:19,880
the prom come well my name is Nikhil I'm

00:00:18,170 --> 00:00:22,700
from Mont a stick and together with my

00:00:19,880 --> 00:00:24,529
colleagues and managing maintaining

00:00:22,700 --> 00:00:28,070
automating our production infrastructure

00:00:24,529 --> 00:00:30,260
at runtastic and I want to show tell you

00:00:28,070 --> 00:00:34,220
some story about us how we managed to

00:00:30,260 --> 00:00:36,590
make it on monitoring to Prometheus from

00:00:34,220 --> 00:00:37,940
mag wheels just as a quick overview of

00:00:36,590 --> 00:00:41,540
what their own testing infrastructure

00:00:37,940 --> 00:00:43,280
looks like so basically we are mostly

00:00:41,540 --> 00:00:45,110
open source to research a lot of open

00:00:43,280 --> 00:00:47,660
source technology in our production

00:00:45,110 --> 00:00:49,130
infrastructure most of Linux based and

00:00:47,660 --> 00:00:53,079
they have a lot of virtualization at the

00:00:49,130 --> 00:00:56,660
moment KVM based at the moment we have

00:00:53,079 --> 00:00:58,700
around 40 thousand CPU cores so for

00:00:56,660 --> 00:01:01,309
thousands because I'm sorry 20 terabytes

00:00:58,700 --> 00:01:04,460
of memory in total and 100 terabytes of

00:01:01,309 --> 00:01:06,530
storage provided by our self cluster so

00:01:04,460 --> 00:01:09,320
all of them is managed broke neighbor on

00:01:06,530 --> 00:01:11,869
and we have also some kind of databases

00:01:09,320 --> 00:01:14,690
here MongoDB my sequel Cassandra also

00:01:11,869 --> 00:01:16,910
RabbitMQ so we have to take care of a

00:01:14,690 --> 00:01:19,490
lot of stuff and therefore else I have

00:01:16,910 --> 00:01:22,459
to automate a lot of things we have

00:01:19,490 --> 00:01:24,530
Cisco is a 40% the networking which were

00:01:22,459 --> 00:01:26,660
also responsible of and we will share

00:01:24,530 --> 00:01:29,270
for bootstrapping on machines and

00:01:26,660 --> 00:01:32,660
setting up all the servers and use

00:01:29,270 --> 00:01:35,780
terraform for infrastructure as code in

00:01:32,660 --> 00:01:37,759
the past at fantastic we grew a lot so

00:01:35,780 --> 00:01:42,080
we had a lot of micro services we have

00:01:37,759 --> 00:01:43,910
70 microservices life and yeah we had a

00:01:42,080 --> 00:01:47,630
lot of virtual machines to spawn up and

00:01:43,910 --> 00:01:50,929
so also a monitoring had to scale a bit

00:01:47,630 --> 00:01:53,479
but back in 2017 we started actually

00:01:50,929 --> 00:01:55,700
when realistic started we used a girls

00:01:53,479 --> 00:01:58,880
in the past to maintain our

00:01:55,700 --> 00:02:01,039
infrastructure to check all services to

00:01:58,880 --> 00:02:04,099
check out services with basic checks we

00:02:01,039 --> 00:02:06,410
had basic checks by CPU for Lords disk

00:02:04,099 --> 00:02:08,899
space as you can imagine there's a 75%

00:02:06,410 --> 00:02:11,810
warning state and 90% critical states

00:02:08,899 --> 00:02:14,390
super familiar with Nagios then this

00:02:11,810 --> 00:02:17,239
might be common for you we also use and

00:02:14,390 --> 00:02:19,159
still use new relic for instrumentation

00:02:17,239 --> 00:02:22,910
of our applications so also the crash

00:02:19,159 --> 00:02:25,070
rates error rates response times and

00:02:22,910 --> 00:02:27,560
to be able to alert on that when

00:02:25,070 --> 00:02:29,210
somebody opposes on phone call we use

00:02:27,560 --> 00:02:31,840
Pingdom for that which is an external

00:02:29,210 --> 00:02:36,410
service for doing external HTTP checks

00:02:31,840 --> 00:02:38,840
and we also use ping them to force it to

00:02:36,410 --> 00:02:41,780
check specific national service pages

00:02:38,840 --> 00:02:43,100
and with string matching so if service

00:02:41,780 --> 00:02:46,280
tracking our girls is being an honorable

00:02:43,100 --> 00:02:51,410
critical state it would be 7 SMS to that

00:02:46,280 --> 00:02:54,710
person that is being on it was horrible

00:02:51,410 --> 00:02:56,650
so we use check for preparing all Magna

00:02:54,710 --> 00:02:59,870
set up and you can see screenshot of our

00:02:56,650 --> 00:03:01,610
chef data pack for adding new Nagios

00:02:59,870 --> 00:03:04,550
checks basically this is a configuration

00:03:01,610 --> 00:03:06,950
file we would call this it has three

00:03:04,550 --> 00:03:09,110
five thousand lines when we stop loggers

00:03:06,950 --> 00:03:11,600
basically so if you created a new micro

00:03:09,110 --> 00:03:13,550
service in this case this is our some

00:03:11,600 --> 00:03:15,200
MongoDB checks you would have to add

00:03:13,550 --> 00:03:23,450
someone going to be checks if there is a

00:03:15,200 --> 00:03:26,450
minute it was a mess here is a

00:03:23,450 --> 00:03:28,400
screenshot I made on on a Sunday morning

00:03:26,450 --> 00:03:30,130
where we usually have our Russia I know

00:03:28,400 --> 00:03:35,600
production moments most of our customers

00:03:30,130 --> 00:03:38,060
are running on a Sunday morning and the

00:03:35,600 --> 00:03:40,220
thing here is as you might see that a

00:03:38,060 --> 00:03:42,740
lot of critical it is only some of the

00:03:40,220 --> 00:03:43,880
critical parts here you see and you

00:03:42,740 --> 00:03:45,200
might think that there is an issue in

00:03:43,880 --> 00:03:49,280
our production environments but there

00:03:45,200 --> 00:03:51,770
was no issue so you see a lot of lot

00:03:49,280 --> 00:03:54,440
alerts for example it was a total mess

00:03:51,770 --> 00:03:55,940
and you for and you be here in the team

00:03:54,440 --> 00:03:58,370
it was nearly impossible to find out if

00:03:55,940 --> 00:04:00,290
there is an issue in production so this

00:03:58,370 --> 00:04:03,680
was unattainable

00:04:00,290 --> 00:04:05,180
anymore so we said together in 2018 in

00:04:03,680 --> 00:04:07,100
January and we said okay we need to

00:04:05,180 --> 00:04:09,709
change this there is no way to keep

00:04:07,100 --> 00:04:11,360
nagasaki running like this so we define

00:04:09,709 --> 00:04:14,510
some goals not binding ourselves to a

00:04:11,360 --> 00:04:15,740
specific tool we just said uncle should

00:04:14,510 --> 00:04:19,310
be as comfortable as possible in the

00:04:15,740 --> 00:04:20,359
future also rethinking of how alerting

00:04:19,310 --> 00:04:23,180
should be done in the future

00:04:20,359 --> 00:04:25,670
is it really necessary to alert on

00:04:23,180 --> 00:04:27,590
specific things like load I note or I

00:04:25,670 --> 00:04:29,690
network throughput is this really a

00:04:27,590 --> 00:04:32,330
thing you you need to - an add-on

00:04:29,690 --> 00:04:34,070
basically and we also said since we are

00:04:32,330 --> 00:04:37,850
small team and we need to automate a lot

00:04:34,070 --> 00:04:39,290
of stuff yeah find the solution that is

00:04:37,850 --> 00:04:42,500
being automatable

00:04:39,290 --> 00:04:43,430
as better as much as possible and also

00:04:42,500 --> 00:04:45,500
make use of grass

00:04:43,430 --> 00:04:47,870
so in Nagios if you're familiar with

00:04:45,500 --> 00:04:50,630
that it's not possible really to come

00:04:47,870 --> 00:04:53,180
back in time so let's say you get the

00:04:50,630 --> 00:04:55,430
disk space alert it's at 80% of this

00:04:53,180 --> 00:04:57,200
space is full is it something I need to

00:04:55,430 --> 00:04:59,419
take care of immediately is it something

00:04:57,200 --> 00:05:01,550
that can wait two or three weeks it was

00:04:59,419 --> 00:05:04,190
not possible to distinguish or predict

00:05:01,550 --> 00:05:05,870
if there is something are going on in

00:05:04,190 --> 00:05:08,979
our infrastructure so we wanted to come

00:05:05,870 --> 00:05:12,080
back in time and make some predictions

00:05:08,979 --> 00:05:14,990
and also yeah it should be scalable and

00:05:12,080 --> 00:05:17,120
should be proof for future yeah

00:05:14,990 --> 00:05:19,160
technologies speaking about containers

00:05:17,120 --> 00:05:22,850
for example it's what we started with

00:05:19,160 --> 00:05:24,740
Prometheus our journey so we picked up a

00:05:22,850 --> 00:05:26,870
micro service in production and yeah we

00:05:24,740 --> 00:05:30,169
also do some testing in production v

00:05:26,870 --> 00:05:32,470
necessary and we picked the micro

00:05:30,169 --> 00:05:35,600
service containing a lot of technologies

00:05:32,470 --> 00:05:37,010
MongoDB my sequel this tooks represents

00:05:35,600 --> 00:05:40,280
the node exporter this was the first

00:05:37,010 --> 00:05:41,750
exporter we tested we also brought some

00:05:40,280 --> 00:05:42,620
custom exporters like near a liquor

00:05:41,750 --> 00:05:46,610
sidekick

00:05:42,620 --> 00:05:48,979
which is pretty easy I have to say and

00:05:46,610 --> 00:05:53,229
so we picked up some metrics and you

00:05:48,979 --> 00:05:56,120
will find so the next step was let's

00:05:53,229 --> 00:05:58,250
spawn up our production Prometheus set

00:05:56,120 --> 00:06:00,110
up so we just picked two pair metals

00:05:58,250 --> 00:06:03,020
service we used to pay metals service

00:06:00,110 --> 00:06:05,390
for it running a total amount of 7.5

00:06:03,020 --> 00:06:07,340
terabytes of storage and right now we

00:06:05,390 --> 00:06:09,919
are able to keep seven months of metrics

00:06:07,340 --> 00:06:12,260
it's super important to keep that seven

00:06:09,919 --> 00:06:14,570
months but it's always nice to come back

00:06:12,260 --> 00:06:16,580
in time and see in past events for

00:06:14,570 --> 00:06:18,680
example how the whole specific matrix

00:06:16,580 --> 00:06:20,600
behaves in the past speaking of what

00:06:18,680 --> 00:06:23,960
special events when more more users are

00:06:20,600 --> 00:06:28,310
using our app and we still use the

00:06:23,960 --> 00:06:30,080
internal storage of Prometheus now

00:06:28,310 --> 00:06:34,340
speaking about the automation which is a

00:06:30,080 --> 00:06:36,590
super important topic we had some goals

00:06:34,340 --> 00:06:37,310
basically roll out the export us

00:06:36,590 --> 00:06:39,530
automatically

00:06:37,310 --> 00:06:41,210
well we use chef for preparing the

00:06:39,530 --> 00:06:43,580
virtual machines and setting all this up

00:06:41,210 --> 00:06:45,770
so this was useful first use red for

00:06:43,580 --> 00:06:48,860
that service discovery

00:06:45,770 --> 00:06:49,860
so in Nagios we use chef giant for

00:06:48,860 --> 00:06:51,600
preparing all

00:06:49,860 --> 00:06:54,630
checks all the configuration files and

00:06:51,600 --> 00:06:56,310
we said there's no way we set up

00:06:54,630 --> 00:06:59,790
prometheus like this and using a static

00:06:56,310 --> 00:07:02,220
list so modules chef Ken was running on

00:06:59,790 --> 00:07:05,160
on again system before I think 30

00:07:02,220 --> 00:07:07,860
minutes natla run so he said this is not

00:07:05,160 --> 00:07:11,040
not a good way to do it so we took a

00:07:07,860 --> 00:07:12,660
look at console and what about the HTTP

00:07:11,040 --> 00:07:15,240
health checks so in the past we logged

00:07:12,660 --> 00:07:18,120
in to Pingdom and configured a health

00:07:15,240 --> 00:07:20,070
check manually on the website and since

00:07:18,120 --> 00:07:24,030
we're using terraform a lot it made

00:07:20,070 --> 00:07:27,540
sense to actually do this and the last

00:07:24,030 --> 00:07:29,250
point add silences in the lab manager so

00:07:27,540 --> 00:07:30,420
this was not on our agenda to the first

00:07:29,250 --> 00:07:32,430
place but when you do a lot of

00:07:30,420 --> 00:07:34,320
automation and spawn spawning up new

00:07:32,430 --> 00:07:36,150
micro-services automatically they are

00:07:34,320 --> 00:07:38,760
not ready for production yet so someone

00:07:36,150 --> 00:07:40,710
has to do something here before it gets

00:07:38,760 --> 00:07:42,870
live so when you create a new micro

00:07:40,710 --> 00:07:46,020
service you immediately will fire some

00:07:42,870 --> 00:07:47,460
alerts so it's mandatory for us to also

00:07:46,020 --> 00:07:51,090
create the silence when you prepare a

00:07:47,460 --> 00:07:54,690
new micro zones up front talking about

00:07:51,090 --> 00:07:55,950
control yeah it turns out we already use

00:07:54,690 --> 00:07:58,440
console in our production environment

00:07:55,950 --> 00:08:00,450
for terraforming mode state so if you

00:07:58,440 --> 00:08:02,820
back with terraform in a team then I

00:08:00,450 --> 00:08:06,870
have to pick you have to store the state

00:08:02,820 --> 00:08:08,490
somewhere and through that it's fine we

00:08:06,870 --> 00:08:10,020
can simply roll the console agents and

00:08:08,490 --> 00:08:12,060
all the server's you want to monitor and

00:08:10,020 --> 00:08:13,500
use yet for that and also chef should

00:08:12,060 --> 00:08:16,290
prepare one service definition for each

00:08:13,500 --> 00:08:18,660
expert on each server this is how the

00:08:16,290 --> 00:08:20,880
console using the phase looks like this

00:08:18,660 --> 00:08:22,560
is a sharing service you version 1 in

00:08:20,880 --> 00:08:24,270
that case you see here we have three

00:08:22,560 --> 00:08:27,060
services assigned to this console mode

00:08:24,270 --> 00:08:29,580
so we said for each exporter creating a

00:08:27,060 --> 00:08:31,950
separate service and on the right side

00:08:29,580 --> 00:08:35,100
you see here some tags in which I will

00:08:31,950 --> 00:08:37,290
come immediately so when we talked about

00:08:35,100 --> 00:08:39,570
but worked with Prometheus it was quite

00:08:37,290 --> 00:08:42,780
obvious that labels are quite important

00:08:39,570 --> 00:08:44,850
and so we said together again and

00:08:42,780 --> 00:08:47,280
defined what labels could be interesting

00:08:44,850 --> 00:08:49,740
for us to use and therefore it's also

00:08:47,280 --> 00:08:53,280
important to define what the use cases

00:08:49,740 --> 00:08:55,110
are basically so you could say show me

00:08:53,280 --> 00:08:57,090
the load of all records of the news feed

00:08:55,110 --> 00:08:58,440
servers so it could be interesting to

00:08:57,090 --> 00:09:00,150
have a service label it could be

00:08:58,440 --> 00:09:02,820
interesting to have a role labor and

00:09:00,150 --> 00:09:03,510
only we could contain is it a server is

00:09:02,820 --> 00:09:07,050
a debacle

00:09:03,510 --> 00:09:10,080
proxy note is it an MongoDB note is it

00:09:07,050 --> 00:09:11,580
in elasticsearch note um and we also

00:09:10,080 --> 00:09:14,070
introduced the hostname we're able to

00:09:11,580 --> 00:09:15,800
easily get the host amount of it we knew

00:09:14,070 --> 00:09:18,050
that there was the instance labeled but

00:09:15,800 --> 00:09:21,930
using the hosting believers also quite

00:09:18,050 --> 00:09:24,420
interesting for us and so we defined a

00:09:21,930 --> 00:09:28,500
chasing for console basically to use

00:09:24,420 --> 00:09:29,880
there to add the service to console this

00:09:28,500 --> 00:09:32,790
is an example of the sharing server

00:09:29,880 --> 00:09:35,730
series through one yeah offering the

00:09:32,790 --> 00:09:37,380
MongoDB exporter and we use this text

00:09:35,730 --> 00:09:38,790
here basically tagging with Prometheus

00:09:37,380 --> 00:09:40,590
Oh to teleport visuals hey this is a

00:09:38,790 --> 00:09:43,440
concert service that is relevant for you

00:09:40,590 --> 00:09:46,080
and the other text so you cannot define

00:09:43,440 --> 00:09:48,830
key value pairs in console so you have

00:09:46,080 --> 00:09:51,630
to do kind of a red X metric here to

00:09:48,830 --> 00:09:54,540
define your service Labor's role every

00:09:51,630 --> 00:09:56,730
service they were export the label this

00:09:54,540 --> 00:09:58,860
is how it looks like in Prometheus so

00:09:56,730 --> 00:10:00,900
this is our chopped name Tod that is

00:09:58,860 --> 00:10:01,920
used for all the things here so we say

00:10:00,900 --> 00:10:04,500
here simply which is in the

00:10:01,920 --> 00:10:06,120
documentation already that keeps

00:10:04,500 --> 00:10:09,030
everything that this label with

00:10:06,120 --> 00:10:11,370
Prometheus and then the second records

00:10:09,030 --> 00:10:12,990
here is very magic happens basically so

00:10:11,370 --> 00:10:16,050
everything that goes after the service

00:10:12,990 --> 00:10:19,050
um string here that is being used to

00:10:16,050 --> 00:10:21,990
target labor service and so it was

00:10:19,050 --> 00:10:23,390
finished so far but what about external

00:10:21,990 --> 00:10:26,100
health checks

00:10:23,390 --> 00:10:27,450
this was another great point because we

00:10:26,100 --> 00:10:29,520
use ping them in the past and we said

00:10:27,450 --> 00:10:32,790
actually we don't need to use Pingdom in

00:10:29,520 --> 00:10:35,610
future we said with black box exported

00:10:32,790 --> 00:10:38,310
to do this so we spawned up some ec2

00:10:35,610 --> 00:10:41,340
instances this was quite easy for us and

00:10:38,310 --> 00:10:43,830
also to ensure that some black box

00:10:41,340 --> 00:10:45,170
export is running or office that is a

00:10:43,830 --> 00:10:47,810
very cool actually to the data center

00:10:45,170 --> 00:10:50,910
because there may be some services that

00:10:47,810 --> 00:10:53,880
you also want to check that and it was

00:10:50,910 --> 00:10:55,440
quite nice because you check that HTTP

00:10:53,880 --> 00:10:58,020
response code and also get some metrics

00:10:55,440 --> 00:11:01,230
for the SSL Certificates stops you also

00:10:58,020 --> 00:11:05,250
I have the checks for expiration which

00:11:01,230 --> 00:11:08,550
is also important yeah as we use tariffs

00:11:05,250 --> 00:11:10,980
um a lot it's quite easy to add service

00:11:08,550 --> 00:11:12,620
a health checking console surprise

00:11:10,980 --> 00:11:15,060
surprise it's the same company that

00:11:12,620 --> 00:11:16,460
provides Sarah Foreman console so this

00:11:15,060 --> 00:11:19,160
is an easy one so just

00:11:16,460 --> 00:11:21,470
add a resource console service and add

00:11:19,160 --> 00:11:24,050
the text that we saw into it in the past

00:11:21,470 --> 00:11:25,550
but we use health check as a text to

00:11:24,050 --> 00:11:28,040
indicate provincials hey this is a

00:11:25,550 --> 00:11:30,980
health check page that is needed by the

00:11:28,040 --> 00:11:34,270
black box exporter and we use the second

00:11:30,980 --> 00:11:37,070
tag here URL and here we can define

00:11:34,270 --> 00:11:40,460
health check that is being used but by

00:11:37,070 --> 00:11:41,840
the black box exporter this is how the

00:11:40,460 --> 00:11:44,810
Chop configuration of the black box

00:11:41,840 --> 00:11:47,030
exporter looks like actually what you do

00:11:44,810 --> 00:11:49,220
here is and it's also documented in the

00:11:47,030 --> 00:11:52,130
previous documentation so actually

00:11:49,220 --> 00:11:53,990
everything that we needed to was quite

00:11:52,130 --> 00:11:56,420
documented in there and provisions

00:11:53,990 --> 00:11:59,030
documentation but we had to fill that up

00:11:56,420 --> 00:12:01,730
with some records here also using the

00:11:59,030 --> 00:12:03,650
records well you're replacing it with

00:12:01,730 --> 00:12:05,060
the parent agate so this this is

00:12:03,650 --> 00:12:07,370
something you need to find ultimately

00:12:05,060 --> 00:12:10,610
but then you see that this is actually

00:12:07,370 --> 00:12:12,530
quite nice working and the last one

00:12:10,610 --> 00:12:14,900
which is super important I have to say

00:12:12,530 --> 00:12:18,260
it's the adding the silence to the end

00:12:14,900 --> 00:12:20,360
up manager if you do automation and to

00:12:18,260 --> 00:12:22,820
create new services via tariffs on the

00:12:20,360 --> 00:12:24,860
dunlop deploy to not live yet not ready

00:12:22,820 --> 00:12:27,410
yet and producing some service tunnel

00:12:24,860 --> 00:12:30,020
that's the beginning then add a silence

00:12:27,410 --> 00:12:32,450
in orcas it is a quite simple solution

00:12:30,020 --> 00:12:35,990
just use the NAIRU resource object and

00:12:32,450 --> 00:12:38,300
use hm2 edges which I'm told you can

00:12:35,990 --> 00:12:39,440
easily add the silence actually this

00:12:38,300 --> 00:12:41,180
would be also nice to have it as a

00:12:39,440 --> 00:12:44,120
telephone provider so this could be a

00:12:41,180 --> 00:12:45,220
nice thing here and there you go you're

00:12:44,120 --> 00:12:49,660
finished

00:12:45,220 --> 00:12:53,000
so I'm not getting any money for

00:12:49,660 --> 00:12:55,400
mentioning of Jimmy here I have to

00:12:53,000 --> 00:12:58,700
clearly say that we played around a bit

00:12:55,400 --> 00:13:00,140
with p2t of Jeannie because we needed

00:12:58,700 --> 00:13:02,150
alerting at some point so in the past

00:13:00,140 --> 00:13:05,180
people did that for us and we were using

00:13:02,150 --> 00:13:07,300
pinging them a bit for that use case so

00:13:05,180 --> 00:13:10,310
we said we need some kind of alert and

00:13:07,300 --> 00:13:13,040
the initial alerting plan was so we have

00:13:10,310 --> 00:13:15,800
also some alerts with low priority of

00:13:13,040 --> 00:13:18,230
course this would be a free trip can't

00:13:15,800 --> 00:13:20,390
run for example or something like at

00:13:18,230 --> 00:13:22,430
this space we'll get food in four hours

00:13:20,390 --> 00:13:24,320
alert this would be also something that

00:13:22,430 --> 00:13:26,900
this low priority that's informational

00:13:24,320 --> 00:13:28,670
for you but not affecting our customers

00:13:26,900 --> 00:13:30,320
are not affecting an accountant or

00:13:28,670 --> 00:13:31,340
something like this this

00:13:30,320 --> 00:13:33,320
would be used with the snake

00:13:31,340 --> 00:13:36,140
integrations we used the second equation

00:13:33,320 --> 00:13:38,480
a lot actually and a lot of high

00:13:36,140 --> 00:13:40,970
priority which are ankle relevant should

00:13:38,480 --> 00:13:42,920
get forward tubes genie so if someone of

00:13:40,970 --> 00:13:45,620
Alice's on hold an option you should

00:13:42,920 --> 00:13:48,770
contact the uncle person via phone or

00:13:45,620 --> 00:13:50,930
via SMS then we sit together with the

00:13:48,770 --> 00:13:52,280
opportunity guys and the question came

00:13:50,930 --> 00:13:55,370
up by not for what although that's

00:13:52,280 --> 00:13:58,160
basically to Chile because of Chile

00:13:55,370 --> 00:14:01,430
stood on therefore and bicha to think

00:13:58,160 --> 00:14:03,200
also not here only for Android team for

00:14:01,430 --> 00:14:06,110
uncle it's awesome for that management

00:14:03,200 --> 00:14:09,950
incident management and so we came up

00:14:06,110 --> 00:14:11,150
with two integrations basically in

00:14:09,950 --> 00:14:12,800
option you'll define multiple

00:14:11,150 --> 00:14:14,390
integrations it can be peripherals it

00:14:12,800 --> 00:14:16,730
can be elastic search can be also new

00:14:14,390 --> 00:14:19,480
relic in that case you can create

00:14:16,730 --> 00:14:21,740
multiple premiere fuels integrations and

00:14:19,480 --> 00:14:23,870
the easiest thing here is we have a

00:14:21,740 --> 00:14:26,720
previous uncle integration where all the

00:14:23,870 --> 00:14:28,130
high priority let's forward to in that

00:14:26,720 --> 00:14:30,800
case if and let this receive for this

00:14:28,130 --> 00:14:33,650
integration called the pro ops guy that

00:14:30,800 --> 00:14:35,570
is being uncle and also post some alerts

00:14:33,650 --> 00:14:38,870
to specific select channels well she's

00:14:35,570 --> 00:14:41,630
like a lot to announce alerts I usually

00:14:38,870 --> 00:14:43,460
call them stay cold elections because

00:14:41,630 --> 00:14:44,660
there are multiple members in this

00:14:43,460 --> 00:14:46,340
channel who were not part of the

00:14:44,660 --> 00:14:48,710
infrastructure team or may be interested

00:14:46,340 --> 00:14:49,850
in some service outage and we have the

00:14:48,710 --> 00:14:52,040
second one to proof the stops

00:14:49,850 --> 00:14:54,730
integration which has a low priority

00:14:52,040 --> 00:14:59,900
others like the chef can't failed runs

00:14:54,730 --> 00:15:01,610
notice that this modification is has the

00:14:59,900 --> 00:15:03,440
option disable notification so you can

00:15:01,610 --> 00:15:05,510
say that it loves Jimmy if this option

00:15:03,440 --> 00:15:08,150
is set then alerts forward to this

00:15:05,510 --> 00:15:10,790
integration I'm not forward to the uncle

00:15:08,150 --> 00:15:14,630
person so then all phone SMS alerts

00:15:10,790 --> 00:15:16,250
then you don't want to wake up an ops

00:15:14,630 --> 00:15:18,080
guy in the middle of the night panda

00:15:16,250 --> 00:15:20,840
chef can run feel so this is something

00:15:18,080 --> 00:15:22,630
you have to take care of so this is how

00:15:20,840 --> 00:15:24,800
you can integrate if you folks Cheney

00:15:22,630 --> 00:15:26,150
and this is how it looks like a lot

00:15:24,800 --> 00:15:29,030
manager configuration quite simple there

00:15:26,150 --> 00:15:30,860
are two receivers here what we do here

00:15:29,030 --> 00:15:33,410
is basically we distinguish when we

00:15:30,860 --> 00:15:34,970
define alerting rules and match with the

00:15:33,410 --> 00:15:36,680
uncle through labor so if you have any

00:15:34,970 --> 00:15:38,540
nothing world at this important and it's

00:15:36,680 --> 00:15:40,940
match this don't call true then it's on

00:15:38,540 --> 00:15:42,290
core relevant notice also the group by

00:15:40,940 --> 00:15:46,100
top the dots here

00:15:42,290 --> 00:15:47,779
which I will come to later on so this is

00:15:46,100 --> 00:15:49,940
how the receiver basically looks like it

00:15:47,779 --> 00:15:51,649
looks like a bit like a mess but what we

00:15:49,940 --> 00:15:54,199
actually do here is in the description

00:15:51,649 --> 00:15:56,630
we prepare the slack message so we are

00:15:54,199 --> 00:15:58,639
able to also add - bottlings if

00:15:56,630 --> 00:16:02,209
necessary or unhook links you could also

00:15:58,639 --> 00:16:04,880
do this so this is quite it's fitting

00:16:02,209 --> 00:16:08,269
quite easy to implement so why we use

00:16:04,880 --> 00:16:10,130
group I thought that I think this was

00:16:08,269 --> 00:16:13,639
integrated six months ago or something

00:16:10,130 --> 00:16:16,220
like this we had major issues before him

00:16:13,639 --> 00:16:18,410
in the beginning because of the latitude

00:16:16,220 --> 00:16:21,350
application of Ginny does so when you

00:16:18,410 --> 00:16:23,420
forward alert stops Jeanne it also does

00:16:21,350 --> 00:16:25,069
some kind of grouping of lots so if you

00:16:23,420 --> 00:16:27,440
have imagined the chef can field when

00:16:25,069 --> 00:16:29,269
the alert is being sent to Cheney for

00:16:27,440 --> 00:16:31,100
the most insurance I one zero zero one

00:16:29,269 --> 00:16:35,149
okay I'm Cindy crates and a lot of it

00:16:31,100 --> 00:16:37,819
alerts via slack but five minutes later

00:16:35,149 --> 00:16:40,279
another share about the sharing service

00:16:37,819 --> 00:16:42,410
we will through has the same issue since

00:16:40,279 --> 00:16:44,990
then that mention sends alerts Cheney

00:16:42,410 --> 00:16:47,209
and of CV ten says hey already received

00:16:44,990 --> 00:16:49,670
is allowed four zero zero one but anyway

00:16:47,209 --> 00:16:50,810
so don't Creighton and do a top of it so

00:16:49,670 --> 00:16:53,600
it was important for us to use this

00:16:50,810 --> 00:16:56,180
group I then optionally also creates

00:16:53,600 --> 00:16:58,130
dedicated alerts for different chef

00:16:56,180 --> 00:17:00,139
Canfield runs so this is a kind of

00:16:58,130 --> 00:17:01,670
grouping you have to take care of not

00:17:00,139 --> 00:17:04,819
that you overlook alerts in that

00:17:01,670 --> 00:17:08,270
particular case this would be an example

00:17:04,819 --> 00:17:09,620
and let me move our on call alert so

00:17:08,270 --> 00:17:12,620
basically this is from the black box

00:17:09,620 --> 00:17:15,079
export what happens if a service comes

00:17:12,620 --> 00:17:17,750
down so then you have to labor on call

00:17:15,079 --> 00:17:18,770
true that indicates okay this is on call

00:17:17,750 --> 00:17:21,829
relevant if the service is done

00:17:18,770 --> 00:17:24,650
immediately fire up and send out alerts

00:17:21,829 --> 00:17:25,939
to the on-call persons this would be an

00:17:24,650 --> 00:17:28,700
example I'm looking for a low priority

00:17:25,939 --> 00:17:29,380
that I call them also informational

00:17:28,700 --> 00:17:32,049
alerts

00:17:29,380 --> 00:17:34,600
basically this is nothing super critical

00:17:32,049 --> 00:17:37,309
that case it is a MongoDB relevant

00:17:34,600 --> 00:17:40,040
metric that indicates there may be some

00:17:37,309 --> 00:17:42,049
issues with some careers being running

00:17:40,040 --> 00:17:44,090
on the MongoDB itself this could be

00:17:42,049 --> 00:17:46,640
interesting for you as an infrastructure

00:17:44,090 --> 00:17:49,220
database engineer but it's not relevant

00:17:46,640 --> 00:17:51,309
really phone call and they also see that

00:17:49,220 --> 00:17:54,580
we edit dashboard link to or gravano

00:17:51,309 --> 00:17:56,029
instance so if you want to check that

00:17:54,580 --> 00:17:58,519
this could go

00:17:56,029 --> 00:18:00,109
into Cena's neck message this is

00:17:58,519 --> 00:18:03,289
actually how a select message looks like

00:18:00,109 --> 00:18:04,879
in our case we have our Cisco assign it

00:18:03,289 --> 00:18:07,249
working they have an alert here

00:18:04,879 --> 00:18:10,789
this is coming from a dedicated exporter

00:18:07,249 --> 00:18:12,529
V we had to write the nice thing here in

00:18:10,789 --> 00:18:15,200
using an ad management feels like this

00:18:12,529 --> 00:18:17,479
you can easily acknowledge your notes

00:18:15,200 --> 00:18:19,489
and tell your team colleagues hey I'm

00:18:17,479 --> 00:18:23,389
working under the lot and also it's easy

00:18:19,489 --> 00:18:26,779
for you to add notes so that everyone is

00:18:23,389 --> 00:18:28,519
aware of what the issue is about this is

00:18:26,779 --> 00:18:34,009
a my notes better way so this is nothing

00:18:28,519 --> 00:18:36,950
super heartbeat it's also called dead

00:18:34,009 --> 00:18:40,119
man's switch so this was actually the

00:18:36,950 --> 00:18:42,649
reason why we chose of Cheney because

00:18:40,119 --> 00:18:45,710
yeah sending the alerts to to a

00:18:42,649 --> 00:18:47,719
dedicated party provider is nice but

00:18:45,710 --> 00:18:49,729
what if Prometheus itself is done but if

00:18:47,719 --> 00:18:52,719
it isn't the applicants on what if our

00:18:49,729 --> 00:18:55,339
networking is done so there's no really

00:18:52,719 --> 00:18:58,219
yeah a check if the monitoring itself is

00:18:55,339 --> 00:19:00,679
up so the heartbeat is actually quite

00:18:58,219 --> 00:19:02,359
simple so you just use the vector one

00:19:00,679 --> 00:19:05,149
for example a fake alerting rule that

00:19:02,359 --> 00:19:07,099
obscenely expects from you every five

00:19:05,149 --> 00:19:09,289
minutes and if this is not received an

00:19:07,099 --> 00:19:12,440
extremely immediately fires an alert

00:19:09,289 --> 00:19:15,169
also an encore relevant alert saying hey

00:19:12,440 --> 00:19:16,070
I didn't receive the heartbeat and this

00:19:15,169 --> 00:19:18,019
is how it looks like an elephant

00:19:16,070 --> 00:19:20,239
reconfiguration so we met roof the

00:19:18,019 --> 00:19:21,979
heartbeat label basically and this is a

00:19:20,239 --> 00:19:24,589
bit different this is just a simple web

00:19:21,979 --> 00:19:28,099
hook yeah that you are calling the

00:19:24,589 --> 00:19:30,469
obscene API and using you use your API

00:19:28,099 --> 00:19:32,779
key from skinny as a basic off basically

00:19:30,469 --> 00:19:38,239
and then this is up and running and

00:19:32,779 --> 00:19:39,950
working see ICD pipeline so I'm also

00:19:38,239 --> 00:19:41,749
speaking about a bit of the automation

00:19:39,950 --> 00:19:45,830
so as you can imagine Buse you have a

00:19:41,749 --> 00:19:48,739
lots of initial version of rolling up

00:19:45,830 --> 00:19:51,440
permissions itself was using chef for

00:19:48,739 --> 00:19:53,119
that and also all letting rules were put

00:19:51,440 --> 00:19:55,219
via cookbook file resource

00:19:53,119 --> 00:19:57,830
directly on the peripheral servers but

00:19:55,219 --> 00:20:01,070
then other departments came to us saying

00:19:57,830 --> 00:20:03,609
hey this is super nice we want also to

00:20:01,070 --> 00:20:05,960
have some custom entity rules and

00:20:03,609 --> 00:20:06,739
recording rules and we said oh this is

00:20:05,960 --> 00:20:09,799
not really possible

00:20:06,739 --> 00:20:13,159
so we said together again and

00:20:09,799 --> 00:20:14,270
why shouldn't we put all the lighting

00:20:13,159 --> 00:20:16,279
and recording listen to the git

00:20:14,270 --> 00:20:19,100
repository and maybe we have the chance

00:20:16,279 --> 00:20:22,370
to automatically test it for syntax

00:20:19,100 --> 00:20:23,870
errors or maybe unit tests and deploy

00:20:22,370 --> 00:20:26,149
the master functional permission service

00:20:23,870 --> 00:20:28,130
basically and as soon as a martial

00:20:26,149 --> 00:20:29,929
master happens the laterals are

00:20:28,130 --> 00:20:33,789
automatically pushed to all Prometheus

00:20:29,929 --> 00:20:36,380
cells and this is how it basically works

00:20:33,789 --> 00:20:38,450
basically we set up a jenkins instance

00:20:36,380 --> 00:20:40,700
that does nothing else in running prom

00:20:38,450 --> 00:20:43,039
tool against each other finally this in

00:20:40,700 --> 00:20:46,549
the branch and then pit pocket bitbucket

00:20:43,039 --> 00:20:49,010
is what we use for git repositories mid

00:20:46,549 --> 00:20:51,380
packet is sending HTTP calls to each

00:20:49,010 --> 00:20:54,350
Prometheus server as soon as the master

00:20:51,380 --> 00:20:56,020
branch changes so this is nice but there

00:20:54,350 --> 00:20:58,399
is something missing here so as soon as

00:20:56,020 --> 00:20:59,809
someone has to do stage they'd be called

00:20:58,399 --> 00:21:03,860
on the previous server so we had to

00:20:59,809 --> 00:21:04,880
write a ruby based HTTP handler that is

00:21:03,860 --> 00:21:07,309
running on each professor

00:21:04,880 --> 00:21:09,200
accepting those HTTP call from pit

00:21:07,309 --> 00:21:11,750
packets and when this HTTP call is

00:21:09,200 --> 00:21:14,149
received a gift pool is initiated and

00:21:11,750 --> 00:21:15,710
then a provision third of this time so

00:21:14,149 --> 00:21:19,340
not the restart because every stop takes

00:21:15,710 --> 00:21:21,100
up to 20 minutes and all set up but the

00:21:19,340 --> 00:21:24,140
reload will do the thing for you and

00:21:21,100 --> 00:21:26,899
this house how it looks like when you

00:21:24,140 --> 00:21:28,789
developing recording rules of modules I

00:21:26,899 --> 00:21:31,100
work here in the wrong and lattimer's

00:21:28,789 --> 00:21:33,860
branch and you see the build failed

00:21:31,100 --> 00:21:35,840
actually so you can directly see okay

00:21:33,860 --> 00:21:38,120
there's a syntax error in it if you

00:21:35,840 --> 00:21:39,799
click on it then you would see the

00:21:38,120 --> 00:21:41,480
output of the prom tool and saying there

00:21:39,799 --> 00:21:43,730
is a syntax error in Yama file a cheap

00:21:41,480 --> 00:21:45,529
proxy how many for example and this is

00:21:43,730 --> 00:21:48,169
how you basically as a resume when you

00:21:45,529 --> 00:21:50,360
create a new pull request it directly

00:21:48,169 --> 00:21:51,890
get the feedback also if if the syntax

00:21:50,360 --> 00:21:55,549
is fine for you this is the most

00:21:51,890 --> 00:21:57,620
critical thing if the alerting wrong in

00:21:55,549 --> 00:21:59,240
some case and then you are not updating

00:21:57,620 --> 00:22:02,470
yours anymore so we had that situation

00:21:59,240 --> 00:22:08,450
already that's why we decided to go with

00:22:02,470 --> 00:22:10,340
the CIC deeper all right I know I could

00:22:08,450 --> 00:22:12,640
tackle a lot of things here so also

00:22:10,340 --> 00:22:15,919
speaking about specific alerting rules

00:22:12,640 --> 00:22:17,659
yeah time is your add so if you have any

00:22:15,919 --> 00:22:19,399
questions if you have any feedback you

00:22:17,659 --> 00:22:23,120
saw maybe something you found something

00:22:19,399 --> 00:22:24,590
nasty here feel free to approach me and

00:22:23,120 --> 00:22:26,480
you write me an email if you're watching

00:22:24,590 --> 00:22:29,870
this on a live stream or seeing the

00:22:26,480 --> 00:22:39,230
recording here feel free to ask me some

00:22:29,870 --> 00:22:40,730
questions thank you all right thank you

00:22:39,230 --> 00:22:42,559
very much we actually do have some time

00:22:40,730 --> 00:22:46,820
for questions like five to ten minutes

00:22:42,559 --> 00:22:53,300
if anyone has any I'll throw this over

00:22:46,820 --> 00:22:55,370
careful one two okay so what happened to

00:22:53,300 --> 00:22:57,170
Nuggets could you completely shut it

00:22:55,370 --> 00:22:59,179
down or you're still using it actually

00:22:57,170 --> 00:23:02,390
yes there are some pitfalls that is

00:22:59,179 --> 00:23:04,820
actually good question that brings us on

00:23:02,390 --> 00:23:07,160
topics there are some special cases some

00:23:04,820 --> 00:23:09,650
special checks like cron job checks or

00:23:07,160 --> 00:23:11,630
is it the disk space check you're a bit

00:23:09,650 --> 00:23:14,660
worried about but actually we managed to

00:23:11,630 --> 00:23:16,640
do this using the text file export a lot

00:23:14,660 --> 00:23:18,320
so the text file collector itself so we

00:23:16,640 --> 00:23:19,130
rolled up the Nordics port on each

00:23:18,320 --> 00:23:22,130
server

00:23:19,130 --> 00:23:23,870
and we really try to add metrics to

00:23:22,130 --> 00:23:25,850
Prometheus itself to get rid of Lagos

00:23:23,870 --> 00:23:27,950
basically but this also means for you

00:23:25,850 --> 00:23:30,940
that you need to remove a lot of

00:23:27,950 --> 00:23:33,170
alerting rules and this is also a bit of

00:23:30,940 --> 00:23:35,870
changing our lighting strategy

00:23:33,170 --> 00:23:37,520
so in arias you would really add a lot

00:23:35,870 --> 00:23:40,520
of checks a lot of alerts which is

00:23:37,520 --> 00:23:43,429
totally useless because you everyone did

00:23:40,520 --> 00:23:45,070
it like this in the past so this is the

00:23:43,429 --> 00:23:47,660
thing that you really have to think of

00:23:45,070 --> 00:23:50,570
what is really necessary

00:23:47,660 --> 00:23:54,290
Oberth and a lot basically is it really

00:23:50,570 --> 00:23:56,210
necessary to adapt download is it really

00:23:54,290 --> 00:23:56,600
necessary to add on high network

00:23:56,210 --> 00:23:58,400
throughput

00:23:56,600 --> 00:24:01,160
this is something you have to decide but

00:23:58,400 --> 00:24:04,420
you got rid of it down completely and

00:24:01,160 --> 00:24:07,280
thankfully hey do you use some

00:24:04,420 --> 00:24:10,640
Federation or tano's we have higher

00:24:07,280 --> 00:24:12,830
ability for permit user we have two

00:24:10,640 --> 00:24:16,030
instances running running separately

00:24:12,830 --> 00:24:19,370
Griffin is is running in a chain mode

00:24:16,030 --> 00:24:21,860
that is basically all we have so if all

00:24:19,370 --> 00:24:23,750
from if your server would be shutting

00:24:21,860 --> 00:24:25,250
down then the heartbeat would fire up in

00:24:23,750 --> 00:24:27,500
the in that case but the instances are

00:24:25,250 --> 00:24:29,210
running separately this is quite nice

00:24:27,500 --> 00:24:32,240
also the lab manager you can easily

00:24:29,210 --> 00:24:34,760
create the mesh network so there is no

00:24:32,240 --> 00:24:36,110
magic behind that no messes no need for

00:24:34,760 --> 00:24:36,460
setting up a pacemaker set up or

00:24:36,110 --> 00:24:39,330
something

00:24:36,460 --> 00:24:44,740
this so it's actually quite nice to skin

00:24:39,330 --> 00:24:47,860
does it answer your question hi can you

00:24:44,740 --> 00:24:50,740
a little bit elaborate on the exporter

00:24:47,860 --> 00:24:54,580
for a Cisco ACI infrastructure using

00:24:50,740 --> 00:24:56,049
Cisco CI yes yes we can feel free to

00:24:54,580 --> 00:24:59,919
approach me afterwards I can show you

00:24:56,049 --> 00:25:02,230
the cold I used to called Cisco CI but

00:24:59,919 --> 00:25:04,570
basically to sum it up we only grab the

00:25:02,230 --> 00:25:06,190
health score of each of each Leafs which

00:25:04,570 --> 00:25:08,289
of these spines which will form the

00:25:06,190 --> 00:25:11,139
fabric and from what details you have to

00:25:08,289 --> 00:25:13,720
go to the Cisco a CI wait with the face

00:25:11,139 --> 00:25:15,700
or just check the walls occurring there

00:25:13,720 --> 00:25:18,279
if you interested in that and how we

00:25:15,700 --> 00:25:22,350
automated also the telephone stuff we

00:25:18,279 --> 00:25:22,350
can talk of the words if you like thanks

00:25:25,049 --> 00:25:34,029
well I'm wondering with the rule CI CD

00:25:30,039 --> 00:25:36,039
whether the prom tool was accurate or or

00:25:34,029 --> 00:25:38,820
whether that was enough so that you

00:25:36,039 --> 00:25:43,200
always end up with a working Prometheus

00:25:38,820 --> 00:25:43,200
instance after that's integrated okay

00:25:44,669 --> 00:25:52,059
definitely right so this is not the

00:25:48,759 --> 00:25:54,429
final solution so I think we already

00:25:52,059 --> 00:25:56,559
have the unit testing thankfully so

00:25:54,429 --> 00:25:57,669
actually it's quite hard to also you

00:25:56,559 --> 00:25:59,649
should change the alert manager

00:25:57,669 --> 00:26:01,210
configuration to test the the routing of

00:25:59,649 --> 00:26:04,240
the alerts so what I do here basically

00:26:01,210 --> 00:26:06,100
is testing it manually per firing an

00:26:04,240 --> 00:26:08,259
alert but this is not a perfect solution

00:26:06,100 --> 00:26:10,929
so we're still working and finding a

00:26:08,259 --> 00:26:12,820
solution to test alerting words and also

00:26:10,929 --> 00:26:14,860
changing stuff in the lab manager

00:26:12,820 --> 00:26:17,019
configuration because this is getting

00:26:14,860 --> 00:26:18,909
more and more important and if you break

00:26:17,019 --> 00:26:24,970
something here before a Friday for

00:26:18,909 --> 00:26:28,029
example that maybe not so moist but that

00:26:24,970 --> 00:26:30,580
I'm not really helpful did you do

00:26:28,029 --> 00:26:38,860
something in that direction testing unit

00:26:30,580 --> 00:26:41,169
testing so it's the same approach let me

00:26:38,860 --> 00:26:45,609
come up with some better solution in the

00:26:41,169 --> 00:26:47,980
future I'm wondering if you could speak

00:26:45,609 --> 00:26:49,359
about the slack integration a little bit

00:26:47,980 --> 00:26:50,269
more I'm wondering if your handling like

00:26:49,359 --> 00:26:52,489
external web hooks

00:26:50,269 --> 00:26:53,809
or something or if you're spent up on

00:26:52,489 --> 00:26:56,059
micro service in ec2 or something to

00:26:53,809 --> 00:26:58,940
handle at it you mean how we followed

00:26:56,059 --> 00:27:01,279
the lots from Luchini to tools like yes

00:26:58,940 --> 00:27:03,169
is that built-in table genie or yes so

00:27:01,279 --> 00:27:05,029
basically in the past in the beginning

00:27:03,169 --> 00:27:07,339
we did we used to stack integration

00:27:05,029 --> 00:27:09,529
directly in the lab manager but then we

00:27:07,339 --> 00:27:11,419
decided to use opportunity for that so

00:27:09,529 --> 00:27:13,489
Jeannie receives all alerts and in

00:27:11,419 --> 00:27:16,070
you've seen you can find a lot routing

00:27:13,489 --> 00:27:18,049
and create multiple selections slack

00:27:16,070 --> 00:27:20,989
integrations this is all super fine

00:27:18,049 --> 00:27:22,570
handled with ops genie and yeah for the

00:27:20,989 --> 00:27:25,129
stakeholder cons you can really define

00:27:22,570 --> 00:27:27,829
but which kind of alerts which string

00:27:25,129 --> 00:27:30,169
matching do I need to post an alert in

00:27:27,829 --> 00:27:31,909
this channel for example I can show you

00:27:30,169 --> 00:27:33,619
some configurations afterwards if you're

00:27:31,909 --> 00:27:39,919
interested yeah I'll come find you it's

00:27:33,619 --> 00:27:41,570
quite quite easy to set up any more

00:27:39,919 --> 00:27:48,829
questions oh yeah back there okay one

00:27:41,570 --> 00:27:51,139
more yeah and you talk about the Sunday

00:27:48,829 --> 00:27:54,200
morning farmer you have a lot of upload

00:27:51,139 --> 00:27:57,139
of people running do you have some sort

00:27:54,200 --> 00:28:06,950
of behavior left so you you must have so

00:27:57,139 --> 00:28:09,320
many a weekend a lot of traffic actually

00:28:06,950 --> 00:28:11,149
some of them so if there's really a lot

00:28:09,320 --> 00:28:12,799
of traffic be nude for example the

00:28:11,149 --> 00:28:14,499
trebling queue and this is also

00:28:12,799 --> 00:28:17,149
interesting

00:28:14,499 --> 00:28:19,669
RabbitMQ exporter for example that is

00:28:17,149 --> 00:28:22,549
directly connecting to rebel mq web user

00:28:19,669 --> 00:28:24,139
interface this has more priority to

00:28:22,549 --> 00:28:26,929
reveal in queue so if there is high load

00:28:24,139 --> 00:28:28,639
on rabbitmq itself that the exporter is

00:28:26,929 --> 00:28:30,589
not able to connect to you anymore

00:28:28,639 --> 00:28:31,729
because of low priority and this means

00:28:30,589 --> 00:28:33,950
basically that you don't have any

00:28:31,729 --> 00:28:35,779
metrics and we say okay rabbit in

00:28:33,950 --> 00:28:36,709
Houston so they export this upon but

00:28:35,779 --> 00:28:38,779
actually your everything queue is

00:28:36,709 --> 00:28:41,539
working so this is some kind of expected

00:28:38,779 --> 00:28:45,159
behavior that we see okay revenue queues

00:28:41,539 --> 00:28:48,529
and I hope we manage to do this by yeah

00:28:45,159 --> 00:28:50,809
prioritizing the UI for the export but I

00:28:48,529 --> 00:28:53,089
hope to get export integration of

00:28:50,809 --> 00:28:56,450
revenue in the newest version so it's

00:28:53,089 --> 00:29:00,229
directly integrated but yeah actually we

00:28:56,450 --> 00:29:01,940
don't have any false positives since we

00:29:00,229 --> 00:29:04,169
use promises but this is also some kind

00:29:01,940 --> 00:29:06,809
of sticking to your L etting strategy

00:29:04,169 --> 00:29:08,730
and really be hard with defining

00:29:06,809 --> 00:29:10,559
alerting also only fire and that's if

00:29:08,730 --> 00:29:12,330
this is really necessary that was our

00:29:10,559 --> 00:29:14,609
goal and we somehow managed to do this

00:29:12,330 --> 00:29:17,039
still we have some disk space this will

00:29:14,609 --> 00:29:20,129
get fooled for hours thing is because of

00:29:17,039 --> 00:29:22,919
lock tries getting fool cron drops are

00:29:20,129 --> 00:29:24,960
rounding let's do some magic on the

00:29:22,919 --> 00:29:26,789
surface basically so this is still

00:29:24,960 --> 00:29:29,549
something that pops up so the disk space

00:29:26,789 --> 00:29:31,289
will get fully for us basically but this

00:29:29,549 --> 00:29:32,940
is actually nice alert you can read this

00:29:31,289 --> 00:29:34,830
you're almost perception block that this

00:29:32,940 --> 00:29:36,929
is a perfect example what prometheus is

00:29:34,830 --> 00:29:40,289
capable of with doing predictions

00:29:36,929 --> 00:29:41,970
basically all right we have one last

00:29:40,289 --> 00:29:45,330
question here sorry and could Adam

00:29:41,970 --> 00:29:49,109
already come up to plug in I think if we

00:29:45,330 --> 00:29:51,090
attack a how much memory do the service

00:29:49,109 --> 00:29:53,220
that you run promise you have how many

00:29:51,090 --> 00:29:55,289
time series are you managing right now

00:29:53,220 --> 00:29:58,320
and do you have any contingency plan for

00:29:55,289 --> 00:30:02,129
when that service cannot handle all

00:29:58,320 --> 00:30:04,859
those time series at the moment yes for

00:30:02,129 --> 00:30:07,499
our use case we have 32 gigs of memory

00:30:04,859 --> 00:30:09,749
on each server at the moment which is a

00:30:07,499 --> 00:30:12,179
fee for the amount of data we have not

00:30:09,749 --> 00:30:14,220
that much I cannot really say how much

00:30:12,179 --> 00:30:17,999
matrix we are currently have in there I

00:30:14,220 --> 00:30:22,739
think we have interesting 200k of matrix

00:30:17,999 --> 00:30:24,269
per minute so we have more than 5500

00:30:22,739 --> 00:30:28,289
export is currently integrate into

00:30:24,269 --> 00:30:30,869
promises it is fine for doing an RT and

00:30:28,289 --> 00:30:34,049
for analyzing Inc Ravana but as soon as

00:30:30,869 --> 00:30:35,850
you do some data analytics how many CPU

00:30:34,049 --> 00:30:38,100
cores are used by time or something like

00:30:35,850 --> 00:30:41,299
this if you need to to go back seven

00:30:38,100 --> 00:30:43,980
months then it is really hard then CPU

00:30:41,299 --> 00:30:46,980
usage is rising so CPU usage is the most

00:30:43,980 --> 00:30:49,649
problem here and we also had to use the

00:30:46,980 --> 00:30:51,450
query timeout finger so I clear is being

00:30:49,649 --> 00:30:55,259
killed after 60 seconds because Ravana

00:30:51,450 --> 00:30:57,480
is used by many employees now and not

00:30:55,259 --> 00:31:00,570
often know that using specific queries

00:30:57,480 --> 00:31:03,029
can kill prometheus so make make sure

00:31:00,570 --> 00:31:06,029
that you set timeouts on the permission

00:31:03,029 --> 00:31:07,799
set with these seconds but yeah we need

00:31:06,029 --> 00:31:10,230
to find a solution in future of maybe

00:31:07,799 --> 00:31:14,149
using an external storage for high loads

00:31:10,230 --> 00:31:15,680
furies that's an answer your question

00:31:14,149 --> 00:31:17,620
all right

00:31:15,680 --> 00:31:21,540
cool thank you very much

00:31:17,620 --> 00:31:31,250
[Applause]

00:31:21,540 --> 00:31:31,250

YouTube URL: https://www.youtube.com/watch?v=xSPNYXbAhQE


