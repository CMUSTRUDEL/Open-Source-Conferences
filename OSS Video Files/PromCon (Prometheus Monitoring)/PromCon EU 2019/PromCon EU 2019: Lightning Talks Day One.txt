Title: PromCon EU 2019: Lightning Talks Day One
Publication date: 2019-12-29
Playlist: PromCon EU 2019
Description: 
	Lightning talks are 5 minutes each.

#1 - Tips and Tricks with the TextFile Collector
Speaker: Adrien Fillon

#2 - Conprof - Prometheus for Profiles
Speaker: Frederic Branczyk

#3 - Integrate Monitoring of Microservices with Ticketing System
Speaker: Sanjay Singh

#4 - Prometheus for Aggregation of Realtime Data of a Wind Farm
Speaker: Burkhard Noltensmeier

#5 - Announcing the SLO Libsonnet
Speaker: Matthias Loibl
Slides: https://promcon.io/2019-munich/slides/lt1-05_slo-libsonnet.pdf

#6 - SAF and Closed-Loop Automatioon
Speaker: Matthias Runge
Slides: https://promcon.io/2019-munich/slides/lt1-06_saf-and-closed-loop-automation.pdf

#7 - New UI
Speaker: Julius Volz

#8 - LogQL in 5 Minutes
Speaker: Cyril Tovena
Slides: https://promcon.io/2019-munich/slides/lt1-08_logql-in-5-minutes.pdf

#9 - Reducing remote read footprint
Speaker: Bartlomiej Plotka
Slides: https://promcon.io/2019-munich/slides/lt1-09_reducing-remote-read-footprint.pdf

#10 - Prometheus in Debian
Speaker: Martina Ferrari
Slides: https://promcon.io/2019-munich/slides/lt1-10_prometheus-in-debian.pdf

#11 - "Fixing" Remote Write
Speaker: Callum Styan

#12 - Tanka
Speaker: Tom Braack
Captions: 
	00:00:00,650 --> 00:00:08,910
[Music]

00:00:13,590 --> 00:00:18,400
okay hi guys I wanted to talk quickly

00:00:16,630 --> 00:00:21,400
about the text file character that I

00:00:18,400 --> 00:00:23,890
think is often ignore feature of the

00:00:21,400 --> 00:00:25,180
Windows and Linux based character so

00:00:23,890 --> 00:00:27,240
basically anything that looks like

00:00:25,180 --> 00:00:30,490
appropriate primitives metric format

00:00:27,240 --> 00:00:32,050
ending in a file with that prom in a

00:00:30,490 --> 00:00:34,750
right folder we'd get corrected one

00:00:32,050 --> 00:00:37,079
script so for example if I were to echo

00:00:34,750 --> 00:00:40,090
from current attendees are about 250

00:00:37,079 --> 00:00:42,850
into that folder by default in Debian

00:00:40,090 --> 00:00:45,010
then this will be exported right now you

00:00:42,850 --> 00:00:47,620
can see also that export the time of the

00:00:45,010 --> 00:00:50,440
modification of the file so that could

00:00:47,620 --> 00:00:52,360
be really interesting for other thing so

00:00:50,440 --> 00:00:54,250
sometimes you could need it because you

00:00:52,360 --> 00:00:56,350
don't want to bother to open a new port

00:00:54,250 --> 00:00:57,160
across your security groups of ours

00:00:56,350 --> 00:00:58,990
iptables

00:00:57,160 --> 00:01:02,830
you need to update from 80s

00:00:58,990 --> 00:01:06,610
configuration as well extracts all you

00:01:02,830 --> 00:01:08,350
want to a new exporter develop so you

00:01:06,610 --> 00:01:11,290
have to copy paste some boilerplate and

00:01:08,350 --> 00:01:13,329
get started so you could do several

00:01:11,290 --> 00:01:15,520
check really quickly you have course

00:01:13,329 --> 00:01:18,280
that's really useful for that but you

00:01:15,520 --> 00:01:20,590
can also look into your application at

00:01:18,280 --> 00:01:22,540
the check if it's like legacy or

00:01:20,590 --> 00:01:25,570
something with the simple curl you could

00:01:22,540 --> 00:01:28,479
put the HTTP code it's not really

00:01:25,570 --> 00:01:29,080
perfect but it works and it gets picked

00:01:28,479 --> 00:01:32,409
up quickly

00:01:29,080 --> 00:01:34,990
you can also matrix for exotic raid

00:01:32,409 --> 00:01:39,570
systems at VM or the devices maybe

00:01:34,990 --> 00:01:42,490
mudbath as we've seen and you can also

00:01:39,570 --> 00:01:44,530
track job prophecies like a long a sink

00:01:42,490 --> 00:01:46,390
or something like what's the last time

00:01:44,530 --> 00:01:49,750
that was a synchronization or minifiers

00:01:46,390 --> 00:01:51,880
what's the size and if you don't have

00:01:49,750 --> 00:01:54,729
any news from your job for some time you

00:01:51,880 --> 00:01:57,610
can look up the text file endtime metric

00:01:54,729 --> 00:01:59,950
and other than that it's sometimes

00:01:57,610 --> 00:02:04,780
easier for those systems to write a fire

00:01:59,950 --> 00:02:07,330
than to make a request to Gateway it

00:02:04,780 --> 00:02:09,879
depends there's also something that's

00:02:07,330 --> 00:02:12,099
why I wanted to speak about this is the

00:02:09,879 --> 00:02:13,959
case of Nativity demons and I think we

00:02:12,099 --> 00:02:16,569
have a talk tomorrow afternoon but this

00:02:13,959 --> 00:02:18,280
for example in bit if you need to have

00:02:16,569 --> 00:02:20,019
several workers processing files and

00:02:18,280 --> 00:02:21,760
stuff like that it will just open a new

00:02:20,019 --> 00:02:24,250
port where you add the promiscuous

00:02:21,760 --> 00:02:26,530
matrix so if it's dynamic you will have

00:02:24,250 --> 00:02:26,800
to reconfigure Prometheus to scrap the

00:02:26,530 --> 00:02:30,540
number

00:02:26,800 --> 00:02:33,580
workers so you could do something like

00:02:30,540 --> 00:02:36,160
curling all the matrix and put it in a

00:02:33,580 --> 00:02:39,700
file put some rejects so you can append

00:02:36,160 --> 00:02:41,730
to TIG or the pod number and then you

00:02:39,700 --> 00:02:45,250
have all the matrix in so in some place

00:02:41,730 --> 00:02:47,290
you could also update n bit and add that

00:02:45,250 --> 00:02:50,170
feature that's something we tried but

00:02:47,290 --> 00:02:53,620
I'm not very good expert in secret but

00:02:50,170 --> 00:02:55,780
it's very easy to do sometimes I can set

00:02:53,620 --> 00:02:57,940
a shell script or can be Python now

00:02:55,780 --> 00:03:00,460
might be even go scripts but just watch

00:02:57,940 --> 00:03:01,930
the next FATA they're easy to deploy for

00:03:00,460 --> 00:03:04,450
example you can just put all of them in

00:03:01,930 --> 00:03:08,500
the cron folder and then they get

00:03:04,450 --> 00:03:11,470
executed periodically and it gets the

00:03:08,500 --> 00:03:13,210
result done in a few minutes and also

00:03:11,470 --> 00:03:14,830
and it's something that's why I really

00:03:13,210 --> 00:03:17,830
like from this community there's already

00:03:14,830 --> 00:03:21,040
a few scripts that have been contributed

00:03:17,830 --> 00:03:23,890
contributed so for example for apt you

00:03:21,040 --> 00:03:27,100
could have the inventor did a great

00:03:23,890 --> 00:03:28,740
account something like that or nvme

00:03:27,100 --> 00:03:32,620
matrix obviously stems

00:03:28,740 --> 00:03:35,560
that's pretty all I wanted to talk about

00:03:32,620 --> 00:03:41,160
it's really a feature that needs to be

00:03:35,560 --> 00:03:41,160
used more okay thank you okay thank you

00:03:41,400 --> 00:03:46,420
okay so my talk already gives it away a

00:03:44,440 --> 00:03:47,530
little bit but there's something that

00:03:46,420 --> 00:03:50,260
I've been thinking about for quite a

00:03:47,530 --> 00:03:52,030
while and if you maybe we're a coop con

00:03:50,260 --> 00:03:54,340
earlier this year you may have already

00:03:52,030 --> 00:03:57,000
heard about this but here's the scenario

00:03:54,340 --> 00:04:00,040
and many of us have probably noticed or

00:03:57,000 --> 00:04:01,930
experienced this before so let's say you

00:04:00,040 --> 00:04:03,250
get some sort of an alert and this is

00:04:01,930 --> 00:04:04,570
actually something that we have in our

00:04:03,250 --> 00:04:06,430
infrastructure for example if there's

00:04:04,570 --> 00:04:08,440
something that actually has been crushed

00:04:06,430 --> 00:04:10,720
looping for quite some time in our

00:04:08,440 --> 00:04:11,860
infrastructure we get a warning alert we

00:04:10,720 --> 00:04:13,600
don't get woken up about this

00:04:11,860 --> 00:04:18,000
necessarily if it doesn't violate our

00:04:13,600 --> 00:04:20,709
day but we do get notified about this so

00:04:18,000 --> 00:04:23,830
what do we do when we do this we look at

00:04:20,709 --> 00:04:27,310
actually the data that tells us that

00:04:23,830 --> 00:04:30,340
this pod is in this particular instance

00:04:27,310 --> 00:04:32,070
it's a pod on Corinna DS so we look at

00:04:30,340 --> 00:04:34,870
the data and it tells us it was killed

00:04:32,070 --> 00:04:36,970
and what do we do then we look at the

00:04:34,870 --> 00:04:40,870
memory usage of this particular workload

00:04:36,970 --> 00:04:42,790
and when our workload is boom killed

00:04:40,870 --> 00:04:45,100
that is when it's allocating so much

00:04:42,790 --> 00:04:46,300
memory that it wants to allocate more

00:04:45,100 --> 00:04:48,010
than the machine for example has

00:04:46,300 --> 00:04:50,980
available or that is available to this

00:04:48,010 --> 00:04:53,919
process it gets killed by the Linux

00:04:50,980 --> 00:04:55,570
kernel so this is a very typical typical

00:04:53,919 --> 00:04:59,080
picture that you will see from an

00:04:55,570 --> 00:05:04,300
unkilled process and wouldn't it be nice

00:04:59,080 --> 00:05:06,490
if we knew exactly the profile right

00:05:04,300 --> 00:05:08,410
before this program was boomed right

00:05:06,490 --> 00:05:10,030
that's usually when there's a memory

00:05:08,410 --> 00:05:12,430
leak or something that's the most

00:05:10,030 --> 00:05:14,139
interesting data point that we want to

00:05:12,430 --> 00:05:17,979
look at but this process has already

00:05:14,139 --> 00:05:20,620
died so we can't profile it anymore so

00:05:17,979 --> 00:05:22,900
the problem the solution to this problem

00:05:20,620 --> 00:05:24,700
isn't particularly new google has

00:05:22,900 --> 00:05:26,020
written about this for a couple of years

00:05:24,700 --> 00:05:28,030
already and the concept is called

00:05:26,020 --> 00:05:31,180
continuous profiling so it's actually

00:05:28,030 --> 00:05:33,940
quite simple just like with metrics that

00:05:31,180 --> 00:05:36,870
we collect metrics over time we collect

00:05:33,940 --> 00:05:39,340
profiles over time and as it happens the

00:05:36,870 --> 00:05:41,800
go runtime for example and there are

00:05:39,340 --> 00:05:44,620
many other integrations - for other

00:05:41,800 --> 00:05:46,600
languages out there actually exposes an

00:05:44,620 --> 00:05:48,820
HTTP server much like a prometheus end

00:05:46,600 --> 00:05:53,919
point but for profiles and this is the p

00:05:48,820 --> 00:05:55,979
proof format that it exposes so what do

00:05:53,919 --> 00:05:59,050
we do typically when we look at these

00:05:55,979 --> 00:06:02,229
profile at profiles over time right we

00:05:59,050 --> 00:06:04,270
look at one profile after the other and

00:06:02,229 --> 00:06:07,630
we look at where has this profile

00:06:04,270 --> 00:06:10,990
changed over time and we kind of look at

00:06:07,630 --> 00:06:12,940
this and identify this with our human

00:06:10,990 --> 00:06:17,400
brain but wouldn't it be nice if we

00:06:12,940 --> 00:06:20,110
could more automatically do this so

00:06:17,400 --> 00:06:22,410
basically what I've already built is a

00:06:20,110 --> 00:06:25,360
tool called comprar that's very

00:06:22,410 --> 00:06:27,930
imaginative of myself to call it con Pro

00:06:25,360 --> 00:06:30,580
because like continues profiling right

00:06:27,930 --> 00:06:32,470
but I want to show you a really quick

00:06:30,580 --> 00:06:34,870
example from our infrastructure so we

00:06:32,470 --> 00:06:37,570
have a process that actually did m here

00:06:34,870 --> 00:06:41,190
and then was respawn here as it happens

00:06:37,570 --> 00:06:43,860
this is actually con probe itself and so

00:06:41,190 --> 00:06:46,510
wouldn't it be interesting to

00:06:43,860 --> 00:06:49,000
continuously profile con probe to find a

00:06:46,510 --> 00:06:50,409
memory leak in cron proof right so right

00:06:49,000 --> 00:06:54,580
here we see something this happened

00:06:50,409 --> 00:06:56,740
yesterday evening or yesterday night and

00:06:54,580 --> 00:06:58,720
we could we don't have a profile of this

00:06:56,740 --> 00:07:00,970
process or this process in isn't running

00:06:58,720 --> 00:07:02,770
anymore but because we have contra we

00:07:00,970 --> 00:07:04,180
have continued to be profiled it so

00:07:02,770 --> 00:07:07,960
let's look at couple that a couple of

00:07:04,180 --> 00:07:10,810
these profiles over time so we've got

00:07:07,960 --> 00:07:12,729
this one and we've got this one for

00:07:10,810 --> 00:07:15,069
example so I already did the hard work

00:07:12,729 --> 00:07:17,259
of choosing a time rich roughly where

00:07:15,069 --> 00:07:18,969
this process was still available and we

00:07:17,259 --> 00:07:21,069
can see that the majority of this

00:07:18,969 --> 00:07:24,099
process keeps being allocated by this

00:07:21,069 --> 00:07:26,020
particular part of the program so now I

00:07:24,099 --> 00:07:27,849
know that this is actually where should

00:07:26,020 --> 00:07:31,599
be looking first when I troubleshoot my

00:07:27,849 --> 00:07:34,240
application and want to fix it

00:07:31,599 --> 00:07:36,819
so that's basically the functionality of

00:07:34,240 --> 00:07:40,449
con craft that exists today and you can

00:07:36,819 --> 00:07:42,659
check it out today with for all this

00:07:40,449 --> 00:07:45,099
functionality that you just seen but

00:07:42,659 --> 00:07:46,840
actually there's something because I

00:07:45,099 --> 00:07:48,310
still have roughly a minute of time that

00:07:46,840 --> 00:07:49,870
I want to talk to you about which i

00:07:48,310 --> 00:07:51,759
think is pretty super cool and if you

00:07:49,870 --> 00:07:53,830
want to help me implement this then

00:07:51,759 --> 00:07:56,580
please approach me afterwards or just go

00:07:53,830 --> 00:07:58,479
for it and implement and sent me a PR so

00:07:56,580 --> 00:08:01,449
Prometheus has the remote read

00:07:58,479 --> 00:08:03,370
functionality where you could convert

00:08:01,449 --> 00:08:05,469
any kind of data into time series data

00:08:03,370 --> 00:08:07,719
and then preemie and query it and

00:08:05,469 --> 00:08:10,000
visualize it with Prometheus right so we

00:08:07,719 --> 00:08:12,190
could convert multiple profiles over

00:08:10,000 --> 00:08:14,710
time and that as it happens sampled in a

00:08:12,190 --> 00:08:19,330
PF profile are also identified by a key

00:08:14,710 --> 00:08:21,370
key value label pair so we can have the

00:08:19,330 --> 00:08:23,889
exact same data model over time and if

00:08:21,370 --> 00:08:25,839
there is any sample that is growing over

00:08:23,889 --> 00:08:28,870
time then we've just found identified

00:08:25,839 --> 00:08:29,889
our memory league automatically so if

00:08:28,870 --> 00:08:36,070
you want to help me implement that

00:08:29,889 --> 00:08:39,070
please approach me thank you I'm from

00:08:36,070 --> 00:08:42,370
IBM and I have been working with the

00:08:39,070 --> 00:08:44,680
largest telecom provider in India for

00:08:42,370 --> 00:08:47,070
IBM so basically I was part of

00:08:44,680 --> 00:08:49,420
application modernization team there and

00:08:47,070 --> 00:08:52,470
after the application modernization was

00:08:49,420 --> 00:08:55,089
done we were using promises for

00:08:52,470 --> 00:08:57,579
basically monitoring the data for all

00:08:55,089 --> 00:09:00,279
the micro services so today I will be

00:08:57,579 --> 00:09:02,260
sharing my experience that we have gone

00:09:00,279 --> 00:09:04,750
through and how we overcome the

00:09:02,260 --> 00:09:07,580
challenges that we got in monitoring the

00:09:04,750 --> 00:09:10,130
micro services so

00:09:07,580 --> 00:09:11,780
after all the application was developed

00:09:10,130 --> 00:09:13,730
in basically the although monolithic

00:09:11,780 --> 00:09:16,060
applications got converted to micro

00:09:13,730 --> 00:09:20,230
services the challenge that we faced in

00:09:16,060 --> 00:09:20,230
monitoring those micro services was

00:09:20,670 --> 00:09:25,060
so as you can see all the operations

00:09:23,470 --> 00:09:26,980
team before in the staging environment

00:09:25,060 --> 00:09:29,170
when we delivered it to them they told

00:09:26,980 --> 00:09:31,270
us that okay this is monolithic

00:09:29,170 --> 00:09:33,690
applications monitoring was bit was

00:09:31,270 --> 00:09:37,540
easier because we have new dimensions in

00:09:33,690 --> 00:09:39,100
having micro services so basically we

00:09:37,540 --> 00:09:41,170
were getting issues like horizontal pod

00:09:39,100 --> 00:09:43,330
auto scaling not working and disk

00:09:41,170 --> 00:09:46,720
available disk available to a specific

00:09:43,330 --> 00:09:49,690
application is as its full capacity some

00:09:46,720 --> 00:09:51,580
nodes in clusters are oversubscribed and

00:09:49,690 --> 00:09:53,710
network bottleneck caused by dropped

00:09:51,580 --> 00:09:55,210
packets so basically the issues that we

00:09:53,710 --> 00:09:58,180
were getting in the promises at that

00:09:55,210 --> 00:10:00,450
point of time we used that data to

00:09:58,180 --> 00:10:04,420
basically replan our infrastructure

00:10:00,450 --> 00:10:07,930
based on the data that we received so

00:10:04,420 --> 00:10:10,420
basically the data that we receive based

00:10:07,930 --> 00:10:13,060
on that we do we basically replant our

00:10:10,420 --> 00:10:15,820
infrastructure CPU Ram network planning

00:10:13,060 --> 00:10:19,120
deployment everything and after this is

00:10:15,820 --> 00:10:21,100
done still our life was not easy so

00:10:19,120 --> 00:10:23,620
basically the day-to-day scenario that

00:10:21,100 --> 00:10:27,190
we were seeing is I let manager was

00:10:23,620 --> 00:10:30,490
sending alerts to for unhealthy services

00:10:27,190 --> 00:10:32,320
and there were some fake alerts so I the

00:10:30,490 --> 00:10:34,030
admin team has to identify which was

00:10:32,320 --> 00:10:36,300
that taker whose ticket to be open and

00:10:34,030 --> 00:10:38,530
that's the biggest challenge was that

00:10:36,300 --> 00:10:40,360
ticket that they need to open their need

00:10:38,530 --> 00:10:43,750
to first make sure that it is not for

00:10:40,360 --> 00:10:46,660
admin it is not for it is not a fake

00:10:43,750 --> 00:10:49,120
alert and also he need to basically go

00:10:46,660 --> 00:10:50,560
manually and open that ticket and also

00:10:49,120 --> 00:10:54,520
you need to make sure that he don't open

00:10:50,560 --> 00:10:59,190
the duplicate tickets so how we overcome

00:10:54,520 --> 00:10:59,190
it so basically first of all we

00:11:01,470 --> 00:11:05,730
yeah so the fake alert that we were

00:11:04,080 --> 00:11:08,070
getting we basically overcome those

00:11:05,730 --> 00:11:10,260
things by using grouping inhibition and

00:11:08,070 --> 00:11:13,260
silences so in grouping what we define

00:11:10,260 --> 00:11:15,120
basically what what all alerts are

00:11:13,260 --> 00:11:16,920
similar so basically those alerts will

00:11:15,120 --> 00:11:18,840
go in a group only one ticket will be

00:11:16,920 --> 00:11:21,290
open for that only one alert will be

00:11:18,840 --> 00:11:23,790
generated basically and the inhibition

00:11:21,290 --> 00:11:25,350
basically if I alert is being open so

00:11:23,790 --> 00:11:27,060
the same alert should not be open for a

00:11:25,350 --> 00:11:28,860
certain period of time and then silences

00:11:27,060 --> 00:11:30,450
basically those alerts that we suppose

00:11:28,860 --> 00:11:32,190
the node is under maintenance and we

00:11:30,450 --> 00:11:34,650
know that the alert will come so

00:11:32,190 --> 00:11:36,810
basically we don't send those alert

00:11:34,650 --> 00:11:39,350
manager don't send this to the intended

00:11:36,810 --> 00:11:40,970
target like alert like the webhook our

00:11:39,350 --> 00:11:44,790
emails

00:11:40,970 --> 00:11:46,620
so after this was done the next

00:11:44,790 --> 00:11:48,720
challenge that we were facing was when

00:11:46,620 --> 00:11:50,700
there is any issue basically our

00:11:48,720 --> 00:11:52,740
prisoners operations team will need to

00:11:50,700 --> 00:11:54,660
go and when we'll check what are the

00:11:52,740 --> 00:11:56,490
alerts are and they need to open the

00:11:54,660 --> 00:11:58,380
ticket so basically as we know in case

00:11:56,490 --> 00:12:00,720
of my crew services and it is a very new

00:11:58,380 --> 00:12:02,550
for all that email so basically if they

00:12:00,720 --> 00:12:04,200
take time to open that ticket like they

00:12:02,550 --> 00:12:06,870
take around 2 to 3 minutes or 5 minutes

00:12:04,200 --> 00:12:08,250
10 minutes when we want that that team

00:12:06,870 --> 00:12:10,350
which I developed the micro service

00:12:08,250 --> 00:12:12,600
should be involved right after the issue

00:12:10,350 --> 00:12:14,040
has come so we wanted that that

00:12:12,600 --> 00:12:16,740
ticketing system should be integrated

00:12:14,040 --> 00:12:18,900
with the basically the alerts should

00:12:16,740 --> 00:12:20,880
directly go to detecting system but the

00:12:18,900 --> 00:12:25,080
challenge was it is not out-of-the-box

00:12:20,880 --> 00:12:27,090
supported for us so we defined one more

00:12:25,080 --> 00:12:29,490
component between a technique system and

00:12:27,090 --> 00:12:33,290
alert manager that was fitting system

00:12:29,490 --> 00:12:33,290
provider so what it does basically

00:12:33,310 --> 00:12:39,129
it basically collects all the data that

00:12:36,069 --> 00:12:42,879
comes and then after collecting the data

00:12:39,129 --> 00:12:44,410
it basically opens basically the format

00:12:42,879 --> 00:12:46,360
the data comes for the ticketing system

00:12:44,410 --> 00:12:48,850
provider it converts it to the format

00:12:46,360 --> 00:12:50,709
that ticketing system understands like

00:12:48,850 --> 00:12:52,449
ServiceNow has a different standard

00:12:50,709 --> 00:12:54,610
format in which if data goes then only

00:12:52,449 --> 00:12:56,110
they open the ticket so that part is

00:12:54,610 --> 00:12:59,279
done by ticketing system provider and

00:12:56,110 --> 00:13:02,350
also the but still the challenge was

00:12:59,279 --> 00:13:07,029
they are at there are ten to twenty

00:13:02,350 --> 00:13:09,310
teams in the internal system so we so in

00:13:07,029 --> 00:13:11,589
the system so basically how we will make

00:13:09,310 --> 00:13:13,569
sure that the ticketing system should

00:13:11,589 --> 00:13:15,930
open that ticket to the particular team

00:13:13,569 --> 00:13:18,370
so for that in the micro services

00:13:15,930 --> 00:13:21,480
development we have defined such such a

00:13:18,370 --> 00:13:24,910
way that the matrixes should have the

00:13:21,480 --> 00:13:27,279
organization and team name okay so the

00:13:24,910 --> 00:13:29,649
team name with which that ticket will be

00:13:27,279 --> 00:13:33,129
open and that team will immediately

00:13:29,649 --> 00:13:35,170
start working on the issues so largely

00:13:33,129 --> 00:13:37,209
as you can see there the notification

00:13:35,170 --> 00:13:40,209
received by the Bev hook the receiver is

00:13:37,209 --> 00:13:42,339
this and then we for ticketing system

00:13:40,209 --> 00:13:44,439
provider the code that we have written

00:13:42,339 --> 00:13:47,170
so that it converts it to the former

00:13:44,439 --> 00:13:50,240
detecting system understands and this is

00:13:47,170 --> 00:13:53,699
it thank you very much Sanjay

00:13:50,240 --> 00:13:56,589
[Applause]

00:13:53,699 --> 00:14:00,699
my name is spoken Norton samaya I'm the

00:13:56,589 --> 00:14:01,990
founder and CEO of Tata net we were

00:14:00,699 --> 00:14:04,809
working in the northern part of Germany

00:14:01,990 --> 00:14:07,540
and delivering services with open ii

00:14:04,809 --> 00:14:09,519
communities so our customers and we got

00:14:07,540 --> 00:14:12,670
in talked with them went to beam

00:14:09,519 --> 00:14:15,939
operator and our region miss fondant and

00:14:12,670 --> 00:14:18,189
they operate operate these huge winter

00:14:15,939 --> 00:14:20,740
beans they are 120 meters in height and

00:14:18,189 --> 00:14:25,930
have power amount of 2 or 3 megawatts

00:14:20,740 --> 00:14:28,240
and yeah we got to talk to them and they

00:14:25,930 --> 00:14:31,740
already have some kind of SCADA system

00:14:28,240 --> 00:14:34,569
which have this data in place and

00:14:31,740 --> 00:14:36,670
they've got an engine dialect a sope

00:14:34,569 --> 00:14:40,509
dialect OPC XML da

00:14:36,670 --> 00:14:44,110
and fortunately we find a library in the

00:14:40,509 --> 00:14:47,020
internet with 0.1 the last release from

00:14:44,110 --> 00:14:51,070
2006 I I think

00:14:47,020 --> 00:14:52,990
so we managed to write and export for

00:14:51,070 --> 00:14:55,570
that and you can see what we get out of

00:14:52,990 --> 00:14:58,470
this operator like the middle wind speed

00:14:55,570 --> 00:15:01,630
the yaw that the axis of the nacelle and

00:14:58,470 --> 00:15:06,670
the power of the plants and this is what

00:15:01,630 --> 00:15:08,320
we export from the winter beans and we

00:15:06,670 --> 00:15:10,950
have some other metrics like operating

00:15:08,320 --> 00:15:15,010
hours temperature of the nacelle and

00:15:10,950 --> 00:15:17,080
yeah power so and this is the

00:15:15,010 --> 00:15:18,790
architecture that we can come up with so

00:15:17,080 --> 00:15:20,770
we get the data from from the left from

00:15:18,790 --> 00:15:24,640
the enecon system and then we have got a

00:15:20,770 --> 00:15:27,700
small metrics extractor and then we pull

00:15:24,640 --> 00:15:30,820
it into a Prometheus database and yeah

00:15:27,700 --> 00:15:32,770
then we put some more effort in to have

00:15:30,820 --> 00:15:36,940
some dashboards and visualizing the data

00:15:32,770 --> 00:15:39,430
which I will show now so yeah graph Anna

00:15:36,940 --> 00:15:43,300
as you may know and you bit on notebook

00:15:39,430 --> 00:15:47,890
and I will switch now and try if it's

00:15:43,300 --> 00:15:51,250
working so and yeah oh thank you very

00:15:47,890 --> 00:15:55,510
much yeah yeah here we have got a

00:15:51,250 --> 00:15:58,870
dashboard so we did dashboard in in

00:15:55,510 --> 00:16:02,470
Jupiter notebook and this is the Kennedy

00:15:58,870 --> 00:16:03,820
Nia inch word so it has got the the

00:16:02,470 --> 00:16:05,950
kilowatts of them went to beam and the

00:16:03,820 --> 00:16:09,550
wind speed on the other on the x-axis

00:16:05,950 --> 00:16:12,310
and you might see here the the Python

00:16:09,550 --> 00:16:15,400
code and in the middle there is a

00:16:12,310 --> 00:16:19,110
Prometheus query here that we get the

00:16:15,400 --> 00:16:24,640
the regions the plants and then we can

00:16:19,110 --> 00:16:28,540
populate the dashboard directly from

00:16:24,640 --> 00:16:31,620
Prometheus so I can select here a plant

00:16:28,540 --> 00:16:35,610
and then it will fetch the data and

00:16:31,620 --> 00:16:39,790
calculate so it to take a second or two

00:16:35,610 --> 00:16:43,540
hopefully it's working well okay usually

00:16:39,790 --> 00:16:45,190
it does and it will fetch that their

00:16:43,540 --> 00:16:47,530
knowledge fetch some data as you see

00:16:45,190 --> 00:16:49,660
here sometimes the window beans are kept

00:16:47,530 --> 00:16:53,400
to a certain amount so this went to bean

00:16:49,660 --> 00:16:55,810
was kept there at two one megawatt and

00:16:53,400 --> 00:16:58,270
yeah this comes from the the grid

00:16:55,810 --> 00:16:59,329
operator sometimes the wind speed is so

00:16:58,270 --> 00:17:01,489
high

00:16:59,329 --> 00:17:03,470
the power to means are kept from the

00:17:01,489 --> 00:17:06,319
grid operator because of the gate is not

00:17:03,470 --> 00:17:10,689
able to consumption of the power so here

00:17:06,319 --> 00:17:14,779
I have the raw data in Prometheus and

00:17:10,689 --> 00:17:16,579
what we may do is we can get the angle

00:17:14,779 --> 00:17:18,470
of a tech of the window beam

00:17:16,579 --> 00:17:20,860
unfortunately the window beans can turn

00:17:18,470 --> 00:17:26,329
several times so we have got three bends

00:17:20,860 --> 00:17:31,039
C and a 360 degrees apart but we can do

00:17:26,329 --> 00:17:36,409
some simple operations and do a model

00:17:31,039 --> 00:17:38,299
ooh and then we can have all this bands

00:17:36,409 --> 00:17:42,110
joint together

00:17:38,299 --> 00:17:45,649
so unfortunately mistake so so we've got

00:17:42,110 --> 00:17:50,149
now one band so this is what Prometheus

00:17:45,649 --> 00:17:55,730
enables us to have a better sight on the

00:17:50,149 --> 00:17:59,779
physical data and on this slide yeah you

00:17:55,730 --> 00:18:02,570
can see the the average went to beam

00:17:59,779 --> 00:18:05,570
average wind speed which we consolidate

00:18:02,570 --> 00:18:07,340
by wind park so there are four wind

00:18:05,570 --> 00:18:09,620
parks here and you can see the

00:18:07,340 --> 00:18:11,840
consolidated speed and on the top you

00:18:09,620 --> 00:18:14,269
can see the the power.keeping from the

00:18:11,840 --> 00:18:16,909
grid operator so every time there's a

00:18:14,269 --> 00:18:19,340
lot of wind energy a lot of winter the

00:18:16,909 --> 00:18:23,570
grid operator kept the power limit so

00:18:19,340 --> 00:18:25,820
it's kind of not so good so the grid's

00:18:23,570 --> 00:18:27,909
really have to be expended extended in

00:18:25,820 --> 00:18:34,820
Germany to take more advantage of these

00:18:27,909 --> 00:18:37,399
renewable energy yeah so we also managed

00:18:34,820 --> 00:18:40,669
to build a graph on our dashboard here

00:18:37,399 --> 00:18:43,130
you can see the consolidated power of

00:18:40,669 --> 00:18:45,769
the wind park so it's got 25 megawatts

00:18:43,130 --> 00:18:49,669
here this company operates 100 window

00:18:45,769 --> 00:18:52,549
beam so it's a lot of power that can be

00:18:49,669 --> 00:18:56,090
potentially generated so you can run

00:18:52,549 --> 00:18:59,299
several data centers from that and I've

00:18:56,090 --> 00:19:02,299
got one video from top of the wind

00:18:59,299 --> 00:19:05,210
turbine that might be interesting so the

00:19:02,299 --> 00:19:08,860
110 meter it's a little bit scary so I'm

00:19:05,210 --> 00:19:08,860
a little bit concerned there

00:19:10,570 --> 00:19:21,820
so it's a huge operation yeah thank you

00:19:13,280 --> 00:19:23,900
very much so first of all big things

00:19:21,820 --> 00:19:26,300
thanks to the video people they just

00:19:23,900 --> 00:19:28,280
brought a smaller screen backstage to

00:19:26,300 --> 00:19:36,140
test the video and doing a overall great

00:19:28,280 --> 00:19:39,050
job though thank you alright so I want

00:19:36,140 --> 00:19:41,150
to talk to you about SLO sand first of

00:19:39,050 --> 00:19:42,500
all I'm Matthias I work at Red Hat

00:19:41,150 --> 00:19:44,450
maintaining some projects like the

00:19:42,500 --> 00:19:48,200
Prometheus operator queue Prometheus and

00:19:44,450 --> 00:19:53,210
became a reason Pharaohs team member for

00:19:48,200 --> 00:19:55,400
maintaining cube Thanos so SLO are known

00:19:53,210 --> 00:19:59,030
from the sree book or many other places

00:19:55,400 --> 00:20:02,990
and they specify a target level for

00:19:59,030 --> 00:20:04,670
reliability for a service and they make

00:20:02,990 --> 00:20:06,680
it happen so that you can make

00:20:04,670 --> 00:20:08,270
data-driven decisions about the

00:20:06,680 --> 00:20:10,970
reliability of your system so they're

00:20:08,270 --> 00:20:12,890
quite essential as a loss however I

00:20:10,970 --> 00:20:15,410
actually build on top of

00:20:12,890 --> 00:20:17,330
SL ice and you can think of them if

00:20:15,410 --> 00:20:20,960
we're talking about red for example of

00:20:17,330 --> 00:20:22,580
requests errors and duration and in

00:20:20,960 --> 00:20:25,220
Prometheus terms this might be something

00:20:22,580 --> 00:20:28,520
like this where we have a rate of the

00:20:25,220 --> 00:20:30,560
HTTP requests total in five minutes and

00:20:28,520 --> 00:20:33,410
then building on top of that again we

00:20:30,560 --> 00:20:36,020
come back to SL O's so you might want to

00:20:33,410 --> 00:20:39,620
have an SLO where you want to be alerted

00:20:36,020 --> 00:20:41,540
if the error rate is above 1% we could

00:20:39,620 --> 00:20:45,260
call this kind of like a service level

00:20:41,540 --> 00:20:47,060
alert in Prometheus trances well yeah so

00:20:45,260 --> 00:20:48,740
we can be smart because we have an SLO

00:20:47,060 --> 00:20:51,560
we also have an error budget and we can

00:20:48,740 --> 00:20:54,140
be smart about alerting on our services

00:20:51,560 --> 00:20:56,900
as a loss and there's something called a

00:20:54,140 --> 00:21:00,160
multi burn rate alert in the sree book

00:20:56,900 --> 00:21:04,040
and we actually combine multiple

00:21:00,160 --> 00:21:07,490
five-minute and 60-minute alerts in this

00:21:04,040 --> 00:21:10,100
case to to come up with something clever

00:21:07,490 --> 00:21:13,940
to not overlook low error rates as well

00:21:10,100 --> 00:21:15,950
and if anything from from prom cone you

00:21:13,940 --> 00:21:18,470
want to take home I really encourage

00:21:15,950 --> 00:21:20,300
everyone to put SLS in place so you can

00:21:18,470 --> 00:21:21,830
actually talk to people about the

00:21:20,300 --> 00:21:22,860
reliability of your servers and have

00:21:21,830 --> 00:21:25,440
data on that

00:21:22,860 --> 00:21:28,169
so yeah let's keep moving forward and

00:21:25,440 --> 00:21:30,990
look at how you can actually make use of

00:21:28,169 --> 00:21:32,490
this in your production system so we

00:21:30,990 --> 00:21:34,740
want to implement this multi burn error

00:21:32,490 --> 00:21:36,450
rate and we need some prompt cure and as

00:21:34,740 --> 00:21:38,490
you can see we have one hour and five

00:21:36,450 --> 00:21:40,950
hours in here

00:21:38,490 --> 00:21:44,159
and this is recording routes actually

00:21:40,950 --> 00:21:47,460
based on something some other recording

00:21:44,159 --> 00:21:48,929
rules again and these are yeah just one

00:21:47,460 --> 00:21:50,730
of the recording rules so this is the

00:21:48,929 --> 00:21:53,399
five-minute recording rule to get the

00:21:50,730 --> 00:21:55,590
ratio of 500 errors in your system and

00:21:53,399 --> 00:21:58,769
we need these four times cause 5 minutes

00:21:55,590 --> 00:22:00,840
30 minutes 1 hour 3 hours and there are

00:21:58,769 --> 00:22:03,200
plenty of more and these are actually

00:22:00,840 --> 00:22:06,240
built on yet another set of alerting

00:22:03,200 --> 00:22:09,149
recording rules to group HTTP requests

00:22:06,240 --> 00:22:13,049
into multiple status groups so 200 300

00:22:09,149 --> 00:22:15,960
400 500 so there's a lot of repetition

00:22:13,049 --> 00:22:19,860
so as many of you heard previously today

00:22:15,960 --> 00:22:22,950
before we have Jason it so there's a

00:22:19,860 --> 00:22:25,260
templating language for for data

00:22:22,950 --> 00:22:27,240
templating and we pretty much use it all

00:22:25,260 --> 00:22:29,669
across the board to template kubernetes

00:22:27,240 --> 00:22:34,200
and Prometheus things and how we can

00:22:29,669 --> 00:22:37,110
also template or SLS with this yeah I

00:22:34,200 --> 00:22:41,399
actually came up with a library a helper

00:22:37,110 --> 00:22:43,860
library in in Jason ed that allows you

00:22:41,399 --> 00:22:46,799
to to generate alerts routes and

00:22:43,860 --> 00:22:50,070
dashboards and I'm here to talk about

00:22:46,799 --> 00:22:52,230
this so this is basically just a small

00:22:50,070 --> 00:22:54,480
object you define and everything else

00:22:52,230 --> 00:23:04,919
will be generated from that here's a

00:22:54,480 --> 00:23:06,990
demo of that right cool so you can see

00:23:04,919 --> 00:23:09,570
on the left hand side you can see the

00:23:06,990 --> 00:23:11,789
actual object as I said previously and

00:23:09,570 --> 00:23:18,090
here's the generator llaman which is

00:23:11,789 --> 00:23:20,909
quite repetitive as I said and we can

00:23:18,090 --> 00:23:23,240
generate this I'm able to tweak

00:23:20,909 --> 00:23:23,240
something

00:23:25,290 --> 00:23:30,670
we can just regenerate everything put in

00:23:28,420 --> 00:23:33,130
different selectors and have a different

00:23:30,670 --> 00:23:35,530
SLO so we can also just change the arrow

00:23:33,130 --> 00:23:38,589
budget everything will be updated

00:23:35,530 --> 00:23:38,589
[Music]

00:23:43,250 --> 00:23:50,600
great my notes a gun but that's not too

00:23:47,570 --> 00:23:52,340
different yeah so plenty of people don't

00:23:50,600 --> 00:23:54,820
like Jason it maybe I don't know you

00:23:52,340 --> 00:23:56,960
should I know these technologies you can

00:23:54,820 --> 00:23:59,330
or I thought let's use these

00:23:56,960 --> 00:24:03,890
technologies to build something so yet

00:23:59,330 --> 00:24:07,130
another demo and here's a web interface

00:24:03,890 --> 00:24:10,159
to generate the llaman files you can

00:24:07,130 --> 00:24:13,250
just put in the availability the metric

00:24:10,159 --> 00:24:15,919
put in some some selectors that you

00:24:13,250 --> 00:24:20,299
might might want to have and generate

00:24:15,919 --> 00:24:23,059
that thing copy it to your entire yeah

00:24:20,299 --> 00:24:34,000
Prometheus and use it thank you very

00:24:23,059 --> 00:24:37,190
much so welcome I am Matthias at redhead

00:24:34,000 --> 00:24:40,159
I'm a senior software engineer and I'm

00:24:37,190 --> 00:24:43,010
working for the OpenStack group there so

00:24:40,159 --> 00:24:50,470
this is a bit different for the main

00:24:43,010 --> 00:24:54,140
audience here so the idea was to get

00:24:50,470 --> 00:24:57,770
metrics from OpenStack clusters and you

00:24:54,140 --> 00:24:59,450
just see one cluster or one compute node

00:24:57,770 --> 00:25:04,030
on the left-hand side but it could be

00:24:59,450 --> 00:25:07,190
one or ten or thousand or however and

00:25:04,030 --> 00:25:10,900
the idea is to use for example collecti

00:25:07,190 --> 00:25:14,990
or the traditional telemetry in

00:25:10,900 --> 00:25:18,620
OpenStack to collect data and to send it

00:25:14,990 --> 00:25:21,409
over to a micro service application

00:25:18,620 --> 00:25:27,230
called the service assurance framework

00:25:21,409 --> 00:25:29,090
and it uses a MQ one bus which has major

00:25:27,230 --> 00:25:31,850
benefits I'm going to get into that and

00:25:29,090 --> 00:25:36,230
also a component we are calling the

00:25:31,850 --> 00:25:40,610
smart gateway it's an awesome name so it

00:25:36,230 --> 00:25:44,299
both provides metrics and also locks or

00:25:40,610 --> 00:25:46,820
events to an endpoint in our or in this

00:25:44,299 --> 00:25:51,220
case here we are using the endpoint for

00:25:46,820 --> 00:25:51,220
metrics to send the data to Prometheus

00:25:52,270 --> 00:26:02,130
the MQ bus as smart enough to provide

00:25:56,740 --> 00:26:05,710
some high availability and just in case

00:26:02,130 --> 00:26:08,710
some core goes a core note goes down it

00:26:05,710 --> 00:26:11,320
gets replaced by other rooters that's

00:26:08,710 --> 00:26:15,940
the basic takeaway you need need here

00:26:11,320 --> 00:26:19,690
and the another benefit of this is it

00:26:15,940 --> 00:26:22,090
also works on edge notes which is the

00:26:19,690 --> 00:26:25,300
next big thing or you could replace edge

00:26:22,090 --> 00:26:29,050
by a 5g or whatever it's just in another

00:26:25,300 --> 00:26:31,840
buzzword but in any case you could just

00:26:29,050 --> 00:26:34,180
send the data through your network which

00:26:31,840 --> 00:26:37,870
is connected from remote computes to

00:26:34,180 --> 00:26:42,880
your central data center and you'll get

00:26:37,870 --> 00:26:48,400
the metrics there so what has it to do

00:26:42,880 --> 00:26:52,630
with OpenStack in cloud environments you

00:26:48,400 --> 00:26:57,190
would also want something like auto

00:26:52,630 --> 00:26:59,410
scaling and since the core components in

00:26:57,190 --> 00:27:00,460
acoustic are a bit slow or a bit a pain

00:26:59,410 --> 00:27:04,450
in the ass

00:27:00,460 --> 00:27:07,600
the idea was to rip off like a eh or no

00:27:04,450 --> 00:27:13,060
key and replace it with primitives and

00:27:07,600 --> 00:27:16,600
alert manager so the way these works is

00:27:13,060 --> 00:27:19,750
you define a software stack like a stack

00:27:16,600 --> 00:27:21,760
or VMs or whatever in template language

00:27:19,750 --> 00:27:25,890
named heat or heat auto-scaling

00:27:21,760 --> 00:27:29,020
templates you define an alarm from when

00:27:25,890 --> 00:27:34,480
some more servers need to be scared up

00:27:29,020 --> 00:27:36,880
or scale down and you check that alarm

00:27:34,480 --> 00:27:40,210
and fire an event if that happens so

00:27:36,880 --> 00:27:45,310
that basically translates to Prometheus

00:27:40,210 --> 00:27:47,650
and alert manager as well but heat heat

00:27:45,310 --> 00:27:50,170
a or eh and no key and so on they are

00:27:47,650 --> 00:27:53,380
all multi-tenant and this is nothing

00:27:50,170 --> 00:28:01,630
known in primitives and alert manager so

00:27:53,380 --> 00:28:04,360
that is becomes a pain as well so the

00:28:01,630 --> 00:28:05,990
other takeaway here is even if you could

00:28:04,360 --> 00:28:09,760
solve the multi-tenant ish

00:28:05,990 --> 00:28:12,350
you you would also add another

00:28:09,760 --> 00:28:15,710
technologist technology stack like

00:28:12,350 --> 00:28:18,470
OpenShift or communities kubernetes just

00:28:15,710 --> 00:28:20,440
to run OpenStack and that's nothing you

00:28:18,470 --> 00:28:25,850
really want to go

00:28:20,440 --> 00:28:27,530
since the auto scaling is a basic cloud

00:28:25,850 --> 00:28:28,820
functionality it should be provided by

00:28:27,530 --> 00:28:34,160
the cloud or by the infrastructure

00:28:28,820 --> 00:28:37,970
itself so so the framework is still a

00:28:34,160 --> 00:28:40,280
useful component for collecting metrics

00:28:37,970 --> 00:28:42,640
and events from your OpenStack

00:28:40,280 --> 00:28:48,140
infrastructure and it is designed for

00:28:42,640 --> 00:28:51,460
consuming loads of data like a hundred

00:28:48,140 --> 00:28:54,860
metrics per second from 5,000 nodes also

00:28:51,460 --> 00:29:00,810
but the auto scaling use case is nothing

00:28:54,860 --> 00:29:06,200
this stack should solve thank you

00:29:00,810 --> 00:29:06,200
[Applause]

00:29:08,900 --> 00:29:14,570
this is improvised I just wanted to talk

00:29:12,290 --> 00:29:15,980
a bit about the new UI so this is you

00:29:14,570 --> 00:29:17,780
know the current UI we have in

00:29:15,980 --> 00:29:19,220
prometheus it's neither particularly

00:29:17,780 --> 00:29:21,590
great nor particularly bad it kind of

00:29:19,220 --> 00:29:23,510
does its job pretty simple we do want to

00:29:21,590 --> 00:29:25,670
keep it simple but we always had the

00:29:23,510 --> 00:29:27,350
wish to you know add some kind of

00:29:25,670 --> 00:29:29,030
state-of-the-art features to it like

00:29:27,350 --> 00:29:31,550
Auto completion syntax highlighting just

00:29:29,030 --> 00:29:34,250
have a bit more of a modern framework to

00:29:31,550 --> 00:29:36,620
actually build on because so far it's

00:29:34,250 --> 00:29:39,410
all been based on basically most of it

00:29:36,620 --> 00:29:41,809
at least on this one huge jquery based

00:29:39,410 --> 00:29:44,210
javascript file which i started in 2012

00:29:41,809 --> 00:29:47,320
and then people just added croft onto it

00:29:44,210 --> 00:29:49,580
and so we had various contribute

00:29:47,320 --> 00:29:52,460
discussions around like building it

00:29:49,580 --> 00:29:53,690
properly in a new framework for various

00:29:52,460 --> 00:29:56,540
reasons they never really ended up

00:29:53,690 --> 00:29:58,730
anywhere but in february i had some time

00:29:56,540 --> 00:29:59,840
and i just you know it's like motivation

00:29:58,730 --> 00:30:01,370
was like okay I'm going to teach myself

00:29:59,840 --> 00:30:02,900
react and I'm going to teach myself

00:30:01,370 --> 00:30:05,530
typescript I'm not really front-end

00:30:02,900 --> 00:30:08,929
person but ended up kind of doing that

00:30:05,530 --> 00:30:12,470
ended up building a new graphing and

00:30:08,929 --> 00:30:14,660
like console tab table UI in react and

00:30:12,470 --> 00:30:17,240
typescript loved it but didn't really

00:30:14,660 --> 00:30:19,010
get to actually integrate it into like

00:30:17,240 --> 00:30:19,730
three weeks ago

00:30:19,010 --> 00:30:21,730
where

00:30:19,730 --> 00:30:25,130
was kind of hairy to do in Prometheus

00:30:21,730 --> 00:30:28,820
it's this huge PR about like 13,000

00:30:25,130 --> 00:30:31,430
lines added and quite you know tough and

00:30:28,820 --> 00:30:33,560
in some ways but the great thing is once

00:30:31,430 --> 00:30:37,310
I managed to do that I tweeted that this

00:30:33,560 --> 00:30:39,170
was three weeks ago and since then my

00:30:37,310 --> 00:30:41,360
god I think the new UI has gotten like

00:30:39,170 --> 00:30:43,850
way more contributions than the old UI

00:30:41,360 --> 00:30:46,220
has gotten in the whole you know many

00:30:43,850 --> 00:30:49,220
years before that so like all these poor

00:30:46,220 --> 00:30:52,070
requests I just search for UI the whole

00:30:49,220 --> 00:30:53,510
page plus like half a page more like

00:30:52,070 --> 00:30:55,520
from five or six different people who

00:30:53,510 --> 00:30:58,010
all flocked in and we're like helping to

00:30:55,520 --> 00:31:00,410
implement various parts of it some are

00:30:58,010 --> 00:31:02,270
in the audience here this is still the

00:31:00,410 --> 00:31:04,700
old your eye but there's a new there's

00:31:02,270 --> 00:31:07,850
in the release that's going to be

00:31:04,700 --> 00:31:09,850
released now we're like roughly today

00:31:07,850 --> 00:31:12,860
right Chris

00:31:09,850 --> 00:31:15,830
release really oh yeah they're the

00:31:12,860 --> 00:31:17,720
release candidates out today as the main

00:31:15,830 --> 00:31:20,090
UI is still the old one there's a link

00:31:17,720 --> 00:31:23,030
to the new experimental UI if you click

00:31:20,090 --> 00:31:25,520
it you get to the experimental one which

00:31:23,030 --> 00:31:27,530
has yeah it has the graphing parts and

00:31:25,520 --> 00:31:31,670
everything that's the amount of percent

00:31:27,530 --> 00:31:34,190
zoom yeah well this and you know some of

00:31:31,670 --> 00:31:35,750
the sub pages by the way the whole knife

00:31:34,190 --> 00:31:38,240
and all the other pages like they were

00:31:35,750 --> 00:31:40,700
added by contributors now it's all a

00:31:38,240 --> 00:31:44,260
single page app now it's based on api's

00:31:40,700 --> 00:31:49,580
and just fetching the data from api's

00:31:44,260 --> 00:31:51,530
and yeah the graph view you know right

00:31:49,580 --> 00:31:54,410
now it's still pretty much what the old

00:31:51,530 --> 00:31:57,800
UI was it's probably also going to stay

00:31:54,410 --> 00:32:00,590
roughly along those lines maybe a bit

00:31:57,800 --> 00:32:02,120
prettier here and there you know there's

00:32:00,590 --> 00:32:04,790
a time picker and all that that you're

00:32:02,120 --> 00:32:06,950
used to and you know but now you've got

00:32:04,790 --> 00:32:08,990
this like very beautiful rotating

00:32:06,950 --> 00:32:13,460
spinner on the left which okay it looks

00:32:08,990 --> 00:32:15,140
too fast but yeah and maybe like

00:32:13,460 --> 00:32:17,180
slightly more beautiful and of course

00:32:15,140 --> 00:32:19,970
it's just the beginning and there's

00:32:17,180 --> 00:32:22,670
already plans like there's an internet

00:32:19,970 --> 00:32:24,110
redhead working on a language server

00:32:22,670 --> 00:32:26,510
protocol implementation on the

00:32:24,110 --> 00:32:29,570
prometheus server side which could then

00:32:26,510 --> 00:32:31,190
on the UI sell side help us to things

00:32:29,570 --> 00:32:33,080
like proper Auto completion syntax

00:32:31,190 --> 00:32:34,610
highlighting we do

00:32:33,080 --> 00:32:37,279
I think we should probably keep them

00:32:34,610 --> 00:32:40,309
very lightweight and maybe even optional

00:32:37,279 --> 00:32:43,159
because auto-complete gone wrong can

00:32:40,309 --> 00:32:44,659
really be frustrating but yeah that's

00:32:43,159 --> 00:32:46,639
kind of the way where we want to go and

00:32:44,659 --> 00:32:48,860
maybe do some smaller other features

00:32:46,639 --> 00:32:51,590
like you know tie pins here and there

00:32:48,860 --> 00:32:53,029
and things like that but we don't want

00:32:51,590 --> 00:32:55,669
to be a competitor with crow fauna of

00:32:53,029 --> 00:32:58,059
course like it's just for this ad hoc

00:32:55,669 --> 00:33:02,289
kind of exploration of data we want to

00:32:58,059 --> 00:33:05,029
give a bit more state of the art new UI

00:33:02,289 --> 00:33:08,840
and yeah so basically try it out file

00:33:05,029 --> 00:33:10,460
box if you see some pages that are not

00:33:08,840 --> 00:33:12,230
implemented yet you get like this kind

00:33:10,460 --> 00:33:16,159
of under construction notice you can go

00:33:12,230 --> 00:33:18,169
back to the classic UI well this should

00:33:16,159 --> 00:33:20,299
work okay yeah maybe we should fix that

00:33:18,169 --> 00:33:23,239
the react Rooter doesn't do that

00:33:20,299 --> 00:33:26,119
properly yet actually I think no okay

00:33:23,239 --> 00:33:28,429
yeah that's basically it I tagged issues

00:33:26,119 --> 00:33:30,440
related to it with the components less

00:33:28,429 --> 00:33:33,169
reactive I label most of these are kind

00:33:30,440 --> 00:33:34,820
of being worked on already so at the

00:33:33,169 --> 00:33:36,590
moment there's a lot of planes in flight

00:33:34,820 --> 00:33:38,210
but I hope it kind of settles down in

00:33:36,590 --> 00:33:40,369
the next week's and then we can

00:33:38,210 --> 00:33:48,309
stabilize it and give you more cool

00:33:40,369 --> 00:33:52,039
features so cool thank you okay i'm

00:33:48,309 --> 00:33:54,470
sailin elapsed on Loki and today I want

00:33:52,039 --> 00:33:58,509
to talk about local to give you a quick

00:33:54,470 --> 00:34:02,929
introduction so I want to start by a

00:33:58,509 --> 00:34:06,980
quick question I know you know okay so

00:34:02,929 --> 00:34:12,049
who knows about Lukey nice who is using

00:34:06,980 --> 00:34:16,429
Lukey nice who has tried lucky even more

00:34:12,049 --> 00:34:19,399
nice okay so for those who don't know

00:34:16,429 --> 00:34:20,780
look he's a log aggregation system that

00:34:19,399 --> 00:34:24,619
we built a goth night it's all open

00:34:20,780 --> 00:34:27,500
source so give it a try if you want to

00:34:24,619 --> 00:34:31,220
check it out so what is locked well you

00:34:27,500 --> 00:34:33,379
see luck shell is the language to query

00:34:31,220 --> 00:34:36,530
for logs unlucky

00:34:33,379 --> 00:34:39,710
it's a be inspired by Punk you'll that

00:34:36,530 --> 00:34:40,549
Yan talked today and I actually had a

00:34:39,710 --> 00:34:43,570
lot of slidable

00:34:40,549 --> 00:34:45,340
gel to explain that thank you

00:34:43,570 --> 00:34:46,570
that's awesome so I'm not gonna have to

00:34:45,340 --> 00:34:48,610
talk about this

00:34:46,570 --> 00:34:50,710
so it's heavily inspired by puncture so

00:34:48,610 --> 00:34:53,679
you don't have to learn learn another

00:34:50,710 --> 00:34:55,270
language as Yun explained today learning

00:34:53,679 --> 00:34:59,230
another language is sometimes difficult

00:34:55,270 --> 00:35:01,440
and we want it to be distributed clap so

00:34:59,230 --> 00:35:06,520
you can you know query across principal

00:35:01,440 --> 00:35:08,620
acosta workload so you know in puncture

00:35:06,520 --> 00:35:11,140
right now when you want to select data

00:35:08,620 --> 00:35:13,630
you have a metric name label matches

00:35:11,140 --> 00:35:17,920
that defines your workload so you know

00:35:13,630 --> 00:35:20,980
cluster job instances all of this is you

00:35:17,920 --> 00:35:22,750
know you know about this in luxury it's

00:35:20,980 --> 00:35:24,490
pretty much the same except that we

00:35:22,750 --> 00:35:27,400
don't have a magic name because there's

00:35:24,490 --> 00:35:31,180
a metric so we still have labeled

00:35:27,400 --> 00:35:34,120
matches and you know if you use the same

00:35:31,180 --> 00:35:36,760
set of label with your metrics you can

00:35:34,120 --> 00:35:41,950
also have an you can switch in logs and

00:35:36,760 --> 00:35:44,440
metrics and we also added filter

00:35:41,950 --> 00:35:47,430
expression so filter expression allows

00:35:44,440 --> 00:35:50,230
you to actually shoot on the log itself

00:35:47,430 --> 00:35:54,040
so we support the same kind of filter

00:35:50,230 --> 00:35:57,490
has punctual as so you know contain a

00:35:54,040 --> 00:36:01,660
string does not contain and much as

00:35:57,490 --> 00:36:04,420
regular expression we recently added a

00:36:01,660 --> 00:36:07,150
new feature where you can count logs so

00:36:04,420 --> 00:36:08,440
it works using regiment Ranger vector so

00:36:07,150 --> 00:36:10,030
I'm not gonna explain what's range

00:36:08,440 --> 00:36:12,970
vector again today I think we have this

00:36:10,030 --> 00:36:17,320
twice but the idea is you can have the

00:36:12,970 --> 00:36:18,520
per second rate of your logs and also

00:36:17,320 --> 00:36:21,580
you can combine that with the filter

00:36:18,520 --> 00:36:24,070
expression so that's about filtering on

00:36:21,580 --> 00:36:28,480
logs and so you can check you know how

00:36:24,070 --> 00:36:32,140
many air holes a whole logs I have in in

00:36:28,480 --> 00:36:35,440
a time frame so since this outputs

00:36:32,140 --> 00:36:37,270
matrix you can use the same aggregation

00:36:35,440 --> 00:36:41,410
as we seen today

00:36:37,270 --> 00:36:44,040
so the sum min max top k and we're gonna

00:36:41,410 --> 00:36:44,040
do a quick demo

00:36:47,090 --> 00:36:52,640
so let's let's imagine that I have a

00:36:49,670 --> 00:36:55,070
choice right I'm trying to figure out

00:36:52,640 --> 00:36:57,710
what's going on you may not be able to

00:36:55,070 --> 00:37:00,560
see it but this tracer is actually free

00:36:57,710 --> 00:37:03,440
different services and I want to if I

00:37:00,560 --> 00:37:04,880
want to actually look at the logs I'm

00:37:03,440 --> 00:37:08,060
gonna I'm gonna actually have too many

00:37:04,880 --> 00:37:10,730
logs so I use the the deaf namespace in

00:37:08,060 --> 00:37:12,410
US central and there's too many logs for

00:37:10,730 --> 00:37:15,980
me currently so that's not what I'm

00:37:12,410 --> 00:37:19,730
working on and I want to filter only on

00:37:15,980 --> 00:37:25,730
the queries that I'm I'm walking so for

00:37:19,730 --> 00:37:30,200
that I can use a filter expression right

00:37:25,730 --> 00:37:32,030
so I'm giving the choice ID and now I

00:37:30,200 --> 00:37:35,240
can see only the logs related to that

00:37:32,030 --> 00:37:38,290
trace so Griffin has a nice UI we can

00:37:35,240 --> 00:37:45,320
see it's actually showing the matches

00:37:38,290 --> 00:37:48,080
and sorry last one so this is the so

00:37:45,320 --> 00:37:49,760
since the loki is one or two percent

00:37:48,080 --> 00:37:55,190
compatible with the poem it is that a

00:37:49,760 --> 00:37:56,660
sauce API you can use Loki as a data

00:37:55,190 --> 00:37:58,850
source and that's that's what we can see

00:37:56,660 --> 00:38:01,100
here so it's a Loki permit is that the

00:37:58,850 --> 00:38:04,010
source and I'm you know you probably

00:38:01,100 --> 00:38:05,990
know about this keyword in golang you

00:38:04,010 --> 00:38:10,130
know the air hockey world so I'm gonna

00:38:05,990 --> 00:38:11,420
look at you know in exactly I think it's

00:38:10,130 --> 00:38:14,390
free our yeah

00:38:11,420 --> 00:38:24,740
three hours what about the job doing

00:38:14,390 --> 00:38:27,770
arrows and yeah that's it okay hello

00:38:24,740 --> 00:38:29,630
everyone again I think those issues I

00:38:27,770 --> 00:38:31,550
don't know like I just joined wet cuts

00:38:29,630 --> 00:38:33,400
and I have a federal laptop from two

00:38:31,550 --> 00:38:36,980
months so I'm learning those tricks

00:38:33,400 --> 00:38:38,390
hopefully better now it's actually

00:38:36,980 --> 00:38:43,340
pretty deterministic it was my fault

00:38:38,390 --> 00:38:45,500
anyway so okay I want to just quickly go

00:38:43,340 --> 00:38:47,300
through spend five minutes we'd be proud

00:38:45,500 --> 00:38:48,740
to go through the remote switch

00:38:47,300 --> 00:38:52,010
improvements we did this year on the

00:38:48,740 --> 00:38:54,110
prom to use side and particularly why

00:38:52,010 --> 00:38:55,700
it's important and what we improved so

00:38:54,110 --> 00:38:58,850
first of all what is remote with API

00:38:55,700 --> 00:39:00,830
remote read is this endpoint slash

00:38:58,850 --> 00:39:02,750
read that is available on a prompt you

00:39:00,830 --> 00:39:05,510
server and it allows you to select a

00:39:02,750 --> 00:39:08,300
series from the internal database the

00:39:05,510 --> 00:39:11,210
prom Hughes is using which is TS DB it

00:39:08,300 --> 00:39:12,980
is it fetches serious much a series and

00:39:11,210 --> 00:39:14,060
in the same time query range that's

00:39:12,980 --> 00:39:16,340
exactly the same

00:39:14,060 --> 00:39:17,810
however query range goes to the Frida

00:39:16,340 --> 00:39:19,940
pro prom Quayle

00:39:17,810 --> 00:39:22,910
engine that does the evaluation and

00:39:19,940 --> 00:39:25,370
staff however the important is the

00:39:22,910 --> 00:39:27,890
important part is both remote read and

00:39:25,370 --> 00:39:31,100
the query endpoints are using exactly

00:39:27,890 --> 00:39:34,700
the same interface endpoints to the tcp

00:39:31,100 --> 00:39:36,770
internal database now what what are the

00:39:34,700 --> 00:39:38,420
use cases for remote read so it is used

00:39:36,770 --> 00:39:41,780
mainly for the integrating with other

00:39:38,420 --> 00:39:43,940
systems so it was built to mainly for

00:39:41,780 --> 00:39:45,590
the service to service communication

00:39:43,940 --> 00:39:49,790
that's why it was designed from scratch

00:39:45,590 --> 00:39:53,080
in the protobuf over HTTP and the

00:39:49,790 --> 00:39:56,750
protobuf is like one of manager

00:39:53,080 --> 00:39:59,030
serialization mechanism from google and

00:39:56,750 --> 00:40:01,880
actually pretty neat one so what this

00:39:59,030 --> 00:40:04,250
message returns it returns you the

00:40:01,880 --> 00:40:06,620
requested series all the cigarettes

00:40:04,250 --> 00:40:08,720
series within one message with all

00:40:06,620 --> 00:40:11,840
correspondence labels and row samples

00:40:08,720 --> 00:40:13,550
and the use cases would be you know the

00:40:11,840 --> 00:40:15,890
Prometheus reading from another permit

00:40:13,550 --> 00:40:19,280
use may be different versions or tano's

00:40:15,890 --> 00:40:23,750
reading from from juice that's very well

00:40:19,280 --> 00:40:25,990
use kind of use user of the very popular

00:40:23,750 --> 00:40:29,120
user of the remote region nowadays and

00:40:25,990 --> 00:40:30,830
and also you might have a prompt you

00:40:29,120 --> 00:40:34,520
Sweeting photogra totally from different

00:40:30,830 --> 00:40:36,680
system that is totally different than

00:40:34,520 --> 00:40:39,200
the primitives in internal database

00:40:36,680 --> 00:40:42,800
however expose metrics in their similar

00:40:39,200 --> 00:40:44,180
form so prom ql can avail a that data so

00:40:42,800 --> 00:40:47,900
what is the problem with this approach

00:40:44,180 --> 00:40:50,780
so first of all yeah during the request

00:40:47,900 --> 00:40:53,900
you can see that the internals

00:40:50,780 --> 00:40:55,930
we select the data from the internal

00:40:53,900 --> 00:40:59,630
database right and the syria and the

00:40:55,930 --> 00:41:01,070
response is a series by series stream of

00:40:59,630 --> 00:41:03,020
series by series right and because

00:41:01,070 --> 00:41:05,150
everything we store are mostly

00:41:03,020 --> 00:41:07,970
compressed in chunks also the samples

00:41:05,150 --> 00:41:10,250
are compressed in that Sora and convict

00:41:07,970 --> 00:41:12,650
format called chunks on each iteration

00:41:10,250 --> 00:41:15,680
we have to decode those into Rosa

00:41:12,650 --> 00:41:18,860
and put into huge buffer to feel this

00:41:15,680 --> 00:41:21,440
protobuf message so you can imagine that

00:41:18,860 --> 00:41:24,530
you know it takes time just to feel that

00:41:21,440 --> 00:41:27,580
there are the requests and only after

00:41:24,530 --> 00:41:30,830
it's after all the series are done

00:41:27,580 --> 00:41:32,960
returned we can march out this big

00:41:30,830 --> 00:41:35,450
protobuf and and compress it and send to

00:41:32,960 --> 00:41:37,700
the client so what's the outcome yeah we

00:41:35,450 --> 00:41:38,900
use excessive amount of resources for

00:41:37,700 --> 00:41:41,810
example for like eight hours ten

00:41:38,900 --> 00:41:43,190
thousand Syria series both from tears

00:41:41,810 --> 00:41:45,410
and tunnels were using a lot of memory

00:41:43,190 --> 00:41:48,770
mainly because it has to buffer all the

00:41:45,410 --> 00:41:51,260
road samples on both color and and the

00:41:48,770 --> 00:41:53,660
server side now what we did to improve

00:41:51,260 --> 00:41:55,850
that we introduced a new protocol of

00:41:53,660 --> 00:41:59,860
message response so you can use exactly

00:41:55,850 --> 00:42:04,160
the same remote reach but from profused

00:41:59,860 --> 00:42:07,190
2.13 you could ask for the chunk streams

00:42:04,160 --> 00:42:09,890
or chunks response which gives you what

00:42:07,190 --> 00:42:11,450
chunks instead of pharaoh samples so

00:42:09,890 --> 00:42:14,450
this allows you to reuse exactly the

00:42:11,450 --> 00:42:17,030
same format we use from juice use on the

00:42:14,450 --> 00:42:20,030
storage level now we also allow this

00:42:17,030 --> 00:42:21,830
message to be chunked and chunk by whole

00:42:20,030 --> 00:42:24,230
series or portion of the series with

00:42:21,830 --> 00:42:27,170
correspondent labels and chunks and we

00:42:24,230 --> 00:42:29,960
stream those frames into a maximum of

00:42:27,170 --> 00:42:34,040
one megabyte so what we can now do we

00:42:29,960 --> 00:42:36,200
can stream this this we can stream

00:42:34,040 --> 00:42:38,660
directly through all the all the steps

00:42:36,200 --> 00:42:40,130
we select and we grab all the series by

00:42:38,660 --> 00:42:42,110
series and for each we frame it

00:42:40,130 --> 00:42:44,240
compressed Marshall and send it or

00:42:42,110 --> 00:42:46,850
Martian compress and send it what's the

00:42:44,240 --> 00:42:50,300
outcome we reduced the amount of memory

00:42:46,850 --> 00:42:52,760
for the same request and normally so

00:42:50,300 --> 00:42:55,700
we're just this selection takes four

00:42:52,760 --> 00:42:57,950
primitives some amount of memory

00:42:55,700 --> 00:42:59,510
however itano's is like almost zero and

00:42:57,950 --> 00:43:03,410
the constant amount of memory because it

00:42:59,510 --> 00:43:05,450
frames it puts that remotes with API

00:43:03,410 --> 00:43:06,080
response and put into j RPC streams and

00:43:05,450 --> 00:43:08,660
nothing more

00:43:06,080 --> 00:43:11,990
so this also reduce latency and and and

00:43:08,660 --> 00:43:15,200
to more and like the CPU time uses so

00:43:11,990 --> 00:43:17,150
anyway the over the outcome of this that

00:43:15,200 --> 00:43:18,920
is that use the newest protein to use if

00:43:17,150 --> 00:43:21,050
you integrate with promy to use use the

00:43:18,920 --> 00:43:22,790
newest new remote with api as well and

00:43:21,050 --> 00:43:26,250
here are the links and one of the link

00:43:22,790 --> 00:43:29,160
is like goal and client that already

00:43:26,250 --> 00:43:34,920
have benefit of using streamed version

00:43:29,160 --> 00:43:38,010
of the remote feed table tag hello

00:43:34,920 --> 00:43:40,680
and you know I talked to you about my

00:43:38,010 --> 00:43:44,190
work in Debian for Prometheus packages

00:43:40,680 --> 00:43:46,290
um it's the fourth time I do this talk I

00:43:44,190 --> 00:43:48,210
probably bore apple see me but well keep

00:43:46,290 --> 00:43:51,300
on working so I keep on reporting and

00:43:48,210 --> 00:43:54,030
also like this community so um I'm a

00:43:51,300 --> 00:43:56,580
Debian contributor since 2005 became a

00:43:54,030 --> 00:43:59,820
developer in 2008 I used to work at

00:43:56,580 --> 00:44:03,869
Google that's where I got into Boorman

00:43:59,820 --> 00:44:06,540
and then Prometheus and for past 6 years

00:44:03,869 --> 00:44:08,730
have been a freelancer so I gave a talk

00:44:06,540 --> 00:44:10,800
about that last year you can watch it

00:44:08,730 --> 00:44:16,950
it's great yeah freelancer don't work

00:44:10,800 --> 00:44:19,590
for companies so a bit more about me

00:44:16,950 --> 00:44:22,619
yeah probably many people here know me

00:44:19,590 --> 00:44:26,099
from before I look different and I use a

00:44:22,619 --> 00:44:35,609
different name I'm 42 almost and I'm

00:44:26,099 --> 00:44:37,440
trans and I'm very happy thank you so

00:44:35,609 --> 00:44:41,369
little show hands who is this Debian or

00:44:37,440 --> 00:44:43,650
derivatives in production Hey who uses

00:44:41,369 --> 00:44:44,220
my packages we might permit those

00:44:43,650 --> 00:44:47,580
packages

00:44:44,220 --> 00:44:52,500
well a few that's great my work is still

00:44:47,580 --> 00:44:55,170
useful I'm glad well by Debian they

00:44:52,500 --> 00:44:58,760
usually think I say we're not hip but we

00:44:55,170 --> 00:45:02,190
still relevant mmm we run on everything

00:44:58,760 --> 00:45:06,480
on space stations and on cell phones and

00:45:02,190 --> 00:45:08,490
embedded devices mmm many release arches

00:45:06,480 --> 00:45:10,410
so that means like nine nine

00:45:08,490 --> 00:45:10,710
architectures are released every two

00:45:10,410 --> 00:45:14,339
years

00:45:10,710 --> 00:45:16,710
yeah with like 99% of the packages and

00:45:14,339 --> 00:45:20,359
we have like 120 derivative

00:45:16,710 --> 00:45:20,359
distributions in Korea new boom - means

00:45:24,950 --> 00:45:28,760
we provide trusted binaries that must be

00:45:27,990 --> 00:45:30,960
reproducible

00:45:28,760 --> 00:45:32,400
golang is still no reproducible because

00:45:30,960 --> 00:45:36,359
still some issues with the compiler but

00:45:32,400 --> 00:45:37,800
almost there we provide back package

00:45:36,359 --> 00:45:39,180
management so what you don't need to

00:45:37,800 --> 00:45:40,300
deal with installing and installing

00:45:39,180 --> 00:45:44,170
things behind

00:45:40,300 --> 00:45:46,270
integration proper defaults security

00:45:44,170 --> 00:45:48,760
support hopefully forego long this time

00:45:46,270 --> 00:45:52,690
we're gonna have security support if we

00:45:48,760 --> 00:45:53,800
manage to get a ego a libraries to

00:45:52,690 --> 00:45:57,010
behave a bit better

00:45:53,800 --> 00:45:59,950
and even if it the release cycles two

00:45:57,010 --> 00:46:05,890
years we do a lot of work on backporting

00:45:59,950 --> 00:46:07,330
stuff so during these two years of the

00:46:05,890 --> 00:46:09,220
really cycle I will be keeping and

00:46:07,330 --> 00:46:13,810
updating the parameters packages for

00:46:09,220 --> 00:46:17,770
their really released Debian so in

00:46:13,810 --> 00:46:19,180
Debian nowadays in the last two releases

00:46:17,770 --> 00:46:21,040
you can just do that and you have

00:46:19,180 --> 00:46:23,650
Prometheus up and running in no time

00:46:21,040 --> 00:46:26,640
a reconfigure atonement or yourself and

00:46:23,650 --> 00:46:26,640
a local not exporter

00:46:27,000 --> 00:46:30,970
most of the packages are available in

00:46:29,380 --> 00:46:34,030
the night architecture's promises

00:46:30,970 --> 00:46:39,010
doesn't work in 386 nowadays but all the

00:46:34,030 --> 00:46:40,750
others work I almost finished by porting

00:46:39,010 --> 00:46:44,200
the latest versions to stretch which is

00:46:40,750 --> 00:46:46,780
their release from M over two years ago

00:46:44,200 --> 00:46:48,210
and about to start backporting to the

00:46:46,780 --> 00:46:50,589
current release with the new versions

00:46:48,210 --> 00:46:52,660
that's a lot of work so whole thing me

00:46:50,589 --> 00:46:56,430
well we still a very small group of

00:46:52,660 --> 00:47:00,220
maintainer so always help is welcome

00:46:56,430 --> 00:47:03,040
these the current status inversions so

00:47:00,220 --> 00:47:05,380
you have the what was released earlier

00:47:03,040 --> 00:47:07,510
this year is this table and the column

00:47:05,380 --> 00:47:10,960
on the right is where I've been working

00:47:07,510 --> 00:47:14,920
the last month or two the ones at risk

00:47:10,960 --> 00:47:18,060
is because I've loaded them today but

00:47:14,920 --> 00:47:21,369
those are all current versions I think

00:47:18,060 --> 00:47:24,700
we have their client libraries also :

00:47:21,369 --> 00:47:27,730
Perl Python Ruby we had like 23

00:47:24,700 --> 00:47:29,760
exporters those are maintained by

00:47:27,730 --> 00:47:33,280
different people not only me

00:47:29,760 --> 00:47:36,040
you can see Apache bind bird black the

00:47:33,280 --> 00:47:39,310
black boxes Porter H a proxy a PMI was

00:47:36,040 --> 00:47:42,369
uploaded like two hours ago and thanks

00:47:39,310 --> 00:47:47,349
to Daniel M mail exporter MongoDB my

00:47:42,369 --> 00:47:50,710
sequel nginx and no post pounds or

00:47:47,349 --> 00:47:54,180
postfix both Postgres process SNMP SQL

00:47:50,710 --> 00:47:58,900
espied traffic server and varnish loads

00:47:54,180 --> 00:48:00,790
so it's a lot of work M Julius is gonna

00:47:58,900 --> 00:48:07,750
make me my life a lot more difficult

00:48:00,790 --> 00:48:09,280
soon and yeah as I always say it's

00:48:07,750 --> 00:48:11,380
difficult to package all this because of

00:48:09,280 --> 00:48:15,310
JavaScript angolan the way these things

00:48:11,380 --> 00:48:19,150
work is constant work constant chasing

00:48:15,310 --> 00:48:21,010
behind the changes and as I said I still

00:48:19,150 --> 00:48:23,319
want to work on JSON net to provide very

00:48:21,010 --> 00:48:26,589
integration but no time many things to

00:48:23,319 --> 00:48:28,450
worry about this year M after

00:48:26,589 --> 00:48:30,490
complications well there's not much time

00:48:28,450 --> 00:48:33,339
to talk about that so if you're curious

00:48:30,490 --> 00:48:34,780
the is the link there with all the

00:48:33,339 --> 00:48:37,150
versions and current status and it's an

00:48:34,780 --> 00:48:39,549
IRC channel you can chat with us

00:48:37,150 --> 00:48:42,039
and thank you very much I'm still

00:48:39,549 --> 00:48:50,950
available for hiring and see you next

00:48:42,039 --> 00:48:53,410
year my next talk hello okay cool so

00:48:50,950 --> 00:48:55,450
just really quick I'm gonna talk about

00:48:53,410 --> 00:48:57,339
some improvements to remote write

00:48:55,450 --> 00:49:02,650
similar to what Bartek just talked about

00:48:57,339 --> 00:49:04,839
earlier all right so this slide I guess

00:49:02,650 --> 00:49:07,630
probably isn't necessary but the the

00:49:04,839 --> 00:49:09,010
gist is that remote write will if you

00:49:07,630 --> 00:49:11,799
configure it to send all of your metrics

00:49:09,010 --> 00:49:13,270
somewhere else other than prometheus for

00:49:11,799 --> 00:49:14,950
various reasons you might want long-term

00:49:13,270 --> 00:49:17,140
storage and some other system for a

00:49:14,950 --> 00:49:19,869
different query language or in our case

00:49:17,140 --> 00:49:22,510
we want it for cortex and a different

00:49:19,869 --> 00:49:24,279
query realization really quickly this is

00:49:22,510 --> 00:49:26,859
the Prometheus architecture and pretty

00:49:24,279 --> 00:49:31,349
sure julius made this diagram thank you

00:49:26,859 --> 00:49:32,589
for this no this is a different one ah

00:49:31,349 --> 00:49:36,609
thank you

00:49:32,589 --> 00:49:39,430
okay so previously I think on the next

00:49:36,609 --> 00:49:41,799
slide previously remote right over in

00:49:39,430 --> 00:49:44,049
this retrieval bit on the left would

00:49:41,799 --> 00:49:47,529
basically make a copy of every scrape

00:49:44,049 --> 00:49:49,359
sample and buffer it all in memory until

00:49:47,529 --> 00:49:53,920
it could successfully send it onto the

00:49:49,359 --> 00:49:55,690
remote right system you can imagine that

00:49:53,920 --> 00:49:57,609
might not be a good idea what happens if

00:49:55,690 --> 00:50:02,140
the remote right endpoint goes down and

00:49:57,609 --> 00:50:04,710
you continually buffer data that buffer

00:50:02,140 --> 00:50:06,940
was a fixed size so if that fixed size

00:50:04,710 --> 00:50:08,470
didn't happen to boom kill your

00:50:06,940 --> 00:50:10,059
Prometheus you would eventually just

00:50:08,470 --> 00:50:10,960
start dropping data which is something

00:50:10,059 --> 00:50:13,569
you don't want either

00:50:10,960 --> 00:50:14,529
if this remote storage system is

00:50:13,569 --> 00:50:16,089
supposed to be for your long-term

00:50:14,529 --> 00:50:18,569
storage but you don't end up with all

00:50:16,089 --> 00:50:23,920
your data there that's not good either

00:50:18,569 --> 00:50:25,809
so in order to fix that the way remote

00:50:23,920 --> 00:50:28,240
ROI works now is it just reads the same

00:50:25,809 --> 00:50:30,460
right ahead log that Prometheus is

00:50:28,240 --> 00:50:32,500
already generating all of your data is

00:50:30,460 --> 00:50:36,579
scraped the right ahead log is written

00:50:32,500 --> 00:50:38,260
that all that data is there until

00:50:36,579 --> 00:50:41,680
eventually something's written to long

00:50:38,260 --> 00:50:43,210
term stored on disk so we just tail the

00:50:41,680 --> 00:50:46,480
same redhead log that is already being

00:50:43,210 --> 00:50:49,599
written there's still an internal fixed

00:50:46,480 --> 00:50:50,980
size buffer the catch is that we don't

00:50:49,599 --> 00:50:54,130
continue to read the write head low

00:50:50,980 --> 00:50:57,460
if the buffer is already full so in

00:50:54,130 --> 00:51:00,940
theory remote right will no longer boom

00:50:57,460 --> 00:51:06,430
your Prometheus the the one side effect

00:51:00,940 --> 00:51:09,369
is that when you scrape your endpoints

00:51:06,430 --> 00:51:12,609
you get the metric name all the possible

00:51:09,369 --> 00:51:14,770
labels and then the value but in the

00:51:12,609 --> 00:51:18,160
right head log format you get the labels

00:51:14,770 --> 00:51:20,170
and metric name once and then from there

00:51:18,160 --> 00:51:21,700
on out you just have a reference to an

00:51:20,170 --> 00:51:24,609
ID and you have to go look up those

00:51:21,700 --> 00:51:26,320
labels so if you've noticed newer

00:51:24,609 --> 00:51:30,460
versions of remote right in the happy

00:51:26,320 --> 00:51:34,960
pass use more memory but in worst case

00:51:30,460 --> 00:51:37,869
scenario it won't crash alright this was

00:51:34,960 --> 00:51:39,880
the the you know end of like six months

00:51:37,869 --> 00:51:43,180
of work but there's been more going on

00:51:39,880 --> 00:51:45,760
since then we're still working on the

00:51:43,180 --> 00:51:49,810
sharding bits but thanks to Chris first

00:51:45,760 --> 00:51:53,290
Chris it mostly works now other than

00:51:49,810 --> 00:51:55,510
when you restart prometheus it doesn't

00:51:53,290 --> 00:51:57,400
necessarily catch up

00:51:55,510 --> 00:51:58,540
very well which is unfortunate but I'm

00:51:57,400 --> 00:52:00,700
working on that

00:51:58,540 --> 00:52:02,319
if you have other questions about Romo

00:52:00,700 --> 00:52:03,010
right find either one of us because

00:52:02,319 --> 00:52:11,079
we're working on it

00:52:03,010 --> 00:52:13,089
that's it thank you I'm Tom I've been in

00:52:11,079 --> 00:52:16,180
internet kohana labs during the summer

00:52:13,089 --> 00:52:18,339
in which I've worked at Tonka which is

00:52:16,180 --> 00:52:21,210
basically this is the somebody around

00:52:18,339 --> 00:52:21,210
you know case on it

00:52:21,900 --> 00:52:26,460
for everyone when happy was acquired by

00:52:24,599 --> 00:52:28,440
the Embera case when it was abandoned

00:52:26,460 --> 00:52:31,380
what we dependent internally on it

00:52:28,440 --> 00:52:35,490
so we decided to basically rebuilt it

00:52:31,380 --> 00:52:37,349
and that's the result of six months

00:52:35,490 --> 00:52:40,380
working on it and six weeks work on it

00:52:37,349 --> 00:52:42,799
it's tanka we're just effectively the

00:52:40,380 --> 00:52:46,529
same city the same thing just and simple

00:52:42,799 --> 00:52:48,960
so the main problem we were about to

00:52:46,529 --> 00:52:49,890
solve is when you're in the world of

00:52:48,960 --> 00:52:52,220
kubernetes you better

00:52:49,890 --> 00:52:55,140
you usually deploy things using search

00:52:52,220 --> 00:52:57,599
manifests but these are environment for

00:52:55,140 --> 00:53:00,630
specifics so you have things like a

00:52:57,599 --> 00:53:03,960
namespace hard-coded or some ingress

00:53:00,630 --> 00:53:06,690
annotation or especially a hostname but

00:53:03,960 --> 00:53:08,490
you might want to reuse these pieces of

00:53:06,690 --> 00:53:11,460
code in other environments like prod or

00:53:08,490 --> 00:53:13,470
deaf or whatever but the kubernetes yamo

00:53:11,460 --> 00:53:16,740
tuning does not really help you there

00:53:13,470 --> 00:53:21,680
but as some other people mentioned today

00:53:16,740 --> 00:53:24,779
as well just trace on it which for this

00:53:21,680 --> 00:53:27,299
basically its trace form but with some

00:53:24,779 --> 00:53:30,299
additional things like it supports

00:53:27,299 --> 00:53:32,609
variables you can see here the local is

00:53:30,299 --> 00:53:35,339
a variable and it's used in JSON it

00:53:32,609 --> 00:53:39,480
supports functions it's especially

00:53:35,339 --> 00:53:41,520
supposed import and one of the most

00:53:39,480 --> 00:53:43,770
interesting features is deep merging so

00:53:41,520 --> 00:53:45,829
if you have one dict you can merge

00:53:43,770 --> 00:53:49,770
another one on it without overwriting

00:53:45,829 --> 00:53:51,180
while over only over writing what is

00:53:49,770 --> 00:53:53,150
specified in the other one and leaving

00:53:51,180 --> 00:53:57,750
the rest intact

00:53:53,150 --> 00:53:59,400
what does it help us well it's basically

00:53:57,750 --> 00:54:03,150
the same as of case on it we can

00:53:59,400 --> 00:54:05,779
modernize our whole kubernetes config we

00:54:03,150 --> 00:54:08,490
can do a high level of code reuse and

00:54:05,779 --> 00:54:10,140
tonker still supports the main workflow

00:54:08,490 --> 00:54:12,270
of chaos on edge you write your dress on

00:54:10,140 --> 00:54:14,400
it you show it as yeah Mel you dip it to

00:54:12,270 --> 00:54:15,990
the kubernetes cluster you apply to the

00:54:14,400 --> 00:54:18,210
cluster once you're happy you have

00:54:15,990 --> 00:54:22,109
environments so you have high-level

00:54:18,210 --> 00:54:24,359
abstracted code outside and only it was

00:54:22,109 --> 00:54:27,180
specific to a single environment is put

00:54:24,359 --> 00:54:29,819
into another dress on that file it's

00:54:27,180 --> 00:54:32,130
rendered you can use your own libraries

00:54:29,819 --> 00:54:35,130
in the lib folder and using JSON at

00:54:32,130 --> 00:54:36,020
bundler from some folks of a Red Hat you

00:54:35,130 --> 00:54:38,660
can use it

00:54:36,020 --> 00:54:41,450
pull in other libraries lycra fondant or

00:54:38,660 --> 00:54:43,430
what has been already mentioned but we

00:54:41,450 --> 00:54:45,530
simplified it a little but we removed

00:54:43,430 --> 00:54:48,530
some concepts that we didn't feel were

00:54:45,530 --> 00:54:50,210
right and all of the package management

00:54:48,530 --> 00:54:52,160
and registry is handed over to JSON

00:54:50,210 --> 00:54:55,550
bundle which leaves you with JSON net

00:54:52,160 --> 00:54:59,060
and environments so comparison with the

00:54:55,550 --> 00:54:59,750
original case on it only one hand

00:54:59,060 --> 00:55:01,339
address on it

00:54:59,750 --> 00:55:03,290
what we want to use and there's

00:55:01,339 --> 00:55:06,200
kubernetes or creep channel where we

00:55:03,290 --> 00:55:08,599
want to get that and tonka is really

00:55:06,200 --> 00:55:11,450
just a small thing in between that

00:55:08,599 --> 00:55:13,849
bridges these two tools and improve the

00:55:11,450 --> 00:55:17,060
user experience so what we have added

00:55:13,849 --> 00:55:20,390
some new cool things is the first one is

00:55:17,060 --> 00:55:23,210
context discovery so tanker really makes

00:55:20,390 --> 00:55:26,750
sure that you at any chance never will

00:55:23,210 --> 00:55:29,630
be able to apply the wrong config to

00:55:26,750 --> 00:55:31,490
your own cluster so you won't apply the

00:55:29,630 --> 00:55:33,349
DEF CON to the broad cluster it will

00:55:31,490 --> 00:55:36,740
warn you and we will hopefully see it in

00:55:33,349 --> 00:55:39,349
the big message at the bottom we

00:55:36,740 --> 00:55:42,380
enhanced it if so when you use case on

00:55:39,349 --> 00:55:45,859
it if you basically got this which is

00:55:42,380 --> 00:55:48,830
not that helpful at all tanker uses nice

00:55:45,859 --> 00:55:52,310
coloring it uses actual output from the

00:55:48,830 --> 00:55:56,150
diff UNIX utility and maybe personally I

00:55:52,310 --> 00:55:58,819
feel it makes it really much simpler

00:55:56,150 --> 00:56:00,440
and of course cube Caudill supported and

00:55:58,819 --> 00:56:03,470
we use it so it's server site if you

00:56:00,440 --> 00:56:05,390
don't have any annotations miss mess or

00:56:03,470 --> 00:56:08,390
whatever and for all terminated versions

00:56:05,390 --> 00:56:11,020
below 1.13 were your subjective which is

00:56:08,390 --> 00:56:13,819
not that nice but it at least works

00:56:11,020 --> 00:56:17,059
another nice thing we had a tab complete

00:56:13,819 --> 00:56:19,940
tab completion to the CLI so if you TK

00:56:17,059 --> 00:56:21,710
DIF some environment it just suggests

00:56:19,940 --> 00:56:23,359
all the environments you can use which

00:56:21,710 --> 00:56:24,950
is real handy because these names are

00:56:23,359 --> 00:56:29,180
long and you really don't want to type

00:56:24,950 --> 00:56:31,099
them another thing targets which is a

00:56:29,180 --> 00:56:36,619
pretty cool thing it allows you to do

00:56:31,099 --> 00:56:38,990
staged rollout so if if you want to be

00:56:36,619 --> 00:56:41,420
able to just like it so a part of it

00:56:38,990 --> 00:56:46,010
before all of it you can specify which

00:56:41,420 --> 00:56:48,470
one the last thing is static analysis we

00:56:46,010 --> 00:56:50,930
have a bot build of a github if you

00:56:48,470 --> 00:56:53,299
change some dress on the library you can

00:56:50,930 --> 00:56:57,049
tell which of your classes is affected

00:56:53,299 --> 00:56:59,599
by it so for the future the first part

00:56:57,049 --> 00:57:01,819
it was an internal presentation before

00:56:59,599 --> 00:57:03,859
we have already done that its life it's

00:57:01,819 --> 00:57:06,470
on github it's on it it'll come from a

00:57:03,859 --> 00:57:10,670
chancre what what I'm currently working

00:57:06,470 --> 00:57:12,109
on is adding a Hershey cobalt secret

00:57:10,670 --> 00:57:14,450
support so that you can pull secrets

00:57:12,109 --> 00:57:17,390
directly from board and another thing is

00:57:14,450 --> 00:57:18,319
that we will add support for deleting or

00:57:17,390 --> 00:57:20,390
chip removed from address for that

00:57:18,319 --> 00:57:22,279
config so you delete something there and

00:57:20,390 --> 00:57:24,020
it will be removed for a cluster as well

00:57:22,279 --> 00:57:26,210
so things we could work on in the future

00:57:24,020 --> 00:57:28,160
would be compiler optimizations and

00:57:26,210 --> 00:57:30,770
tight integration of cube Carol so that

00:57:28,160 --> 00:57:32,660
you just use to curl apply - I have made

00:57:30,770 --> 00:57:37,420
no trace on it thank you

00:57:32,660 --> 00:57:47,130
[Applause]

00:57:37,420 --> 00:57:47,130

YouTube URL: https://www.youtube.com/watch?v=kRVE15j1zxQ


