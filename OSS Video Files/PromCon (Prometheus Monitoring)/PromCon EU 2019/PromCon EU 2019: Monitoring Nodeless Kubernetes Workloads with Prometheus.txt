Title: PromCon EU 2019: Monitoring Nodeless Kubernetes Workloads with Prometheus
Publication date: 2019-12-29
Playlist: PromCon EU 2019
Description: 
	Speaker: Madhuri Yechuri

Emerging Nodeless Kubernetes solutions like virtual-kubelet and Kiyot simplify public cloud k8s node management, save cost, and improve multi-tenant security. This talk explores how Prometheus can be used to monitor Nodeless Kubernetes workloads on public cloud. Demo included.

Slides: https://promcon.io/2019-munich/slides/monitoring-nodeless-kubernetes-workloads-with-prometheus.pdf
Captions: 
	00:00:00,650 --> 00:00:08,910
[Music]

00:00:11,080 --> 00:00:17,000
and everybody my name is Madhuri I am

00:00:14,270 --> 00:00:19,310
the founder makers of nodelist

00:00:17,000 --> 00:00:21,950
kubernetes prior to that I've been a

00:00:19,310 --> 00:00:24,260
systems engineer for 20 years worked on

00:00:21,950 --> 00:00:26,540
Flocka which some of you might remember

00:00:24,260 --> 00:00:28,910
from the early container ecosystem days

00:00:26,540 --> 00:00:31,160
where we helped run stateful

00:00:28,910 --> 00:00:33,440
applications inside containers and prior

00:00:31,160 --> 00:00:36,650
to that worked on resource management

00:00:33,440 --> 00:00:39,170
for the investig-- on initial placement

00:00:36,650 --> 00:00:41,510
load balancing of virtual machines you

00:00:39,170 --> 00:00:44,270
know VMware truster and also VMware

00:00:41,510 --> 00:00:46,790
hypervisor product prior to that worked

00:00:44,270 --> 00:00:49,040
on oracle database server technologies

00:00:46,790 --> 00:00:50,660
in cluster way software so mostly

00:00:49,040 --> 00:00:54,100
systems and distributed systems

00:00:50,660 --> 00:01:01,000
engineering how many of you use

00:00:54,100 --> 00:01:01,000
kubernetes currently with prometheus Wow

00:01:01,570 --> 00:01:06,979
kubernetes capacity planning on public

00:01:04,640 --> 00:01:10,250
cloud is super hard when you take a

00:01:06,979 --> 00:01:13,040
regular kubernetes cluster instead of

00:01:10,250 --> 00:01:15,619
focusing on your applications your pods

00:01:13,040 --> 00:01:17,899
your resource manifests you as the

00:01:15,619 --> 00:01:20,840
DevOps and asari folks spend a lot of

00:01:17,899 --> 00:01:23,450
time on hand curating the worker nodes

00:01:20,840 --> 00:01:27,409
you have to select the base worker node

00:01:23,450 --> 00:01:29,149
type and what price point of the worker

00:01:27,409 --> 00:01:32,179
node you want to select whether it's on

00:01:29,149 --> 00:01:35,390
demand pre-emptive spot or forget kind

00:01:32,179 --> 00:01:37,609
of of compute launch types and also you

00:01:35,390 --> 00:01:41,210
have to hand curate the auto scaling

00:01:37,609 --> 00:01:43,549
knobs and maintain monitor and update

00:01:41,210 --> 00:01:47,119
the auto scaling knobs based on your

00:01:43,549 --> 00:01:49,909
applications workload changes and also

00:01:47,119 --> 00:01:52,579
when a worker node dies it is a pet

00:01:49,909 --> 00:01:54,889
worker node so you have to someone has

00:01:52,579 --> 00:01:58,549
to be on on-call rotation to make sure

00:01:54,889 --> 00:02:01,549
that the the compute capacity is still

00:01:58,549 --> 00:02:04,189
enough to run your worker nodes the

00:02:01,549 --> 00:02:07,399
compute capacity on public cloud is

00:02:04,189 --> 00:02:10,399
ephemeral and it doesn't have the same

00:02:07,399 --> 00:02:13,370
static sticky qualities of server blades

00:02:10,399 --> 00:02:16,520
on on-premise data centers so why are we

00:02:13,370 --> 00:02:18,830
continuing to to manage and provision

00:02:16,520 --> 00:02:20,629
compute on public cloud as if we owned

00:02:18,830 --> 00:02:22,030
the machines is sitting in a private

00:02:20,629 --> 00:02:24,970
datacenter

00:02:22,030 --> 00:02:26,830
so with the over-provision the compute

00:02:24,970 --> 00:02:29,050
capacity nodes we are wasting resources

00:02:26,830 --> 00:02:32,050
we are wasting money if we under

00:02:29,050 --> 00:02:34,660
provision the parts will be in pending

00:02:32,050 --> 00:02:37,500
state and won't have the compute node

00:02:34,660 --> 00:02:39,940
available when it wants to run and

00:02:37,500 --> 00:02:42,670
unexpected spikes during rolling up

00:02:39,940 --> 00:02:46,540
upgrades and horizontal part of scaling

00:02:42,670 --> 00:02:47,200
will be very lazy to react no less

00:02:46,540 --> 00:02:50,230
kubernetes

00:02:47,200 --> 00:02:52,959
is flavor of kubernetes which basically

00:02:50,230 --> 00:02:56,650
makes sure that you consume public cloud

00:02:52,959 --> 00:02:58,720
capacity as and when it is needed for

00:02:56,650 --> 00:03:01,650
your kubernetes clusters and focus on

00:02:58,720 --> 00:03:04,570
your kubernetes applications instead of

00:03:01,650 --> 00:03:06,400
focusing on managing the operational

00:03:04,570 --> 00:03:09,160
complexity of your kubernetes cluster

00:03:06,400 --> 00:03:11,920
worker nodes so what noodle s

00:03:09,160 --> 00:03:14,590
communities does is that the control

00:03:11,920 --> 00:03:17,830
plane looks down and sees a worker node

00:03:14,590 --> 00:03:21,040
which advertises very large capacity it

00:03:17,830 --> 00:03:24,640
advertises let's say thousand 24 V CPUs

00:03:21,040 --> 00:03:27,310
4 terabytes of RAM tens of GPU devices

00:03:24,640 --> 00:03:29,920
and it says that I can run up to 1,000

00:03:27,310 --> 00:03:32,470
parts but in actuality it is the virtual

00:03:29,920 --> 00:03:35,320
capacity of the cloud provider that it

00:03:32,470 --> 00:03:39,160
is advertising it's not pre provisioned

00:03:35,320 --> 00:03:42,370
always on work a node that is that is

00:03:39,160 --> 00:03:44,860
advertising this capacity so when a pod

00:03:42,370 --> 00:03:47,489
is provisioned to this worker node the

00:03:44,860 --> 00:03:50,620
worker node will provision just-in-time

00:03:47,489 --> 00:03:53,320
right-sized cost optimized compute

00:03:50,620 --> 00:03:55,230
launch type for the pod on the cloud

00:03:53,320 --> 00:03:58,269
provider that you've configured and the

00:03:55,230 --> 00:04:00,310
just-in-time compute it started up and

00:03:58,269 --> 00:04:02,680
the pod is dispatched over to the

00:04:00,310 --> 00:04:05,380
compute node as long as the pod is

00:04:02,680 --> 00:04:07,720
running the pod appears to be running on

00:04:05,380 --> 00:04:09,280
the large virtual work an old and once

00:04:07,720 --> 00:04:11,230
the pod terminates the underlying

00:04:09,280 --> 00:04:13,299
compute capacity is automatically

00:04:11,230 --> 00:04:16,060
terminated the computer launch type

00:04:13,299 --> 00:04:17,799
could be sourced from all the different

00:04:16,060 --> 00:04:20,290
flavors of launch types that are

00:04:17,799 --> 00:04:22,479
available on your cloud provider so it

00:04:20,290 --> 00:04:24,520
could be an on-demand compute node it

00:04:22,479 --> 00:04:26,680
could be a spot instance it could be a

00:04:24,520 --> 00:04:28,660
far gate launch type or it could be a

00:04:26,680 --> 00:04:30,880
newer better cheaper launch type that

00:04:28,660 --> 00:04:32,919
the cloud provider has brought to market

00:04:30,880 --> 00:04:35,569
three months from now so you don't have

00:04:32,919 --> 00:04:37,580
to keep like a hand curated list

00:04:35,569 --> 00:04:40,129
of the various computer launch types

00:04:37,580 --> 00:04:42,949
that are available so one of the

00:04:40,129 --> 00:04:46,429
questions that we get asked quite a lot

00:04:42,949 --> 00:04:49,129
is that here how do we manage node less

00:04:46,429 --> 00:04:52,009
kubernetes applications does Prometheus

00:04:49,129 --> 00:04:55,550
work but notice kubernetes and the short

00:04:52,009 --> 00:04:57,889
answer is is yes it works with node less

00:04:55,550 --> 00:04:59,990
kubernetes just like it works with a

00:04:57,889 --> 00:05:02,809
regular kubernetes cluster so there is

00:04:59,990 --> 00:05:05,599
absolutely no change from the user point

00:05:02,809 --> 00:05:08,839
of view and this is a question that gets

00:05:05,599 --> 00:05:11,330
asked most often when we talk about

00:05:08,839 --> 00:05:13,969
nodelist kubernetes so let's go ahead

00:05:11,330 --> 00:05:16,180
and see a demo of how this is going to

00:05:13,969 --> 00:05:16,180
work

00:05:32,440 --> 00:05:38,690
so out here I have a single master

00:05:35,810 --> 00:05:45,560
single virtual worker node cluster that

00:05:38,690 --> 00:05:48,020
is an odorless worker so we have we have

00:05:45,560 --> 00:05:51,110
single control plane node and a single

00:05:48,020 --> 00:06:02,210
virtual worker node let's describe node

00:05:51,110 --> 00:06:03,950
on the virtual worker node so if we look

00:06:02,210 --> 00:06:07,370
at the capacity of the virtual work

00:06:03,950 --> 00:06:12,200
alone it's saying that it has 1024 V

00:06:07,370 --> 00:06:16,310
CPUs it has 4 terabytes of RAM it has 10

00:06:12,200 --> 00:06:19,550
GPU devices and it can run up to 1k pods

00:06:16,310 --> 00:06:28,100
so let's actually look at this host name

00:06:19,550 --> 00:06:31,460
in our AWS console it's actually running

00:06:28,100 --> 00:06:34,670
on a t2 medium instance which has 2 V

00:06:31,460 --> 00:06:37,220
CPUs and 4 gigs of ram so the worker

00:06:34,670 --> 00:06:40,730
node actually has very little capacity

00:06:37,220 --> 00:06:44,680
but it's advertising 1024 V CPUs 4

00:06:40,730 --> 00:06:48,940
terabytes of RAM so as the first

00:06:44,680 --> 00:06:53,020
workload let's go ahead and deploy our

00:06:48,940 --> 00:06:53,020
Prometheus stack

00:07:02,180 --> 00:07:05,180
sorry

00:07:14,550 --> 00:07:19,900
so we are trying to create Prometheus

00:07:17,710 --> 00:07:23,980
start with graph on R cube state matrix

00:07:19,900 --> 00:07:26,740
and Prometheus deployment so we have

00:07:23,980 --> 00:07:30,820
services associated with the graph on R

00:07:26,740 --> 00:07:33,550
cube state matrix and Prometheus and I

00:07:30,820 --> 00:07:36,790
chose to externally expose all these

00:07:33,550 --> 00:07:39,310
three services for demo purposes and we

00:07:36,790 --> 00:07:41,490
have deployments associated with again

00:07:39,310 --> 00:07:44,710
graph on R cube state matrix and

00:07:41,490 --> 00:07:47,530
Prometheus and we have the three

00:07:44,710 --> 00:07:51,040
replicas sets corresponding to graph on

00:07:47,530 --> 00:07:54,280
R cube state matrix and Prometheus so

00:07:51,040 --> 00:07:56,950
all of these three components are being

00:07:54,280 --> 00:07:59,290
deployed into monitoring namespace so

00:07:56,950 --> 00:08:05,950
what is happening under the covers is

00:07:59,290 --> 00:08:08,830
that in our single node cluster the

00:08:05,950 --> 00:08:10,720
three deployment requests enter the

00:08:08,830 --> 00:08:13,720
kubernetes control plane the control

00:08:10,720 --> 00:08:15,970
plane looks down and sees just this one

00:08:13,720 --> 00:08:20,680
worker node which has this very large

00:08:15,970 --> 00:08:23,169
capacity so it schedules all three all

00:08:20,680 --> 00:08:25,360
three pods on to the virtual worker node

00:08:23,169 --> 00:08:28,300
and virtual worker node will make the

00:08:25,360 --> 00:08:30,700
call as to what is the right compute

00:08:28,300 --> 00:08:33,159
launch type that fits the resource

00:08:30,700 --> 00:08:35,440
requirements for each of these parts and

00:08:33,159 --> 00:08:37,450
adjusting time launched just-in-time

00:08:35,440 --> 00:08:42,010
compute launch type is provision and

00:08:37,450 --> 00:08:43,930
each pod is is is scheduled to the

00:08:42,010 --> 00:08:47,650
just-in-time provisioning compute launch

00:08:43,930 --> 00:08:50,260
type as of now no less kubernetes can

00:08:47,650 --> 00:08:53,410
only run stateless applications so what

00:08:50,260 --> 00:08:55,660
it does is there is an admission webhook

00:08:53,410 --> 00:08:57,940
that will dispatch the stateless

00:08:55,660 --> 00:09:00,850
applications in an odourless fashion and

00:08:57,940 --> 00:09:03,690
the stateful apps are started up on the

00:09:00,850 --> 00:09:06,520
virtual worker nodes compute node itself

00:09:03,690 --> 00:09:08,740
support for stateful apps will be coming

00:09:06,520 --> 00:09:12,220
in a couple of months so this is a

00:09:08,740 --> 00:09:15,880
temporary dispatching hybrid model that

00:09:12,220 --> 00:09:18,280
is being implemented right now so what

00:09:15,880 --> 00:09:21,870
is what will happen now is that the

00:09:18,280 --> 00:09:24,970
control plane is going to start the

00:09:21,870 --> 00:09:27,580
Prometheus deployment on the worker node

00:09:24,970 --> 00:09:29,950
itself and just in time

00:09:27,580 --> 00:09:32,410
compute cells noodler cells are

00:09:29,950 --> 00:09:36,130
provision to run cube state matrix and

00:09:32,410 --> 00:09:38,200
graph owner so let's see if our parts

00:09:36,130 --> 00:09:40,570
are running all the parts are running so

00:09:38,200 --> 00:09:43,779
let's try to access a graph on our

00:09:40,570 --> 00:09:45,670
dashboard it typically takes a little

00:09:43,779 --> 00:09:47,950
longer time for the external load

00:09:45,670 --> 00:09:59,079
balancer to be available so let's see if

00:09:47,950 --> 00:09:59,440
that is have been running right now all

00:09:59,079 --> 00:10:01,959
right

00:09:59,440 --> 00:10:14,769
so let's go ahead and add the data

00:10:01,959 --> 00:10:16,720
source for Prometheus since all the

00:10:14,769 --> 00:10:19,720
three components are running in the same

00:10:16,720 --> 00:10:22,690
cluster I don't need to use the external

00:10:19,720 --> 00:10:37,060
IP so I will use the service address for

00:10:22,690 --> 00:10:39,370
internal communication and there is a

00:10:37,060 --> 00:10:42,160
very simple graph on our dashboard for

00:10:39,370 --> 00:10:51,430
monitoring Nautilus cells so let's go

00:10:42,160 --> 00:10:55,810
ahead and import that dashboard so what

00:10:51,430 --> 00:10:58,420
this dashboards shows is there are four

00:10:55,810 --> 00:11:01,149
columns so it the first column is the

00:10:58,420 --> 00:11:03,700
number of nodes in the cluster which is

00:11:01,149 --> 00:11:06,250
two we have one control plane node and

00:11:03,700 --> 00:11:08,800
one which will work a node and the

00:11:06,250 --> 00:11:10,899
second column is components that are

00:11:08,800 --> 00:11:14,110
running on control plane so a control

00:11:10,899 --> 00:11:15,910
plane node name and the number of pods

00:11:14,110 --> 00:11:19,180
that are running on the control plane

00:11:15,910 --> 00:11:22,750
it's mostly the control plane components

00:11:19,180 --> 00:11:25,269
API server queue proxy etc and they said

00:11:22,750 --> 00:11:27,430
on the third column is the worker node

00:11:25,269 --> 00:11:29,820
components there are seven parts that

00:11:27,430 --> 00:11:32,649
are running on the worker node itself

00:11:29,820 --> 00:11:35,079
including Prometheus deployment because

00:11:32,649 --> 00:11:37,329
it's a stateful application it's running

00:11:35,079 --> 00:11:39,010
on the worker node itself and the

00:11:37,329 --> 00:11:41,410
interesting column is the fourth column

00:11:39,010 --> 00:11:43,000
where which lists the node list

00:11:41,410 --> 00:11:44,680
pure cells that are running in the

00:11:43,000 --> 00:11:47,769
system so there are three nor less

00:11:44,680 --> 00:11:51,790
compute cells they correspond to graph

00:11:47,769 --> 00:11:54,069
on R cube state matrix and cube proxy

00:11:51,790 --> 00:11:56,019
which is a system part that we can

00:11:54,069 --> 00:12:00,430
ignore for the purpose of this demo so

00:11:56,019 --> 00:12:04,540
as if we want to create an engine X

00:12:00,430 --> 00:12:07,269
deployment for example which is

00:12:04,540 --> 00:12:10,089
stateless to start with let's look at

00:12:07,269 --> 00:12:13,569
the engine X deployment which is asking

00:12:10,089 --> 00:12:16,209
for three replicas and it doesn't have

00:12:13,569 --> 00:12:19,149
any persistent volume needs once we

00:12:16,209 --> 00:12:26,860
create this deployment once the pods are

00:12:19,149 --> 00:12:31,540
up and running once the parts are up and

00:12:26,860 --> 00:12:34,209
running because the the control plane is

00:12:31,540 --> 00:12:37,480
going to schedule the work the engine

00:12:34,209 --> 00:12:40,629
exports or no Lascelles because it is

00:12:37,480 --> 00:12:42,730
not it doesn't have they don't have any

00:12:40,629 --> 00:12:44,740
persistent volume needs once the three

00:12:42,730 --> 00:12:47,170
node Lascelles are up and running and

00:12:44,740 --> 00:12:51,069
the ports are dispatched over to the

00:12:47,170 --> 00:12:54,630
node Lascelles the graph on our

00:12:51,069 --> 00:12:57,790
dashboard should should reflect that our

00:12:54,630 --> 00:13:01,029
compute cells no less compute cell count

00:12:57,790 --> 00:13:03,850
should go up from 3 to 6 eventually I'm

00:13:01,029 --> 00:13:08,620
going to pause here for questions this

00:13:03,850 --> 00:13:11,529
is the gist of the demo the key

00:13:08,620 --> 00:13:15,130
takeaways that if you're using

00:13:11,529 --> 00:13:17,949
kubernetes on public cloud not having to

00:13:15,130 --> 00:13:20,800
hand curate and babysit these pet worker

00:13:17,949 --> 00:13:24,699
nodes is a cleaner way to consume public

00:13:20,800 --> 00:13:26,680
cloud compute capacity and Prometheus

00:13:24,699 --> 00:13:28,329
and Ravana are the perfect fit for

00:13:26,680 --> 00:13:30,880
monitoring notice kubernetes

00:13:28,329 --> 00:13:32,410
applications and there is no difference

00:13:30,880 --> 00:13:34,809
in monitoring no less kubernetes

00:13:32,410 --> 00:13:38,110
applications versus your regular vanilla

00:13:34,809 --> 00:13:40,240
kubernetes applications so if you want

00:13:38,110 --> 00:13:42,430
to give this a try i'm going to share

00:13:40,240 --> 00:13:45,670
the github location for running this

00:13:42,430 --> 00:13:47,550
tutorial yourself thank you so much and

00:13:45,670 --> 00:13:54,890
I look forward to the questions

00:13:47,550 --> 00:13:54,890
[Applause]

00:14:09,040 --> 00:14:19,340
yes I am thankful talk yes this is more

00:14:16,190 --> 00:14:21,620
talked about notice I just wonder how

00:14:19,340 --> 00:14:24,710
you do all the affinity and definitive

00:14:21,620 --> 00:14:31,010
stuff as the scheduler doesn't know how

00:14:24,710 --> 00:14:33,730
to work with it compute node isolation

00:14:31,010 --> 00:14:36,410
so you get 90 affinity out-of-the-box

00:14:33,730 --> 00:14:38,450
affinity there is there can be an

00:14:36,410 --> 00:14:40,850
admission controller with schedules the

00:14:38,450 --> 00:14:45,370
if party and pod we have an affinity

00:14:40,850 --> 00:14:48,560
rule they become one notice pods so they

00:14:45,370 --> 00:14:52,420
scheduled as one notice basically so

00:14:48,560 --> 00:14:52,420
they go locate on the same computer

00:14:59,800 --> 00:15:09,710
questions or the a.m. is that are

00:15:06,500 --> 00:15:14,270
running on the nodes are those special

00:15:09,710 --> 00:15:15,740
notice ones or just the barebone ones

00:15:14,270 --> 00:15:17,720
that we would run for you guys for

00:15:15,740 --> 00:15:19,880
example yeah that's a good question one

00:15:17,720 --> 00:15:22,130
of the motivations for node less is to

00:15:19,880 --> 00:15:25,160
eliminate the need for maintaining the

00:15:22,130 --> 00:15:27,770
operating system the security patches

00:15:25,160 --> 00:15:30,380
and all of that overhead so by default

00:15:27,770 --> 00:15:34,700
the node Lascelles run an Alpine base

00:15:30,380 --> 00:15:36,950
image but some enterprises have pre

00:15:34,700 --> 00:15:39,530
listed lest images that they want to

00:15:36,950 --> 00:15:42,320
consume instead of Alpine so you can

00:15:39,530 --> 00:15:45,860
override that ami by default it's an

00:15:42,320 --> 00:15:48,230
alpha Nami so the the key idea is to

00:15:45,860 --> 00:15:51,410
keep the over system overhead of the

00:15:48,230 --> 00:15:53,810
cell to keep it pretty minimal so that

00:15:51,410 --> 00:15:56,990
you get majority of the resources in the

00:15:53,810 --> 00:15:59,170
cell for your application thank you very

00:15:56,990 --> 00:15:59,170
much

00:15:59,440 --> 00:16:06,560
it was hi great talk

00:16:04,610 --> 00:16:08,750
so what is the latency of provisioning

00:16:06,560 --> 00:16:10,670
the actual compute nodes because we're

00:16:08,750 --> 00:16:12,470
advertising a lot of compute but we

00:16:10,670 --> 00:16:14,180
don't have it exactly what is the

00:16:12,470 --> 00:16:16,730
latency and as a follow-up if you are

00:16:14,180 --> 00:16:18,940
not able to provision that compute what

00:16:16,730 --> 00:16:21,850
would happen yeah that's a good question

00:16:18,940 --> 00:16:24,610
the the computer launch type latency

00:16:21,850 --> 00:16:26,680
varies between cloud providers and even

00:16:24,610 --> 00:16:28,420
on a single cloud provider it varies

00:16:26,680 --> 00:16:30,760
between the various launch types so for

00:16:28,420 --> 00:16:33,160
example on demand on AWS can take

00:16:30,760 --> 00:16:35,680
anywhere between five seconds up to two

00:16:33,160 --> 00:16:38,920
minutes whereas fire can launch really

00:16:35,680 --> 00:16:40,510
quickly so there is a knob that you can

00:16:38,920 --> 00:16:43,590
turn on in an odourless kubernetes

00:16:40,510 --> 00:16:45,820
cluster where you specify the SLA and

00:16:43,590 --> 00:16:48,100
depending on the SLA for your

00:16:45,820 --> 00:16:50,920
application startup if your app has a

00:16:48,100 --> 00:16:54,310
five-second delay between cube CTL

00:16:50,920 --> 00:16:56,290
create and the app responding then the

00:16:54,310 --> 00:17:00,340
node less engine will maintain a set of

00:16:56,290 --> 00:17:02,740
pre-warmed cells that are treated so

00:17:00,340 --> 00:17:05,020
that your SaaS on it so it's more of an

00:17:02,740 --> 00:17:09,420
SLA driven capacity management rather

00:17:05,020 --> 00:17:17,200
than having to provision it beforehand

00:17:09,420 --> 00:17:19,030
I am if you configure a daemon set to

00:17:17,200 --> 00:17:20,740
have four node exporter running on each

00:17:19,030 --> 00:17:23,200
node what happens in this node list

00:17:20,740 --> 00:17:23,710
scenario with that yeah so that's a good

00:17:23,200 --> 00:17:38,950
question

00:17:23,710 --> 00:17:40,750
so 'man sites odd so daemon sets came

00:17:38,950 --> 00:17:43,060
into existence with the assumption that

00:17:40,750 --> 00:17:45,250
you have these very large server blades

00:17:43,060 --> 00:17:47,620
that are comprising of your worker nodes

00:17:45,250 --> 00:17:50,530
so the idea of the daemon set is to

00:17:47,620 --> 00:17:54,250
collect all of so to represent the

00:17:50,530 --> 00:17:56,770
compute nodes agent correct so that is

00:17:54,250 --> 00:17:59,830
what we are trying to achieve so what

00:17:56,770 --> 00:18:02,350
you can still run a daemon set on the

00:17:59,830 --> 00:18:04,360
virtual worker node itself but it will

00:18:02,350 --> 00:18:08,530
run on the virtual worker node it won't

00:18:04,360 --> 00:18:10,660
run on every single cell so depending on

00:18:08,530 --> 00:18:13,270
the motivation for your demon set you

00:18:10,660 --> 00:18:16,780
can have for example you can run elastic

00:18:13,270 --> 00:18:19,120
beads file beat I think node beat etc if

00:18:16,780 --> 00:18:21,130
it is something super lightweight then

00:18:19,120 --> 00:18:23,860
it can be injected into the cell itself

00:18:21,130 --> 00:18:26,140
so it totally depends on what is the

00:18:23,860 --> 00:18:28,820
purpose that your demon set is trying to

00:18:26,140 --> 00:18:30,710
address does that make sense

00:18:28,820 --> 00:18:39,640
if you have a specific use case I'd love

00:18:30,710 --> 00:18:39,640
to chat afterwards just a second

00:18:43,480 --> 00:18:57,519
a question similar direction what about

00:18:53,019 --> 00:18:59,710
running a custom interest controller on

00:18:57,519 --> 00:19:02,590
this infrastructure basically same

00:18:59,710 --> 00:19:05,320
problem as with the diamond set but also

00:19:02,590 --> 00:19:08,679
regarding networking so do you have some

00:19:05,320 --> 00:19:11,500
extra hops in it no so the one of the

00:19:08,679 --> 00:19:13,600
key design goals of nodelist is for the

00:19:11,500 --> 00:19:15,610
behavior for networking and storage and

00:19:13,600 --> 00:19:18,820
everything else to be exactly the same

00:19:15,610 --> 00:19:21,880
as regular kubernetes cluster so we in

00:19:18,820 --> 00:19:24,100
fact have there's a blog post that that

00:19:21,880 --> 00:19:26,139
talks about how is tio would work so

00:19:24,100 --> 00:19:28,539
your site cars are still running on the

00:19:26,139 --> 00:19:30,850
cells they're not running on the virtual

00:19:28,539 --> 00:19:33,190
work a node so each cell at all the

00:19:30,850 --> 00:19:36,399
networking site cars will run on the

00:19:33,190 --> 00:19:38,350
compute cell they aboard is running so

00:19:36,399 --> 00:19:48,880
it it works just like your vanilla

00:19:38,350 --> 00:19:49,680
kubernetes basically anyone else thank

00:19:48,880 --> 00:19:53,980
you

00:19:49,680 --> 00:20:03,690
[Applause]

00:19:53,980 --> 00:20:03,690

YouTube URL: https://www.youtube.com/watch?v=JcVOuCBRN8s


