Title: PromCon EU 2019: Managing Prometheus in a Security-focused Environment Linux Monitoring @ HUK-COBURG
Publication date: 2019-12-29
Playlist: PromCon EU 2019
Description: 
	Speaker: Christian Hoffmann

Prometheus offers lots of features and flexibility which often leaves newcomers with lots of open questions: Which service discovery to use? How to secure access to metrics? Which architecture to implement? In order to provide some input back to the community, this talk will outline how we run Prometheus at HUK-COBURG in order to provide a multi-tenant turn-key monitoring experience in a security-focused environment. It covers basic decisions like scrape intervals and service discovery and extends to the implementation of encrypted and authenticated connections to monitoring targets using sshified and team-aware access to Prometheus/Grafana access via prometheus-filter-proxy.

Slides: https://promcon.io/2019-munich/slides/managing-prometheus-in-a-security-focused-environment-linux-monitoring-at-huk-coburg.pdf
Captions: 
	00:00:00,650 --> 00:00:08,910
[Music]

00:00:09,700 --> 00:00:14,590
so hello everyone my name is Kristen

00:00:12,130 --> 00:00:17,260
Hoffman and before I actually dive into

00:00:14,590 --> 00:00:20,290
the technical part of my talk I've got a

00:00:17,260 --> 00:00:24,790
quick question who has heard of Conway's

00:00:20,290 --> 00:00:27,460
law ok surprisingly much people I'm

00:00:24,790 --> 00:00:28,029
still going to give my interpretation of

00:00:27,460 --> 00:00:31,990
Conway's law

00:00:28,029 --> 00:00:35,199
it basically says the system designs are

00:00:31,990 --> 00:00:37,420
going to resemble the organizational

00:00:35,199 --> 00:00:38,800
structures at some point in time so this

00:00:37,420 --> 00:00:40,900
is why I'm going to give a short

00:00:38,800 --> 00:00:44,350
introduction into the structure of our

00:00:40,900 --> 00:00:46,690
company which is the cool book we are a

00:00:44,350 --> 00:00:48,610
German influence company targeted as

00:00:46,690 --> 00:00:50,979
consumers we are the Lockett largest

00:00:48,610 --> 00:00:53,170
kauri ensure efficient you know

00:00:50,979 --> 00:00:57,400
consumers we've got about 12 million

00:00:53,170 --> 00:00:58,930
customers and 10,000 employees within

00:00:57,400 --> 00:01:01,449
October there are multiple departments

00:00:58,930 --> 00:01:03,549
even multiple IT related departments and

00:01:01,449 --> 00:01:06,010
we've got about 800 people working there

00:01:03,549 --> 00:01:08,590
in IT related shops so we are certainly

00:01:06,010 --> 00:01:11,110
not some kind of startup rather we have

00:01:08,590 --> 00:01:14,500
lots of teams lots of policies lots of

00:01:11,110 --> 00:01:18,280
regulatory stuff high data security

00:01:14,500 --> 00:01:20,229
policies and so on one of these IT

00:01:18,280 --> 00:01:23,740
related departments is the Linux

00:01:20,229 --> 00:01:25,450
platform development team and we would

00:01:23,740 --> 00:01:27,250
describe ourselves as some kind of

00:01:25,450 --> 00:01:31,270
managed infrastructure as a service

00:01:27,250 --> 00:01:34,570
provider so we provide Linux machines to

00:01:31,270 --> 00:01:36,490
our internal customers and we manage

00:01:34,570 --> 00:01:38,909
around 900 service based on wretched

00:01:36,490 --> 00:01:41,950
Enterprise Linux these services are

00:01:38,909 --> 00:01:45,429
distributed across two main datacenters

00:01:41,950 --> 00:01:48,969
I am one of the 10 people in exactly

00:01:45,429 --> 00:01:50,740
this team I join in 2016 and I would

00:01:48,969 --> 00:01:53,520
describe myself as some kind of Linux

00:01:50,740 --> 00:01:56,859
and open source and fields and SUSE yes

00:01:53,520 --> 00:01:58,509
so much for the organizational part

00:01:56,859 --> 00:02:00,819
where I work we've got lots of other

00:01:58,509 --> 00:02:03,130
teams as I said so we've got application

00:02:00,819 --> 00:02:05,579
owners who run databases who run rep

00:02:03,130 --> 00:02:08,050
servers who run insurance software and

00:02:05,579 --> 00:02:10,650
these application owners are the ones

00:02:08,050 --> 00:02:14,140
who use our Linux servers as the base

00:02:10,650 --> 00:02:16,900
we've also got an dedicated operations

00:02:14,140 --> 00:02:19,409
team which handle the on-call situations

00:02:16,900 --> 00:02:22,480
which I'm going to describe later

00:02:19,409 --> 00:02:22,990
so now let's dive into the actual

00:02:22,480 --> 00:02:24,540
technical

00:02:22,990 --> 00:02:27,520
parts I'm going to describe our

00:02:24,540 --> 00:02:29,920
Prometheus setup how we run it and what

00:02:27,520 --> 00:02:31,600
our basic configuration looks like but

00:02:29,920 --> 00:02:34,270
I'm going to focus on the parts where I

00:02:31,600 --> 00:02:36,280
think our separate setup is special so

00:02:34,270 --> 00:02:39,310
let's start with scraping or with the

00:02:36,280 --> 00:02:41,700
actual Prometheus service when we

00:02:39,310 --> 00:02:43,900
started we thought about how many

00:02:41,700 --> 00:02:46,240
Prometheus servers are we going to run

00:02:43,900 --> 00:02:48,550
and where should we place them the

00:02:46,240 --> 00:02:50,200
common recommendation seems to be run

00:02:48,550 --> 00:02:52,900
your Prometheus service as close to the

00:02:50,200 --> 00:02:54,730
monitoring target as possible but what

00:02:52,900 --> 00:02:57,130
does close actually mean we thought

00:02:54,730 --> 00:02:59,230
about this and we came up with multiple

00:02:57,130 --> 00:03:01,120
ideas what this could mean we first

00:02:59,230 --> 00:03:02,800
thought about firewall zones and we

00:03:01,120 --> 00:03:05,250
quickly noticed that we've got lots of

00:03:02,800 --> 00:03:07,570
firewall environment environments and

00:03:05,250 --> 00:03:09,220
some of these environments don't even

00:03:07,570 --> 00:03:11,020
contain any virtualization or content

00:03:09,220 --> 00:03:13,120
container in fact infrastructure they

00:03:11,020 --> 00:03:14,710
only contain two or three servers and it

00:03:13,120 --> 00:03:16,840
certainly would not make sense to run

00:03:14,710 --> 00:03:19,270
Prometheus instances for I will

00:03:16,840 --> 00:03:21,940
abilities there so we thought about

00:03:19,270 --> 00:03:23,410
other meanings of the word closed should

00:03:21,940 --> 00:03:25,330
we run it on the same network switch

00:03:23,410 --> 00:03:27,760
cluster should we run it in the same

00:03:25,330 --> 00:03:30,340
data center which which most people seem

00:03:27,760 --> 00:03:33,340
to do and what we actually decided is

00:03:30,340 --> 00:03:35,110
that we should run from a responsibility

00:03:33,340 --> 00:03:37,780
perspective so we decided that we should

00:03:35,110 --> 00:03:40,560
run a central Prometheus instance and we

00:03:37,780 --> 00:03:43,420
would run this as a pair of two servers

00:03:40,560 --> 00:03:46,680
distributed across to our two main

00:03:43,420 --> 00:03:50,260
datacenters so this is how it looks and

00:03:46,680 --> 00:03:52,900
we monitor all servers from both data

00:03:50,260 --> 00:03:55,390
centers so there's not - two pairs of

00:03:52,900 --> 00:03:57,850
these as just one pair this has the

00:03:55,390 --> 00:04:00,460
benefit of providing insight into the

00:03:57,850 --> 00:04:01,870
network so if we really should have some

00:04:00,460 --> 00:04:04,060
discrepancies between the data centers

00:04:01,870 --> 00:04:07,240
we are going to see this using these

00:04:04,060 --> 00:04:08,560
instances so I'm going to put up some

00:04:07,240 --> 00:04:10,660
numbers and you will notice that our

00:04:08,560 --> 00:04:12,670
Prometheus setup is actually not that

00:04:10,660 --> 00:04:14,980
large and we're running on two virtual

00:04:12,670 --> 00:04:16,780
machines with four cores with two and

00:04:14,980 --> 00:04:22,000
friendly gigabytes of RAM and one point

00:04:16,780 --> 00:04:23,640
terabyte well disk space we handle 1.7

00:04:22,000 --> 00:04:27,430
million times serious and with about

00:04:23,640 --> 00:04:29,350
30,000 samples per second we've got a

00:04:27,430 --> 00:04:31,150
pretty standard scribe interval of 60

00:04:29,350 --> 00:04:34,120
seconds we use the file service

00:04:31,150 --> 00:04:36,639
discovery for for service discovery and

00:04:34,120 --> 00:04:39,729
have half about our to 2006

00:04:36,639 --> 00:04:41,919
entries there we've got lots of alert

00:04:39,729 --> 00:04:43,599
rules which we write by by hand

00:04:41,919 --> 00:04:47,110
obviously and we've got lots of

00:04:43,599 --> 00:04:49,150
recording rules these are partly

00:04:47,110 --> 00:04:51,729
generated from other systems so these

00:04:49,150 --> 00:04:56,379
are used to provide for the meter data

00:04:51,729 --> 00:04:57,969
into Prometheus so now that we we've got

00:04:56,379 --> 00:05:00,550
a Prometheus service where should we

00:04:57,969 --> 00:05:03,759
scrape the metrics how should we reach

00:05:00,550 --> 00:05:05,590
our our monitoring targets as that we

00:05:03,759 --> 00:05:07,360
have a high security environment

00:05:05,590 --> 00:05:09,849
everybody's going to talk about well you

00:05:07,360 --> 00:05:11,590
can't do the plain text formats so what

00:05:09,849 --> 00:05:14,349
the common recommendation seems to be

00:05:11,590 --> 00:05:16,629
well place a reverse proxy in front of

00:05:14,349 --> 00:05:18,699
it use TLS and get all these

00:05:16,629 --> 00:05:20,710
certificates and stuff we thought about

00:05:18,699 --> 00:05:23,129
this approach and we quickly noticed

00:05:20,710 --> 00:05:25,750
we've got already lots of monitoring on

00:05:23,129 --> 00:05:27,340
agents on our servers we've got security

00:05:25,750 --> 00:05:29,349
scanners with we've got license minute

00:05:27,340 --> 00:05:31,960
manage like license management tools and

00:05:29,349 --> 00:05:34,150
it's only getting worse that we really

00:05:31,960 --> 00:05:38,020
try to keep the number of extra agents

00:05:34,150 --> 00:05:40,690
on these servers down so how do other

00:05:38,020 --> 00:05:43,060
well monitoring systems solve this

00:05:40,690 --> 00:05:45,250
problem or what does a typical little

00:05:43,060 --> 00:05:46,870
server look for us we were an nmap and

00:05:45,250 --> 00:05:47,500
we noticed there's one service running

00:05:46,870 --> 00:05:51,039
there already

00:05:47,500 --> 00:05:54,099
it's SSH hmm other monitoring tools use

00:05:51,039 --> 00:05:56,469
SSH and run some kind of command on the

00:05:54,099 --> 00:05:58,539
target server to get their data and we

00:05:56,469 --> 00:06:00,490
thought well can we do something similar

00:05:58,539 --> 00:06:03,879
maybe not running a command but using at

00:06:00,490 --> 00:06:07,270
least SSH SSH provides us authentication

00:06:03,879 --> 00:06:09,819
encryption it's easy to lock down and we

00:06:07,270 --> 00:06:13,270
have an established mechanism for SSH

00:06:09,819 --> 00:06:15,009
keys we don't have to well get TLS

00:06:13,270 --> 00:06:17,139
certificates from different CAS in our

00:06:15,009 --> 00:06:20,400
environments so SH looked like a good

00:06:17,139 --> 00:06:24,610
idea but for me this does not support Sh

00:06:20,400 --> 00:06:25,930
luckily Prometheus is really flexible so

00:06:24,610 --> 00:06:27,759
we've got our Prometheus server we've

00:06:25,930 --> 00:06:30,069
got our monitoring target and the

00:06:27,759 --> 00:06:32,620
monitoring target runs SSH E and it runs

00:06:30,069 --> 00:06:35,319
all the exporters so how do we get

00:06:32,620 --> 00:06:38,680
Prometheus to use this way for access

00:06:35,319 --> 00:06:40,900
all we need is to use Prometheus proxy

00:06:38,680 --> 00:06:44,860
feature it has a config option which

00:06:40,900 --> 00:06:46,539
says proxy URL and we can well plug in

00:06:44,860 --> 00:06:47,949
anything we want in this case and we

00:06:46,539 --> 00:06:50,350
came up with a small daemon which we

00:06:47,949 --> 00:06:52,510
call SS 85 which simply

00:06:50,350 --> 00:06:54,880
an HTTP listener to Prometheus and

00:06:52,510 --> 00:06:57,460
translates all requests from Prometheus

00:06:54,880 --> 00:06:59,890
to an SSH tunneling request

00:06:57,460 --> 00:07:03,130
so this way we rep all the metrics

00:06:59,890 --> 00:07:05,470
excesses within this SSH tunnel which is

00:07:03,130 --> 00:07:07,150
easy from a buy one point of view which

00:07:05,470 --> 00:07:09,640
provides us the authentication and

00:07:07,150 --> 00:07:11,650
encryption of course this also only

00:07:09,640 --> 00:07:13,900
makes sense if we limit access to the

00:07:11,650 --> 00:07:15,490
actual exporters so node exporter will

00:07:13,900 --> 00:07:17,500
probably start with binding to the

00:07:15,490 --> 00:07:20,380
public interface by default and exposing

00:07:17,500 --> 00:07:23,110
all the metrics on a public port this is

00:07:20,380 --> 00:07:26,200
something we had to keep down so we look

00:07:23,110 --> 00:07:28,560
all the exporters to localhost and we

00:07:26,200 --> 00:07:31,150
even deploy IP tables rules to further

00:07:28,560 --> 00:07:33,690
restrict access to only some specific

00:07:31,150 --> 00:07:36,430
user on the system

00:07:33,690 --> 00:07:38,380
so what exporters do we actually run

00:07:36,430 --> 00:07:41,140
well no export is something you probably

00:07:38,380 --> 00:07:43,420
all know we also make heavy use of the

00:07:41,140 --> 00:07:45,820
text file collector feature we mainly

00:07:43,420 --> 00:07:48,760
use it to fill gaps where note exported

00:07:45,820 --> 00:07:51,370
does not yet support specific parts such

00:07:48,760 --> 00:07:53,950
as and certain error situations in LBM

00:07:51,370 --> 00:07:56,590
in multipath constructions in sound

00:07:53,950 --> 00:07:58,780
related monitoring we've also got a

00:07:56,590 --> 00:08:02,290
small script framework which makes it

00:07:58,780 --> 00:08:04,150
easy to plug in scripts so that other

00:08:02,290 --> 00:08:06,370
people from the other application teams

00:08:04,150 --> 00:08:08,680
can just write some small script which

00:08:06,370 --> 00:08:11,890
outputs the metrics and note exporter

00:08:08,680 --> 00:08:14,880
will pick them up we do run node

00:08:11,890 --> 00:08:17,470
exporters route this may surprise you as

00:08:14,880 --> 00:08:20,320
there are recommendations not to do this

00:08:17,470 --> 00:08:22,630
but we have reasons for that one of the

00:08:20,320 --> 00:08:25,690
reason is that we like to lock down or a

00:08:22,630 --> 00:08:29,020
file systems really match so a normal

00:08:25,690 --> 00:08:30,640
user would not be able to see each file

00:08:29,020 --> 00:08:32,830
system on the server we would have to

00:08:30,640 --> 00:08:34,330
teach people to configure monitoring

00:08:32,830 --> 00:08:36,250
access for them and we would not even

00:08:34,330 --> 00:08:38,650
notice if someone forgets to do that so

00:08:36,250 --> 00:08:41,919
prometheus or the not exporter in this

00:08:38,650 --> 00:08:44,440
case is is able to see all file systems

00:08:41,919 --> 00:08:46,630
this way also we make heavy use of this

00:08:44,440 --> 00:08:49,510
system D exporter to monitor services

00:08:46,630 --> 00:08:53,340
and the systems exporter has a custom

00:08:49,510 --> 00:08:57,790
feature which uses a private soccer -

00:08:53,340 --> 00:08:59,740
well accesses to T so the the connection

00:08:57,790 --> 00:09:01,750
between node exporter can either you go

00:08:59,740 --> 00:09:02,840
over T bus or it can go over the private

00:09:01,750 --> 00:09:04,100
socket and privates

00:09:02,840 --> 00:09:05,420
could turn out to be much more

00:09:04,100 --> 00:09:09,230
frequently reliable for us especially

00:09:05,420 --> 00:09:11,690
under load situations we still about me

00:09:09,230 --> 00:09:14,000
we still have lots of services which are

00:09:11,690 --> 00:09:15,860
not within system D we also got some

00:09:14,000 --> 00:09:18,290
legacy system so this is where we run

00:09:15,860 --> 00:09:20,420
closes exporter to do some process based

00:09:18,290 --> 00:09:23,660
monitoring but we really like to get rid

00:09:20,420 --> 00:09:25,520
of this part we use a model exporter

00:09:23,660 --> 00:09:27,610
called mighty logic spotter which is

00:09:25,520 --> 00:09:30,500
used to extract metrics from well

00:09:27,610 --> 00:09:32,330
proprietary application locks and this

00:09:30,500 --> 00:09:34,970
is comparable with the MTL exporters

00:09:32,330 --> 00:09:39,020
from Google or the graphics part that we

00:09:34,970 --> 00:09:40,670
support and a custom matching support we

00:09:39,020 --> 00:09:42,530
also use a blackbox exporter and we do

00:09:40,670 --> 00:09:44,510
not run it in a central place mainly

00:09:42,530 --> 00:09:46,580
only as well for fievel reasons that we

00:09:44,510 --> 00:09:48,830
run this part as close to the target as

00:09:46,580 --> 00:09:51,560
possible so a machine exporting of

00:09:48,830 --> 00:09:53,240
exposing web server may get all the

00:09:51,560 --> 00:09:56,570
black box exported and we will at least

00:09:53,240 --> 00:09:58,010
see if the web server works correctly as

00:09:56,570 --> 00:10:00,430
this is okay because we have other

00:09:58,010 --> 00:10:03,590
monitoring tools which monitor the whole

00:10:00,430 --> 00:10:05,570
well placed from the customer view to

00:10:03,590 --> 00:10:09,860
the actual when load balancers which

00:10:05,570 --> 00:10:11,990
which return the actual data so now we

00:10:09,860 --> 00:10:14,390
then we've got the premier setup covered

00:10:11,990 --> 00:10:16,700
I'm going to dive into the alerting part

00:10:14,390 --> 00:10:20,300
we use a read manager as probably most

00:10:16,700 --> 00:10:22,550
of you do and our Prometheus instances

00:10:20,300 --> 00:10:25,370
send all the other dementia and we've

00:10:22,550 --> 00:10:27,820
got a fairly simple routing tree our

00:10:25,370 --> 00:10:31,790
routing is based mainly on the algorithm

00:10:27,820 --> 00:10:33,560
we've got three categories we've got

00:10:31,790 --> 00:10:35,600
Linux platform alerts which are targeted

00:10:33,560 --> 00:10:37,610
at my team and we've got Linux server

00:10:35,600 --> 00:10:39,530
application alerts which are targeted at

00:10:37,610 --> 00:10:42,260
the application owners so a custom

00:10:39,530 --> 00:10:45,020
example would be a platform a lab could

00:10:42,260 --> 00:10:47,660
be something like well SSH daemon going

00:10:45,020 --> 00:10:49,190
down or the connection to our Active

00:10:47,660 --> 00:10:51,080
Directory going down this is something

00:10:49,190 --> 00:10:53,060
that we would resolve but we also have

00:10:51,080 --> 00:10:55,520
lots of alerts which the application

00:10:53,060 --> 00:10:59,330
owners have to solve such as filling

00:10:55,520 --> 00:11:01,910
filesystem or something like that we use

00:10:59,330 --> 00:11:06,230
two integrations we use a receiver and

00:11:01,910 --> 00:11:08,150
the email receiver and the receiver is

00:11:06,230 --> 00:11:12,140
also the place which we use to forward

00:11:08,150 --> 00:11:14,510
all alert manager other words to to our

00:11:12,140 --> 00:11:15,990
central monitoring so in our case for me

00:11:14,510 --> 00:11:17,970
this is targeted at the

00:11:15,990 --> 00:11:19,770
NOx monitoring but at work or book we

00:11:17,970 --> 00:11:22,110
run lots of other systems like Network

00:11:19,770 --> 00:11:24,450
Devices and Windows servers and so on

00:11:22,110 --> 00:11:26,760
and this is all collected in a central

00:11:24,450 --> 00:11:28,860
place and we have a custom daemon which

00:11:26,760 --> 00:11:31,230
translates translates the alert manager

00:11:28,860 --> 00:11:33,980
webhook requests into a suit locally

00:11:31,230 --> 00:11:36,420
went for this central event management

00:11:33,980 --> 00:11:39,690
the central event management is also the

00:11:36,420 --> 00:11:41,459
place where incident creation happens

00:11:39,690 --> 00:11:43,440
where paging happens and where we've got

00:11:41,459 --> 00:11:48,750
some money trying to catch the case when

00:11:43,440 --> 00:11:50,399
to me this time we've also we also make

00:11:48,750 --> 00:11:53,100
use of silences and this is done by

00:11:50,399 --> 00:11:54,810
using our proprietary stop inventory and

00:11:53,100 --> 00:11:56,490
the server inventory' uses some kind of

00:11:54,810 --> 00:11:58,200
maintenance mode and we swing this

00:11:56,490 --> 00:12:02,720
maintenance mode and translated to

00:11:58,200 --> 00:12:05,310
silences in using a custom Python script

00:12:02,720 --> 00:12:09,620
for graphing it might surprise you but

00:12:05,310 --> 00:12:12,120
we use graph Anna our main challenge was

00:12:09,620 --> 00:12:15,209
building some kind of multi-tenancy sale

00:12:12,120 --> 00:12:18,029
so we have this graph Anna setup but

00:12:15,209 --> 00:12:19,680
what happens if john x's excesses well

00:12:18,029 --> 00:12:22,680
our graph Anna instance we wanted him to

00:12:19,680 --> 00:12:24,480
see only the service which are relevant

00:12:22,680 --> 00:12:26,010
for him on the dashboards which are

00:12:24,480 --> 00:12:27,860
relevant for him and this is a little

00:12:26,010 --> 00:12:31,800
solution we came up with so john

00:12:27,860 --> 00:12:34,200
excesses web server hthe HTTP in our

00:12:31,800 --> 00:12:36,870
case we authenticate a museum what leop

00:12:34,200 --> 00:12:38,370
and what of Kirk and after that we took

00:12:36,870 --> 00:12:41,329
off farah where this is John

00:12:38,370 --> 00:12:45,240
so kuffara knows this is John we've got

00:12:41,329 --> 00:12:46,770
an organization in Havana fortune and

00:12:45,240 --> 00:12:50,010
we've we've got this for all our

00:12:46,770 --> 00:12:54,540
employees which use Ravana and we do

00:12:50,010 --> 00:12:56,160
this because we do not point the this

00:12:54,540 --> 00:12:58,260
organization's data source and Ravana

00:12:56,160 --> 00:13:00,000
directly to Prometheus but instead we

00:12:58,260 --> 00:13:02,520
pointed to a custom service called from

00:13:00,000 --> 00:13:06,180
it is filter proxy and from easiest

00:13:02,520 --> 00:13:09,329
filter filter proxy translates all these

00:13:06,180 --> 00:13:12,029
requests and maps a special label onto

00:13:09,329 --> 00:13:14,670
all parts of the query so if John is

00:13:12,029 --> 00:13:16,320
going to ask for the app metric he for

00:13:14,670 --> 00:13:19,140
me the filter proxies going translated

00:13:16,320 --> 00:13:21,810
to up but only for time serials labeled

00:13:19,140 --> 00:13:23,760
with owner John this is not some idea

00:13:21,810 --> 00:13:25,800
which I came up with this an idea which

00:13:23,760 --> 00:13:27,810
I stole or borrowed from some of the

00:13:25,800 --> 00:13:29,640
tickets it may be Brian who posted it

00:13:27,810 --> 00:13:33,900
there so this is Oh

00:13:29,640 --> 00:13:35,460
some implementation of this idea of

00:13:33,900 --> 00:13:37,860
course we do not do with this all

00:13:35,460 --> 00:13:40,050
manually and we've got lots of employees

00:13:37,860 --> 00:13:41,550
and so we've got lots of gravano users

00:13:40,050 --> 00:13:44,430
so we use some kind of template

00:13:41,550 --> 00:13:46,770
organizations where we do our dashboard

00:13:44,430 --> 00:13:48,810
and changes and we've got a job which

00:13:46,770 --> 00:13:54,240
managers of the dashboards of the actual

00:13:48,810 --> 00:13:55,470
owner organizations we also run this

00:13:54,240 --> 00:13:58,080
setup twice

00:13:55,470 --> 00:14:00,360
basically for high availability so that

00:13:58,080 --> 00:14:03,900
we can perform maintenance work and we

00:14:00,360 --> 00:14:06,090
accomplish this by running floating a P

00:14:03,900 --> 00:14:07,680
address so this is always on the left

00:14:06,090 --> 00:14:10,560
side except when there's maintenance it

00:14:07,680 --> 00:14:13,290
is moved to the other side and we keep

00:14:10,560 --> 00:14:15,360
Gravano in sync by just are swinging the

00:14:13,290 --> 00:14:17,810
curve on a SQLite database we know that

00:14:15,360 --> 00:14:20,700
they're more sophisticated ways such as

00:14:17,810 --> 00:14:23,160
running a product and together with some

00:14:20,700 --> 00:14:25,320
kind of database which is highly

00:14:23,160 --> 00:14:26,970
available on its own but we really

00:14:25,320 --> 00:14:32,700
wanted to cut down dependencies at this

00:14:26,970 --> 00:14:34,320
place so and lastly these were the parts

00:14:32,700 --> 00:14:35,580
about the the common Prometheus

00:14:34,320 --> 00:14:37,200
infrastructure but I also want to

00:14:35,580 --> 00:14:39,060
provide some insight into how we

00:14:37,200 --> 00:14:42,030
integrate Prometheus in our into our

00:14:39,060 --> 00:14:44,000
environment so I've got two examples

00:14:42,030 --> 00:14:47,220
there and the first example is about

00:14:44,000 --> 00:14:50,160
integrating Prometheus into our existing

00:14:47,220 --> 00:14:53,880
configure manage node we use puppet

00:14:50,160 --> 00:14:56,220
others have used chef in their talks we

00:14:53,880 --> 00:14:57,660
use puppet and we use puppet to deploy

00:14:56,220 --> 00:15:00,240
all this service so it was a natural

00:14:57,660 --> 00:15:02,130
choice to use puppet to deploy all the

00:15:00,240 --> 00:15:04,590
exporters we IND on these machines and

00:15:02,130 --> 00:15:08,430
it was also a natural choice to use

00:15:04,590 --> 00:15:11,610
puppet to configure Prometheus itself so

00:15:08,430 --> 00:15:13,470
puppet takes its own will database of

00:15:11,610 --> 00:15:16,470
flat file database which is called

00:15:13,470 --> 00:15:18,540
Huayra in puppet speak and this contains

00:15:16,470 --> 00:15:20,760
all our escaped convicts and our

00:15:18,540 --> 00:15:22,770
platform alerts so these Linux platform

00:15:20,760 --> 00:15:25,170
where let's come from this place and we

00:15:22,770 --> 00:15:27,240
can also use puppet to distribute them

00:15:25,170 --> 00:15:29,640
server all specific alerts in this case

00:15:27,240 --> 00:15:31,470
so that we can have custom alerts for

00:15:29,640 --> 00:15:37,310
databases or custom alerts for that

00:15:31,470 --> 00:15:39,780
service this is an example for

00:15:37,310 --> 00:15:41,940
Prometheus being integrated into the

00:15:39,780 --> 00:15:43,260
conservation management but if a second

00:15:41,940 --> 00:15:45,090
example where we

00:15:43,260 --> 00:15:48,630
made use of the features of Prometheus

00:15:45,090 --> 00:15:50,970
in another process I hope that most

00:15:48,630 --> 00:15:52,950
companies do have some kind of patch

00:15:50,970 --> 00:15:57,510
management patch management process at

00:15:52,950 --> 00:15:59,790
least we do and we try to automate as

00:15:57,510 --> 00:16:01,350
much as possible this means that our

00:15:59,790 --> 00:16:03,750
patch management process should be

00:16:01,350 --> 00:16:06,360
automated as well so we've got some

00:16:03,750 --> 00:16:08,340
internals the development and staging

00:16:06,360 --> 00:16:10,260
environments where we pull new operating

00:16:08,340 --> 00:16:12,210
system patches or leases new system

00:16:10,260 --> 00:16:14,100
software and so on and we use Prometheus

00:16:12,210 --> 00:16:16,110
to detect well common problems there

00:16:14,100 --> 00:16:18,540
like the server no longer coming in

00:16:16,110 --> 00:16:21,390
coming online after such an update but

00:16:18,540 --> 00:16:22,740
we also run custom benchmarks to see if

00:16:21,390 --> 00:16:24,480
there are some performance regressions

00:16:22,740 --> 00:16:26,340
and we started to introduce this and

00:16:24,480 --> 00:16:28,290
when all the hyper inspector and

00:16:26,340 --> 00:16:31,850
measurement came up so this is the place

00:16:28,290 --> 00:16:35,750
where we run from easiest and can detect

00:16:31,850 --> 00:16:38,220
anomalies before they we roll them out

00:16:35,750 --> 00:16:39,960
this is the first place where we use

00:16:38,220 --> 00:16:41,610
Prometheus in the patch management and

00:16:39,960 --> 00:16:43,950
we use it also in a second place and

00:16:41,610 --> 00:16:47,670
when we actually update application

00:16:43,950 --> 00:16:50,040
servers such as main website then the

00:16:47,670 --> 00:16:52,020
changes will first go out into a

00:16:50,040 --> 00:16:54,000
development machine and if there is

00:16:52,020 --> 00:16:55,350
anything which which comes up in the

00:16:54,000 --> 00:16:57,840
alerting then our patch management

00:16:55,350 --> 00:16:59,880
process talks to Prometheus asks is

00:16:57,840 --> 00:17:01,980
there anything special about the

00:16:59,880 --> 00:17:03,660
development server and if there is it's

00:17:01,980 --> 00:17:05,310
going to stop the deployment into the

00:17:03,660 --> 00:17:07,800
other environments and this even works

00:17:05,310 --> 00:17:10,530
in production so we can ensure that if

00:17:07,800 --> 00:17:12,780
one server is down because of well other

00:17:10,530 --> 00:17:14,280
maintenance or some problem then we are

00:17:12,780 --> 00:17:16,350
not going to patch the other one and

00:17:14,280 --> 00:17:20,670
leave us with with with no service lose

00:17:16,350 --> 00:17:22,829
of the actual customers so these are the

00:17:20,670 --> 00:17:26,880
parts which were good for us but we are

00:17:22,829 --> 00:17:28,079
also not done so we do have some ideas

00:17:26,880 --> 00:17:31,230
about long-term storage we currently

00:17:28,079 --> 00:17:34,470
keep data for about one year and this

00:17:31,230 --> 00:17:38,070
can be problematic if people try to well

00:17:34,470 --> 00:17:40,200
process large time time ranges and we

00:17:38,070 --> 00:17:42,120
would really love some some solution for

00:17:40,200 --> 00:17:43,860
down sampling the data we currently have

00:17:42,120 --> 00:17:45,900
some kind of channeler process which

00:17:43,860 --> 00:17:47,850
talks to the Prometheus API and release

00:17:45,900 --> 00:17:50,010
all metrics which are not used in any

00:17:47,850 --> 00:17:53,160
dashboards or are not used in any rules

00:17:50,010 --> 00:17:55,590
so that these these metrics are only

00:17:53,160 --> 00:17:56,650
available for 30 days and only the other

00:17:55,590 --> 00:17:58,480
metrics are canceled

00:17:56,650 --> 00:18:00,370
we also want to look into dashboard

00:17:58,480 --> 00:18:05,350
performance optimizations and have lots

00:18:00,370 --> 00:18:06,910
of other ideas so to sum it up and we

00:18:05,350 --> 00:18:07,660
are very happy to have implemented

00:18:06,910 --> 00:18:10,300
prometheus

00:18:07,660 --> 00:18:12,580
Prometheus provides us flexibility even

00:18:10,300 --> 00:18:13,900
in our regulated environment in high

00:18:12,580 --> 00:18:16,780
security environment with lots of

00:18:13,900 --> 00:18:18,970
policies it made it possible to off

00:18:16,780 --> 00:18:21,160
about my two tenancy for our teams so

00:18:18,970 --> 00:18:23,260
that each person only sees the relevant

00:18:21,160 --> 00:18:25,270
parts for them and for me this is really

00:18:23,260 --> 00:18:28,240
good to integrate into other processes

00:18:25,270 --> 00:18:30,850
which also helped us so this being said

00:18:28,240 --> 00:18:32,679
thank you very much to the Prometheus

00:18:30,850 --> 00:18:35,510
developers community the prompt con

00:18:32,679 --> 00:18:44,650
organizers and thank you for listening

00:18:35,510 --> 00:18:44,650
[Applause]

00:18:46,559 --> 00:18:52,480
hi I have two questions about the filter

00:18:49,809 --> 00:18:55,630
the Prometheus filter proxy

00:18:52,480 --> 00:18:58,059
the first is descent work does it inject

00:18:55,630 --> 00:19:00,130
the correct owner label until all like

00:18:58,059 --> 00:19:02,500
any kind of complex query that that

00:19:00,130 --> 00:19:04,300
comes through and the second is how do

00:19:02,500 --> 00:19:06,460
you ensure that the owner labels are

00:19:04,300 --> 00:19:07,780
actually part of the scraping logic how

00:19:06,460 --> 00:19:08,980
do you make sure they actually get added

00:19:07,780 --> 00:19:11,980
get it added in

00:19:08,980 --> 00:19:15,250
okay first question

00:19:11,980 --> 00:19:17,530
I hope that we have a robust solution in

00:19:15,250 --> 00:19:19,150
this place and we are using the actual

00:19:17,530 --> 00:19:21,190
prom killer passes so this is not some

00:19:19,150 --> 00:19:23,710
kind of a regular expression Hickory or

00:19:21,190 --> 00:19:26,530
something we use the official from

00:19:23,710 --> 00:19:28,929
parser and try to match each prom cool

00:19:26,530 --> 00:19:30,730
well data type and so on and inject this

00:19:28,929 --> 00:19:32,800
label there and we have no found cases

00:19:30,730 --> 00:19:35,440
yet but this project is open source you

00:19:32,800 --> 00:19:37,210
can have a look at it and if you find

00:19:35,440 --> 00:19:39,130
something please tell me so that we can

00:19:37,210 --> 00:19:41,140
work it out and for the second part of

00:19:39,130 --> 00:19:43,179
how we ensure that the honest label is

00:19:41,140 --> 00:19:45,640
attached to all the metrics this is part

00:19:43,179 --> 00:19:47,920
of our file finest D service discovery

00:19:45,640 --> 00:19:49,630
making mechanism so we use puppet to

00:19:47,920 --> 00:19:51,700
pull this data from our server inventory

00:19:49,630 --> 00:19:53,500
and use popper to write a config file

00:19:51,700 --> 00:19:58,540
which is used by Prometheus and contains

00:19:53,500 --> 00:20:03,910
this owners label third shope oh thank

00:19:58,540 --> 00:20:07,690
you oh yeah I'm here yeah thank you

00:20:03,910 --> 00:20:09,940
I have a question about this fight so I

00:20:07,690 --> 00:20:13,960
call 85 and I notice the name is

00:20:09,940 --> 00:20:14,980
really good fun okay I have two

00:20:13,960 --> 00:20:18,190
questions

00:20:14,980 --> 00:20:21,670
first question do you restrict access on

00:20:18,190 --> 00:20:23,920
the target host mmm I don't know that

00:20:21,670 --> 00:20:27,160
you don't have every every comment maybe

00:20:23,920 --> 00:20:31,330
or something to exist and the second

00:20:27,160 --> 00:20:33,790
question is why not on some maybe white

00:20:31,330 --> 00:20:36,550
brace it to a spokes you on the target

00:20:33,790 --> 00:20:39,130
host as well not giving for such access

00:20:36,550 --> 00:20:46,570
for someone who has access to parameters

00:20:39,130 --> 00:20:49,000
host and the first part how we lock down

00:20:46,570 --> 00:20:52,450
access well basically we use basic

00:20:49,000 --> 00:20:54,090
normal OpenSSH methods so we use public

00:20:52,450 --> 00:20:56,080
key authentication and we can put some

00:20:54,090 --> 00:20:59,020
restrictions in the authorized keys file

00:20:56,080 --> 00:21:02,110
such as disabling all command usage and

00:20:59,020 --> 00:21:04,030
only permitting the the port forwarding

00:21:02,110 --> 00:21:06,100
which is what what is required in this

00:21:04,030 --> 00:21:07,930
case well there's no no shell or

00:21:06,100 --> 00:21:10,270
anything open is chapter the port

00:21:07,930 --> 00:21:12,730
forwarding yeah okay the second part was

00:21:10,270 --> 00:21:15,310
about or why not use something like a

00:21:12,730 --> 00:21:19,870
jinx on my bill wine jinx on the target

00:21:15,310 --> 00:21:21,670
host yeah well basically because we

00:21:19,870 --> 00:21:23,560
would have to run it in on all the

00:21:21,670 --> 00:21:25,270
target host and as that this is the

00:21:23,560 --> 00:21:27,430
common best practice you can do that and

00:21:25,270 --> 00:21:29,830
in our case we decided that it would be

00:21:27,430 --> 00:21:31,600
too much overhead even by using puppet

00:21:29,830 --> 00:21:33,250
we would have to run it everywhere they

00:21:31,600 --> 00:21:35,800
can be failures it would have to been

00:21:33,250 --> 00:21:38,230
updated and so on so we just want to try

00:21:35,800 --> 00:21:40,570
to keep the number of agents on these

00:21:38,230 --> 00:21:43,840
servers low so we went for this solution

00:21:40,570 --> 00:21:45,910
which places in a central place one more

00:21:43,840 --> 00:21:49,000
question here do you find any hard limit

00:21:45,910 --> 00:21:51,910
on this sh pipe you try and you using

00:21:49,000 --> 00:21:54,940
like how much can you ingest not yet in

00:21:51,910 --> 00:21:57,190
production and the CPU usage is also ok

00:21:54,940 --> 00:21:59,340
and we ran into one problem once and it

00:21:57,190 --> 00:22:02,290
was related to the file descriptors

00:21:59,340 --> 00:22:04,750
since then we have implemented the

00:22:02,290 --> 00:22:06,220
metrics in phone for SSA chief ID so we

00:22:04,750 --> 00:22:07,510
know about its state and we have an

00:22:06,220 --> 00:22:09,010
alert on it and it doesn't happen

00:22:07,510 --> 00:22:11,470
anymore and regarding the actual

00:22:09,010 --> 00:22:13,210
throughput which works I don't have have

00:22:11,470 --> 00:22:15,220
any numbers ready but the github page

00:22:13,210 --> 00:22:16,960
maybe contains the actual numbered

00:22:15,220 --> 00:22:18,370
numbers at least it contains some

00:22:16,960 --> 00:22:19,900
scripts for benchmarking so you can

00:22:18,370 --> 00:22:23,580
quickly run it yourself and try to see

00:22:19,900 --> 00:22:23,580
how how many connections it will handle

00:22:25,260 --> 00:22:30,100
hey sorry with satified do you have a

00:22:28,600 --> 00:22:32,860
way to restrict only the port that

00:22:30,100 --> 00:22:35,440
exports matrix or do like does the user

00:22:32,860 --> 00:22:38,770
have access to all the ports on the

00:22:35,440 --> 00:22:42,940
machine on do you mean the machine where

00:22:38,770 --> 00:22:45,880
whether we are monitoring the target yes

00:22:42,940 --> 00:22:48,010
and we use a exporter extreme

00:22:45,880 --> 00:22:49,660
restriction for binding to localhost

00:22:48,010 --> 00:22:50,320
only which limits all access to the

00:22:49,660 --> 00:22:52,300
machine itself

00:22:50,320 --> 00:22:55,240
and in addition we deploy a features

00:22:52,300 --> 00:22:57,100
rules where we have an own image target

00:22:55,240 --> 00:22:59,020
which limits all exes only to the

00:22:57,100 --> 00:23:04,980
specific user user which we use for SS

00:22:59,020 --> 00:23:04,980
85 access anymore

00:23:05,490 --> 00:23:09,480
3 2 1

00:23:07,960 --> 00:23:13,090
thank you very much

00:23:09,480 --> 00:23:17,350
[Applause]

00:23:13,090 --> 00:23:17,690
[Music]

00:23:17,350 --> 00:23:22,770
you

00:23:17,690 --> 00:23:22,770

YouTube URL: https://www.youtube.com/watch?v=qwWw41LEoSU


