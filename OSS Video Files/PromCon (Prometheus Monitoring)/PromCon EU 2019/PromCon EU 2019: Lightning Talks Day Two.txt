Title: PromCon EU 2019: Lightning Talks Day Two
Publication date: 2019-12-29
Playlist: PromCon EU 2019
Description: 
	#1 - Metric Storage for Capacity Management of Kubernetes/OpenShift Clusters
Speaker: Ulrike Klusik
Slides: https://promcon.io/2019-munich/slides/lt2-01_metric-storage-for-capacity-management.pdf

#2 - PromQL for Security
Speaker: Carlos Arilla
Slides: https://promcon.io/2019-munich/slides/lt2-02_promql-for-security.pdf

#3 - From Basement to Skyscraper - Monitoring Distributed Micro-DCs
Speaker: Joris Baum
Slides: https://promcon.io/2019-munich/slides/lt2-03_from-basement-to-skyscraper.pdf

#4 - Monitoring Networking Infrastructure with Prometheus ecosystem
Speaker: Artem Nedoshepa
Slides: https://promcon.io/2019-munich/slides/lt2-04_monitoring-networking-infrastructure-with-prometheus-ecosystem.pdf

#5 - The Numbers Behind Prometheus and Grafana
Speaker: Manuel Craciun
Slides: https://promcon.io/2019-munich/slides/lt2-05_the-numbers-behind-prometheus-and-grafana.pdf

#6 - Metrics, and Logs, and Metadata, Oh My!
Speaker: Mike Freedman

#7 - How to Manage 600 Prometheus Instances?
Speaker: Geoffrey Beausire
Slides: https://promcon.io/2019-munich/slides/lt2-07_how-to-manage-600-prometheus-instances.pdf

#8 - Monitoring at CCC NOC - How the Internetmanufaktur Uses Prometheus
Speaker: Frederic Jaeckel
Slides: https://promcon.io/2019-munich/slides/lt2-08_monitoring-at-ccc-noc.pdf

#9 - How to Monitor a Music Festival
Speaker: Christian Simon

#10 - PromQL Aggregation Sharding
Speaker: Tom Wilkie
Slides: https://promcon.io/2019-munich/slides/lt2-10_promql-aggregation-sharding.pdf

#11 - Getting Non-Infrastructure Engineers to Use Prometheus
Speaker: Sally Lehman
Slides: https://promcon.io/2019-munich/slides/lt2-11_getting-non-infrastructure-engineers-to-use-prometheus.pdf

#12 - Victoria Metrics Cluster Architecture
Speaker: Alexander Danilov
Slides: https://promcon.io/2019-munich/slides/lt2-12_victoria-metrics-cluster-architecture.pdf

#13 - A Deeper Look into Victoria Metrics Benchmarks
Speaker: Brian Brazil

#14 - Pain Points with M3 and How Replication Works
Speaker: Rob Skillington
Slides: https://promcon.io/2019-munich/slides/lt2-14_pain-points-with-m3-and-how-replication-works.pdf

#15 - Alert Acknowledgment with Alertmanager
Speaker: Łukasz Mierzwa
Slides: https://promcon.io/2019-munich/slides/lt2-15_alert_acknowledgement_with_alertmanager.pdf

#16 - Cacheability of PromQL Queries
Speaker: Björn Rabenstein

#17 - Automated Canaries with Prometheus, Kubernetes and Service Mesh
Speaker: Bryan Boreham
Slides: https://promcon.io/2019-munich/slides/lt2-17_automated-canaries-with-prometheus-kubernetes-and-service-mesh.pdf
Captions: 
	00:00:00,650 --> 00:00:08,910
[Music]

00:00:11,440 --> 00:00:16,750
okay hey hello I'm talking about metric

00:00:14,860 --> 00:00:20,020
storage for capacity management of

00:00:16,750 --> 00:00:25,860
openshift clusters we've heard something

00:00:20,020 --> 00:00:28,359
about capacity management so the part is

00:00:25,860 --> 00:00:32,080
capacity management needs highly

00:00:28,359 --> 00:00:35,500
aggregated metrics for not a capacity

00:00:32,080 --> 00:00:37,780
quota and resource usages probably you

00:00:35,500 --> 00:00:39,370
want to keep this over several months

00:00:37,780 --> 00:00:42,129
and years

00:00:39,370 --> 00:00:44,829
unfortunately Pro meters has only one

00:00:42,129 --> 00:00:48,219
retention policies for also we have a

00:00:44,829 --> 00:00:51,370
conflict here in our meter system for

00:00:48,219 --> 00:00:55,059
once we want to have detailed metrics

00:00:51,370 --> 00:00:59,170
for post-mortem analysis which we only

00:00:55,059 --> 00:01:01,480
want to need days or up to months and on

00:00:59,170 --> 00:01:06,640
the other hand we need these highly

00:01:01,480 --> 00:01:10,899
aggregated metrics for a long term so we

00:01:06,640 --> 00:01:14,350
heard also before the one solution to

00:01:10,899 --> 00:01:16,240
this is to have long-term storage why

00:01:14,350 --> 00:01:18,789
are the which was first why are they

00:01:16,240 --> 00:01:25,030
provide us with preemie size remote

00:01:18,789 --> 00:01:30,160
right feature and with this we can we

00:01:25,030 --> 00:01:47,380
can keep the capacity management metrics

00:01:30,160 --> 00:01:49,720
in storage and we can essentially use

00:01:47,380 --> 00:01:53,830
the same dashboards directly on premiere

00:01:49,720 --> 00:01:57,190
OS and on the long term storage so next

00:01:53,830 --> 00:02:01,599
thing is a small architecture for this

00:01:57,190 --> 00:02:04,209
so for Cuban ages up shift clusters we

00:02:01,599 --> 00:02:08,250
have the classical set up here have the

00:02:04,209 --> 00:02:13,120
promised OS within within the cluster

00:02:08,250 --> 00:02:16,510
set up and for the optional part another

00:02:13,120 --> 00:02:19,209
component is added shift state metrics

00:02:16,510 --> 00:02:23,080
which provides us additionally with

00:02:19,209 --> 00:02:29,860
object specific metric

00:02:23,080 --> 00:02:32,680
and of course so the remote side we put

00:02:29,860 --> 00:02:37,270
outside our clusters to be monitored

00:02:32,680 --> 00:02:39,280
because we want to keep the capacity

00:02:37,270 --> 00:02:42,400
management data or for several clusters

00:02:39,280 --> 00:02:45,250
in one place so we looked for a central

00:02:42,400 --> 00:02:48,100
storage system so this can be set up on

00:02:45,250 --> 00:02:51,520
a virtual machine or in another class

00:02:48,100 --> 00:02:55,270
than you would like so forth uh no

00:02:51,520 --> 00:02:58,990
solution we need a couple of containers

00:02:55,270 --> 00:03:01,450
to to run them so the first is the

00:02:58,990 --> 00:03:04,930
itano's receiver which receives the

00:03:01,450 --> 00:03:10,180
metrics by a remote write and then

00:03:04,930 --> 00:03:13,600
stores it in a compatible bucket in our

00:03:10,180 --> 00:03:21,070
case we used the Menuhin Mineo container

00:03:13,600 --> 00:03:24,160
to do this and we need regularly running

00:03:21,070 --> 00:03:27,880
from us compactor and to get the storage

00:03:24,160 --> 00:03:31,239
efficient for creaming we wanted to use

00:03:27,880 --> 00:03:33,940
a fauna as visualization pack and

00:03:31,239 --> 00:03:35,910
therefore we need a saunas query which

00:03:33,940 --> 00:03:38,470
can which provides the query

00:03:35,910 --> 00:03:41,830
interferometers from qf query interface

00:03:38,470 --> 00:03:46,840
and this tunnel store container to talk

00:03:41,830 --> 00:03:48,660
to the s3 bucket so what's the result of

00:03:46,840 --> 00:03:53,290
this is this capacity management

00:03:48,660 --> 00:03:56,950
overview where we keep an overview of

00:03:53,290 --> 00:04:00,760
the whole cluster for memory CPU and

00:03:56,950 --> 00:04:07,810
ports which are the typical resources

00:04:00,760 --> 00:04:11,650
and yeah what we distinguished what we

00:04:07,810 --> 00:04:15,510
see on the on the right hand side here

00:04:11,650 --> 00:04:15,510
is that we distinguish between

00:04:16,049 --> 00:04:21,430
application and infrastructure

00:04:17,980 --> 00:04:27,490
containers so we can keep an eye on both

00:04:21,430 --> 00:04:33,070
of them the tricky part for for

00:04:27,490 --> 00:04:34,830
kubernetes is the quotas there are some

00:04:33,070 --> 00:04:38,770
different

00:04:34,830 --> 00:04:43,390
writing possibilities to define quotas

00:04:38,770 --> 00:04:47,650
and to have several quotas namespace so

00:04:43,390 --> 00:04:50,380
I needed some weird for from QL formulas

00:04:47,650 --> 00:04:52,270
if you're interested in the details you

00:04:50,380 --> 00:04:54,520
can come to me later on thank you very

00:04:52,270 --> 00:04:58,000
much thank you

00:04:54,520 --> 00:05:00,310
Arika my name is Carlos Correa I'm

00:04:58,000 --> 00:05:03,760
working for city gas Dutch marketing

00:05:00,310 --> 00:05:08,170
engineer and I'm a father of three

00:05:03,760 --> 00:05:10,810
children you have you notice my white

00:05:08,170 --> 00:05:14,950
hair well let's talk about monitoring

00:05:10,810 --> 00:05:17,980
but in a different way because we are

00:05:14,950 --> 00:05:20,100
talking now about security are

00:05:17,980 --> 00:05:24,310
monitoring our security so different

00:05:20,100 --> 00:05:27,120
well sometimes the first indicator of

00:05:24,310 --> 00:05:31,030
our security risk is to an increase off

00:05:27,120 --> 00:05:33,130
resources you say it so if you receive a

00:05:31,030 --> 00:05:37,260
computer taking attack you will not it

00:05:33,130 --> 00:05:40,110
because your associates will rise so

00:05:37,260 --> 00:05:42,730
many times is a an entry point of

00:05:40,110 --> 00:05:45,610
knowledge and knowledge is very

00:05:42,730 --> 00:05:49,360
important for security many times you

00:05:45,610 --> 00:05:51,850
imagine a security people as hackers in

00:05:49,360 --> 00:05:55,300
a terminal but most of the time security

00:05:51,850 --> 00:05:58,180
means knowing what you have inventories

00:05:55,300 --> 00:06:00,340
things like that that will provide you

00:05:58,180 --> 00:06:01,930
information about what you have in order

00:06:00,340 --> 00:06:06,280
to get information about what

00:06:01,930 --> 00:06:07,090
vulnerabilities can you have and in

00:06:06,280 --> 00:06:10,000
recent years

00:06:07,090 --> 00:06:14,560
DevOps teams have been the champions of

00:06:10,000 --> 00:06:16,570
the monitoring but now they are starting

00:06:14,560 --> 00:06:19,390
to have responsibilities in security too

00:06:16,570 --> 00:06:23,770
because when you are in containers or

00:06:19,390 --> 00:06:25,270
containers immutable in theory so if you

00:06:23,770 --> 00:06:27,880
do it run at the beginning of the

00:06:25,270 --> 00:06:31,210
process you cannot change that in in a

00:06:27,880 --> 00:06:34,150
further process so a DevOps team will

00:06:31,210 --> 00:06:38,050
have security responsibilities and the

00:06:34,150 --> 00:06:41,700
ability to have tools that can working

00:06:38,050 --> 00:06:45,729
in both a expect can be very helpful

00:06:41,700 --> 00:06:48,190
let's go with the examples a very very

00:06:45,729 --> 00:06:50,110
very easy example low

00:06:48,190 --> 00:06:54,340
if you have a log increase you can

00:06:50,110 --> 00:06:59,560
probably have a security issue crypto

00:06:54,340 --> 00:07:03,250
mining a spam whatever so if you have an

00:06:59,560 --> 00:07:05,590
increase of resources you can be

00:07:03,250 --> 00:07:10,170
probably being attack so you should

00:07:05,590 --> 00:07:13,600
investigate that a next level is that a

00:07:10,170 --> 00:07:15,520
Inc unit is an example a positive

00:07:13,600 --> 00:07:18,790
running without limit are especially

00:07:15,520 --> 00:07:23,110
vulnerable to increase of those

00:07:18,790 --> 00:07:26,890
resources so I will leave you that has

00:07:23,110 --> 00:07:29,290
to mix those two queries to create one

00:07:26,890 --> 00:07:30,880
that could if you information about the

00:07:29,290 --> 00:07:35,380
posts that are more vulnerable to load

00:07:30,880 --> 00:07:40,120
increase and if they have been using

00:07:35,380 --> 00:07:42,760
more resources a second example we we

00:07:40,120 --> 00:07:44,080
saw recently a vulnerability ingo that

00:07:42,760 --> 00:07:48,580
affected kubernetes and

00:07:44,080 --> 00:07:50,620
everybody cried and so on so I found

00:07:48,580 --> 00:07:53,920
this in Twitter that in fact is very

00:07:50,620 --> 00:07:56,800
clever all the exporters in go are

00:07:53,920 --> 00:07:59,110
exposing our supposing information of

00:07:56,800 --> 00:08:01,510
the version so maybe you can use your

00:07:59,110 --> 00:08:04,240
own Prometheus to gather information

00:08:01,510 --> 00:08:05,470
about the version and to know where is

00:08:04,240 --> 00:08:09,150
running the things that are vulnerable

00:08:05,470 --> 00:08:11,140
you can extrapolate this to different

00:08:09,150 --> 00:08:13,060
languages or libraries and you can

00:08:11,140 --> 00:08:15,340
instrument your own versions here to

00:08:13,060 --> 00:08:18,690
have that information at hand another

00:08:15,340 --> 00:08:22,210
example can be a change in desperation

00:08:18,690 --> 00:08:24,580
date of a certificate maybe that can

00:08:22,210 --> 00:08:28,090
give you information about someone that

00:08:24,580 --> 00:08:31,840
is trying to supply to put in the middle

00:08:28,090 --> 00:08:34,690
of your connections and try to make a

00:08:31,840 --> 00:08:38,050
man-in-the-middle attacks so unexpected

00:08:34,690 --> 00:08:40,539
changes can can make a you suspect of of

00:08:38,050 --> 00:08:43,210
the attack and the next level you can

00:08:40,539 --> 00:08:46,300
check to with blackbox Porter they

00:08:43,210 --> 00:08:49,150
recently put a TLS version so you can

00:08:46,300 --> 00:08:50,890
check that you are running a safe

00:08:49,150 --> 00:08:55,480
version of you're TLS in your

00:08:50,890 --> 00:08:58,750
application and another one important

00:08:55,480 --> 00:09:00,950
thing is that usually one of the

00:08:58,750 --> 00:09:03,830
problems of

00:09:00,950 --> 00:09:08,420
a security security issue an attack is

00:09:03,830 --> 00:09:10,700
that it will give you what cost you

00:09:08,420 --> 00:09:14,510
money that's most of the concerns of the

00:09:10,700 --> 00:09:17,960
security teams they can span you know in

00:09:14,510 --> 00:09:22,610
acronyms cluster or new easy to teach

00:09:17,960 --> 00:09:24,590
much in AWS so a with this Porter you

00:09:22,610 --> 00:09:27,560
can track the daily cost of your

00:09:24,590 --> 00:09:30,650
infrastructure in AWS so you can see if

00:09:27,560 --> 00:09:34,660
the cost increase unexpectedly you can

00:09:30,650 --> 00:09:38,570
be in attack and the last one is like a

00:09:34,660 --> 00:09:41,240
error rate you can see that here the

00:09:38,570 --> 00:09:46,580
number of rate rise so you have been

00:09:41,240 --> 00:09:48,410
attack so conclusions DevOps need to be

00:09:46,580 --> 00:09:51,230
in power to have security

00:09:48,410 --> 00:09:53,480
responsibilities and you should have

00:09:51,230 --> 00:09:55,760
that in mind to instrument your code

00:09:53,480 --> 00:10:00,260
with security futures thank you very

00:09:55,760 --> 00:10:03,200
much thank you very much Carlos ok so I

00:10:00,260 --> 00:10:05,840
work as a DevOps engineer at cloud and

00:10:03,200 --> 00:10:08,390
heat my responsibilities are

00:10:05,840 --> 00:10:11,690
configuration management of prometheus

00:10:08,390 --> 00:10:13,630
and operations of our data centers in my

00:10:11,690 --> 00:10:15,650
talk from basement to skyscraper

00:10:13,630 --> 00:10:17,810
monitoring distributed Micro data

00:10:15,650 --> 00:10:20,600
centers I will talk about our experience

00:10:17,810 --> 00:10:26,870
with Prometheus what we do to set it up

00:10:20,600 --> 00:10:30,110
and yeah my company we what we do is we

00:10:26,870 --> 00:10:33,890
put servers in the base in basements or

00:10:30,110 --> 00:10:36,680
in skyscrapers and yeah we call those

00:10:33,890 --> 00:10:39,350
the micro data centers and we have

00:10:36,680 --> 00:10:43,490
different data centers across Germany

00:10:39,350 --> 00:10:45,320
mostly some also in other countries so

00:10:43,490 --> 00:10:50,300
this is the heart of the technology that

00:10:45,320 --> 00:10:53,930
my company does so we have a computer a

00:10:50,300 --> 00:10:58,000
plate here what we can see to the right

00:10:53,930 --> 00:11:02,690
is a 55 hot degree hot water comes in 60

00:10:58,000 --> 00:11:06,140
degree hot water comes out and what the

00:11:02,690 --> 00:11:09,500
plate does so we capture 90 percent of

00:11:06,140 --> 00:11:13,010
the emitted heat of the blade from CPU

00:11:09,500 --> 00:11:14,270
RAM and mainboard and we used we use

00:11:13,010 --> 00:11:17,089
this heat

00:11:14,270 --> 00:11:21,230
we use the water - this water is being

00:11:17,089 --> 00:11:24,470
used in buildings for heating up

00:11:21,230 --> 00:11:25,790
buildings and stuff so of course my

00:11:24,470 --> 00:11:27,740
drawing is very important for us we need

00:11:25,790 --> 00:11:31,209
to collect info about pressure

00:11:27,740 --> 00:11:33,709
temperature flow of the water we need to

00:11:31,209 --> 00:11:36,740
collect a lot of info about temperature

00:11:33,709 --> 00:11:38,779
of CPU RAM and their usage and also

00:11:36,740 --> 00:11:42,589
monitor their high level services that

00:11:38,779 --> 00:11:47,060
run on top of the hardware we mostly run

00:11:42,589 --> 00:11:48,560
OpenStack so we have a very he trigonis

00:11:47,060 --> 00:11:52,130
infrastructure we have a bunch of

00:11:48,560 --> 00:11:53,770
different data center types we now with

00:11:52,130 --> 00:11:58,240
the newest addition addition being

00:11:53,770 --> 00:12:02,290
containers we also like we have a legacy

00:11:58,240 --> 00:12:04,459
data center we also have a

00:12:02,290 --> 00:12:07,910
state-of-the-art our data centers

00:12:04,459 --> 00:12:11,330
prometheus needs to be ready for all of

00:12:07,910 --> 00:12:14,000
those kind of types we have a big

00:12:11,330 --> 00:12:16,580
variety of racks per data center we have

00:12:14,000 --> 00:12:18,740
different OpenStack versions and what's

00:12:16,580 --> 00:12:20,870
maybe also special from our case

00:12:18,740 --> 00:12:25,220
compared to yours is that we deploy on

00:12:20,870 --> 00:12:27,320
pair metal and on VMs with chef so what

00:12:25,220 --> 00:12:30,050
did we do to make optimises work we

00:12:27,320 --> 00:12:32,649
wrote an exporter we've got a bunch of

00:12:30,050 --> 00:12:36,200
exporters that gave us low-level data

00:12:32,649 --> 00:12:38,329
that we needed for monitoring pressure

00:12:36,200 --> 00:12:41,450
and temperature and so on

00:12:38,329 --> 00:12:44,709
this is OPC UA is for example another

00:12:41,450 --> 00:12:49,820
standard for that is used in buildings

00:12:44,709 --> 00:12:53,329
we use that to monitor pressure of the

00:12:49,820 --> 00:12:54,770
of the pipes for example and we have

00:12:53,329 --> 00:12:58,040
bigge energy exporter and heat

00:12:54,770 --> 00:12:59,950
controller own exporters that also to

00:12:58,040 --> 00:13:02,990
similar things then we also have a

00:12:59,950 --> 00:13:04,490
written a smart exporter which is on

00:13:02,990 --> 00:13:08,270
github you can check it out if you want

00:13:04,490 --> 00:13:10,339
and a NYX feed exporter with all with a

00:13:08,270 --> 00:13:12,829
bunch of other exporters we can put this

00:13:10,339 --> 00:13:16,610
into chef our configuration management

00:13:12,829 --> 00:13:20,600
this takes care of we give this we give

00:13:16,610 --> 00:13:23,620
chef the roles for each node and takes

00:13:20,600 --> 00:13:26,449
care of the targets and creating the

00:13:23,620 --> 00:13:27,750
Prometheus configuration and we

00:13:26,449 --> 00:13:30,450
basically then collectively

00:13:27,750 --> 00:13:33,690
apply button and hope that everything

00:13:30,450 --> 00:13:35,220
works yeah so promises was very

00:13:33,690 --> 00:13:38,310
convenient for us and is still very

00:13:35,220 --> 00:13:40,580
convenient for us to use what's also

00:13:38,310 --> 00:13:44,160
very convenient skra fauna and here is a

00:13:40,580 --> 00:13:46,830
example of panel of what happens in our

00:13:44,160 --> 00:13:50,970
data centers from time to time so here

00:13:46,830 --> 00:13:53,700
this graph here is the flow of of the

00:13:50,970 --> 00:13:55,770
water and at this moment shed suddenly

00:13:53,700 --> 00:13:59,670
the flow just stopped and we can see a

00:13:55,770 --> 00:14:02,820
whole rack of nodes the increasing in

00:13:59,670 --> 00:14:05,130
huge increase in CPU temperature and the

00:14:02,820 --> 00:14:09,660
notes is suddenly dropping out from time

00:14:05,130 --> 00:14:11,970
to time for example so prometheus is

00:14:09,660 --> 00:14:13,860
still up and running for us for a long

00:14:11,970 --> 00:14:17,850
like we have used we've used it since

00:14:13,860 --> 00:14:19,650
2017 it collects roughly 1.5 million

00:14:17,850 --> 00:14:23,730
series per minute across all our data

00:14:19,650 --> 00:14:25,530
centers we had issues with it mostly we

00:14:23,730 --> 00:14:29,070
have a retention time of three years so

00:14:25,530 --> 00:14:32,430
sometimes the disk space runs full yeah

00:14:29,070 --> 00:14:35,339
and yes we know that's configured that's

00:14:32,430 --> 00:14:37,280
a configuration issue of our our site

00:14:35,339 --> 00:14:39,870
and we are working on using talus and

00:14:37,280 --> 00:14:42,150
now we also have this new project called

00:14:39,870 --> 00:14:45,540
cracker this is an Orchestrator that

00:14:42,150 --> 00:14:48,750
uses Prometheus to as a basis to collect

00:14:45,540 --> 00:14:51,839
metrics for own scheduling decisions in

00:14:48,750 --> 00:14:53,460
Cuba Cuba nature's environment yeah so

00:14:51,839 --> 00:14:56,280
here are some more links if you want to

00:14:53,460 --> 00:14:56,750
check them out yeah that's it thank you

00:14:56,280 --> 00:15:01,110
very much

00:14:56,750 --> 00:15:03,510
[Applause]

00:15:01,110 --> 00:15:06,080
what else can we do is from ethers how

00:15:03,510 --> 00:15:12,950
about try to more internet work so I'm

00:15:06,080 --> 00:15:15,450
I'm network engineer and so we try to

00:15:12,950 --> 00:15:20,400
spin up primitives to basically improve

00:15:15,450 --> 00:15:22,560
our monitoring for networking stack this

00:15:20,400 --> 00:15:24,120
is the motivation kind of like

00:15:22,560 --> 00:15:26,790
high-level motivation while actual try

00:15:24,120 --> 00:15:30,380
it but in short for network engineer you

00:15:26,790 --> 00:15:32,640
want to be able to find the problem like

00:15:30,380 --> 00:15:35,339
whatever problem could be related to

00:15:32,640 --> 00:15:37,740
either something oversubscribed

00:15:35,339 --> 00:15:39,470
interface for apps BGP flaps in like

00:15:37,740 --> 00:15:42,650
super short time

00:15:39,470 --> 00:15:46,190
and whisper missus echo system actually

00:15:42,650 --> 00:15:49,360
it's pretty easy to do gruffly easy and

00:15:46,190 --> 00:15:54,500
also ability to create dashboards and

00:15:49,360 --> 00:15:56,540
capacity planning all that stuff so this

00:15:54,500 --> 00:15:58,670
is like simplified over simplified view

00:15:56,540 --> 00:16:01,850
of our existing infrastructure let's say

00:15:58,670 --> 00:16:04,460
so it's already based systems that

00:16:01,850 --> 00:16:08,030
ingest metrics then we also have a

00:16:04,460 --> 00:16:09,860
syslog that basically and all this then

00:16:08,030 --> 00:16:12,890
comes in into the centralized systems

00:16:09,860 --> 00:16:15,860
that make sense of both worlds like six

00:16:12,890 --> 00:16:17,600
large events versus metric base events

00:16:15,860 --> 00:16:18,950
with syslog there's a whole bunch of

00:16:17,600 --> 00:16:22,040
issues where like let's say you have

00:16:18,950 --> 00:16:25,910
some is configured or the pod improperly

00:16:22,040 --> 00:16:27,620
not properly deployed and let's say this

00:16:25,910 --> 00:16:30,410
clock never reaches the collector so you

00:16:27,620 --> 00:16:34,040
have the whole problem there with hourly

00:16:30,410 --> 00:16:37,400
by system it's very like I would say

00:16:34,040 --> 00:16:40,450
outdated it's not dynamic and it's hard

00:16:37,400 --> 00:16:44,500
to use so what we try to do instead

00:16:40,450 --> 00:16:50,240
basically typical setup where you have

00:16:44,500 --> 00:16:52,490
parameters clusters so all this work

00:16:50,240 --> 00:16:54,440
pretty well until we start sending too

00:16:52,490 --> 00:16:56,900
much data to Federation we open it a few

00:16:54,440 --> 00:16:59,780
times then we had to add mcdb cluster

00:16:56,900 --> 00:17:03,530
and this way engineers could hammered

00:16:59,780 --> 00:17:05,120
with queries all day long so this works

00:17:03,530 --> 00:17:07,280
pretty well but actually the hardest

00:17:05,120 --> 00:17:09,800
part was do you connect to existing

00:17:07,280 --> 00:17:12,410
system so when we planned all this

00:17:09,800 --> 00:17:13,450
we didn't budget like timewise for how

00:17:12,410 --> 00:17:16,610
much time it would take to actually

00:17:13,450 --> 00:17:19,430
agree with everybody and having all

00:17:16,610 --> 00:17:21,950
these discussions to send because we had

00:17:19,430 --> 00:17:25,580
to have two systems in parallel and to

00:17:21,950 --> 00:17:28,930
actually send all the alerts to existing

00:17:25,580 --> 00:17:33,080
system took I would say the longest time

00:17:28,930 --> 00:17:34,880
so it's our like I would say when you

00:17:33,080 --> 00:17:37,400
have when you were trying to build a

00:17:34,880 --> 00:17:41,300
system budget for this so when you all

00:17:37,400 --> 00:17:44,300
teams in line and some some challenges

00:17:41,300 --> 00:17:46,670
who is like let's say typical least bian

00:17:44,300 --> 00:17:49,070
topology is everything else three so

00:17:46,670 --> 00:17:51,140
when one link goes down you will have at

00:17:49,070 --> 00:17:53,030
least four events you will hairlike each

00:17:51,140 --> 00:17:55,400
side of the link it's like

00:17:53,030 --> 00:17:58,310
physically flap then each side will

00:17:55,400 --> 00:18:00,740
report BGP flap so we you have four

00:17:58,310 --> 00:18:02,300
events and this is just one physical

00:18:00,740 --> 00:18:04,370
link if one of the boxes in the middle

00:18:02,300 --> 00:18:06,650
goes down you can imagine how many

00:18:04,370 --> 00:18:09,860
events that will produce and so

00:18:06,650 --> 00:18:11,330
correlation becomes like key and we kind

00:18:09,860 --> 00:18:13,340
of like try to attempt to do it in

00:18:11,330 --> 00:18:18,500
premises which actually Bryant helped us

00:18:13,340 --> 00:18:20,990
with an idea come up with lldpe name so

00:18:18,500 --> 00:18:23,600
what we do we basically run a cache of

00:18:20,990 --> 00:18:25,840
lldp because if the doors goes down you

00:18:23,600 --> 00:18:28,490
don't have anymore

00:18:25,840 --> 00:18:30,320
metric in the system so with this cache

00:18:28,490 --> 00:18:31,880
like let's say it's some kind of cron

00:18:30,320 --> 00:18:34,510
job the trance I don't know every daily

00:18:31,880 --> 00:18:37,370
every 10 days depends on a system and

00:18:34,510 --> 00:18:40,610
the LDP name is the same on each side of

00:18:37,370 --> 00:18:43,820
the link as basically short name device

00:18:40,610 --> 00:18:45,470
a interface device B interface and based

00:18:43,820 --> 00:18:47,900
on this label you can actually make

00:18:45,470 --> 00:18:51,740
sense that these two events are they

00:18:47,900 --> 00:18:55,130
came from the same link and so in the I

00:18:51,740 --> 00:18:57,320
would say you are do some kind of like

00:18:55,130 --> 00:19:00,760
interface webs so you can reach the

00:18:57,320 --> 00:19:05,180
e-class change with this LDP name and

00:19:00,760 --> 00:19:06,890
then the alert for each side you can

00:19:05,180 --> 00:19:10,220
group them together in the world manager

00:19:06,890 --> 00:19:13,810
who is like just simple group by like in

00:19:10,220 --> 00:19:16,970
this case let's say OTP name and for the

00:19:13,810 --> 00:19:18,470
you can you also use this ODP name when

00:19:16,970 --> 00:19:20,330
one of the boxes comes down you don't

00:19:18,470 --> 00:19:22,730
want to send any like interface labs or

00:19:20,330 --> 00:19:25,670
BGP flaps if the device is physical

00:19:22,730 --> 00:19:28,250
physically down so it's some samples we

00:19:25,670 --> 00:19:30,710
did the same idea for BGP like you come

00:19:28,250 --> 00:19:32,690
up with BGP ID which is the same on each

00:19:30,710 --> 00:19:36,110
side and that way at least you reduce it

00:19:32,690 --> 00:19:38,930
to one one a lot instead of multiple

00:19:36,110 --> 00:19:41,030
ones alright and this is just some

00:19:38,930 --> 00:19:44,030
examples of like how easy it is for an

00:19:41,030 --> 00:19:47,720
engineer to let say troubleshoot some

00:19:44,030 --> 00:19:49,940
pages from a series like this particular

00:19:47,720 --> 00:19:51,770
time you can clearly see that it wasn't

00:19:49,940 --> 00:19:54,740
caused by a physical event it was just

00:19:51,770 --> 00:19:58,010
spike in traffic and it's like super

00:19:54,740 --> 00:20:00,080
easy to use and dynamic once you show

00:19:58,010 --> 00:20:03,670
the engineer what they can do they

00:20:00,080 --> 00:20:07,650
cannot go back to the old system

00:20:03,670 --> 00:20:07,650
all right thank you very much are Tim

00:20:08,970 --> 00:20:15,570
cool so my name is Manuel I work for a

00:20:12,400 --> 00:20:15,570
company that you might be familiar with

00:20:15,630 --> 00:20:21,100
so I'm an account executive each

00:20:18,370 --> 00:20:22,960
actually that means sales so kind of

00:20:21,100 --> 00:20:24,850
intruder here and I'll explain you why

00:20:22,960 --> 00:20:28,630
exactly I'm here and why we have a sales

00:20:24,850 --> 00:20:30,100
guy talking about numbers usually this

00:20:28,630 --> 00:20:31,420
is something that we normally do for

00:20:30,100 --> 00:20:33,160
those folks that have been here for the

00:20:31,420 --> 00:20:35,470
last year's we have a tradition going

00:20:33,160 --> 00:20:36,910
through the numbers behind grifone and

00:20:35,470 --> 00:20:38,920
Prometheus and what we see in terms of

00:20:36,910 --> 00:20:40,270
number of data sources in your phone are

00:20:38,920 --> 00:20:42,610
coming from Prometheus and number of

00:20:40,270 --> 00:20:46,090
installs if we go find out during the

00:20:42,610 --> 00:20:48,340
last few years so this is something that

00:20:46,090 --> 00:20:50,110
normally Carl does Carl is an engineer

00:20:48,340 --> 00:20:52,090
one of the primary engineers in the

00:20:50,110 --> 00:20:55,480
fauna team he was here a couple of years

00:20:52,090 --> 00:20:58,030
ago during the this talk only that Carl

00:20:55,480 --> 00:21:00,370
last year was not here because he was

00:20:58,030 --> 00:21:03,310
important to leave now well he's

00:21:00,370 --> 00:21:06,040
important to leave again

00:21:03,310 --> 00:21:07,750
you'd probably it probably exactly has

00:21:06,040 --> 00:21:10,390
twin so he probably if you want to have

00:21:07,750 --> 00:21:13,330
kids go in Sweden he's probably watching

00:21:10,390 --> 00:21:15,880
I know because he's missing it he's

00:21:13,330 --> 00:21:18,310
tweeting about it he's he has this form

00:21:15,880 --> 00:21:21,400
of missing pumpkin this year at the same

00:21:18,310 --> 00:21:25,750
time he's doing some soap or soup with

00:21:21,400 --> 00:21:27,850
the kids so Carl say hi to Carla hope

00:21:25,750 --> 00:21:30,000
you'll be back next year and not since

00:21:27,850 --> 00:21:32,290
and the cell guy talking about numbers

00:21:30,000 --> 00:21:34,000
last year was David because Carl wasn't

00:21:32,290 --> 00:21:38,410
leave but David decided to spend some

00:21:34,000 --> 00:21:41,230
time with his kids fall so talking about

00:21:38,410 --> 00:21:43,240
numbers we say we look into the numbers

00:21:41,230 --> 00:21:45,790
that we see in terms of installs of

00:21:43,240 --> 00:21:47,950
cremona and numbers of Prometheus that

00:21:45,790 --> 00:21:50,860
as versus integral for nine 2016 the

00:21:47,950 --> 00:21:52,930
first prompt on we had 36,000 install so

00:21:50,860 --> 00:21:54,730
go fauna in two point well two thousand

00:21:52,930 --> 00:21:58,510
and eight hundred primitives data

00:21:54,730 --> 00:22:00,490
sources the next year we had 90 2015

00:21:58,510 --> 00:22:02,170
super fauna is sixteen thousands of

00:22:00,490 --> 00:22:06,790
parameters data sources quite increase

00:22:02,170 --> 00:22:08,260
there 2018 last year how do we ninety

00:22:06,790 --> 00:22:10,470
six so doubling that

00:22:08,260 --> 00:22:13,720
Congrats to someone's doing a great job

00:22:10,470 --> 00:22:16,660
and three pulling the group reaches

00:22:13,720 --> 00:22:18,030
instances now the question is what

00:22:16,660 --> 00:22:21,170
happens in two

00:22:18,030 --> 00:22:26,700
and I guess so who says like we had to

00:22:21,170 --> 00:22:33,240
186 ger fauna installs who says we

00:22:26,700 --> 00:22:35,640
doubled at least that was we said more

00:22:33,240 --> 00:22:41,870
than so double would be 360 who said

00:22:35,640 --> 00:22:55,290
would say four hundred thousand four

00:22:41,870 --> 00:22:56,520
four fifty four fifty five so this is

00:22:55,290 --> 00:22:58,080
how many good for instances are

00:22:56,520 --> 00:22:59,580
somewhere in the world thanks to you

00:22:58,080 --> 00:23:02,490
guys as well and a bunch of other people

00:22:59,580 --> 00:23:04,530
back home let's move to prometheus how

00:23:02,490 --> 00:23:09,690
many data sources we see affirmative

00:23:04,530 --> 00:23:11,940
inside go fauna so 54 going from 16

00:23:09,690 --> 00:23:16,490
I'm not say doubled because probably we

00:23:11,940 --> 00:23:16,490
did more than that because a triple 150

00:23:19,340 --> 00:23:27,390
did it did you share the numbers before

00:23:21,890 --> 00:23:34,280
yeah I'm not gonna ask Tom I'm gonna ask

00:23:27,390 --> 00:23:39,140
you so we said we get 200,000 okay okay

00:23:34,280 --> 00:23:39,140
alright actually forgot the numbers here

00:23:40,730 --> 00:23:55,070
250 okay okay well 244 242 actually

00:23:48,990 --> 00:23:58,380
quite close so Ron allows for you guys

00:23:55,070 --> 00:24:00,929
that was pretty much old for me I hope

00:23:58,380 --> 00:24:04,799
Carl will be back in 2020 if David

00:24:00,929 --> 00:24:06,179
doesn't want to come back and I'll get

00:24:04,799 --> 00:24:08,820
the previous slides up if you want to

00:24:06,179 --> 00:24:10,740
take a picture and let's get the numbers

00:24:08,820 --> 00:24:12,580
next year thanks and continue to do a

00:24:10,740 --> 00:24:14,850
great job here thank you

00:24:12,580 --> 00:24:16,290
[Applause]

00:24:14,850 --> 00:24:17,610
thanks for having me a lot of this talk

00:24:16,290 --> 00:24:20,040
obviously I'm Prometheus has been a

00:24:17,610 --> 00:24:22,200
metrics but we've heard again and again

00:24:20,040 --> 00:24:23,700
that there's often a lot of value by

00:24:22,200 --> 00:24:25,920
looking across the different data types

00:24:23,700 --> 00:24:27,840
you have metrics and and logs we don't

00:24:25,920 --> 00:24:29,490
hear as much on as we should I think a

00:24:27,840 --> 00:24:32,040
metadata and so I don't talk together

00:24:29,490 --> 00:24:34,380
about actually taking these three

00:24:32,040 --> 00:24:36,390
disparate data sources and possibly

00:24:34,380 --> 00:24:38,040
thinking about them together in fact we

00:24:36,390 --> 00:24:39,470
start to see this there's kind of you

00:24:38,040 --> 00:24:42,690
know there's really kind of two

00:24:39,470 --> 00:24:44,550
philosophical ways you can do this and I

00:24:42,690 --> 00:24:46,110
think they both have values so many of

00:24:44,550 --> 00:24:47,610
you could find them both both parts one

00:24:46,110 --> 00:24:49,110
is obviously to do it at the

00:24:47,610 --> 00:24:51,210
visualization layer where you could have

00:24:49,110 --> 00:24:52,740
a unified visualization platform that

00:24:51,210 --> 00:24:55,200
could then talk to different disparate

00:24:52,740 --> 00:24:56,610
data sources what we believe you know

00:24:55,200 --> 00:24:58,200
we're we build a database and we

00:24:56,610 --> 00:25:00,180
actually see also a lot of value in

00:24:58,200 --> 00:25:02,610
bringing all of these different types of

00:25:00,180 --> 00:25:04,770
data into a centralized data store and

00:25:02,610 --> 00:25:06,750
from that you could put on top of this

00:25:04,770 --> 00:25:08,940
graph on ax you could allow your

00:25:06,750 --> 00:25:10,920
business analysts to maybe use things

00:25:08,940 --> 00:25:12,900
like look or or tableau you could use

00:25:10,920 --> 00:25:14,880
your data scientists to Python in fact

00:25:12,900 --> 00:25:15,840
you could reach draw sequel and so in

00:25:14,880 --> 00:25:18,060
some sense that's what I want to talk

00:25:15,840 --> 00:25:19,500
about today a little bit into a quick

00:25:18,060 --> 00:25:23,040
demo of doing this on top of core fauna

00:25:19,500 --> 00:25:24,810
is using a database and protect one we

00:25:23,040 --> 00:25:26,490
build time scale DB to provide a

00:25:24,810 --> 00:25:28,890
centralized store for all of your

00:25:26,490 --> 00:25:30,240
observability data and what this allows

00:25:28,890 --> 00:25:32,640
you to do now is to unify these

00:25:30,240 --> 00:25:35,490
different data types to kind of review

00:25:32,640 --> 00:25:38,220
reveal the whole picture of your system

00:25:35,490 --> 00:25:39,450
at a specific time so I this is a

00:25:38,220 --> 00:25:41,100
developer conference so certainly

00:25:39,450 --> 00:25:43,200
actually want to show you a little bit

00:25:41,100 --> 00:25:46,470
of this working in practice and so this

00:25:43,200 --> 00:25:48,330
is an example of data coming out of time

00:25:46,470 --> 00:25:50,460
scale cloud and we're actually showing

00:25:48,330 --> 00:25:53,130
it as one picture both kind of query

00:25:50,460 --> 00:25:55,170
latency over a few things in normal

00:25:53,130 --> 00:25:58,860
graph view metrics this is actually

00:25:55,170 --> 00:25:59,460
collected through Prometheus as well as

00:25:58,860 --> 00:26:00,930
the logs

00:25:59,460 --> 00:26:02,940
um I should add in fact both of these

00:26:00,930 --> 00:26:05,610
things are actually using something that

00:26:02,940 --> 00:26:08,340
time scale has built to the connector

00:26:05,610 --> 00:26:10,590
from Prometheus the adapter to Postgres

00:26:08,340 --> 00:26:11,970
on which time scale runs we built as

00:26:10,590 --> 00:26:14,460
well as actually the refinement

00:26:11,970 --> 00:26:16,680
connector for time scale and Postgres we

00:26:14,460 --> 00:26:17,610
also we also contributed but you can

00:26:16,680 --> 00:26:19,080
look at this you know one of the

00:26:17,610 --> 00:26:21,660
interesting things is now you actually

00:26:19,080 --> 00:26:23,790
get this one view of these both logs and

00:26:21,660 --> 00:26:26,220
it's actually pretty simple to do you

00:26:23,790 --> 00:26:26,950
know you could for example you could

00:26:26,220 --> 00:26:29,030
wrong but

00:26:26,950 --> 00:26:31,460
you could just look at your logs and

00:26:29,030 --> 00:26:33,560
it's really as simple as basically you

00:26:31,460 --> 00:26:35,540
know here you know it's really just

00:26:33,560 --> 00:26:38,510
selecting what container so in fact if

00:26:35,540 --> 00:26:39,830
you go back you'll see that in fact this

00:26:38,510 --> 00:26:41,870
is just times this is one of the

00:26:39,830 --> 00:26:45,080
containers you could switch your logs

00:26:41,870 --> 00:26:46,640
will change your graphs will change but

00:26:45,080 --> 00:26:48,440
it's all pulling buffs in this actual

00:26:46,640 --> 00:26:50,180
same data store now the interesting

00:26:48,440 --> 00:26:51,950
thing is let's say you Annie you

00:26:50,180 --> 00:26:54,620
actually want to go down on what is that

00:26:51,950 --> 00:26:56,090
effectively a Prometheus myth label we

00:26:54,620 --> 00:26:58,310
actually have a consistent set of

00:26:56,090 --> 00:27:00,020
Prometheus labels now across our metrics

00:26:58,310 --> 00:27:02,030
and logs and and that's actually what I

00:27:00,020 --> 00:27:04,490
just showed you above in the container

00:27:02,030 --> 00:27:06,230
name and so I might say show me all my

00:27:04,490 --> 00:27:08,060
Prometheus labels in fact that latency

00:27:06,230 --> 00:27:10,370
spike I don't really like so show me a

00:27:08,060 --> 00:27:11,720
bunch our logs in this graph except

00:27:10,370 --> 00:27:13,250
there's a lot of them you know you could

00:27:11,720 --> 00:27:15,650
in fact I don't want to actually try to

00:27:13,250 --> 00:27:18,590
pull the hundred thousand log entries

00:27:15,650 --> 00:27:20,570
from this time period I pre across the

00:27:18,590 --> 00:27:22,670
broad area internet from our data center

00:27:20,570 --> 00:27:25,070
back in a US will actually pre did that

00:27:22,670 --> 00:27:27,350
and that's what my laptop might crash

00:27:25,070 --> 00:27:28,880
soon from Griffin ax but that's what it

00:27:27,350 --> 00:27:32,000
looks like to try to visualize three

00:27:28,880 --> 00:27:34,130
hundred thousand annotations in this one

00:27:32,000 --> 00:27:36,050
time period but the nice thing is we

00:27:34,130 --> 00:27:38,840
could actually dig dig a little deeper

00:27:36,050 --> 00:27:41,360
and in fact I you know you know I just

00:27:38,840 --> 00:27:44,600
using again the annotation I built what

00:27:41,360 --> 00:27:48,380
is effectively just a little suspicious

00:27:44,600 --> 00:27:51,800
logs query and all I'm saying is I just

00:27:48,380 --> 00:27:54,020
want the set of metrics that I want the

00:27:51,800 --> 00:27:56,000
logs corresponding to the metrics that

00:27:54,020 --> 00:27:57,500
are above two hundred thousand and this

00:27:56,000 --> 00:27:59,600
is not something you could really select

00:27:57,500 --> 00:28:01,760
if you just have labels you're actually

00:27:59,600 --> 00:28:03,410
doing a query on the data on the

00:28:01,760 --> 00:28:05,480
structured ad itself you do not want to

00:28:03,410 --> 00:28:10,250
express that in terms of cardinality but

00:28:05,480 --> 00:28:12,470
once I have that super small set I can

00:28:10,250 --> 00:28:13,400
in fact just look down and that's what

00:28:12,470 --> 00:28:15,230
you're actually seeing on his log

00:28:13,400 --> 00:28:16,640
entries just see suspicious metrics and

00:28:15,230 --> 00:28:17,990
what's a little debugging you've

00:28:16,640 --> 00:28:20,390
actually discovered that this is due to

00:28:17,990 --> 00:28:22,220
a high availability failover between two

00:28:20,390 --> 00:28:23,630
running instances of time scale and the

00:28:22,220 --> 00:28:25,670
nice thing is I'm running out of time is

00:28:23,630 --> 00:28:28,310
that it's not just all so about metadata

00:28:25,670 --> 00:28:30,380
in fact what I also did here is ice

00:28:28,310 --> 00:28:33,140
don't store this directly in the

00:28:30,380 --> 00:28:35,420
Prometheus log but we also actually have

00:28:33,140 --> 00:28:36,980
view of which are the users which are

00:28:35,420 --> 00:28:39,320
the tenants mapping to that particular

00:28:36,980 --> 00:28:40,640
container and again this is all coming

00:28:39,320 --> 00:28:42,260
in the same data store

00:28:40,640 --> 00:28:44,660
now I could learn that these two users

00:28:42,260 --> 00:28:46,610
are affected on this container this uses

00:28:44,660 --> 00:28:48,500
an effect on that container and so forth

00:28:46,610 --> 00:28:50,090
now how did that sign nail this I'm

00:28:48,500 --> 00:28:51,560
quickly running out of time it's because

00:28:50,090 --> 00:28:53,330
this is not the we think the value of

00:28:51,560 --> 00:28:54,590
this is not just long-term storage it's

00:28:53,330 --> 00:28:56,660
actually building the power of true

00:28:54,590 --> 00:28:58,160
database time scale in some sense is

00:28:56,660 --> 00:29:00,200
built as a cloud native time series

00:28:58,160 --> 00:29:01,340
database we actually built on Postgres

00:29:00,200 --> 00:29:03,530
but we've kind of changed its

00:29:01,340 --> 00:29:05,210
architecture really significantly to

00:29:03,530 --> 00:29:07,430
really optimize for time series data

00:29:05,210 --> 00:29:09,050
like metrics we first launched in April

00:29:07,430 --> 00:29:11,060
2017 and since had millions of downloads

00:29:09,050 --> 00:29:13,520
and really since there have really

00:29:11,060 --> 00:29:14,750
expanded it to kind of petabyte scale so

00:29:13,520 --> 00:29:16,610
you get full sequel with the entire

00:29:14,750 --> 00:29:18,350
Postgres ecosystem but you could deal

00:29:16,610 --> 00:29:20,480
with huge cardinality elastic scale at

00:29:18,350 --> 00:29:21,860
native compression now and data tearing

00:29:20,480 --> 00:29:23,330
in fact you can move things from hot

00:29:21,860 --> 00:29:24,650
storage to cold storage all in the cloud

00:29:23,330 --> 00:29:44,150
and it's super simple

00:29:24,650 --> 00:29:46,750
big future work including interface so

00:29:44,150 --> 00:29:49,850
first a bit about the Crito because

00:29:46,750 --> 00:29:51,680
understand the scale we do online

00:29:49,850 --> 00:29:54,500
advertisement so the ads that you can

00:29:51,680 --> 00:29:58,190
see on websites on the technical side we

00:29:54,500 --> 00:29:59,990
have more than 40,000 servers a question

00:29:58,190 --> 00:30:02,600
that has some tiles so we do a lot of

00:29:59,990 --> 00:30:05,960
Germantown we have two large clusters

00:30:02,600 --> 00:30:08,840
and we under six million requests per

00:30:05,960 --> 00:30:15,320
second that should be read in less than

00:30:08,840 --> 00:30:18,950
100 millisecond so it's a dedicated team

00:30:15,320 --> 00:30:22,360
of four persons and they're responsible

00:30:18,950 --> 00:30:25,850
for maintaining the object is tagged so

00:30:22,360 --> 00:30:27,110
metrics logs and tracing on matrix we

00:30:25,850 --> 00:30:29,350
have graphite and primitives and

00:30:27,110 --> 00:30:34,220
monitors on primitives that were about

00:30:29,350 --> 00:30:36,410
642 instances containing two million

00:30:34,220 --> 00:30:38,900
samples per second and the most common

00:30:36,410 --> 00:30:40,310
resolution is one minute and we have

00:30:38,900 --> 00:30:44,000
more than 200 meters on the

00:30:40,310 --> 00:30:46,130
configuration file mono repository so

00:30:44,000 --> 00:30:46,640
how do we manage 600 instances of

00:30:46,130 --> 00:30:50,300
foreign tools

00:30:46,640 --> 00:30:53,640
well we don't each team is responsible

00:30:50,300 --> 00:30:56,670
for their own instances

00:30:53,640 --> 00:31:00,480
so basically what we do is we have this

00:30:56,670 --> 00:31:03,810
notion of perimeters want is one team

00:31:00,480 --> 00:31:06,240
has one or maybe more parameters and

00:31:03,810 --> 00:31:10,770
basically it's a separation a perimeter

00:31:06,240 --> 00:31:15,840
scrubs the service of the team and it's

00:31:10,770 --> 00:31:18,270
a general global local topology so one

00:31:15,840 --> 00:31:22,890
primitives in every data center or less

00:31:18,270 --> 00:31:25,860
and federated by global parameters and

00:31:22,890 --> 00:31:28,830
everything is running in missiles so why

00:31:25,860 --> 00:31:32,010
do we do that so it's a lot maintenance

00:31:28,830 --> 00:31:34,530
cost for a subsidy team it provides a

00:31:32,010 --> 00:31:39,270
new solution between teams because you

00:31:34,530 --> 00:31:44,610
don't want somebody miss that messed up

00:31:39,270 --> 00:31:47,250
with rules target to impact the alerting

00:31:44,610 --> 00:31:50,730
another team the freedom of she's edge

00:31:47,250 --> 00:31:53,580
so all the users can use all the

00:31:50,730 --> 00:31:56,490
capabilities of printers often they do

00:31:53,580 --> 00:31:59,370
and there is a clean ownership

00:31:56,490 --> 00:32:02,760
separation that obscenity will never

00:31:59,370 --> 00:32:05,460
answer for another feminism and it's

00:32:02,760 --> 00:32:08,270
valid valid for all the teams and

00:32:05,460 --> 00:32:11,160
different ages so obviously it's the

00:32:08,270 --> 00:32:13,410
workload on the can teams and there is a

00:32:11,160 --> 00:32:15,630
high contrast because you have to learn

00:32:13,410 --> 00:32:18,990
how primitives works and how to use it

00:32:15,630 --> 00:32:23,160
inside p2 so how to make it easier for

00:32:18,990 --> 00:32:24,600
the teams to use the primitives so you

00:32:23,160 --> 00:32:26,730
can reduce your workload by providing

00:32:24,600 --> 00:32:28,350
shared services so we provide the one

00:32:26,730 --> 00:32:29,790
that manager you can ask for one

00:32:28,350 --> 00:32:33,420
dedicated for you if you have special

00:32:29,790 --> 00:32:36,680
needs and we use generic routes that

00:32:33,420 --> 00:32:39,840
will route the labs depending on

00:32:36,680 --> 00:32:42,080
syllables given by primitives so

00:32:39,840 --> 00:32:44,760
basically we almost never touch the

00:32:42,080 --> 00:32:46,290
configuration we have command expertos

00:32:44,760 --> 00:32:48,660
like the blue boxes Potter that are

00:32:46,290 --> 00:32:50,340
shown that everybody can use if you need

00:32:48,660 --> 00:32:54,030
specific spotter or if you need special

00:32:50,340 --> 00:32:56,160
computation you can build your own and

00:32:54,030 --> 00:32:58,140
it's very easy because everything is

00:32:56,160 --> 00:33:01,110
ready find and we have the garage ruff

00:32:58,140 --> 00:33:03,420
ruff I dream adapter that receive

00:33:01,110 --> 00:33:07,350
metrics and parameters and passes I'm

00:33:03,420 --> 00:33:09,750
inside guava we can also reduce

00:33:07,350 --> 00:33:11,340
by using efficient tuning so we have a

00:33:09,750 --> 00:33:12,840
reliable mate I mean it turning so it's

00:33:11,340 --> 00:33:14,970
basically parameter dedicated to watch

00:33:12,840 --> 00:33:16,890
these are parameters and users can

00:33:14,970 --> 00:33:19,740
choose how they want to be alerted and

00:33:16,890 --> 00:33:21,870
all the data actionable and what the

00:33:19,740 --> 00:33:24,419
content so the users know what to do if

00:33:21,870 --> 00:33:26,700
they receive something to do tooling is

00:33:24,419 --> 00:33:28,620
available to debug more complex issues

00:33:26,700 --> 00:33:35,309
like out of other issues that can be

00:33:28,620 --> 00:33:36,630
tricky for users to like fine and we

00:33:35,309 --> 00:33:41,100
have graphin and as well for parents who

00:33:36,630 --> 00:33:43,080
stats we also reduced or code by acting

00:33:41,100 --> 00:33:46,140
as the constituting so we don't let the

00:33:43,080 --> 00:33:48,539
users alone we give users advice on how

00:33:46,140 --> 00:33:50,070
to best manage the application we dig

00:33:48,539 --> 00:33:51,600
deep arrived into complicated issue

00:33:50,070 --> 00:33:55,409
basically everything that is not trivial

00:33:51,600 --> 00:33:57,150
to fix we take care of upgrades and we

00:33:55,409 --> 00:34:01,650
provide workshops to unbox the new user

00:33:57,150 --> 00:34:03,870
to explain how it works we also use a

00:34:01,650 --> 00:34:06,299
lot of automation side service basically

00:34:03,870 --> 00:34:08,190
creating a new parameter so those

00:34:06,299 --> 00:34:11,250
configurations the pipelining for CAE

00:34:08,190 --> 00:34:13,740
and the primitives are placed there

00:34:11,250 --> 00:34:16,200
it's done by your bunch people German

00:34:13,740 --> 00:34:18,030
from Jenkins primitives is simple to

00:34:16,200 --> 00:34:21,450
test locally basically we have Windows

00:34:18,030 --> 00:34:23,909
Mac and Linux so we can do it very

00:34:21,450 --> 00:34:26,639
easily and Co efficient is that tested

00:34:23,909 --> 00:34:30,030
when creating a review so we check the

00:34:26,639 --> 00:34:32,780
roads we check the documentation so on

00:34:30,030 --> 00:34:34,590
alert it's enforced and we have

00:34:32,780 --> 00:34:39,030
sectional and of journeys that are

00:34:34,590 --> 00:34:39,389
validated that exists so to finish start

00:34:39,030 --> 00:34:41,639
small

00:34:39,389 --> 00:34:42,450
it took say yes to build its ultimate

00:34:41,639 --> 00:34:44,340
flexibly

00:34:42,450 --> 00:34:46,290
listen to your feedback to know what to

00:34:44,340 --> 00:34:50,850
emitted

00:34:46,290 --> 00:34:53,970
thank you very much thank you hello I'm

00:34:50,850 --> 00:34:57,480
Fredrik I work at a small company called

00:34:53,970 --> 00:35:00,090
github some of you might know but that's

00:34:57,480 --> 00:35:02,970
not about my work github it's more about

00:35:00,090 --> 00:35:05,640
my work at the CCC knock if you don't

00:35:02,970 --> 00:35:07,800
know what the CCC is the CCC is a

00:35:05,640 --> 00:35:12,510
nonprofit organization mostly by hackers

00:35:07,800 --> 00:35:14,910
and nerds and IT professionals it's a

00:35:12,510 --> 00:35:18,089
completely non non profit run organism

00:35:14,910 --> 00:35:19,500
like organization and event the event

00:35:18,089 --> 00:35:21,480
usually runs for four to six days

00:35:19,500 --> 00:35:24,690
depending on the event

00:35:21,480 --> 00:35:27,750
the Congress is usually between

00:35:24,690 --> 00:35:32,609
Christmas and New Year's Eve what

00:35:27,750 --> 00:35:35,579
usually I mean always the camp is every

00:35:32,609 --> 00:35:38,369
four years but some other camps are like

00:35:35,579 --> 00:35:39,750
between the year or like on every other

00:35:38,369 --> 00:35:42,770
year

00:35:39,750 --> 00:35:46,230
those are run by Dutch colleagues and

00:35:42,770 --> 00:35:47,880
British colleagues and so forth and but

00:35:46,230 --> 00:35:50,250
the cross collaboration is very large

00:35:47,880 --> 00:35:55,410
and so we attend those events usually as

00:35:50,250 --> 00:35:57,660
well the Congress is like 17 17 thousand

00:35:55,410 --> 00:36:05,220
people so you can imagine like this room

00:35:57,660 --> 00:36:07,410
a lot bigger and we're now at the CC CCL

00:36:05,220 --> 00:36:10,380
that's a delight tiga Messer which was a

00:36:07,410 --> 00:36:12,780
pretty big venue and we used to be in

00:36:10,380 --> 00:36:16,260
Hamburg now we're in Leipzig the presale

00:36:12,780 --> 00:36:18,540
started today and went out

00:36:16,260 --> 00:36:21,390
or was it today was it yesterday I know

00:36:18,540 --> 00:36:25,440
today I believe and it went out in 17

00:36:21,390 --> 00:36:27,450
minutes that was the first stage of the

00:36:25,440 --> 00:36:30,900
presale there will be another stage

00:36:27,450 --> 00:36:35,220
later the campus about 5,000 people that

00:36:30,900 --> 00:36:37,109
attend and we run the knock or I run the

00:36:35,220 --> 00:36:41,640
knock together with others and we're

00:36:37,109 --> 00:36:43,559
about I believe like 20 40 people that

00:36:41,640 --> 00:36:46,920
take the time to do it and usually

00:36:43,559 --> 00:36:49,589
that's that's like at least a month or

00:36:46,920 --> 00:36:51,380
two or three in advance sometimes even

00:36:49,589 --> 00:36:54,780
half year to a year in advance of

00:36:51,380 --> 00:36:56,180
organization and what that comes

00:36:54,780 --> 00:36:58,200
permanent infrastructure but also

00:36:56,180 --> 00:36:59,760
temporary ins for infrastructure as I

00:36:58,200 --> 00:37:02,960
said is running for four to six days

00:36:59,760 --> 00:37:07,140
means from ethers retention is two weeks

00:37:02,960 --> 00:37:08,880
isn't that a beautiful thing right no

00:37:07,140 --> 00:37:11,549
long-term storage and because at the end

00:37:08,880 --> 00:37:13,859
of the event we delete everything we'd

00:37:11,549 --> 00:37:15,119
keep nothing because we like privacy if

00:37:13,859 --> 00:37:18,299
you like we don't know what we're

00:37:15,119 --> 00:37:21,450
storing what could be related to user so

00:37:18,299 --> 00:37:23,130
we delete everything we have about more

00:37:21,450 --> 00:37:25,890
than foreigners switches and access

00:37:23,130 --> 00:37:28,920
points access point is probably like 200

00:37:25,890 --> 00:37:30,210
or something many routers lots of spawns

00:37:28,920 --> 00:37:32,730
or it's multiple hundreds of gigabits

00:37:30,210 --> 00:37:35,250
per second capacity last year Congress

00:37:32,730 --> 00:37:38,040
we had 500 gigabyte capacity also

00:37:35,250 --> 00:37:40,410
we didn't pay for anything which was

00:37:38,040 --> 00:37:41,400
pretty awesome this is what it looks

00:37:40,410 --> 00:37:43,800
like in hardware

00:37:41,400 --> 00:37:45,090
lots of switches lots of spaghetti

00:37:43,800 --> 00:37:47,550
cables everywhere

00:37:45,090 --> 00:37:51,800
and it's all temporary so we set it up

00:37:47,550 --> 00:37:51,800
like a week in advance and run with it

00:37:51,950 --> 00:37:56,460
so infrastructure layout for prometheus

00:37:54,660 --> 00:37:57,990
so all that infrastructure also needs to

00:37:56,460 --> 00:38:00,090
be somewhat monitored we need to be

00:37:57,990 --> 00:38:03,780
looking at utilized links and all that

00:38:00,090 --> 00:38:05,640
and we have a public Ravana that is

00:38:03,780 --> 00:38:07,619
fronted by and by an NGO next to

00:38:05,640 --> 00:38:10,230
terminate a cell fronted by a varnish

00:38:07,619 --> 00:38:12,270
because 17,000 people or more on the

00:38:10,230 --> 00:38:14,190
internet will access that dashboard and

00:38:12,270 --> 00:38:16,680
look at statistics and we don't want

00:38:14,190 --> 00:38:18,750
them to wade through every graph that we

00:38:16,680 --> 00:38:20,310
have on this dashboard so we shield it

00:38:18,750 --> 00:38:22,170
off by some rewrite rules and all that

00:38:20,310 --> 00:38:24,240
but we have an internal version of that

00:38:22,170 --> 00:38:25,950
as well that directly communicates with

00:38:24,240 --> 00:38:28,619
preemies their server where we can do

00:38:25,950 --> 00:38:31,710
dashboards we have lots of exporters we

00:38:28,619 --> 00:38:34,710
do lots of SNMP hopefully some more

00:38:31,710 --> 00:38:36,830
telemetry we use an auto export are very

00:38:34,710 --> 00:38:41,609
extensively to the extent that we have

00:38:36,830 --> 00:38:43,680
17 servers not a lot we have use of

00:38:41,609 --> 00:38:45,330
junus exporter black boxes for about the

00:38:43,680 --> 00:38:47,849
more important pieces people write their

00:38:45,330 --> 00:38:49,830
own exporters so we have the vogue

00:38:47,849 --> 00:38:52,410
providing us if they believe some of

00:38:49,830 --> 00:38:54,240
them are here provide us with

00:38:52,410 --> 00:38:57,270
information for the public therefore the

00:38:54,240 --> 00:38:59,820
angel system usually well in advance we

00:38:57,270 --> 00:39:01,530
have power utilization water pressure at

00:38:59,820 --> 00:39:04,170
Kemps so you know when you have to take

00:39:01,530 --> 00:39:06,990
a shower so no one is there and all that

00:39:04,170 --> 00:39:10,109
and dhcp you'll use it which is great

00:39:06,990 --> 00:39:12,599
for capacity planning for the next year

00:39:10,109 --> 00:39:15,390
yeah service discovery we operate by the

00:39:12,599 --> 00:39:17,040
principle of net box and chill means we

00:39:15,390 --> 00:39:19,560
put everything in that box it's a system

00:39:17,040 --> 00:39:24,390
that looks broadly like this lots of

00:39:19,560 --> 00:39:26,510
links computers switches ok and yeah

00:39:24,390 --> 00:39:29,160
what's in there is some Prometheus

00:39:26,510 --> 00:39:30,690
dashboards and graphs this is part of

00:39:29,160 --> 00:39:32,099
the public dashboard I always forget to

00:39:30,690 --> 00:39:33,839
take a screenshot at the end of the

00:39:32,099 --> 00:39:36,660
event if you can remember for me please

00:39:33,839 --> 00:39:39,380
do so and send me the link that's radio

00:39:36,660 --> 00:39:42,210
capacity that was a little bit of a

00:39:39,380 --> 00:39:45,270
cliff there for 10 gigabits of radio

00:39:42,210 --> 00:39:46,950
capacity that we had on the camp meaning

00:39:45,270 --> 00:39:48,920
a radio link that goes to the next

00:39:46,950 --> 00:39:53,270
village giving 10 gigabits of capacity

00:39:48,920 --> 00:39:55,550
he just like two seconds that's like our

00:39:53,270 --> 00:39:58,130
availability dashboard which we use for

00:39:55,550 --> 00:39:59,870
build up and build and during the event

00:39:58,130 --> 00:40:02,420
for all the switches and access points

00:39:59,870 --> 00:40:04,760
and yeah some other dashboards that

00:40:02,420 --> 00:40:07,730
people build more importantly people use

00:40:04,760 --> 00:40:13,850
it for mascots on the public dashboard

00:40:07,730 --> 00:40:15,980
so thank you very much I run a set I'm

00:40:13,850 --> 00:40:18,320
Christian Simon and I'm speaking about a

00:40:15,980 --> 00:40:21,020
slightly smaller event so we have around

00:40:18,320 --> 00:40:23,450
2,000 visitors and it's something I'm

00:40:21,020 --> 00:40:26,120
doing in my hometown maybe an hour away

00:40:23,450 --> 00:40:28,430
from here for the last 15 years I guess

00:40:26,120 --> 00:40:31,160
with a couple of friends and at some

00:40:28,430 --> 00:40:34,210
point I switched from Pro from zabbix to

00:40:31,160 --> 00:40:37,280
Prometheus I think three years ago and

00:40:34,210 --> 00:40:40,100
basically the main purpose I'd like to

00:40:37,280 --> 00:40:42,470
look at is because I'm responsible for

00:40:40,100 --> 00:40:45,560
the power supply for the stages and all

00:40:42,470 --> 00:40:50,120
catering and stuff so basically I want

00:40:45,560 --> 00:40:51,980
to know when our CD fails because there

00:40:50,120 --> 00:40:56,120
are all sorts of reasons we have quite a

00:40:51,980 --> 00:41:00,500
lot of connections outside and like some

00:40:56,120 --> 00:41:02,600
of the catering places they they are not

00:41:00,500 --> 00:41:05,050
really done well from electrical

00:41:02,600 --> 00:41:06,950
standpoint I would say and so like they

00:41:05,050 --> 00:41:09,730
there are all sorts of problems

00:41:06,950 --> 00:41:13,130
throughout the evening and you really

00:41:09,730 --> 00:41:14,810
struggled you just get phone signal if

00:41:13,130 --> 00:41:16,760
there are that many people sometimes and

00:41:14,810 --> 00:41:19,910
so it's really hard to get anything so

00:41:16,760 --> 00:41:23,330
basically my approach to this was wrt

00:41:19,910 --> 00:41:26,510
and cheap access points that I just why

00:41:23,330 --> 00:41:28,900
I got into the cloud and then yeah

00:41:26,510 --> 00:41:31,970
export some metrics

00:41:28,900 --> 00:41:33,260
oh yeah I'm not really using them much I

00:41:31,970 --> 00:41:34,520
think I'm using a bit of port state

00:41:33,260 --> 00:41:38,360
source of the switches and things like

00:41:34,520 --> 00:41:41,810
that but yeah I'm using the open wrt Lua

00:41:38,360 --> 00:41:45,190
node exporter that someone wrote I just

00:41:41,810 --> 00:41:48,350
packaged it at some point I think and

00:41:45,190 --> 00:41:50,300
yeah so that's the main use case and

00:41:48,350 --> 00:41:53,810
basically the main thing I care about is

00:41:50,300 --> 00:41:56,270
if something is down on us and a nice

00:41:53,810 --> 00:41:58,670
thing I just wanted to show so it was

00:41:56,270 --> 00:42:00,430
just two weeks ago the festival and I

00:41:58,670 --> 00:42:02,360
discovered that a graph on our diagram

00:42:00,430 --> 00:42:05,540
plug-in and

00:42:02,360 --> 00:42:08,150
you can use mermaid with just like a GS

00:42:05,540 --> 00:42:10,340
graphing library and you can color it

00:42:08,150 --> 00:42:12,230
based on previous metrics so I've found

00:42:10,340 --> 00:42:13,130
that quite interesting and well set up

00:42:12,230 --> 00:42:14,960
in no time

00:42:13,130 --> 00:42:17,750
so basically like yeah that's the rough

00:42:14,960 --> 00:42:19,460
logical layout and yeah the numbers here

00:42:17,750 --> 00:42:23,420
for example would show how much uptime

00:42:19,460 --> 00:42:26,810
they had over the festival the mermaid

00:42:23,420 --> 00:42:28,850
itself looks a bit like that so that's

00:42:26,810 --> 00:42:33,410
basically draws the logical graph and

00:42:28,850 --> 00:42:35,330
you then replace the colors with some

00:42:33,410 --> 00:42:38,020
more bespoke reformer config that I

00:42:35,330 --> 00:42:40,550
think doesn't fit the screen really well

00:42:38,020 --> 00:42:41,810
so I think yeah that's the main thing I

00:42:40,550 --> 00:42:43,790
wanted to show I really liked the

00:42:41,810 --> 00:42:48,470
plug-in I'm probably gonna use it a bit

00:42:43,790 --> 00:42:52,280
more in my real life we're also looking

00:42:48,470 --> 00:42:54,170
at Prometheus and yeah so basically I

00:42:52,280 --> 00:42:56,000
also have a couple more ideas because if

00:42:54,170 --> 00:42:59,480
everything goes well I have quite a lot

00:42:56,000 --> 00:43:01,820
of time Festool so basically maybe

00:42:59,480 --> 00:43:04,040
measure the actual consumptions add so

00:43:01,820 --> 00:43:06,950
we have two connection points to the

00:43:04,040 --> 00:43:13,130
grid maybe measure the consumption there

00:43:06,950 --> 00:43:15,620
so we always talked about like some kind

00:43:13,130 --> 00:43:17,090
of OpenCV monitoring how many people are

00:43:15,620 --> 00:43:18,740
inside because there are strict limits

00:43:17,090 --> 00:43:21,440
from the authorities so we actually

00:43:18,740 --> 00:43:24,560
recorded some video stream there we need

00:43:21,440 --> 00:43:27,860
to figure out if we can count that with

00:43:24,560 --> 00:43:29,480
CV at some point and yeah right now all

00:43:27,860 --> 00:43:32,360
the kind of true easiest stuff is pretty

00:43:29,480 --> 00:43:34,430
much answerable static and I'd like to

00:43:32,360 --> 00:43:39,680
do that in a bit more dynamic way and

00:43:34,430 --> 00:43:44,720
yeah used to fall discovery cool thank

00:43:39,680 --> 00:43:46,130
you very much christian logan bronchiole

00:43:44,720 --> 00:43:48,290
aggregation shouting so we've heard from

00:43:46,130 --> 00:43:50,510
Chris why high cardinality queries are

00:43:48,290 --> 00:43:53,600
bad and I'm gonna tell you a different

00:43:50,510 --> 00:43:55,250
way of potentially maybe fixing that so

00:43:53,600 --> 00:43:56,540
I'm gonna for the sake of this

00:43:55,250 --> 00:43:58,280
five-minute lightning talk I'm gonna

00:43:56,540 --> 00:44:02,180
argue that all prompt ql queries look

00:43:58,280 --> 00:44:04,700
like this I know that's not true but

00:44:02,180 --> 00:44:06,500
like probably 80% maybe 70% look like

00:44:04,700 --> 00:44:08,420
this and and the reasoning behind it is

00:44:06,500 --> 00:44:09,800
if you're querying millions of

00:44:08,420 --> 00:44:11,930
time-series you're not displaying them

00:44:09,800 --> 00:44:13,550
right there's too many time series to

00:44:11,930 --> 00:44:15,740
you know to fit on even on a

00:44:13,550 --> 00:44:17,780
high-definition display so you

00:44:15,740 --> 00:44:20,090
doing some kind of aggregation to reduce

00:44:17,780 --> 00:44:22,280
the cardinality of the output set right

00:44:20,090 --> 00:44:24,560
so that's why they all look like this

00:44:22,280 --> 00:44:27,020
now one of the nice things you can do

00:44:24,560 --> 00:44:29,930
with this query is you can turn it into

00:44:27,020 --> 00:44:31,670
this query and I've I've quoted the plus

00:44:29,930 --> 00:44:34,040
because it's some kind of merge

00:44:31,670 --> 00:44:35,600
operation and obviously you you know you

00:44:34,040 --> 00:44:37,100
can't actually shard stuff like this

00:44:35,600 --> 00:44:39,680
unless you've sharded your time series

00:44:37,100 --> 00:44:41,480
but imagine you could imagine you could

00:44:39,680 --> 00:44:43,550
do this and then you could execute each

00:44:41,480 --> 00:44:45,230
one of those can't use the term sub

00:44:43,550 --> 00:44:48,410
query have to use a different term but

00:44:45,230 --> 00:44:49,610
each of those sub queries partial

00:44:48,410 --> 00:44:52,369
queries thank you

00:44:49,610 --> 00:44:54,680
in parallel independently imagine if you

00:44:52,369 --> 00:44:56,840
could do this right imagine if there was

00:44:54,680 --> 00:44:58,490
a way of guaranteeing that time series

00:44:56,840 --> 00:44:59,930
would be equally distributed amongst

00:44:58,490 --> 00:45:04,490
these shards and it was all automatic

00:44:59,930 --> 00:45:05,960
well we can well we we can almost like

00:45:04,490 --> 00:45:09,470
these these results came out of a dev

00:45:05,960 --> 00:45:11,060
system from from Owen and he gave me

00:45:09,470 --> 00:45:12,350
them last night it's really super early

00:45:11,060 --> 00:45:14,840
days but this is what we're working on

00:45:12,350 --> 00:45:17,270
at the moment this is again it's a dev

00:45:14,840 --> 00:45:18,530
system it's running over only a few

00:45:17,270 --> 00:45:21,230
thousand times series so this is

00:45:18,530 --> 00:45:22,760
artificially slow we're kind of working

00:45:21,230 --> 00:45:24,740
on it but this is the query dad we did

00:45:22,760 --> 00:45:26,180
we did contain a CPU seconds total which

00:45:24,740 --> 00:45:27,710
everyone will know is a pretty high

00:45:26,180 --> 00:45:29,990
cardinality metric if you use Cuba

00:45:27,710 --> 00:45:32,170
Nettie's and this is before it took ten

00:45:29,990 --> 00:45:34,609
seconds and after the sharding it takes

00:45:32,170 --> 00:45:36,440
2.9 seconds which is an improvement

00:45:34,609 --> 00:45:38,990
right we're shouting it into sixteen

00:45:36,440 --> 00:45:41,330
shards it's not sixteen times faster

00:45:38,990 --> 00:45:42,800
which we want we want to understand why

00:45:41,330 --> 00:45:44,420
this but these results are super early

00:45:42,800 --> 00:45:46,880
and this is the same query or slightly

00:45:44,420 --> 00:45:49,280
different query just for an example so

00:45:46,880 --> 00:45:51,530
it's repeatable at least this is a trace

00:45:49,280 --> 00:45:53,180
as Gotham already showed this morning we

00:45:51,530 --> 00:45:54,740
have we have Yaeger tracing for

00:45:53,180 --> 00:45:58,220
everything so you can see the queries

00:45:54,740 --> 00:45:59,390
being executed in parallel and yeah I

00:45:58,220 --> 00:46:01,430
mean that's pretty much all I wanted to

00:45:59,390 --> 00:46:03,619
show you I don't know why I have another

00:46:01,430 --> 00:46:08,359
slide there it's just going in a loop

00:46:03,619 --> 00:46:09,680
now yes so oh yes thank you so right now

00:46:08,359 --> 00:46:11,570
we're developing this in cortex in the

00:46:09,680 --> 00:46:13,369
crew front-end there's nothing here

00:46:11,570 --> 00:46:14,720
that's cortex specific so we'll make

00:46:13,369 --> 00:46:16,220
this work for Thanos we'll make this

00:46:14,720 --> 00:46:18,410
work for m3 we'll make this work for

00:46:16,220 --> 00:46:20,240
just plain old Prometheus in stores and

00:46:18,410 --> 00:46:21,710
actually in a Thanos environment it

00:46:20,240 --> 00:46:23,780
makes even more sense because you can

00:46:21,710 --> 00:46:26,150
push the individual what did we call

00:46:23,780 --> 00:46:27,800
them partial queries down into each of

00:46:26,150 --> 00:46:29,540
the edge locations into each of the edge

00:46:27,800 --> 00:46:31,100
Promethea that are running in your

00:46:29,540 --> 00:46:33,170
cluster so yeah that's gonna be pretty

00:46:31,100 --> 00:46:33,740
cool one of the other interesting things

00:46:33,170 --> 00:46:36,560
here

00:46:33,740 --> 00:46:38,360
the results are slightly different now

00:46:36,560 --> 00:46:39,560
we checked we don't think it's a bug we

00:46:38,360 --> 00:46:41,720
think it's just the order in which you

00:46:39,560 --> 00:46:43,270
add floating point numbers matters so

00:46:41,720 --> 00:46:45,560
they're only ever so slightly different

00:46:43,270 --> 00:46:47,150
but yeah that's kind of all I want to

00:46:45,560 --> 00:46:48,140
show you I wanted to I didn't do this

00:46:47,150 --> 00:46:49,310
work by the way

00:46:48,140 --> 00:46:50,690
the only reason I'm up here talking

00:46:49,310 --> 00:46:52,280
about it's cuz Owen who's been doing the

00:46:50,690 --> 00:46:53,750
work couldn't make it to prom con and

00:46:52,280 --> 00:46:54,860
see what's been helping him so I want to

00:46:53,750 --> 00:46:59,360
give a big shout out to these two have

00:46:54,860 --> 00:47:02,900
been doing the work thank you hi so I

00:46:59,360 --> 00:47:06,560
would like to tell you what its or what

00:47:02,900 --> 00:47:09,470
it's like to get people who don't care

00:47:06,560 --> 00:47:15,170
as much about infrastructure as we do to

00:47:09,470 --> 00:47:19,700
use Prometheus when I went to prom con

00:47:15,170 --> 00:47:22,100
in 2017 I was super excited to be

00:47:19,700 --> 00:47:25,430
solving a whole bunch of cardinality

00:47:22,100 --> 00:47:28,520
explosions when I got home well it turns

00:47:25,430 --> 00:47:31,070
out that if no one's using your service

00:47:28,520 --> 00:47:39,050
you don't get to solve these fun

00:47:31,070 --> 00:47:42,170
problems so this is me I like computers

00:47:39,050 --> 00:47:44,840
I like weightlifting I also like being a

00:47:42,170 --> 00:47:49,340
captain of a starship but I don't have a

00:47:44,840 --> 00:47:50,870
ship and I don't have a crew here are

00:47:49,340 --> 00:47:55,490
all the titles that I've had and the

00:47:50,870 --> 00:48:00,670
places that I worked so in my previous

00:47:55,490 --> 00:48:05,090
job this was what I was asked to do to

00:48:00,670 --> 00:48:07,250
have a combined prometheus set of

00:48:05,090 --> 00:48:09,980
deployments for a whole bunch of

00:48:07,250 --> 00:48:12,380
companies that were being put together a

00:48:09,980 --> 00:48:14,090
whole bunch of varied developers in

00:48:12,380 --> 00:48:17,120
different languages and different

00:48:14,090 --> 00:48:19,760
services I had a bunch of pre-existing

00:48:17,120 --> 00:48:25,040
monitoring tools a lot of them were in

00:48:19,760 --> 00:48:29,390
the about to die lots of micro services

00:48:25,040 --> 00:48:31,670
a ton and a whole ton of notes so it's

00:48:29,390 --> 00:48:35,900
my job and my team's job go monitor it

00:48:31,670 --> 00:48:40,220
all of it six months go you can't do

00:48:35,900 --> 00:48:43,820
that okay I need to help each team do

00:48:40,220 --> 00:48:45,710
this for themselves so how do I

00:48:43,820 --> 00:48:51,290
operate this and then make this

00:48:45,710 --> 00:48:55,250
accessible so my solution we did end up

00:48:51,290 --> 00:48:58,670
hosting Prometheus as an infrastructure

00:48:55,250 --> 00:49:01,310
team but what we did is we made a walled

00:48:58,670 --> 00:49:06,710
garden we made the barrier to entry as

00:49:01,310 --> 00:49:10,850
low as possible and we exposed alerting

00:49:06,710 --> 00:49:13,970
recording rules ability to add exporters

00:49:10,850 --> 00:49:18,860
and good testing environments and all

00:49:13,970 --> 00:49:22,340
you had to do was commit and push to put

00:49:18,860 --> 00:49:24,380
rules out on the system so that was the

00:49:22,340 --> 00:49:26,930
first experience that people would have

00:49:24,380 --> 00:49:32,390
is implementing alerting and recording

00:49:26,930 --> 00:49:35,770
rules so they had instant feedback so

00:49:32,390 --> 00:49:38,990
that left scaling and tuning and

00:49:35,770 --> 00:49:41,660
Prometheus performance problems to me

00:49:38,990 --> 00:49:44,620
and and yes we did have them but at

00:49:41,660 --> 00:49:49,690
least we had customers so that was great

00:49:44,620 --> 00:49:53,240
communicating was super important for

00:49:49,690 --> 00:49:56,000
onboarding it was a lot of application

00:49:53,240 --> 00:49:58,810
talking about applications and exporters

00:49:56,000 --> 00:50:03,200
and and how to write lyrics and rules

00:49:58,810 --> 00:50:08,920
what did I do wrong what woke me up at

00:50:03,200 --> 00:50:12,740
night so I got really ambitious about

00:50:08,920 --> 00:50:14,750
paralyzing paralyzation my deploys i've

00:50:12,740 --> 00:50:21,260
had branch deploys i had all sorts of

00:50:14,750 --> 00:50:23,060
things and i was too too shy to merge to

00:50:21,260 --> 00:50:26,480
master when i should have I should have

00:50:23,060 --> 00:50:28,970
put that in in the automated deployment

00:50:26,480 --> 00:50:31,730
flow that I had so I end up having to

00:50:28,970 --> 00:50:34,310
resolve drift problems early on and that

00:50:31,730 --> 00:50:38,150
was a huge pain the other thing was I

00:50:34,310 --> 00:50:41,510
didn't put in initially in my workflow

00:50:38,150 --> 00:50:43,550
with silences so it's either me waking

00:50:41,510 --> 00:50:45,200
up or them waking up and because I want

00:50:43,550 --> 00:50:51,170
them to use my service it was me waking

00:50:45,200 --> 00:50:55,460
up and I also don't know why but I

00:50:51,170 --> 00:50:57,060
didn't initially expose or educate

00:50:55,460 --> 00:50:58,980
people about prom tool and

00:50:57,060 --> 00:51:00,270
tool so I ended up having yourself those

00:50:58,980 --> 00:51:04,020
sorts of problems which they could

00:51:00,270 --> 00:51:07,250
easily do on their own so what you

00:51:04,020 --> 00:51:12,000
should do from learning from what I did

00:51:07,250 --> 00:51:15,000
is get people to who know their

00:51:12,000 --> 00:51:18,900
applications to do their own

00:51:15,000 --> 00:51:20,820
instrumentation give them a workflow get

00:51:18,900 --> 00:51:23,760
out of the business of writing alerts

00:51:20,820 --> 00:51:26,430
and rules yourself and I'm about to do

00:51:23,760 --> 00:51:28,410
this again at my new company so I'd love

00:51:26,430 --> 00:51:30,180
to hear about your experiences and we

00:51:28,410 --> 00:51:32,070
can all learn together and do this

00:51:30,180 --> 00:51:36,930
better

00:51:32,070 --> 00:51:40,530
[Applause]

00:51:36,930 --> 00:51:43,520
it's actually my first prom cone so I'm

00:51:40,530 --> 00:51:48,030
going to be here I want to talk about

00:51:43,520 --> 00:51:52,830
Victoria metrics cluster actually I love

00:51:48,030 --> 00:51:55,010
complex things but as operations I have

00:51:52,830 --> 00:51:59,160
to work with simple things and I think

00:51:55,010 --> 00:52:04,820
Victoria metrics is pretty simple in

00:51:59,160 --> 00:52:10,010
forecaster actually here Graham from the

00:52:04,820 --> 00:52:13,770
github repo you can look at comes up

00:52:10,010 --> 00:52:14,700
there are only three things which you'll

00:52:13,770 --> 00:52:17,940
be talking about

00:52:14,700 --> 00:52:21,510
it's a vamp select women cert and RAM

00:52:17,940 --> 00:52:24,210
storage so other just how you connect

00:52:21,510 --> 00:52:29,190
some other things to Austin

00:52:24,210 --> 00:52:31,860
yeah it's place a simple talk so

00:52:29,190 --> 00:52:35,720
actually have women's yet it's where you

00:52:31,860 --> 00:52:40,260
write you matrix from parameters you can

00:52:35,720 --> 00:52:43,800
use just some remote right you need to

00:52:40,260 --> 00:52:47,210
configure on the remote write and women

00:52:43,800 --> 00:52:50,640
set as binary will split evenly between

00:52:47,210 --> 00:52:52,890
ovum storages which you have so it's

00:52:50,640 --> 00:52:56,580
pretty simple one select is where you

00:52:52,890 --> 00:52:59,370
point you graph honest data source so if

00:52:56,580 --> 00:53:02,280
you want to show this matrix in your

00:52:59,370 --> 00:53:09,540
commanders boss you just point to this

00:53:02,280 --> 00:53:10,920
one story is very magic happens so it's

00:53:09,540 --> 00:53:18,119
not as the bear

00:53:10,920 --> 00:53:22,740
and for written for storing data so you

00:53:18,119 --> 00:53:25,530
can use RAM storage solution for long

00:53:22,740 --> 00:53:30,900
term long term metrics for your

00:53:25,530 --> 00:53:33,540
parameters they're using compression and

00:53:30,900 --> 00:53:36,500
it's really good if you tell you so a

00:53:33,540 --> 00:53:41,190
talk from yesterday from adidas

00:53:36,500 --> 00:53:44,579
Compassion's pretty good yeah

00:53:41,190 --> 00:53:46,710
scanning scanning cluster actually

00:53:44,579 --> 00:53:49,650
scaling cluster is quite easier and

00:53:46,710 --> 00:53:51,960
that's why I like it because if you want

00:53:49,650 --> 00:53:55,760
to have more variance or more firm

00:53:51,960 --> 00:53:58,770
selects you just create one of them and

00:53:55,760 --> 00:54:03,240
pointed to existing home storages and it

00:53:58,770 --> 00:54:05,670
just works so they are stateless when

00:54:03,240 --> 00:54:10,170
storages you can scale them up easily

00:54:05,670 --> 00:54:13,770
you just add a new one and you have new

00:54:10,170 --> 00:54:14,520
metrics there but it's simple and it's

00:54:13,770 --> 00:54:18,569
good and it's bad

00:54:14,520 --> 00:54:21,540
so no sharding nothing like after

00:54:18,569 --> 00:54:24,900
automatic emigration of data new data

00:54:21,540 --> 00:54:27,329
just happens there and that's all it's

00:54:24,900 --> 00:54:31,349
cool but if you need some complex ingots

00:54:27,329 --> 00:54:33,980
it doesn't doesn't this so but if you

00:54:31,349 --> 00:54:37,230
need to scale down it's hard part and

00:54:33,980 --> 00:54:40,230
actually I don't know how to do it well

00:54:37,230 --> 00:54:43,650
you just need to maybe turn it off from

00:54:40,230 --> 00:54:46,500
women's shirt and you can remove this

00:54:43,650 --> 00:54:48,299
storage from women's shirt and wait till

00:54:46,500 --> 00:54:52,200
retention pollutant moves all the data

00:54:48,299 --> 00:54:55,380
and then remove you from storage so it

00:54:52,200 --> 00:55:01,230
can take it you and can take time okay

00:54:55,380 --> 00:55:03,890
what bow accent you need some for for

00:55:01,230 --> 00:55:08,700
her ability you need some buoyancy

00:55:03,890 --> 00:55:13,859
before you worm select women set so if

00:55:08,700 --> 00:55:16,109
you use kubernetes and they have come

00:55:13,859 --> 00:55:17,940
chat and you can just use them guess for

00:55:16,109 --> 00:55:20,490
this but if you don't have kubernetes

00:55:17,940 --> 00:55:21,030
maybe you need to use jinx on something

00:55:20,490 --> 00:55:24,450
like this

00:55:21,030 --> 00:55:27,900
so women set bounces

00:55:24,450 --> 00:55:30,210
twin storages just just from

00:55:27,900 --> 00:55:33,079
cash-on-cash automatic so if you have a

00:55:30,210 --> 00:55:37,740
new storage some metrics will go there

00:55:33,079 --> 00:55:42,450
and like I said in my test setup my test

00:55:37,740 --> 00:55:45,570
cluster there is difference between new

00:55:42,450 --> 00:55:50,550
new storage nodes and all storage nodes

00:55:45,570 --> 00:55:54,599
and it it depends on when you Adam so in

00:55:50,550 --> 00:55:57,450
some in terms of notes you don't have a

00:55:54,599 --> 00:56:00,480
replication there so if you worst one

00:55:57,450 --> 00:56:03,630
not he was matrix promises not so you

00:56:00,480 --> 00:56:06,930
need to make the cups and but you still

00:56:03,630 --> 00:56:09,750
get as a metric so you cost your cost

00:56:06,930 --> 00:56:10,530
will be operable but without a matrix

00:56:09,750 --> 00:56:14,130
okay

00:56:10,530 --> 00:56:16,200
as I said configuring room cost for you

00:56:14,130 --> 00:56:18,900
long term storage is easy you just need

00:56:16,200 --> 00:56:21,270
to devote to remote right into your

00:56:18,900 --> 00:56:25,050
parameters configuration configured at

00:56:21,270 --> 00:56:26,869
the source and pretty much it's setup

00:56:25,050 --> 00:56:32,400
form cluster it's really easy

00:56:26,869 --> 00:56:34,410
yeah that's all thank you so obviously

00:56:32,400 --> 00:56:35,790
from yesterday's talk from Amazon drove

00:56:34,410 --> 00:56:37,950
from a DDoS you're talking about

00:56:35,790 --> 00:56:39,510
Victoria metrics and like those right

00:56:37,950 --> 00:56:41,730
numbers were pretty impressive well

00:56:39,510 --> 00:56:43,650
we're also asking questions about okay

00:56:41,730 --> 00:56:45,750
it's a question from over there you know

00:56:43,650 --> 00:56:48,060
what about reads and so on so I figured

00:56:45,750 --> 00:56:50,430
let's look at things and figure it out

00:56:48,060 --> 00:56:52,920
so I I did some looking last night after

00:56:50,430 --> 00:56:54,150
I got home from the event and I've

00:56:52,920 --> 00:57:02,130
actually been running some stuff as well

00:56:54,150 --> 00:57:04,770
oh good so I've since this morning been

00:57:02,130 --> 00:57:07,170
running medias here which is just

00:57:04,770 --> 00:57:10,079
scraping itself and my exposed many

00:57:07,170 --> 00:57:11,880
metrics script which is everyone writes

00:57:10,079 --> 00:57:13,260
a script to expose some metrics or

00:57:11,880 --> 00:57:15,300
benchmarking by the looks what I was

00:57:13,260 --> 00:57:16,770
last looking at table sizes with it but

00:57:15,300 --> 00:57:18,660
I've got outputting at 10,000 metrics

00:57:16,770 --> 00:57:20,400
and basically every time you scrape it

00:57:18,660 --> 00:57:22,829
increments by a random number between

00:57:20,400 --> 00:57:24,060
Norton would I

00:57:22,829 --> 00:57:25,980
so that's there and it's also got

00:57:24,060 --> 00:57:27,569
scraping for Matias so I've got a

00:57:25,980 --> 00:57:30,060
Victoria metrics here it's been running

00:57:27,569 --> 00:57:32,520
since well at the same time so should

00:57:30,060 --> 00:57:35,880
all be the same data so we can go over

00:57:32,520 --> 00:57:36,660
it to snapshots so I can take a snapshot

00:57:35,880 --> 00:57:38,190
for Boetsch

00:57:36,660 --> 00:57:41,070
cuz we want to compare like

00:57:38,190 --> 00:57:43,980
like and like comparing the Wow from

00:57:41,070 --> 00:57:45,270
freakiest isn't quite data sizes because

00:57:43,980 --> 00:57:47,700
it's not long-term stuff and

00:57:45,270 --> 00:57:51,030
apples-to-apples comparisons are tricky

00:57:47,700 --> 00:57:55,100
so I think this is apples to apples so I

00:57:51,030 --> 00:57:55,100
will now ask both systems to snapshot

00:57:56,750 --> 00:58:03,770
one of these takes longer than the other

00:57:58,740 --> 00:58:07,700
and we can now look at how big these are

00:58:03,770 --> 00:58:12,660
yes I made it a bit too big

00:58:07,700 --> 00:58:15,510
eh there we go so here we can see that

00:58:12,660 --> 00:58:18,570
previous one I just did is a bit over a

00:58:15,510 --> 00:58:20,400
gig and Victoria metrics is 1.1 gigs so

00:58:18,570 --> 00:58:22,890
it looks like these are basically the

00:58:20,400 --> 00:58:25,980
same in fact I started out this morning

00:58:22,890 --> 00:58:27,420
and medias was a bit bigger now Victoria

00:58:25,980 --> 00:58:29,700
metrics seems to have tried it to be a

00:58:27,420 --> 00:58:31,830
bit larger so it looks like things are

00:58:29,700 --> 00:58:33,450
about the same in center size because

00:58:31,830 --> 00:58:35,730
it's the same data the previous is

00:58:33,450 --> 00:58:37,320
standing out by a remote right so in

00:58:35,730 --> 00:58:39,390
principle at least I expect a nose to be

00:58:37,320 --> 00:58:40,980
comparable and probably also cortex

00:58:39,390 --> 00:58:43,200
because it's also using for meatiest

00:58:40,980 --> 00:58:45,300
chunks and I know that Emma tree is

00:58:43,200 --> 00:58:47,160
using something different so it might

00:58:45,300 --> 00:58:49,020
well be using more so it looks as though

00:58:47,160 --> 00:58:51,180
because this varies from the Victoria

00:58:49,020 --> 00:58:54,120
metrics is diversion was released like

00:58:51,180 --> 00:58:55,920
well three days ago now version and 128

00:58:54,120 --> 00:58:58,830
tree and Prometheus is the latest stable

00:58:55,920 --> 00:59:00,750
release of 213 one so it looks as though

00:58:58,830 --> 00:59:02,820
things have changed since whenever those

00:59:00,750 --> 00:59:03,990
benchmarks were last done but I have

00:59:02,820 --> 00:59:08,460
another two minutes I'm going to look at

00:59:03,990 --> 00:59:12,110
some other stuff so question was asked

00:59:08,460 --> 00:59:12,110
can we get the same data back ouch

00:59:21,250 --> 00:59:26,180
so we'll do process CPU seconds total

00:59:24,200 --> 00:59:29,270
and this is going against vitória

00:59:26,180 --> 00:59:35,359
metrics hope alle data what we always

00:59:29,270 --> 00:59:37,850
love to see sorry process CPU seconds

00:59:35,359 --> 00:59:39,740
total over the last five minutes so

00:59:37,850 --> 00:59:41,420
there we have some numbers all these

00:59:39,740 --> 00:59:44,000
timestamps are convenient yes every

00:59:41,420 --> 00:59:47,240
second ending in point four four one oh

00:59:44,000 --> 00:59:52,280
this is wrong order because it's got

00:59:47,240 --> 00:59:53,869
both the I don't mean it that way so

00:59:52,280 --> 00:59:56,660
it's the five three six this is okay up

00:59:53,869 --> 00:59:59,990
here so this is the previous so two

00:59:56,660 --> 01:00:02,480
seven eight oh nine where is data that's

00:59:59,990 --> 01:00:13,880
not here let's see one five seven three

01:00:02,480 --> 01:00:17,750
two two two seven three oh seven three

01:00:13,880 --> 01:00:19,970
eight okay there it is so for four to

01:00:17,750 --> 01:00:21,710
six oh one point zero five four to six

01:00:19,970 --> 01:00:23,450
at one point zero five that looks good

01:00:21,710 --> 01:00:25,640
although it's odd that the latest data

01:00:23,450 --> 01:00:28,820
that Victoria metrics has is a bit back

01:00:25,640 --> 01:00:32,180
down to page and we might consider

01:00:28,820 --> 01:00:35,000
something else so let's say go mem stats

01:00:32,180 --> 01:00:41,030
GC CPU fraction in the thirty seconds I

01:00:35,000 --> 01:00:50,440
have left execute that they're both go

01:00:41,030 --> 01:00:54,460
binaries live demos a 10 seconds left

01:00:50,440 --> 01:00:54,460
sorry I have a minute left

01:01:01,190 --> 01:01:10,980
okay so here you see we can just go one

01:01:04,710 --> 01:01:12,869
five seven grab that phone up and see

01:01:10,980 --> 01:01:17,130
our value here then it's six six eight

01:01:12,869 --> 01:01:18,299
oh two and this is eight six six and it

01:01:17,130 --> 01:01:20,339
doesn't seem like it has the full number

01:01:18,299 --> 01:01:22,799
I have previously they are slightly

01:01:20,339 --> 01:01:24,329
different and it also appears I'm not

01:01:22,799 --> 01:01:26,369
gonna show up at this time that Victoria

01:01:24,329 --> 01:01:27,690
nemetrix is dropping Nan's which means

01:01:26,369 --> 01:01:29,490
it's not even storing all the same data

01:01:27,690 --> 01:01:31,079
that also means staleness isn't working

01:01:29,490 --> 01:01:36,299
I have checked that but I'm out of time

01:01:31,079 --> 01:01:38,670
thank you hello

01:01:36,299 --> 01:01:40,500
yeah so I'm Rob I've got to talk to you

01:01:38,670 --> 01:01:42,329
about why you shouldn't use a long term

01:01:40,500 --> 01:01:44,880
storage and for the first part of this

01:01:42,329 --> 01:01:48,630
talk so I wanted to talk a little bit

01:01:44,880 --> 01:01:51,869
about pain points running m3 and and how

01:01:48,630 --> 01:01:53,750
the replication works so why would you

01:01:51,869 --> 01:01:56,430
want to try to run anything clustered

01:01:53,750 --> 01:02:01,380
most people don't and in my opinion

01:01:56,430 --> 01:02:03,750
Riley so I think that most setups do

01:02:01,380 --> 01:02:07,200
perfectly well with a small number of

01:02:03,750 --> 01:02:10,079
Prometheus's and and medium and larger

01:02:07,200 --> 01:02:11,640
number Prometheus works just as well if

01:02:10,079 --> 01:02:16,770
you have the infrastructure to support

01:02:11,640 --> 01:02:18,930
that at your organization and then you

01:02:16,770 --> 01:02:21,359
know I think to dig a little bit more

01:02:18,930 --> 01:02:24,030
into why would you not run anything

01:02:21,359 --> 01:02:25,980
clustered for your monitoring running

01:02:24,030 --> 01:02:28,619
distributed systems typically it takes a

01:02:25,980 --> 01:02:30,839
lot of work learning concrete types of

01:02:28,619 --> 01:02:33,589
distributed systems distributed stateful

01:02:30,839 --> 01:02:35,970
systems like Cassandra or whatever

01:02:33,589 --> 01:02:37,319
there's tons of other staple distributed

01:02:35,970 --> 01:02:40,950
systems out there too like cough Karen

01:02:37,319 --> 01:02:43,710
and such it it sometimes involves like

01:02:40,950 --> 01:02:45,809
several magnitudes higher amount of time

01:02:43,710 --> 01:02:48,030
to learn that a programming language

01:02:45,809 --> 01:02:49,859
which you know I think like perhaps

01:02:48,030 --> 01:02:51,270
maybe it's it's easier these days to

01:02:49,859 --> 01:02:54,569
pick up new languages because I look

01:02:51,270 --> 01:02:56,309
very similar or maybe maybe not but

01:02:54,569 --> 01:03:00,990
that's that's a long amount of time

01:02:56,309 --> 01:03:02,579
mmm Arjen vestment so alright so coming

01:03:00,990 --> 01:03:06,119
back to why would you try to run

01:03:02,579 --> 01:03:09,580
anything clustered so the use cases i've

01:03:06,119 --> 01:03:11,320
seen is like a single application

01:03:09,580 --> 01:03:14,200
wants to be viewed from a single

01:03:11,320 --> 01:03:16,080
instance has to be collected by more

01:03:14,200 --> 01:03:18,550
than one single like basically needs

01:03:16,080 --> 01:03:21,730
more than one single monitoring node to

01:03:18,550 --> 01:03:24,400
to basically store all that data you

01:03:21,730 --> 01:03:26,170
could do a few things you can not try to

01:03:24,400 --> 01:03:27,460
store that many metrics for a single

01:03:26,170 --> 01:03:32,050
application that's usually the correct

01:03:27,460 --> 01:03:34,180
thing to do so unfortunately like there

01:03:32,050 --> 01:03:35,380
are some use cases aware that is that is

01:03:34,180 --> 01:03:39,100
very difficult and I've seen people

01:03:35,380 --> 01:03:41,680
running you know 500 gigabyte heaps or 1

01:03:39,100 --> 01:03:45,340
terabyte heaps with Prometheus which is

01:03:41,680 --> 01:03:47,290
interesting it works though so you know

01:03:45,340 --> 01:03:48,490
I think like that's also a perfectly

01:03:47,290 --> 01:03:52,330
valid thing to do as well

01:03:48,490 --> 01:03:54,970
scaling up vertically so the other thing

01:03:52,330 --> 01:03:57,040
you might if you you might want to do

01:03:54,970 --> 01:04:00,070
this is if you depend really heavily on

01:03:57,040 --> 01:04:03,670
on metrics themselves so you know if you

01:04:00,070 --> 01:04:06,360
have some kind of rudimentary normally

01:04:03,670 --> 01:04:10,150
detection or perhaps you have automated

01:04:06,360 --> 01:04:12,580
load balancing or you have some other

01:04:10,150 --> 01:04:14,920
things where basically any amount of

01:04:12,580 --> 01:04:16,570
downtime is quite bad and will make your

01:04:14,920 --> 01:04:19,420
load balancing of certain clusters or

01:04:16,570 --> 01:04:21,220
routing of certain network edges fail

01:04:19,420 --> 01:04:23,920
because they rely on the monitor on the

01:04:21,220 --> 01:04:26,620
metrics to actually perform those those

01:04:23,920 --> 01:04:28,840
routing tasks then you really want high

01:04:26,620 --> 01:04:30,820
high reliability because you're

01:04:28,840 --> 01:04:32,890
basically not using metrics as

01:04:30,820 --> 01:04:36,000
monitoring you're in some ways you're

01:04:32,890 --> 01:04:39,280
using as a platform it's almost like how

01:04:36,000 --> 01:04:40,840
databases would use databases I used to

01:04:39,280 --> 01:04:42,190
serve applications there's a core part

01:04:40,840 --> 01:04:43,510
of the application you're building is

01:04:42,190 --> 01:04:47,650
stuff and so ideally you have

01:04:43,510 --> 01:04:49,300
replication on that ideally are three so

01:04:47,650 --> 01:04:51,190
and yes you might want to be able to

01:04:49,300 --> 01:04:52,870
lose and a firmly instance if you're not

01:04:51,190 --> 01:04:54,130
running in the cloud and so using or if

01:04:52,870 --> 01:04:55,930
you're in the cloud and using local SSD

01:04:54,130 --> 01:04:58,660
instances in the cloud so a quick

01:04:55,930 --> 01:05:00,250
background for how in three works before

01:04:58,660 --> 01:05:01,990
I get into the paint like paint points

01:05:00,250 --> 01:05:04,780
is that we store everything in quorum

01:05:01,990 --> 01:05:06,400
across three replicas immediately so we

01:05:04,780 --> 01:05:09,220
don't use cloud disks this runs on creme

01:05:06,400 --> 01:05:10,720
if you wanted to and it basically means

01:05:09,220 --> 01:05:12,820
that immediately as soon as you write it

01:05:10,720 --> 01:05:14,980
a copy it's a it's across three

01:05:12,820 --> 01:05:16,420
availability zones or like three racks

01:05:14,980 --> 01:05:18,700
if that's what you're doing for

01:05:16,420 --> 01:05:20,980
isolation so let's quickly go to

01:05:18,700 --> 01:05:22,730
categories of pain so bootstrapping

01:05:20,980 --> 01:05:25,340
times with large instances

01:05:22,730 --> 01:05:29,060
with m3 I think is a big thing I mean

01:05:25,340 --> 01:05:30,680
obviously large has there's obviously

01:05:29,060 --> 01:05:32,390
been a lot of work that Ganesh covered

01:05:30,680 --> 01:05:33,830
is happening but it's like typically a

01:05:32,390 --> 01:05:38,150
thing with with a lot of these storage

01:05:33,830 --> 01:05:40,450
systems memory and down sampling and so

01:05:38,150 --> 01:05:43,100
on the bootstrapping times you know

01:05:40,450 --> 01:05:44,750
really large heap sizes can take a

01:05:43,100 --> 01:05:46,190
really amount of time I've really

01:05:44,750 --> 01:05:48,650
severely long amount of time this is

01:05:46,190 --> 01:05:51,200
okay sometimes in the entry world since

01:05:48,650 --> 01:05:53,000
if you lose one node then other replicas

01:05:51,200 --> 01:05:55,010
are still up for a quorum reads and

01:05:53,000 --> 01:05:56,690
writes obvious more than two or more

01:05:55,010 --> 01:05:58,160
replicas a time it's still okay because

01:05:56,690 --> 01:06:00,410
we actually accept rights while you're

01:05:58,160 --> 01:06:01,970
bootstrapping and now we're actually

01:06:00,410 --> 01:06:03,470
deferring I'm moving to deferred and

01:06:01,970 --> 01:06:06,320
asynchronous merging of data points and

01:06:03,470 --> 01:06:08,600
that gives us about roughly like a terms

01:06:06,320 --> 01:06:11,600
of 30 minute bootstrap into two minutes

01:06:08,600 --> 01:06:13,730
so how much memory is I'm using we use a

01:06:11,600 --> 01:06:15,830
lot of em maps and these aren't actually

01:06:13,730 --> 01:06:18,020
evicted from the page cache until you

01:06:15,830 --> 01:06:19,640
experience memory pressure so

01:06:18,020 --> 01:06:21,680
essentially now we use em advised to

01:06:19,640 --> 01:06:22,700
tell that the operating system that

01:06:21,680 --> 01:06:24,260
actually we don't need these pages

01:06:22,700 --> 01:06:27,380
anymore because what we do is we

01:06:24,260 --> 01:06:29,030
basically do a checksum across the

01:06:27,380 --> 01:06:32,420
entire page pledged all into the page

01:06:29,030 --> 01:06:34,460
acacia mmediately so basically our

01:06:32,420 --> 01:06:36,140
memory RSS side's is like very heavily

01:06:34,460 --> 01:06:39,080
bloated but now we're going to try and

01:06:36,140 --> 01:06:40,640
Colt don't need and from him advised to

01:06:39,080 --> 01:06:43,340
basically make sure that that doesn't

01:06:40,640 --> 01:06:45,620
happen well it's no mystery it's not

01:06:43,340 --> 01:06:50,270
miss reverbs misrepresenting page caches

01:06:45,620 --> 01:06:53,080
that are not needed anymore Thanks

01:06:50,270 --> 01:06:53,080
thank you very much Rob

01:06:56,180 --> 01:07:01,580
hi I'm Mukesh Mehra and I'm gonna tell

01:06:59,960 --> 01:07:04,490
you about that annual to like wrote

01:07:01,580 --> 01:07:09,380
recently that allows you to acknowledge

01:07:04,490 --> 01:07:11,810
alerts with alert manager so another

01:07:09,380 --> 01:07:15,520
manager any others can have three states

01:07:11,810 --> 01:07:18,860
and processed active and suppressed and

01:07:15,520 --> 01:07:20,900
the unprocessed is the initial state

01:07:18,860 --> 01:07:24,200
before other manager has an ability to

01:07:20,900 --> 01:07:27,440
check any alert against any silences

01:07:24,200 --> 01:07:29,390
that you might have defined and once

01:07:27,440 --> 01:07:32,000
it's checked that it puts the alert in

01:07:29,390 --> 01:07:34,130
the active or suppressed state also

01:07:32,000 --> 01:07:35,810
inhibition rules could put any alert in

01:07:34,130 --> 01:07:38,750
suppressed state

01:07:35,810 --> 01:07:40,670
a lot of other monitoring system have

01:07:38,750 --> 01:07:42,740
also the acknowledged state where you

01:07:40,670 --> 01:07:44,240
can click an alert that you received and

01:07:42,740 --> 01:07:48,050
marked it as something that you're

01:07:44,240 --> 01:07:49,400
working on like yes unfortunately in a

01:07:48,050 --> 01:07:51,380
Larry Manager that's not possible

01:07:49,400 --> 01:07:54,710
because effectively we have only those

01:07:51,380 --> 01:07:56,300
two state active or suppressed and

01:07:54,710 --> 01:07:59,600
there's a reason for that

01:07:56,300 --> 01:08:02,090
which you can find in dogs typically the

01:07:59,600 --> 01:08:04,670
advice is to use systems like pager duty

01:08:02,090 --> 01:08:06,710
or obscene e to push your alerts to

01:08:04,670 --> 01:08:09,620
those systems and then manage all their

01:08:06,710 --> 01:08:12,560
pages in systems which is good advice

01:08:09,620 --> 01:08:14,780
but not everyone uses those extra

01:08:12,560 --> 01:08:18,200
systems or sometimes you don't want to

01:08:14,780 --> 01:08:20,089
push those alerts to those and I did

01:08:18,200 --> 01:08:22,069
want to have an acknowledgement system

01:08:20,089 --> 01:08:26,510
with just alert manager without any

01:08:22,069 --> 01:08:29,230
external services and the usual advice

01:08:26,510 --> 01:08:32,810
for that was to just use a silence which

01:08:29,230 --> 01:08:35,060
is a fair advice but the problem is that

01:08:32,810 --> 01:08:37,240
silences are something that you need to

01:08:35,060 --> 01:08:41,120
add manually and then you need to take

01:08:37,240 --> 01:08:44,980
in to ensure that they are removed when

01:08:41,120 --> 01:08:49,100
the silence is no longer needed so to

01:08:44,980 --> 01:08:51,440
manage alerts and have some way of outer

01:08:49,100 --> 01:08:52,940
expiring silence once the alert is gone

01:08:51,440 --> 01:08:56,089
so that you don't need to manually

01:08:52,940 --> 01:08:58,130
remove that I consider two options the

01:08:56,089 --> 01:09:00,650
first one was to create a silence with a

01:08:58,130 --> 01:09:02,480
very long duration and then once the

01:09:00,650 --> 01:09:07,430
silence doesn't match any alerts expire

01:09:02,480 --> 01:09:08,660
that silence and this option seemed a

01:09:07,430 --> 01:09:11,630
little bit more tricky to implement

01:09:08,660 --> 01:09:14,600
because you have a very long running

01:09:11,630 --> 01:09:17,089
silence then you need to keep some tiny

01:09:14,600 --> 01:09:19,100
Allstate to know when the silence

01:09:17,089 --> 01:09:21,890
doesn't match any alerts and the other

01:09:19,100 --> 01:09:24,230
option was to create a silence with a

01:09:21,890 --> 01:09:27,440
very short duration and then keep

01:09:24,230 --> 01:09:31,490
extending it as long as you find alerts

01:09:27,440 --> 01:09:34,850
firing for it and for that I implemented

01:09:31,490 --> 01:09:37,609
tiny tour which took me like just one

01:09:34,850 --> 01:09:40,549
afternoon to write which you can find on

01:09:37,609 --> 01:09:43,970
github and it does exactly that you

01:09:40,549 --> 01:09:46,160
create silence with alert manager and

01:09:43,970 --> 01:09:48,260
then this doe will keep extending those

01:09:46,160 --> 01:09:51,210
silences as long as there are alerts

01:09:48,260 --> 01:09:53,580
firing further silence

01:09:51,210 --> 01:09:56,370
and this is example of the output you

01:09:53,580 --> 01:09:58,950
can see as you can see there's one

01:09:56,370 --> 01:10:00,990
silence that would expire soon but it's

01:09:58,950 --> 01:10:01,890
still matching some alerts so it's gonna

01:10:00,990 --> 01:10:04,560
be extended

01:10:01,890 --> 01:10:07,080
and there's another silence this one

01:10:04,560 --> 01:10:10,950
doesn't have any alerts for it so little

01:10:07,080 --> 01:10:12,570
will let it expire I also wrote a

01:10:10,950 --> 01:10:14,340
different tool which is karma which is

01:10:12,570 --> 01:10:16,950
our dashboard so I wanted to make it

01:10:14,340 --> 01:10:19,520
even easier for myself so I read ability

01:10:16,950 --> 01:10:24,870
to just silence alerts with one click

01:10:19,520 --> 01:10:26,310
which you can see here hopefully as you

01:10:24,870 --> 01:10:28,470
can see you can just click on one button

01:10:26,310 --> 01:10:31,650
it will create a silence for you and

01:10:28,470 --> 01:10:33,930
then if you have that tiny tool I just

01:10:31,650 --> 01:10:36,150
mentioned running along with your alert

01:10:33,930 --> 01:10:38,070
manager then it will keep that silence

01:10:36,150 --> 01:10:40,380
running as long as the alert is firing

01:10:38,070 --> 01:10:42,090
once that is gone the salads will be

01:10:40,380 --> 01:10:45,740
gone and this way you don't need to

01:10:42,090 --> 01:10:45,740
worry about managing salads yourself

01:10:53,210 --> 01:11:03,150
well the takeaways not everything needs

01:10:58,170 --> 01:11:04,830
to be a core primitives feature so it's

01:11:03,150 --> 01:11:08,340
very easy to extend that manager in

01:11:04,830 --> 01:11:10,320
prometheus by using their IP is it

01:11:08,340 --> 01:11:13,830
especially easy to write alert manager

01:11:10,320 --> 01:11:16,050
API clients I don't need to learn any

01:11:13,830 --> 01:11:19,170
talks about API for alert manager

01:11:16,050 --> 01:11:21,030
because I use the go client libraries

01:11:19,170 --> 01:11:24,410
that are included in alert manager

01:11:21,030 --> 01:11:27,270
itself and it was really easy task to do

01:11:24,410 --> 01:11:30,000
so I encourage everyone to just take a

01:11:27,270 --> 01:11:36,450
look at the elect manager code and use

01:11:30,000 --> 01:11:39,270
it first of all first things first I

01:11:36,450 --> 01:11:41,700
have a really bad memory for names and

01:11:39,270 --> 01:11:44,790
the second rosca was Heinrich Hofmann

01:11:41,700 --> 01:11:47,760
for a catchy name and he life tweeted my

01:11:44,790 --> 01:11:49,170
whole talk and I gave him as my thank

01:11:47,760 --> 01:11:50,700
you at that I forgot his name my

01:11:49,170 --> 01:11:53,880
sincerest apologies Irish if you are

01:11:50,700 --> 01:11:56,670
still listening or watching it was my

01:11:53,880 --> 01:11:59,750
fault we talked for each other every con

01:11:56,670 --> 01:12:02,070
so I should really remember but anyway

01:11:59,750 --> 01:12:03,420
Brian encourage us to give like

01:12:02,070 --> 01:12:05,400
spontaneous lightning talk so I took

01:12:03,420 --> 01:12:06,960
my Prometheus Braun notebook and a made

01:12:05,400 --> 01:12:08,340
of a lightning talk I mean I wanted to

01:12:06,960 --> 01:12:10,440
give this talk at some point in the

01:12:08,340 --> 01:12:12,810
future now I just wing it casual village

01:12:10,440 --> 01:12:15,270
you're from your theories we know

01:12:12,810 --> 01:12:19,170
there's trickster there is the cortex

01:12:15,270 --> 01:12:25,469
query front end okay Tom North your name

01:12:19,170 --> 01:12:27,090
is Tom isn't it right okay so and I was

01:12:25,469 --> 01:12:28,800
always thinking about can we actually

01:12:27,090 --> 01:12:30,870
cash prompt your queries I mean

01:12:28,800 --> 01:12:33,360
obviously the newest data like if you

01:12:30,870 --> 01:12:34,739
have this dashboard that reloads every

01:12:33,360 --> 01:12:36,150
five seconds and the whole company is

01:12:34,739 --> 01:12:37,530
looking in it because we just have an

01:12:36,150 --> 01:12:40,620
outage and they bring down prometheus

01:12:37,530 --> 01:12:42,630
hopefully not but we want to cash that

01:12:40,620 --> 01:12:44,460
right that is the idea and of course the

01:12:42,630 --> 01:12:46,110
newest and hottest data cannot be cash

01:12:44,460 --> 01:12:48,030
but we can kind of manipulate the query

01:12:46,110 --> 01:12:50,790
and just like do a query arranged for

01:12:48,030 --> 01:12:52,230
the like data we already curate before

01:12:50,790 --> 01:12:52,560
or something right we can certainly do

01:12:52,230 --> 01:12:53,969
that

01:12:52,560 --> 01:12:57,960
like who thinks that would be an

01:12:53,969 --> 01:13:02,520
effective caching strategy Tom thinks

01:12:57,960 --> 01:13:04,770
that so what's what's going wrong the

01:13:02,520 --> 01:13:06,449
problem is that at least in the old days

01:13:04,770 --> 01:13:08,280
whenever you reload the dashboard it

01:13:06,449 --> 01:13:10,860
would like ask for the newest data and

01:13:08,280 --> 01:13:13,320
then like every 10 seconds for the later

01:13:10,860 --> 01:13:14,610
later and since you you do this at some

01:13:13,320 --> 01:13:16,650
random point in time you will almost

01:13:14,610 --> 01:13:18,360
never hit the same time stem for

01:13:16,650 --> 01:13:20,190
Prometheus this looks like a completely

01:13:18,360 --> 01:13:21,810
different query and you're you're like

01:13:20,190 --> 01:13:23,340
dumb cache would just see this is a

01:13:21,810 --> 01:13:27,179
different query so why should I even

01:13:23,340 --> 01:13:29,219
cache that right so what you can do you

01:13:27,179 --> 01:13:30,660
can align the timestamps and that's what

01:13:29,219 --> 01:13:31,940
the core it's very front and it's doing

01:13:30,660 --> 01:13:34,620
that's what's trickster doing and

01:13:31,940 --> 01:13:36,510
incidentally Quran is doing this by now

01:13:34,620 --> 01:13:38,640
too just because they don't want noisy

01:13:36,510 --> 01:13:40,260
graphs to jump around too much but very

01:13:38,640 --> 01:13:42,210
convenient so now all the time stems

01:13:40,260 --> 01:13:45,030
online the data should be perfectly

01:13:42,210 --> 01:13:48,679
cashable true or false who thinks that's

01:13:45,030 --> 01:13:51,270
true we can just catch all the occurs

01:13:48,679 --> 01:13:52,949
nobody why not okay I mean there are too

01:13:51,270 --> 01:13:55,350
many things I should first do the easy

01:13:52,949 --> 01:13:57,960
part so there's one thing that's I mean

01:13:55,350 --> 01:13:59,910
most of this stuff you can know from the

01:13:57,960 --> 01:14:02,699
talks we had here there is we talked

01:13:59,910 --> 01:14:04,469
about this ingestion isolation which

01:14:02,699 --> 01:14:06,540
already tells you that samples show up

01:14:04,469 --> 01:14:08,850
like one after another so it can totally

01:14:06,540 --> 01:14:10,800
be that a sample didn't make it into the

01:14:08,850 --> 01:14:12,989
storage so you are caching an incomplete

01:14:10,800 --> 01:14:15,090
query when you run it the next time it

01:14:12,989 --> 01:14:16,800
might look different so that's already a

01:14:15,090 --> 01:14:18,240
bit dangerous that was really dangerous

01:14:16,800 --> 01:14:20,100
so from easiest one because it could

01:14:18,240 --> 01:14:22,110
have like five-minute indexing delays

01:14:20,100 --> 01:14:24,540
and you would only see those queries

01:14:22,110 --> 01:14:26,520
later so you should not cache super

01:14:24,540 --> 01:14:28,140
fresh data so let's say we just cache

01:14:26,520 --> 01:14:34,080
data that is at least one minute old

01:14:28,140 --> 01:14:36,060
it's that okay true or false false okay

01:14:34,080 --> 01:14:42,140
so what I asked what I showed us why who

01:14:36,060 --> 01:14:42,140
said that false okay you should know

01:14:44,030 --> 01:14:48,390
yeah so you can ingest all the later so

01:14:46,650 --> 01:14:50,820
what's the so I mean that should never

01:14:48,390 --> 01:14:53,100
happen because Prometheus dictates the

01:14:50,820 --> 01:14:54,420
time Sam unless you are crazy enough to

01:14:53,100 --> 01:14:55,860
put timestamps into your exposition

01:14:54,420 --> 01:14:58,740
formulas you should never do what you

01:14:55,860 --> 01:15:05,010
could do right so what times when do we

01:14:58,740 --> 01:15:07,170
tolerate how old can it be now he says

01:15:05,010 --> 01:15:10,770
two hours three hours I think it's one

01:15:07,170 --> 01:15:15,300
hour I think we so we do to our bra is

01:15:10,770 --> 01:15:16,650
three hours now when I think it's but I

01:15:15,300 --> 01:15:19,320
think we reject time since that are

01:15:16,650 --> 01:15:21,420
older than one hour yeah so I mean the

01:15:19,320 --> 01:15:23,100
thing is the the writer had locked that

01:15:21,420 --> 01:15:25,080
we also talked about can be two hours

01:15:23,100 --> 01:15:27,660
old and then it's condensed at your real

01:15:25,080 --> 01:15:29,340
immutable block but and then we keep

01:15:27,660 --> 01:15:32,010
doing this for another hour so you

01:15:29,340 --> 01:15:33,990
always have one hour hath Headroom

01:15:32,010 --> 01:15:35,520
essentially and I think even if you have

01:15:33,990 --> 01:15:37,200
three hours at that time we always

01:15:35,520 --> 01:15:40,410
reject anything older than one hour

01:15:37,200 --> 01:15:42,960
that's my knowledge okay so if we only

01:15:40,410 --> 01:15:44,310
cache query data for clear is that like

01:15:42,960 --> 01:15:46,260
for data that is all over an hour

01:15:44,310 --> 01:15:48,660
are we finally done I have only one

01:15:46,260 --> 01:15:51,660
minute so we should be done now we are

01:15:48,660 --> 01:15:55,350
not done so that was mentioned by Ganesh

01:15:51,660 --> 01:15:57,390
why are we not done backfilling League

01:15:55,350 --> 01:15:59,370
Prometheus is now so awesome it can

01:15:57,390 --> 01:16:01,320
backfill so totally somebody could come

01:15:59,370 --> 01:16:04,080
along and backfill data and then your

01:16:01,320 --> 01:16:06,120
categories are all in relative okay so

01:16:04,080 --> 01:16:07,140
if we do backfill if we allow that or

01:16:06,120 --> 01:16:08,700
now the Prometheus sir we need to

01:16:07,140 --> 01:16:10,110
invalidate the cache and I think the

01:16:08,700 --> 01:16:11,280
query front-end isn't doing this and

01:16:10,110 --> 01:16:13,380
Trixie's and doing this because it's a

01:16:11,280 --> 01:16:15,120
new feature okay final thing there's one

01:16:13,380 --> 01:16:17,100
final reason why this caching is still

01:16:15,120 --> 01:16:18,800
not perfect I mean first they're more

01:16:17,100 --> 01:16:23,250
but I didn't couldn't come up with one

01:16:18,800 --> 01:16:25,710
what is it deletion now he knows that of

01:16:23,250 --> 01:16:28,870
course so we have a retention period and

01:16:25,710 --> 01:16:30,610
promise it'll just never respond to like

01:16:28,870 --> 01:16:32,620
we'll never return data after the

01:16:30,610 --> 01:16:34,450
retention period but your cache can't

01:16:32,620 --> 01:16:36,070
know about that right so if you want to

01:16:34,450 --> 01:16:37,840
be super truthful we have to take this

01:16:36,070 --> 01:16:39,220
into account as well and I think that's

01:16:37,840 --> 01:16:45,640
all you have to take into account when

01:16:39,220 --> 01:16:48,070
you cash from your Chris thank you hello

01:16:45,640 --> 01:16:51,240
my name's Brian poram I work at wave

01:16:48,070 --> 01:16:55,390
works I am a cortex maintainer and I

01:16:51,240 --> 01:16:57,220
remove buckets from kubernetes so you

01:16:55,390 --> 01:16:59,980
can thank me for that

01:16:57,220 --> 01:17:00,670
I decided to talk at the very last

01:16:59,980 --> 01:17:03,340
minute

01:17:00,670 --> 01:17:04,710
about a project I've never worked on but

01:17:03,340 --> 01:17:07,990
I think it's really cool

01:17:04,710 --> 01:17:11,730
it's so you know why do why do we do

01:17:07,990 --> 01:17:14,500
this right why do we do all this work

01:17:11,730 --> 01:17:17,710
basically because we get to play with

01:17:14,500 --> 01:17:19,390
the toys and and this particular project

01:17:17,710 --> 01:17:21,640
will let you play with with pretty much

01:17:19,390 --> 01:17:24,040
all of them service meshes and

01:17:21,640 --> 01:17:24,580
kubernetes and Prometheus and and

01:17:24,040 --> 01:17:29,680
everything

01:17:24,580 --> 01:17:31,030
oh my so by the way the thing that the

01:17:29,680 --> 01:17:33,310
tool is called flagger who's heard of

01:17:31,030 --> 01:17:34,690
flagger before yeah just this matter

01:17:33,310 --> 01:17:37,390
okay so it's cool I'll tell the rest of

01:17:34,690 --> 01:17:41,410
you you guys can relax just chill out

01:17:37,390 --> 01:17:43,690
you know about it so you can read about

01:17:41,410 --> 01:17:47,610
it dogs doc doc swagger that app or it's

01:17:43,690 --> 01:17:52,720
on github it is written by my colleague

01:17:47,610 --> 01:17:55,450
Stephan who is really cool but couldn't

01:17:52,720 --> 01:17:57,520
be here today and so what does it

01:17:55,450 --> 01:18:01,030
actually do it well first of all what's

01:17:57,520 --> 01:18:02,110
a canary deployment the basic idea is

01:18:01,030 --> 01:18:04,360
that you have a new version of your

01:18:02,110 --> 01:18:05,590
software and you're going to roll it out

01:18:04,360 --> 01:18:07,180
but you don't want to roll it out to

01:18:05,590 --> 01:18:10,840
everyone all at once all of your end

01:18:07,180 --> 01:18:12,280
users so the idea of a canary is you

01:18:10,840 --> 01:18:14,080
know you take it down a coal mine and if

01:18:12,280 --> 01:18:17,350
it if it falls off its perch then

01:18:14,080 --> 01:18:20,500
there's the oxygens bad so so that's

01:18:17,350 --> 01:18:21,700
what we're gonna do I have this yeah a

01:18:20,500 --> 01:18:25,180
little diagram here we're going to walk

01:18:21,700 --> 01:18:28,060
through the stages so we start off we

01:18:25,180 --> 01:18:31,690
got version 1 running and version 1 is

01:18:28,060 --> 01:18:33,610
receiving all of the traffic and the

01:18:31,690 --> 01:18:35,500
traffic is coming in through a service

01:18:33,610 --> 01:18:37,930
mesh something that can redirect the

01:18:35,500 --> 01:18:40,830
traffic so we need something like a

01:18:37,930 --> 01:18:43,110
linker D or a nice T or something like

01:18:40,830 --> 01:18:47,190
that in your see what I mean you got all

01:18:43,110 --> 01:18:49,950
the toys so we come along with our

01:18:47,190 --> 01:18:52,860
version 2 of the software and we a

01:18:49,950 --> 01:18:55,170
flagger automatically tells the service

01:18:52,860 --> 01:18:58,620
mesh to send you know like like 5% of

01:18:55,170 --> 01:19:00,780
the traffic to the new version and then

01:18:58,620 --> 01:19:04,980
the the even cooler part is it it

01:19:00,780 --> 01:19:06,570
watches metrics in Prometheus so you

01:19:04,980 --> 01:19:08,700
know this metric can be like error rate

01:19:06,570 --> 01:19:10,590
or latency or something like that or you

01:19:08,700 --> 01:19:13,860
could just write your own prompt ql

01:19:10,590 --> 01:19:16,650
query so it rolls it out it waits a

01:19:13,860 --> 01:19:19,170
waits a few seconds and it watches the

01:19:16,650 --> 01:19:21,180
metric and if it's all good you know

01:19:19,170 --> 01:19:23,190
like we're still getting to hundreds or

01:19:21,180 --> 01:19:26,970
whatever latency still good all good and

01:19:23,190 --> 01:19:28,620
we start to ramp up the percentage of

01:19:26,970 --> 01:19:32,130
the traffic that is going to the new

01:19:28,620 --> 01:19:35,670
version of the software so we come up 5%

01:19:32,130 --> 01:19:37,680
10% 20% ramp it up slowly flaggers doing

01:19:35,670 --> 01:19:41,790
all of this automatically talking to the

01:19:37,680 --> 01:19:43,470
kubernetes api you know we get to a

01:19:41,790 --> 01:19:45,900
certain point we you know we still

01:19:43,470 --> 01:19:48,480
watching the metric if the metric goes

01:19:45,900 --> 01:19:52,200
bad latency goes way up it rolls it back

01:19:48,480 --> 01:19:55,440
we go back to version 1 automatically

01:19:52,200 --> 01:19:58,020
and then you get to debug it if not we

01:19:55,440 --> 01:20:01,160
carry on we set the traffic going to

01:19:58,020 --> 01:20:05,700
version 1 to zero so we basically

01:20:01,160 --> 01:20:08,370
undeployed version 1 and now we're

01:20:05,700 --> 01:20:09,960
completely on version 2 and we we take

01:20:08,370 --> 01:20:13,190
away the kind of temporary ones that we

01:20:09,960 --> 01:20:15,240
put in there for canary purposes so

01:20:13,190 --> 01:20:20,220
that's it that's flagger it's really

01:20:15,240 --> 01:20:29,930
cool thanks very much

01:20:20,220 --> 01:20:29,930

YouTube URL: https://www.youtube.com/watch?v=UTODrbR9yxE


