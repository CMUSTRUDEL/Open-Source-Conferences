Title: PromCon EU 2019: Containing Your Cardinality
Publication date: 2019-12-29
Playlist: PromCon EU 2019
Description: 
	Speaker: Chris Marchbanks

Spending time in the Prometheus community quickly instills a fear of high cardinality metrics; however, cardinality provides immense value for certain workloads. Learn when to use high cardinality metrics, how to reduce unnecessary cardinality, and how to find the sources of excessive cardinality in your data.

Slides: https://promcon.io/2019-munich/slides/containing-your-cardinality.pdf
Captions: 
	00:00:00,650 --> 00:00:08,910
[Music]

00:00:11,460 --> 00:00:16,740
hello everyone Brian said I'm Chris

00:00:13,980 --> 00:00:19,140
talking about something that I've been

00:00:16,740 --> 00:00:20,900
working struggling with possibly a lot

00:00:19,140 --> 00:00:24,560
over the last few months which is

00:00:20,900 --> 00:00:27,090
containing our cardinality in for me yes

00:00:24,560 --> 00:00:31,470
so a bit about me I don't know why this

00:00:27,090 --> 00:00:33,449
is being weird yeah so as Brian said I'm

00:00:31,470 --> 00:00:34,949
engineer at spunk we work on the

00:00:33,449 --> 00:00:37,290
internal observability platform for

00:00:34,949 --> 00:00:39,390
splint cloud providing something similar

00:00:37,290 --> 00:00:42,600
to the last presentation so that was

00:00:39,390 --> 00:00:44,250
super neat to see also the most recent

00:00:42,600 --> 00:00:46,410
newest team member prometheus which I'm

00:00:44,250 --> 00:00:47,730
very excited about just having a great

00:00:46,410 --> 00:00:50,399
time working with this community and

00:00:47,730 --> 00:00:53,070
giving back we do all things

00:00:50,399 --> 00:00:56,219
observability step tracing metrics all

00:00:53,070 --> 00:00:58,500
that stuff it's all super exciting and I

00:00:56,219 --> 00:01:00,410
love uphill skiing on the weekends get

00:00:58,500 --> 00:01:01,710
away from lift lines pretty great I

00:01:00,410 --> 00:01:05,309
recommend it

00:01:01,710 --> 00:01:07,080
get a good workout in so yeah so first

00:01:05,309 --> 00:01:11,789
questions who is concerned about

00:01:07,080 --> 00:01:15,860
cardinality in their system that is a

00:01:11,789 --> 00:01:20,160
lot of people me too

00:01:15,860 --> 00:01:22,310
so next off who has seen this warning I

00:01:20,160 --> 00:01:27,149
wish I could figure out was up with that

00:01:22,310 --> 00:01:29,630
on the Prometheus Docs it's a good one

00:01:27,149 --> 00:01:32,550
it provides some decent information

00:01:29,630 --> 00:01:34,979
don't store things like user IDs email

00:01:32,550 --> 00:01:38,190
addresses unbounded sets of data yeah

00:01:34,979 --> 00:01:39,630
definitely avoid that but what about if

00:01:38,190 --> 00:01:43,770
you're running a multi-tenant system

00:01:39,630 --> 00:01:45,420
what about things like tenon ID is that

00:01:43,770 --> 00:01:48,149
okay is it not there's there's a lot of

00:01:45,420 --> 00:01:49,920
fuzzy areas and even if you do avoid all

00:01:48,149 --> 00:01:52,590
of this I've been in situations where I

00:01:49,920 --> 00:01:54,450
have hundreds of thousands of individual

00:01:52,590 --> 00:01:54,989
series for one metric so what can you do

00:01:54,450 --> 00:01:57,319
about that

00:01:54,989 --> 00:02:03,179
that's what I'm hoping to address today

00:01:57,319 --> 00:02:05,459
so what is cardinality so for this talk

00:02:03,179 --> 00:02:07,020
I really define it as the number of

00:02:05,459 --> 00:02:12,349
series held in Prometheus over a given

00:02:07,020 --> 00:02:16,410
timeframe typically that is at least

00:02:12,349 --> 00:02:18,569
okay at least how long sort of series

00:02:16,410 --> 00:02:21,079
are in memory for that's the at least

00:02:18,569 --> 00:02:23,129
two three hours something like that

00:02:21,079 --> 00:02:24,640
contributes a lot to the memory uses of

00:02:23,129 --> 00:02:26,650
Prometheus it's up to however

00:02:24,640 --> 00:02:29,110
long you commonly do queries for maybe

00:02:26,650 --> 00:02:30,340
you do queries that are a week long you

00:02:29,110 --> 00:02:33,520
should kind of consider how many series

00:02:30,340 --> 00:02:35,440
will be in that week-long query oh and

00:02:33,520 --> 00:02:39,040
then a few different sources of

00:02:35,440 --> 00:02:41,140
cardinality targets series on a target

00:02:39,040 --> 00:02:42,490
and then also just how those change over

00:02:41,140 --> 00:02:44,440
time so if you're running kubernetes

00:02:42,490 --> 00:02:46,720
something like that targets are

00:02:44,440 --> 00:02:48,700
constantly coming up and down that adds

00:02:46,720 --> 00:02:50,350
a lot to cardinality that isn't

00:02:48,700 --> 00:02:52,900
necessarily reflected at any one point

00:02:50,350 --> 00:02:54,850
in time and finally carnality is

00:02:52,900 --> 00:02:57,160
valuable like yes it's kind of a scary

00:02:54,850 --> 00:02:58,540
thing but it's incredibly valuable and

00:02:57,160 --> 00:03:03,269
things you can do with it are amazing

00:02:58,540 --> 00:03:07,090
but yes excessive carnality is expensive

00:03:03,269 --> 00:03:09,790
so how does carnality hurt first more

00:03:07,090 --> 00:03:13,260
resource usage lots and lots of memory

00:03:09,790 --> 00:03:18,580
is a big one CPU to some extent as well

00:03:13,260 --> 00:03:20,580
get really slow queries seconds might

00:03:18,580 --> 00:03:22,870
just timeout and I stopped working

00:03:20,580 --> 00:03:24,940
prometheus might reject them after a

00:03:22,870 --> 00:03:26,440
while hopefully rejects them before you

00:03:24,940 --> 00:03:28,510
Ahmir Prometheus which has been pretty

00:03:26,440 --> 00:03:32,739
good recently so thank you for all the

00:03:28,510 --> 00:03:34,660
work there next one just long startup

00:03:32,739 --> 00:03:37,120
time to this Prometheus loads so all

00:03:34,660 --> 00:03:38,829
that in memory data every time you

00:03:37,120 --> 00:03:41,110
restart Prometheus has to be reloaded

00:03:38,829 --> 00:03:43,690
from the right ahead log and I can take

00:03:41,110 --> 00:03:45,250
1520 plus minutes I've talked to a few

00:03:43,690 --> 00:03:47,470
people already at this conference in

00:03:45,250 --> 00:03:48,700
this very similar condition of they're

00:03:47,470 --> 00:03:51,489
just sitting there waiting and then you

00:03:48,700 --> 00:03:52,780
get those big gaps on a on your graphs

00:03:51,489 --> 00:03:54,670
if you don't have some sort of

00:03:52,780 --> 00:03:58,360
deduplication strategy in front of it

00:03:54,670 --> 00:04:00,760
and and finally in a lot of carnality

00:03:58,360 --> 00:04:04,720
issues even if you're running a high

00:04:00,760 --> 00:04:06,640
availability Prometheus since there's

00:04:04,720 --> 00:04:08,350
usually you run to scraping the same

00:04:06,640 --> 00:04:10,810
data they're scraping the same data that

00:04:08,350 --> 00:04:13,209
means similar memory usage if you're

00:04:10,810 --> 00:04:15,160
gonna run out of memory in one probably

00:04:13,209 --> 00:04:22,169
gonna lose the second one at the exact

00:04:15,160 --> 00:04:22,169
same time and that really sucks so oh

00:04:23,400 --> 00:04:29,590
maybe maybe

00:04:27,010 --> 00:04:31,900
how much is too much so this is just

00:04:29,590 --> 00:04:33,910
some things for that I have found in our

00:04:31,900 --> 00:04:36,490
clusters your data is going to vary most

00:04:33,910 --> 00:04:41,110
likely right leaving fullscreen and see

00:04:36,490 --> 00:04:45,250
if that helps no okay

00:04:41,110 --> 00:04:49,270
so for a single metric the idea is about

00:04:45,250 --> 00:04:51,670
a hundred thousand series it's sort of a

00:04:49,270 --> 00:04:53,170
good rule of thumb for some of our class

00:04:51,670 --> 00:04:55,770
are just doing a count query pretty

00:04:53,170 --> 00:04:57,990
simple 100,000 series about 1.5 seconds

00:04:55,770 --> 00:05:01,990
kind of sucks to load a lot of those

00:04:57,990 --> 00:05:02,500
dashboard panels but you can do it it's

00:05:01,990 --> 00:05:05,050
okay

00:05:02,500 --> 00:05:07,180
but by 200 series like I'm seeing

00:05:05,050 --> 00:05:10,930
five-second response times if I go too

00:05:07,180 --> 00:05:13,990
much above that it's just I start

00:05:10,930 --> 00:05:15,490
getting timeouts stuff doesn't work so

00:05:13,990 --> 00:05:18,010
those are some rules of thumb for a

00:05:15,490 --> 00:05:22,120
single single metric and then as far as

00:05:18,010 --> 00:05:23,530
what can a Prometheus scale - for a lot

00:05:22,120 --> 00:05:26,770
of the systems I run I can get up to

00:05:23,530 --> 00:05:29,530
about 10 million series in memory you

00:05:26,770 --> 00:05:31,600
can look at there's some good metrics to

00:05:29,530 --> 00:05:33,340
see that and you can go above that

00:05:31,600 --> 00:05:35,350
depending on how well-behaved your data

00:05:33,340 --> 00:05:37,480
is but you start getting into these are

00:05:35,350 --> 00:05:39,310
pretty big Prometheus instances as I was

00:05:37,480 --> 00:05:43,210
saying 15 plus and minutes to start up

00:05:39,310 --> 00:05:47,050
I'm exceeding hundred gigabytes of heap

00:05:43,210 --> 00:05:50,110
basically constantly and if anything

00:05:47,050 --> 00:05:52,480
goes wrong some maybe not even in your

00:05:50,110 --> 00:05:53,470
code but some third party has some weird

00:05:52,480 --> 00:05:56,170
carnality issue

00:05:53,470 --> 00:05:59,140
I found that it'll quickly just escalate

00:05:56,170 --> 00:06:01,150
and Prometheus is gone past sort of this

00:05:59,140 --> 00:06:06,250
point there's not a whole lot of

00:06:01,150 --> 00:06:07,720
headroom anymore and that's doing this

00:06:06,250 --> 00:06:10,330
sort of multiplicative nature of

00:06:07,720 --> 00:06:11,440
cardinality which here's a good here's

00:06:10,330 --> 00:06:14,950
an example of what I mean by

00:06:11,440 --> 00:06:17,970
multiplicative nature so regular I want

00:06:14,950 --> 00:06:20,110
to know the latency hundred instances

00:06:17,970 --> 00:06:22,450
Prezi they're running hundreds of

00:06:20,110 --> 00:06:25,000
microservices it seems reasonable ten

00:06:22,450 --> 00:06:27,130
buckets on your histogram it's a normal

00:06:25,000 --> 00:06:29,170
amount of buckets times ten in points

00:06:27,130 --> 00:06:31,690
it's good to know how your different end

00:06:29,170 --> 00:06:33,370
points and routes are performing cool

00:06:31,690 --> 00:06:35,410
you're at 10000 series

00:06:33,370 --> 00:06:37,300
totally fine Prometheus handles that

00:06:35,410 --> 00:06:39,370
great if you add in ten different

00:06:37,300 --> 00:06:40,630
response codes it's useful to know if

00:06:39,370 --> 00:06:43,000
you're returning 200

00:06:40,630 --> 00:06:44,530
or fours four ones etc you're at a

00:06:43,000 --> 00:06:46,180
hundred thousand series already and

00:06:44,530 --> 00:06:49,120
that's sort of that that limit it's a

00:06:46,180 --> 00:06:50,530
little little dangerous there and then

00:06:49,120 --> 00:06:52,990
if you want to keep splitting up by

00:06:50,530 --> 00:06:54,930
something like hey yeah it's gets and

00:06:52,990 --> 00:06:57,580
deletes and puts how are they performing

00:06:54,930 --> 00:06:59,560
you're already in the like four hundred

00:06:57,580 --> 00:07:01,900
thousand series if you try and add just

00:06:59,560 --> 00:07:06,220
ten customer tenants you're in forty

00:07:01,900 --> 00:07:09,880
million like it it's it gets bad pretty

00:07:06,220 --> 00:07:11,860
fast at that point and I've not ever

00:07:09,880 --> 00:07:12,970
tried to push 40 million series and

00:07:11,860 --> 00:07:16,650
Prometheus I don't know what would

00:07:12,970 --> 00:07:16,650
happen they're probably not good things

00:07:17,160 --> 00:07:21,400
and then finally before I get into some

00:07:19,510 --> 00:07:23,500
tips of how you can actually prevent

00:07:21,400 --> 00:07:26,140
some of this there's two types of data I

00:07:23,500 --> 00:07:29,830
commonly see in the Prometheus instances

00:07:26,140 --> 00:07:31,960
I run one is operational data so we've

00:07:29,830 --> 00:07:33,520
heard about SLO s recent a fair amount

00:07:31,960 --> 00:07:34,810
at this conference actually you should

00:07:33,520 --> 00:07:38,530
have them surface level objectives

00:07:34,810 --> 00:07:40,060
they're great but and good to know when

00:07:38,530 --> 00:07:43,420
you're meeting those and alert if you're

00:07:40,060 --> 00:07:45,060
not and the second thing is I really

00:07:43,420 --> 00:07:47,620
like this operational a data pointing to

00:07:45,060 --> 00:07:50,350
where the problem is in your system very

00:07:47,620 --> 00:07:51,550
quickly and also providing evidence of

00:07:50,350 --> 00:07:54,550
the things that are working properly

00:07:51,550 --> 00:07:56,890
that you can rule out from this is where

00:07:54,550 --> 00:07:59,590
my issue is and really operational data

00:07:56,890 --> 00:08:01,570
has to be very fast and very reliable

00:07:59,590 --> 00:08:03,730
you can't just be missing alerts because

00:08:01,570 --> 00:08:05,560
they're delayed by fifteen minutes and

00:08:03,730 --> 00:08:07,630
you get customer support cases before

00:08:05,560 --> 00:08:10,930
your alert goes off that's not a fun

00:08:07,630 --> 00:08:12,970
situation but then I also see many teams

00:08:10,930 --> 00:08:16,260
trying to put this more telemetry type

00:08:12,970 --> 00:08:18,190
data into Prometheus and that is

00:08:16,260 --> 00:08:21,670
investigating things like how many users

00:08:18,190 --> 00:08:24,730
are using this endpoint and or tenants

00:08:21,670 --> 00:08:26,260
or things like that and this data is

00:08:24,730 --> 00:08:27,820
very powerful it's really nice to be

00:08:26,260 --> 00:08:30,670
able slice and dice all this data into

00:08:27,820 --> 00:08:32,050
oh you're you they're all using this

00:08:30,670 --> 00:08:34,900
this customers being this type of

00:08:32,050 --> 00:08:37,360
performance great but usually that is

00:08:34,900 --> 00:08:39,130
someone asking about these ad hoc

00:08:37,360 --> 00:08:40,750
queries or they can wait a few minutes

00:08:39,130 --> 00:08:43,599
it's okay it doesn't have to be as fast

00:08:40,750 --> 00:08:45,700
and consider not putting that data in

00:08:43,599 --> 00:08:48,310
prometheus is a big one prometheus is

00:08:45,700 --> 00:08:51,460
really good for the operational data but

00:08:48,310 --> 00:08:53,250
telemetry data a lot of time better sent

00:08:51,460 --> 00:08:55,620
to an event system

00:08:53,250 --> 00:08:57,839
coolly I work a Splunk I can put it in

00:08:55,620 --> 00:09:02,160
Splunk but whatever vent system you want

00:08:57,839 --> 00:09:06,449
works great so yeah tips how do you

00:09:02,160 --> 00:09:11,639
actually contain your carnality there's

00:09:06,449 --> 00:09:14,250
mine there we go it's tip zero first off

00:09:11,639 --> 00:09:16,139
I just like really being pragmatic these

00:09:14,250 --> 00:09:17,850
are all suggestions something else make

00:09:16,139 --> 00:09:19,230
make sense in your systems everyone

00:09:17,850 --> 00:09:21,689
you're the expert of your own systems

00:09:19,230 --> 00:09:24,360
there and a few high cardinality series

00:09:21,689 --> 00:09:25,980
are okay like you have that up to like

00:09:24,360 --> 00:09:28,079
10 million that gets you a pretty long

00:09:25,980 --> 00:09:31,139
way so just think about what it is

00:09:28,079 --> 00:09:32,639
got barred here I will actually you know

00:09:31,139 --> 00:09:34,379
do some work rather than just fighting

00:09:32,639 --> 00:09:38,329
carnality all the time I have to repeat

00:09:34,379 --> 00:09:41,699
that to myself occasionally so but yeah

00:09:38,329 --> 00:09:43,500
first real tip I really like thinking

00:09:41,699 --> 00:09:45,170
about how any label is used this kind of

00:09:43,500 --> 00:09:48,750
gets back to that operational verse

00:09:45,170 --> 00:09:49,829
telemetry type data if I'm getting woken

00:09:48,750 --> 00:09:50,970
up it's 2:00 in the morning

00:09:49,829 --> 00:09:54,839
well first off how many people are on

00:09:50,970 --> 00:09:56,339
call here also a fair amount of people

00:09:54,839 --> 00:09:58,709
if I get woken up at 2:00 in the morning

00:09:56,339 --> 00:10:01,559
and I get a dashboard that has a bunch

00:09:58,709 --> 00:10:06,660
of labels that don't help me not super

00:10:01,559 --> 00:10:08,459
super happy about that um so at that

00:10:06,660 --> 00:10:09,899
point put it in a bin system put it

00:10:08,459 --> 00:10:12,870
somewhere else keep it out of those

00:10:09,899 --> 00:10:15,420
court those core operational metrics is

00:10:12,870 --> 00:10:17,430
there a predictable number comes with

00:10:15,420 --> 00:10:19,379
that Prometheus warning if you don't

00:10:17,430 --> 00:10:21,990
know how many numbers are going to be in

00:10:19,379 --> 00:10:24,360
this label if it's something like raw

00:10:21,990 --> 00:10:27,480
error messages which I have had show up

00:10:24,360 --> 00:10:28,399
in Prometheus before don't do that it

00:10:27,480 --> 00:10:35,220
hurts a lot

00:10:28,399 --> 00:10:37,079
yeah good first way next up something I

00:10:35,220 --> 00:10:41,149
find very powerful is just having your

00:10:37,079 --> 00:10:43,620
metrics really model your system so

00:10:41,149 --> 00:10:45,899
common little architecture example you

00:10:43,620 --> 00:10:48,480
have some sort of application it's

00:10:45,899 --> 00:10:49,860
writing to some sort of cue whatever and

00:10:48,480 --> 00:10:53,220
you're starting it by tenant ID

00:10:49,860 --> 00:10:55,680
things like that and say you know you

00:10:53,220 --> 00:10:58,230
have one of those shards get backed up

00:10:55,680 --> 00:11:00,509
to you do you want to have those

00:10:58,230 --> 00:11:04,470
pertinent metrics a show oh these five

00:11:00,509 --> 00:11:06,690
percent of users are bad are performing

00:11:04,470 --> 00:11:09,660
slowly or do you want to see

00:11:06,690 --> 00:11:11,490
hey this shard is backed up maybe you

00:11:09,660 --> 00:11:14,850
need to rechart or however your queueing

00:11:11,490 --> 00:11:17,070
system works scale it up whatever that

00:11:14,850 --> 00:11:20,280
is I want to see the ladder I don't want

00:11:17,070 --> 00:11:23,730
to start by seeing oh okay these

00:11:20,280 --> 00:11:26,250
customers are bad what now I can maybe

00:11:23,730 --> 00:11:28,740
hypothesize that a shot a shard is

00:11:26,250 --> 00:11:31,200
backing up but it could be maybe those

00:11:28,740 --> 00:11:32,580
customers are just sending weird data it

00:11:31,200 --> 00:11:34,950
could it could be a whole lot of things

00:11:32,580 --> 00:11:37,320
I'm gonna have to go look somewhere it's

00:11:34,950 --> 00:11:39,390
just much much faster to have it model

00:11:37,320 --> 00:11:47,780
your system have your metrics modeling

00:11:39,390 --> 00:11:50,340
your system and then third thing Oh

00:11:47,780 --> 00:11:52,590
using multiple metrics is very very

00:11:50,340 --> 00:11:55,470
powerful so we're back to our example of

00:11:52,590 --> 00:11:57,570
this histogram from earlier 100

00:11:55,470 --> 00:12:00,630
instances 10 buckets 10 response codes

00:11:57,570 --> 00:12:04,050
10 raps 100,000 series if you split that

00:12:00,630 --> 00:12:06,180
into two metrics instead one metric that

00:12:04,050 --> 00:12:08,700
really focuses on the latency of your

00:12:06,180 --> 00:12:10,650
different routes that's 10,000 series

00:12:08,700 --> 00:12:13,470
that's 100 instances 10 buckets 10

00:12:10,650 --> 00:12:16,230
routes in a second metric that is just

00:12:13,470 --> 00:12:17,490
the requests and the response codes that

00:12:16,230 --> 00:12:20,010
still gives you all the same

00:12:17,490 --> 00:12:23,880
availability information you had another

00:12:20,010 --> 00:12:27,690
10,000 20,000 total series 5x reduction

00:12:23,880 --> 00:12:30,480
in your cardinality the only thing you

00:12:27,690 --> 00:12:35,790
lose here is you can no longer figure

00:12:30,480 --> 00:12:38,490
out your response time by by status code

00:12:35,790 --> 00:12:41,610
I have almost never tried to do that

00:12:38,490 --> 00:12:44,820
query in reality Tom's looking at me

00:12:41,610 --> 00:12:46,980
with puppy-dog eyes go use some access

00:12:44,820 --> 00:12:48,270
logs or something for that it's very

00:12:46,980 --> 00:12:52,260
rare that you need that in an

00:12:48,270 --> 00:12:54,500
operational thing and you save 5 X so

00:12:52,260 --> 00:12:56,520
pretty powerful there it's not just HTTP

00:12:54,500 --> 00:12:57,960
that you can do this for there's a

00:12:56,520 --> 00:13:00,060
variety that just you don't have to put

00:12:57,960 --> 00:13:01,740
every single every single way you can

00:13:00,060 --> 00:13:03,960
slice and dice a metric onto the same

00:13:01,740 --> 00:13:09,960
metric split them out a little works

00:13:03,960 --> 00:13:12,300
nicely and then finally if if you find

00:13:09,960 --> 00:13:16,080
some value in your cardinality you have

00:13:12,300 --> 00:13:17,670
those tenant metrics that maybe you do

00:13:16,080 --> 00:13:19,740
have a system that one tenant can

00:13:17,670 --> 00:13:20,670
perform very differently from all other

00:13:19,740 --> 00:13:22,440
tenants

00:13:20,670 --> 00:13:24,180
we actually do we have some of that

00:13:22,440 --> 00:13:25,830
we're like you can define custom

00:13:24,180 --> 00:13:28,740
pipelines and things inside of spawn

00:13:25,830 --> 00:13:30,030
cloud and one tenant can have massively

00:13:28,740 --> 00:13:34,830
different performance and we need to

00:13:30,030 --> 00:13:36,390
know that at that point try to limit how

00:13:34,830 --> 00:13:38,250
many at least try and limit how many

00:13:36,390 --> 00:13:42,120
things how many high carnelli metrics

00:13:38,250 --> 00:13:44,040
you have so maybe keep them just on your

00:13:42,120 --> 00:13:45,990
high-level API calls rather than on

00:13:44,040 --> 00:13:48,780
every single metric in that service and

00:13:45,990 --> 00:13:50,460
then at some point maybe even consider

00:13:48,780 --> 00:13:53,100
breaking it out into separate Prometheus

00:13:50,460 --> 00:13:54,810
instances there's functional sharding

00:13:53,100 --> 00:13:57,170
Federation if you're using long term

00:13:54,810 --> 00:14:00,570
stores maybe that becomes even easier

00:13:57,170 --> 00:14:02,130
but you can just have one Prometheus

00:14:00,570 --> 00:14:04,470
still doing those local alert

00:14:02,130 --> 00:14:07,770
evaluations on this high cardinality

00:14:04,470 --> 00:14:09,810
data and then only Fedder eight into a

00:14:07,770 --> 00:14:11,520
different into the Prometheus that

00:14:09,810 --> 00:14:13,920
you're graphing from or things like that

00:14:11,520 --> 00:14:16,110
the aggregations just have a couple

00:14:13,920 --> 00:14:19,770
layers here at least even if something

00:14:16,110 --> 00:14:21,240
goes horribly wrong with that with that

00:14:19,770 --> 00:14:22,890
risky Prometheus you still have your

00:14:21,240 --> 00:14:25,680
monitoring for the rest of your system

00:14:22,890 --> 00:14:28,620
it's not all going to be down at the

00:14:25,680 --> 00:14:34,980
same time you really get to avoid those

00:14:28,620 --> 00:14:36,660
awful situations so yeah finally some

00:14:34,980 --> 00:14:38,420
tools how do you actually diagnose

00:14:36,660 --> 00:14:42,450
cardinality

00:14:38,420 --> 00:14:44,730
well first off Prometheus to 14 actually

00:14:42,450 --> 00:14:46,350
so that's release candidate out now to

00:14:44,730 --> 00:14:48,000
come check these out what you see on

00:14:46,350 --> 00:14:50,070
your right is actually displayed in the

00:14:48,000 --> 00:14:52,560
UI there's a variety of different ways

00:14:50,070 --> 00:14:55,350
we can slice and dice or the top 10 or

00:14:52,560 --> 00:15:00,090
so carnality metrics and this has been

00:14:55,350 --> 00:15:01,860
very very valuable for for us so this is

00:15:00,090 --> 00:15:04,290
an example as we provide the highest

00:15:01,860 --> 00:15:07,680
cardinality metric names this example is

00:15:04,290 --> 00:15:10,650
actually from the prom bench and you can

00:15:07,680 --> 00:15:13,860
see there is a single that histogram has

00:15:10,650 --> 00:15:16,650
eight point seven million series

00:15:13,860 --> 00:15:19,140
I tried querying that bucketed you can't

00:15:16,650 --> 00:15:20,640
do anything there it does not work but

00:15:19,140 --> 00:15:23,640
that can be very valuable there's other

00:15:20,640 --> 00:15:26,400
ones of these such as the cumulative

00:15:23,640 --> 00:15:29,040
size of all your label values which as I

00:15:26,400 --> 00:15:30,810
said earlier I had a issue where error

00:15:29,040 --> 00:15:34,889
labels were being put into that full

00:15:30,810 --> 00:15:38,879
error messages were putting into my my

00:15:34,889 --> 00:15:42,929
these labels and it exploded very badly

00:15:38,879 --> 00:15:45,299
um this was actually the only place I

00:15:42,929 --> 00:15:46,949
could find that was us another one of

00:15:45,299 --> 00:15:48,809
these you eyes where you will actually

00:15:46,949 --> 00:15:50,850
sum up all of that because it didn't

00:15:48,809 --> 00:15:54,239
even show up in like my top 20 highest

00:15:50,850 --> 00:15:58,350
carnality metrics it was completely

00:15:54,239 --> 00:15:59,790
different um so yeah check this out if

00:15:58,350 --> 00:16:03,959
you want more added here find it useful

00:15:59,790 --> 00:16:06,839
let us know or contribute to it and then

00:16:03,959 --> 00:16:09,509
just a few useful queries um things that

00:16:06,839 --> 00:16:14,009
really help tracking churn is scrape

00:16:09,509 --> 00:16:15,540
series added just taking a look at which

00:16:14,009 --> 00:16:17,369
jobs or adding the most series

00:16:15,540 --> 00:16:19,919
constantly that's been very helpful for

00:16:17,369 --> 00:16:21,779
us another one just scrape sample

00:16:19,919 --> 00:16:24,899
scraped gives you a good idea of which

00:16:21,779 --> 00:16:27,839
jobs are just contributing the most to

00:16:24,899 --> 00:16:31,169
carnality and final that Prometheus TSP

00:16:27,839 --> 00:16:32,879
symbols table size bytes ask that's what

00:16:31,169 --> 00:16:37,649
really exploded for us in that air label

00:16:32,879 --> 00:16:41,429
thing incident it was I I think it got

00:16:37,649 --> 00:16:43,879
to 60 gigabytes uh and you know ideally

00:16:41,429 --> 00:16:47,119
this would be like a few hundred Meg's

00:16:43,879 --> 00:16:52,169
so that's a decent one to keep an eye on

00:16:47,119 --> 00:16:53,669
yeah finally thank you very much for

00:16:52,169 --> 00:16:56,160
your time hope you learned something

00:16:53,669 --> 00:16:58,649
about carnality if you want to hear some

00:16:56,160 --> 00:17:00,360
of the great great disasters Brian Boram

00:16:58,649 --> 00:17:02,790
and I will be talking in two weeks at

00:17:00,360 --> 00:17:05,010
cube con about when things go really

00:17:02,790 --> 00:17:06,800
really wrong so look forward to that and

00:17:05,010 --> 00:17:14,960
questions

00:17:06,800 --> 00:17:14,960
[Applause]

00:17:22,220 --> 00:17:31,520
hmm testing there we go still a box I

00:17:28,549 --> 00:17:33,700
guess so what questions we have for

00:17:31,520 --> 00:17:33,700
Chris

00:17:42,019 --> 00:17:49,769
hi thanks for the talk

00:17:45,470 --> 00:17:52,470
in our company we all will face a lot

00:17:49,769 --> 00:17:53,309
the issue that for example that in

00:17:52,470 --> 00:17:57,629
histograms

00:17:53,309 --> 00:18:00,779
there are buckets are adjusted sometimes

00:17:57,629 --> 00:18:03,649
people who start to work with primitives

00:18:00,779 --> 00:18:06,240
they adjust the bucket in histograms

00:18:03,649 --> 00:18:10,230
incorrectly so do you have any I don't

00:18:06,240 --> 00:18:13,110
know some technique or some query how to

00:18:10,230 --> 00:18:16,350
find out how to better adjust the

00:18:13,110 --> 00:18:19,950
histogram buckets histogram buckets can

00:18:16,350 --> 00:18:23,279
be kind of tricky I'd really try to find

00:18:19,950 --> 00:18:25,080
about 10 or so that makes sense for the

00:18:23,279 --> 00:18:27,360
system and that might depend a lot on

00:18:25,080 --> 00:18:30,090
what you're trying to measure something

00:18:27,360 --> 00:18:32,490
like a DNS lookup needs to be much much

00:18:30,090 --> 00:18:34,619
much faster than maybe how long

00:18:32,490 --> 00:18:37,019
something is sitting in a queue for so

00:18:34,619 --> 00:18:39,059
that ends up being kind of a personal

00:18:37,019 --> 00:18:40,889
choice of what is valuable which makes

00:18:39,059 --> 00:18:42,059
it tricky as you said because people are

00:18:40,889 --> 00:18:46,169
gonna be playing around with these

00:18:42,059 --> 00:18:47,789
seeing what works I would recommend just

00:18:46,169 --> 00:18:50,549
generally for a company practice if you

00:18:47,789 --> 00:18:55,529
can define some sort of SLO something

00:18:50,549 --> 00:18:57,269
saying we need 99% of things going

00:18:55,529 --> 00:19:00,299
through this system to be returned in

00:18:57,269 --> 00:19:01,470
five seconds that should be it that

00:19:00,299 --> 00:19:03,539
should be the grouping for your

00:19:01,470 --> 00:19:04,710
histogram and if you kind of and if you

00:19:03,539 --> 00:19:06,840
can start having those sorts of

00:19:04,710 --> 00:19:09,629
discussions that really narrows down to

00:19:06,840 --> 00:19:11,940
where your histogram should lie rather

00:19:09,629 --> 00:19:14,490
than because if you have this SLO of

00:19:11,940 --> 00:19:17,460
five seconds there's not necessarily as

00:19:14,490 --> 00:19:19,619
much of a need to also measure three

00:19:17,460 --> 00:19:21,299
minutes because you're way out of SLO at

00:19:19,619 --> 00:19:25,320
that point and it doesn't matter you

00:19:21,299 --> 00:19:28,279
haven't you have an issue that has opted

00:19:25,320 --> 00:19:28,279
of some enzymes that help

00:19:34,750 --> 00:19:41,180
okay nice talk thank you do you have any

00:19:38,330 --> 00:19:43,400
ultimate automated mechanism for

00:19:41,180 --> 00:19:46,270
detecting sudden increase in cardinality

00:19:43,400 --> 00:19:48,260
do you watch for that and can you maybe

00:19:46,270 --> 00:19:51,350
recommend something for further

00:19:48,260 --> 00:19:54,490
community yeah so there's a couple of

00:19:51,350 --> 00:19:58,070
queries over here might depend a bit on

00:19:54,490 --> 00:20:00,770
what your system is um we have written

00:19:58,070 --> 00:20:02,810
some alerts around that um those are

00:20:00,770 --> 00:20:06,110
pretty useful for us of just okay if

00:20:02,810 --> 00:20:07,610
symbol size is usually in our systems

00:20:06,110 --> 00:20:09,830
you know I said it should be you know

00:20:07,610 --> 00:20:11,720
yeah and hundreds of megabytes in our

00:20:09,830 --> 00:20:15,530
systems it's like five gigs it's okay

00:20:11,720 --> 00:20:17,690
we've got some problems still oh so but

00:20:15,530 --> 00:20:19,280
if we see that starting to you know be

00:20:17,690 --> 00:20:22,520
2x higher than where we think it should

00:20:19,280 --> 00:20:24,290
be we've got some alerts around that

00:20:22,520 --> 00:20:26,870
sort of thing and usually these are more

00:20:24,290 --> 00:20:29,030
warning levels I don't want to be woken

00:20:26,870 --> 00:20:31,370
up for that but I will go investigate

00:20:29,030 --> 00:20:34,900
and talk to teams we are exploring some

00:20:31,370 --> 00:20:38,420
ideas around is it actually possible to

00:20:34,900 --> 00:20:40,040
maybe provide limits on teams

00:20:38,420 --> 00:20:41,840
specifically make it a bit more like

00:20:40,040 --> 00:20:44,330
okay this is what you get out of the box

00:20:41,840 --> 00:20:46,370
and then if you go above this you either

00:20:44,330 --> 00:20:50,330
get some sort of alert that we're going

00:20:46,370 --> 00:20:51,920
to start truncating soon things like

00:20:50,330 --> 00:20:54,950
that can sort of help um there's a

00:20:51,920 --> 00:20:57,050
Prometheus config where you can actually

00:20:54,950 --> 00:20:58,510
limit the size the number of series you

00:20:57,050 --> 00:21:01,220
scrape from single targets as well

00:20:58,510 --> 00:21:08,360
that can be helpful for just preventing

00:21:01,220 --> 00:21:11,090
if they'll scrapes yes note the initial

00:21:08,360 --> 00:21:14,390
issue that caused that metric to be

00:21:11,090 --> 00:21:16,010
added was only like two tree gigs and

00:21:14,390 --> 00:21:19,280
there's also the TST be analyzed tool

00:21:16,010 --> 00:21:21,140
which is handy yeah yeah yeah this is

00:21:19,280 --> 00:21:23,600
similar to what I'm showing on the right

00:21:21,140 --> 00:21:27,760
here but for older persistent blocks

00:21:23,600 --> 00:21:31,180
rather than just the data very handy

00:21:27,760 --> 00:21:31,180
next question

00:21:36,130 --> 00:21:40,720
well not truly a question but what we've

00:21:38,930 --> 00:21:44,540
done is that we have a script which

00:21:40,720 --> 00:21:46,970
analyzed which queries primitives at the

00:21:44,540 --> 00:21:49,580
API and gets all the levels and the

00:21:46,970 --> 00:21:55,670
number of hole for each levels so if

00:21:49,580 --> 00:21:58,520
something blows out we we noticed I'm

00:21:55,670 --> 00:22:01,550
sure it was a question in there it's

00:21:58,520 --> 00:22:04,070
good good advice for the community yeah

00:22:01,550 --> 00:22:06,080
number series touched at Inc that's

00:22:04,070 --> 00:22:08,750
something we expose should expose and

00:22:06,080 --> 00:22:11,720
from ql yeah there's just a head number

00:22:08,750 --> 00:22:13,060
of series no I mean actual prompt qo

00:22:11,720 --> 00:22:16,160
itself when it's executing the query

00:22:13,060 --> 00:22:18,790
number series touched is yeah I think

00:22:16,160 --> 00:22:18,790
it's in there somewhere

00:22:19,060 --> 00:22:28,470
any other questions three two one thank

00:22:27,230 --> 00:22:31,650
you Chris

00:22:28,470 --> 00:22:35,970
[Applause]

00:22:31,650 --> 00:22:36,290
[Music]

00:22:35,970 --> 00:22:41,369
you

00:22:36,290 --> 00:22:41,369

YouTube URL: https://www.youtube.com/watch?v=49BGvC1coG4


