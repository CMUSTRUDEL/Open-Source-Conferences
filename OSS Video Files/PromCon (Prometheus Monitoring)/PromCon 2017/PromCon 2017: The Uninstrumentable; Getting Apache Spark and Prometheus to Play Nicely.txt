Title: PromCon 2017: The Uninstrumentable; Getting Apache Spark and Prometheus to Play Nicely
Publication date: 2017-09-04
Playlist: PromCon 2017
Description: 
	* Abstract:

Instrumenting your code with Prometheus is simple and easy. Or so we thought until we tried to instrument our Python application running under Apache Spark… The distributed nature of Spark presents some interesting challenges when it comes to instrumenting your code effectively, for example a lack of global state, transient processes and no control over the execution profile.

We’ll talk about our myriad failed attempts at instrumenting under Spark and our journey to finally getting something working effectively, without DOSing Prometheus with millions of time series! :)

* Speaker biography:

Dan Rathbone:
Dan is co-founder and Technical Director of Infinity Works, a 100-strong consultancy and software house based out of Leeds and London. Over the years Dan has held many varied roles focusing on areas from infrastructure to front end development and most things in between. Drawing on a broad skill set Dan now builds and operates high-scale and high-performance systems for Infinity Works’ clients.

Joe Stringer:
Joe recently joined Infinity Works and is currently working alongside Dan on a large health data processing system. From an infrastructure and operations background, Joe has worked towards bringing modern devops practises to businesses and utilising cloud providers. Most of Joe's work lately has been implementing Prometheus across the various applications his team are responsible for.

* Slides:

https://promcon.io/2017-munich/slides/the-uninstrumentable-getting-apache-spark-and-prometheus-to-play-nicely.pdf

* PromCon website:

https://promcon.io/
Captions: 
	00:00:00,380 --> 00:00:16,800
[Music]

00:00:13,130 --> 00:00:18,800
all right so our next talk is gonna have

00:00:16,800 --> 00:00:22,470
a little bit of a personality disorder

00:00:18,800 --> 00:00:24,300
well because there's two speakers I'd

00:00:22,470 --> 00:00:26,519
like to welcome Joe Rathbone and Joe

00:00:24,300 --> 00:00:30,150
stringer from infinite works a little

00:00:26,519 --> 00:00:33,180
bit of story so many many years ago I

00:00:30,150 --> 00:00:35,579
was working for a web company and they

00:00:33,180 --> 00:00:37,440
ran a very popular web portal that had

00:00:35,579 --> 00:00:41,399
stuff like news articles and sports

00:00:37,440 --> 00:00:43,829
articles the platforming ran on was was

00:00:41,399 --> 00:00:47,789
at capacity and it would regularly

00:00:43,829 --> 00:00:49,260
suffer service outages and this was

00:00:47,789 --> 00:00:50,850
particularly bad when there was a really

00:00:49,260 --> 00:00:51,989
interesting football match on because

00:00:50,850 --> 00:00:53,280
people would log in and try to look at

00:00:51,989 --> 00:00:56,520
all the live scores as they were

00:00:53,280 --> 00:01:00,180
happening so one evening

00:00:56,520 --> 00:01:02,670
muggins here is on call trying to

00:01:00,180 --> 00:01:05,100
support this platform so I'm sat at home

00:01:02,670 --> 00:01:07,560
in my apartment trying to eat my dinner

00:01:05,100 --> 00:01:11,580
which is the expertly photoshopped

00:01:07,560 --> 00:01:14,340
meatballs so I've kind of sit there

00:01:11,580 --> 00:01:15,840
eating dinner and all I've got to really

00:01:14,340 --> 00:01:19,290
gauge whether or not this system is on

00:01:15,840 --> 00:01:21,960
fire or not is a captai graph that

00:01:19,290 --> 00:01:24,479
updates once every five minutes which

00:01:21,960 --> 00:01:28,140
shows me the CPU usage aggregated across

00:01:24,479 --> 00:01:29,939
the the web cluster so I'm sat there

00:01:28,140 --> 00:01:31,560
munching away occasionally glance up

00:01:29,939 --> 00:01:33,090
with this graph to see but how

00:01:31,560 --> 00:01:36,210
perilously close to a hundred percent

00:01:33,090 --> 00:01:38,280
are we right now and I think I think I

00:01:36,210 --> 00:01:40,710
like we just just about scraped by with

00:01:38,280 --> 00:01:43,560
90 something percent so it's quite quite

00:01:40,710 --> 00:01:46,710
hairy evening so yeah not a great day

00:01:43,560 --> 00:01:49,579
for me yeah so that's a cool story down

00:01:46,710 --> 00:01:52,920
but why is it relevant to us here today

00:01:49,579 --> 00:01:55,829
that's a good question joke so it's it's

00:01:52,920 --> 00:01:58,049
relevant it's relevant because I think

00:01:55,829 --> 00:01:59,850
the point here is I had no visibility of

00:01:58,049 --> 00:02:01,829
what was going on I knew whether the

00:01:59,850 --> 00:02:03,090
system was on fire or if it wasn't on

00:02:01,829 --> 00:02:04,829
fire that was kind of it

00:02:03,090 --> 00:02:07,079
there was no fine-grained real-time

00:02:04,829 --> 00:02:08,789
monitoring it so it's about really the

00:02:07,079 --> 00:02:10,140
importance of that may not have that so

00:02:08,789 --> 00:02:11,760
I can see what's going on akka

00:02:10,140 --> 00:02:12,500
proactively even fix a problem before it

00:02:11,760 --> 00:02:13,880
happens

00:02:12,500 --> 00:02:14,870
yes so that's no the crooks of what

00:02:13,880 --> 00:02:17,390
we're talking about today is kind of

00:02:14,870 --> 00:02:19,220
fine-grain real-time monitoring yep

00:02:17,390 --> 00:02:21,440
absolutely so we're gonna talk to you a

00:02:19,220 --> 00:02:23,150
bit about our data processing system

00:02:21,440 --> 00:02:26,660
we're on at the minute it runs apache

00:02:23,150 --> 00:02:28,280
spark the pie spark - so we're gonna

00:02:26,660 --> 00:02:30,740
talk a bit about their failed attempts

00:02:28,280 --> 00:02:32,690
we had on getting it instrumented some

00:02:30,740 --> 00:02:35,300
of the crazy ideas we tried to get the

00:02:32,690 --> 00:02:36,530
instrumentation working and then in the

00:02:35,300 --> 00:02:38,090
solution we have at the minute that's

00:02:36,530 --> 00:02:41,120
actually working and gives us all our

00:02:38,090 --> 00:02:44,230
nice graph ana graphs so join us a bit

00:02:41,120 --> 00:02:46,850
about the project down I do yes okay so

00:02:44,230 --> 00:02:50,540
this is kind of it in a nutshell

00:02:46,850 --> 00:02:51,980
so it's we're a consultancy company so

00:02:50,540 --> 00:02:54,890
we work on behalf of clients this is

00:02:51,980 --> 00:02:57,350
this is a system for the the major

00:02:54,890 --> 00:03:01,430
publicly owned healthcare provider in

00:02:57,350 --> 00:03:03,230
the UK someone be able guess you guys so

00:03:01,430 --> 00:03:06,700
principally this receives data from

00:03:03,230 --> 00:03:10,190
hospitals this list files up in the top

00:03:06,700 --> 00:03:12,080
so those files contain a lot of XML

00:03:10,190 --> 00:03:13,940
about what the hospitals have been doing

00:03:12,080 --> 00:03:15,410
so stuff like operations they've

00:03:13,940 --> 00:03:17,570
performed patients they've seen this

00:03:15,410 --> 00:03:19,820
kind of stuff it all goes down is this

00:03:17,570 --> 00:03:22,130
custom at the bottom so that's that's a

00:03:19,820 --> 00:03:24,590
big load of homogeneous service or

00:03:22,130 --> 00:03:27,950
running Apache spark and running react

00:03:24,590 --> 00:03:30,470
there's a new SQL database the other

00:03:27,950 --> 00:03:32,989
side of it is these users at the top

00:03:30,470 --> 00:03:35,090
left there so those users want to be a

00:03:32,989 --> 00:03:37,220
generate reports based off that data

00:03:35,090 --> 00:03:39,070
which tell them how much to pay the

00:03:37,220 --> 00:03:41,750
hospital for the work that they've done

00:03:39,070 --> 00:03:43,700
so that's kind of the the main bulk of

00:03:41,750 --> 00:03:45,860
the work is generating those reports and

00:03:43,700 --> 00:03:47,420
that's what we use spark to do and it's

00:03:45,860 --> 00:03:48,709
really spark that we're talking about

00:03:47,420 --> 00:03:51,049
today how to instrument the stuff

00:03:48,709 --> 00:03:52,130
running on the spark well you've got to

00:03:51,049 --> 00:03:56,600
tell us a little bit more about spark

00:03:52,130 --> 00:03:59,269
then yep good idea let's do that so all

00:03:56,600 --> 00:04:01,820
our codes written in Python which isn't

00:03:59,269 --> 00:04:05,060
a great fit for spot because spark is a

00:04:01,820 --> 00:04:07,340
JVM thing so the way it kind of works is

00:04:05,060 --> 00:04:08,900
it's separated into these two halves

00:04:07,340 --> 00:04:11,600
you've got the local half and the

00:04:08,900 --> 00:04:13,760
cluster half so you start off on the

00:04:11,600 --> 00:04:16,070
left hand side this spark context that's

00:04:13,760 --> 00:04:19,669
the Python call basically which then

00:04:16,070 --> 00:04:21,919
talks to the JVM this is all a single

00:04:19,669 --> 00:04:24,710
point so a single server not JVM then

00:04:21,919 --> 00:04:25,910
spins out to the cluster notes this is

00:04:24,710 --> 00:04:29,420
your many nodes

00:04:25,910 --> 00:04:31,580
starts up these spark worker JVMs they

00:04:29,420 --> 00:04:33,620
in turn then spin up lots of Python

00:04:31,580 --> 00:04:35,810
worker processes so that's this all at

00:04:33,620 --> 00:04:38,510
one machine still replicated many times

00:04:35,810 --> 00:04:41,150
for all the machines you've got so you

00:04:38,510 --> 00:04:43,220
kind of start off in Python and you end

00:04:41,150 --> 00:04:45,010
up in Python and the pythons what

00:04:43,220 --> 00:04:47,030
actually runs all your or your jobs

00:04:45,010 --> 00:04:49,060
applies out your logic and processes

00:04:47,030 --> 00:04:51,020
your data we have this kind of like JVM

00:04:49,060 --> 00:04:53,540
abstraction thing in the middle which

00:04:51,020 --> 00:04:55,070
does all the cluster orchestration yeah

00:04:53,540 --> 00:04:56,800
and that's where some of the times you

00:04:55,070 --> 00:04:59,390
start to come up with trying to

00:04:56,800 --> 00:05:01,190
instrument it the Python workers the

00:04:59,390 --> 00:05:03,440
ones doing the work but they're over on

00:05:01,190 --> 00:05:05,450
this side and there's got all this Java

00:05:03,440 --> 00:05:07,070
going on in the middle so the Python

00:05:05,450 --> 00:05:08,900
workers themselves we don't really know

00:05:07,070 --> 00:05:10,700
much about them we don't really know

00:05:08,900 --> 00:05:12,770
when they spin up we don't know exactly

00:05:10,700 --> 00:05:14,990
how many there are half the time and

00:05:12,770 --> 00:05:16,700
then the workers themselves there's no

00:05:14,990 --> 00:05:17,890
shared States between them they don't

00:05:16,700 --> 00:05:20,390
know about each other

00:05:17,890 --> 00:05:22,040
it's lots of all little bits that make

00:05:20,390 --> 00:05:23,570
it kind of difficult to bring the

00:05:22,040 --> 00:05:24,820
instrumentation together in something

00:05:23,570 --> 00:05:27,890
that makes sense

00:05:24,820 --> 00:05:30,760
indeed so what do we do to get started

00:05:27,890 --> 00:05:33,890
then done that's a good question so

00:05:30,760 --> 00:05:36,290
we're using Python from epheus has a

00:05:33,890 --> 00:05:38,930
very good Python client library so we'll

00:05:36,290 --> 00:05:41,180
just use that yeah so just like normal

00:05:38,930 --> 00:05:42,770
just employ it and there you go you just

00:05:41,180 --> 00:05:44,900
do all the normal stuff import the

00:05:42,770 --> 00:05:47,390
library started at user mentation bit

00:05:44,900 --> 00:05:49,190
teeny counters in your gauges yep so

00:05:47,390 --> 00:05:51,200
let's listen on a pot yeah bunny would

00:05:49,190 --> 00:05:54,710
tell for easiest to scrape that oh yeah

00:05:51,200 --> 00:05:56,390
what else is there well all the work

00:05:54,710 --> 00:06:02,150
happens over here on the right so like

00:05:56,390 --> 00:06:05,390
which one do you scrape mmm another good

00:06:02,150 --> 00:06:07,310
question yeah so for example in this

00:06:05,390 --> 00:06:10,010
diagram we have six workers on a note

00:06:07,310 --> 00:06:11,330
we actually have 24 on our production so

00:06:10,010 --> 00:06:13,700
they're all gonna try and listen on the

00:06:11,330 --> 00:06:16,220
same pot so if that's all you're not

00:06:13,700 --> 00:06:18,050
gonna work yep so could we maybe

00:06:16,220 --> 00:06:20,480
manually configure each process to have

00:06:18,050 --> 00:06:22,160
a different pot well do we have any

00:06:20,480 --> 00:06:24,710
initialization control over these worker

00:06:22,160 --> 00:06:28,730
processes nope so they're gonna work

00:06:24,710 --> 00:06:30,950
nope okay okay if we know there were six

00:06:28,730 --> 00:06:33,440
processes we know where say process

00:06:30,950 --> 00:06:35,330
number three of six can we say adds

00:06:33,440 --> 00:06:36,320
three it's at the base part number so

00:06:35,330 --> 00:06:37,520
they've all got like a sequential

00:06:36,320 --> 00:06:39,559
apartment or something

00:06:37,520 --> 00:06:43,549
again do we know where through six

00:06:39,559 --> 00:06:45,109
okay next idea okay yes as you see we

00:06:43,549 --> 00:06:47,419
kind of fell at the first hurdle like

00:06:45,109 --> 00:06:52,129
even just trying to scrape our metrics

00:06:47,419 --> 00:06:56,569
was a challenge so yeah so what was next

00:06:52,129 --> 00:07:07,159
then next was please don't kick me the

00:06:56,569 --> 00:07:09,229
push gateway so this is an excerpt from

00:07:07,159 --> 00:07:11,329
the Prometheus documentation about the

00:07:09,229 --> 00:07:12,559
push gateway so it says occasionally

00:07:11,329 --> 00:07:15,109
you'll need to monitor components which

00:07:12,559 --> 00:07:17,239
cannot be straight oh yeah that's was

00:07:15,109 --> 00:07:18,919
they might leave behind a firewall they

00:07:17,239 --> 00:07:22,609
might be too short-lived to expose data

00:07:18,919 --> 00:07:24,109
yeah the cells as well so the push

00:07:22,609 --> 00:07:25,729
gateway allows you to push time series

00:07:24,109 --> 00:07:28,789
from these components to an intermediate

00:07:25,729 --> 00:07:29,659
job which Prometheus can scrape it

00:07:28,789 --> 00:07:31,639
sounds brilliant

00:07:29,659 --> 00:07:34,039
yeah so what is the telling me to do is

00:07:31,639 --> 00:07:35,809
to run one push gateway on each of our

00:07:34,039 --> 00:07:39,499
servers and then get all our Python

00:07:35,809 --> 00:07:41,749
workers to push data into it yeah hang

00:07:39,499 --> 00:07:45,169
on it doesn't it also say this in the

00:07:41,749 --> 00:07:47,239
documentation is explicitly not an

00:07:45,169 --> 00:07:49,909
aggregator or distributed counter but

00:07:47,239 --> 00:07:53,419
other metrics cache I think you're right

00:07:49,909 --> 00:07:54,649
it does say that as well yeah so I'm not

00:07:53,419 --> 00:07:56,389
quite sure we understood what this meant

00:07:54,649 --> 00:07:58,340
at the time because we plant we've cloud

00:07:56,389 --> 00:08:00,950
on with this

00:07:58,340 --> 00:08:02,420
we ploughed on with the solution and to

00:08:00,950 --> 00:08:05,420
be honest with you it was a total

00:08:02,420 --> 00:08:08,690
unmitigated disaster so you don't do

00:08:05,420 --> 00:08:10,640
that what was happening was each of our

00:08:08,690 --> 00:08:12,650
Python processes have had a completely

00:08:10,640 --> 00:08:14,300
different and the create the independent

00:08:12,650 --> 00:08:16,430
view of what all account as should be

00:08:14,300 --> 00:08:17,780
set to my gauges should be set to and

00:08:16,430 --> 00:08:19,910
then when they push into the push

00:08:17,780 --> 00:08:23,390
gateway the oldest overwrite each other

00:08:19,910 --> 00:08:28,190
so you end up with junk with no value

00:08:23,390 --> 00:08:31,310
whatsoever yeah not great but so where

00:08:28,190 --> 00:08:34,670
do we go from here well the next idea

00:08:31,310 --> 00:08:38,780
was let's just not bother what

00:08:34,670 --> 00:08:40,460
no metrics at all yeah well look why we

00:08:38,780 --> 00:08:42,350
thought let's not bother instrumenting

00:08:40,460 --> 00:08:44,090
the stuff out on the cluster let's try

00:08:42,350 --> 00:08:46,580
and do it works a bit easier at the

00:08:44,090 --> 00:08:48,440
single point on the local Biff in Sparks

00:08:46,580 --> 00:08:51,260
terminology yep let's just instrument

00:08:48,440 --> 00:08:52,760
there so we tried that but that meant we

00:08:51,260 --> 00:08:54,290
had to kind of do all our counting

00:08:52,760 --> 00:08:56,960
ourselves to all our aggregation

00:08:54,290 --> 00:08:59,330
ourselves so we did it all in Python and

00:08:56,960 --> 00:09:00,950
then used spark to fold it all back to

00:08:59,330 --> 00:09:04,580
the to the single point and then we can

00:09:00,950 --> 00:09:06,470
scrape it the downside of that is we get

00:09:04,580 --> 00:09:10,760
all the counters and stuff right at the

00:09:06,470 --> 00:09:13,490
end of the job like that yeah not ideal

00:09:10,760 --> 00:09:16,400
so obviously we really wanted to get the

00:09:13,490 --> 00:09:17,870
real-time view back so how about any

00:09:16,400 --> 00:09:21,410
crazy ideas to get that working

00:09:17,870 --> 00:09:24,410
we did have some crazy ideas yeah so we

00:09:21,410 --> 00:09:26,210
thought could we do something like the

00:09:24,410 --> 00:09:28,160
push gamer wasn't aggregating for us so

00:09:26,210 --> 00:09:30,290
could we say write our own version of

00:09:28,160 --> 00:09:32,120
the push gateway that did aggregation we

00:09:30,290 --> 00:09:35,510
could maybe send it like counter

00:09:32,120 --> 00:09:39,560
increments over UDP or something yeah

00:09:35,510 --> 00:09:44,450
that sounds not great I think some

00:09:39,560 --> 00:09:46,160
people just got the joke yep really got

00:09:44,450 --> 00:09:48,620
beyond the conversation so we thought

00:09:46,160 --> 00:09:50,690
could we use like something like stats D

00:09:48,620 --> 00:09:54,410
instance or proxy arrangement to do some

00:09:50,690 --> 00:09:56,600
aggregation yeah possible but yeah we're

00:09:54,410 --> 00:09:58,250
at the Prometheus conference so surely

00:09:56,600 --> 00:10:00,410
we can get Prometheus to do everything

00:09:58,250 --> 00:10:03,260
we could possibly dream of let's hope so

00:10:00,410 --> 00:10:05,630
all the are gonna kickers so yeah how

00:10:03,260 --> 00:10:07,490
would we do in Prometheus then and so

00:10:05,630 --> 00:10:08,500
what about this tech Specter stuff it's

00:10:07,490 --> 00:10:10,600
got

00:10:08,500 --> 00:10:12,820
the test collector so that's that's the

00:10:10,600 --> 00:10:14,620
thing where you don't metrics into a

00:10:12,820 --> 00:10:16,570
text file in a directory and then the

00:10:14,620 --> 00:10:17,500
node export or ingest them setup yeah

00:10:16,570 --> 00:10:20,680
exactly

00:10:17,500 --> 00:10:22,480
so we can maybe sort of get all about

00:10:20,680 --> 00:10:25,210
Python workers to write out their

00:10:22,480 --> 00:10:28,060
metrics and then get that picked up by

00:10:25,210 --> 00:10:29,800
the text vector and then obviously node

00:10:28,060 --> 00:10:33,520
exports grades that ends up in from EVs

00:10:29,800 --> 00:10:34,810
it's happy days but we'll still have the

00:10:33,520 --> 00:10:38,080
problem with the sort of overwriting

00:10:34,810 --> 00:10:42,580
counters so we might have to maybe use

00:10:38,080 --> 00:10:45,840
the process ID mmm-hmm as a label okay

00:10:42,580 --> 00:10:48,550
cuz that might work

00:10:45,840 --> 00:10:50,200
okay so we've solved this grating

00:10:48,550 --> 00:10:53,410
problem yeah because now we're just

00:10:50,200 --> 00:10:56,410
scraping the node exporter yep okay so

00:10:53,410 --> 00:10:58,540
using the process ideas label yeah so

00:10:56,410 --> 00:11:01,060
these workers are ephemeral processes

00:10:58,540 --> 00:11:05,100
yeah we can recycle so I'm gonna create

00:11:01,060 --> 00:11:09,160
lots of pros I CDs lots of time series

00:11:05,100 --> 00:11:09,550
is that gonna be a problem well we can

00:11:09,160 --> 00:11:11,800
try it

00:11:09,550 --> 00:11:14,080
we've got a massive test suite we've got

00:11:11,800 --> 00:11:16,960
loads of tests we can run and so we did

00:11:14,080 --> 00:11:18,850
that and so we ran it all locally and it

00:11:16,960 --> 00:11:21,610
looked pretty good so the spark workers

00:11:18,850 --> 00:11:23,890
or stayed around for quite a while so we

00:11:21,610 --> 00:11:25,660
didn't end up with that many pits so not

00:11:23,890 --> 00:11:28,630
that many labels not that many time

00:11:25,660 --> 00:11:31,180
series yeah look looked pretty good

00:11:28,630 --> 00:11:34,710
running the test environments all not

00:11:31,180 --> 00:11:36,640
too bad with the tests okay so

00:11:34,710 --> 00:11:38,860
[Music]

00:11:36,640 --> 00:11:40,810
so you're standing up in front of 200

00:11:38,860 --> 00:11:43,570
odds ops professionals

00:11:40,810 --> 00:11:49,030
yep and saying it worked in dev I worked

00:11:43,570 --> 00:11:52,600
really well on my machine okay

00:11:49,030 --> 00:11:55,390
these kicking is looking quite so what

00:11:52,600 --> 00:12:00,490
about production how did that fair yeah

00:11:55,390 --> 00:12:02,740
no it we really broke it so obviously we

00:12:00,490 --> 00:12:05,110
ended up with quite a lot of labels from

00:12:02,740 --> 00:12:07,600
all of those process IDs we found in our

00:12:05,110 --> 00:12:09,370
long-running jobs that as it's sort of

00:12:07,600 --> 00:12:11,080
moving real world data sets around

00:12:09,370 --> 00:12:13,570
between the cluster state and the local

00:12:11,080 --> 00:12:16,750
state the the private workers were

00:12:13,570 --> 00:12:18,760
getting killed off a lot the spark logic

00:12:16,750 --> 00:12:20,740
around that is quite simple they sort of

00:12:18,760 --> 00:12:23,680
look if they've done any work for 60

00:12:20,740 --> 00:12:26,550
seconds and if not kill them off so yeah

00:12:23,680 --> 00:12:30,970
well lots of processes lots of labels

00:12:26,550 --> 00:12:34,210
not good so how many times did we create

00:12:30,970 --> 00:12:36,670
well we did try and ask the API to tell

00:12:34,210 --> 00:12:38,290
us that but it kind of hung for 45

00:12:36,670 --> 00:12:46,150
minutes and we got a bit bored after

00:12:38,290 --> 00:12:49,540
that okay so so we've gone from quite

00:12:46,150 --> 00:12:52,090
coarse-grained metrics to no metrics

00:12:49,540 --> 00:12:54,520
yeah well just like another idea yeah

00:12:52,090 --> 00:12:57,400
okay there yeah where do we go from here

00:12:54,520 --> 00:13:00,180
then yeah well there's also this thing

00:12:57,400 --> 00:13:02,740
in the way called the multi POC mode

00:13:00,180 --> 00:13:05,680
multi proc mode yeah what's all that

00:13:02,740 --> 00:13:08,500
about them and so it kind of uses a

00:13:05,680 --> 00:13:09,850
directory is the shared registry so each

00:13:08,500 --> 00:13:12,340
of the Python workers could write some

00:13:09,850 --> 00:13:14,800
data and then we sort of follow the same

00:13:12,340 --> 00:13:16,540
partners before ok do still lead the tax

00:13:14,800 --> 00:13:17,980
collector them yeah cuz that's still

00:13:16,540 --> 00:13:20,950
gonna be the bit that we actually scrape

00:13:17,980 --> 00:13:22,510
the metrics from okay it's quite a few

00:13:20,950 --> 00:13:23,860
moving parts now if we got like a pretty

00:13:22,510 --> 00:13:28,750
diagram that explains it

00:13:23,860 --> 00:13:30,490
yes we do well handy yeah just as before

00:13:28,750 --> 00:13:32,590
but we've just got this extra little

00:13:30,490 --> 00:13:34,990
layer in still got the Python workers

00:13:32,590 --> 00:13:37,390
across the top but each time they they

00:13:34,990 --> 00:13:39,390
just write out to a data file that's

00:13:37,390 --> 00:13:42,790
within this multi product directory and

00:13:39,390 --> 00:13:44,320
then every so often in our code we tell

00:13:42,790 --> 00:13:46,120
it to flush out the metrics where it

00:13:44,320 --> 00:13:47,699
relates all that the counters it can

00:13:46,120 --> 00:13:50,160
find

00:13:47,699 --> 00:13:52,889
writes them out as a text that metrics

00:13:50,160 --> 00:13:54,419
file just as before still picked up by

00:13:52,889 --> 00:13:58,470
the node exporters still scraped into

00:13:54,419 --> 00:14:00,480
Prometheus okay so if you wouldn't drink

00:13:58,470 --> 00:14:04,379
like this had loads of complexity to the

00:14:00,480 --> 00:14:08,220
code it isn't that bad so this is this

00:14:04,379 --> 00:14:09,769
is where we start off so the first line

00:14:08,220 --> 00:14:12,359
is just just a totally vanilla

00:14:09,769 --> 00:14:13,859
Prometheus counter increment so that

00:14:12,359 --> 00:14:15,540
prompt cancellation count that assisted

00:14:13,859 --> 00:14:18,689
counter objects ever incrementing it by

00:14:15,540 --> 00:14:20,309
some value the only tax we have to pay

00:14:18,689 --> 00:14:22,199
on that is we have to call this periodic

00:14:20,309 --> 00:14:24,559
flush function every time we touch a

00:14:22,199 --> 00:14:28,199
metric so let's just have a look at that

00:14:24,559 --> 00:14:30,449
yep nothing too crazy in here so we sort

00:14:28,199 --> 00:14:32,850
of just have a sort of arbitrary

00:14:30,449 --> 00:14:34,949
10-second interval of flushing the

00:14:32,850 --> 00:14:36,899
metrics out start sort of a balance

00:14:34,949 --> 00:14:39,209
between not having the kotatsu right

00:14:36,899 --> 00:14:41,669
these files out all the time but still

00:14:39,209 --> 00:14:43,529
within our Prometheus scrape timings so

00:14:41,669 --> 00:14:45,059
we just yeah check the current time

00:14:43,529 --> 00:14:47,069
check the last flush tank and check it's

00:14:45,059 --> 00:14:48,509
outside the interval and then if we do

00:14:47,069 --> 00:14:51,739
need to flush the metrics we just use

00:14:48,509 --> 00:14:54,600
the standard Prometheus Python libraries

00:14:51,739 --> 00:14:56,669
right to text file so just as we were

00:14:54,600 --> 00:14:59,489
doing earlier but without all the crazy

00:14:56,669 --> 00:15:01,439
paid stuff in the way and yeah still

00:14:59,489 --> 00:15:03,359
still work still gets into Prometheus

00:15:01,439 --> 00:15:05,160
what about initializing it yeah that's

00:15:03,359 --> 00:15:07,679
not too bad so you just sort of tell it

00:15:05,160 --> 00:15:09,119
to use a multi-process registry that's

00:15:07,679 --> 00:15:10,889
those a couple of lines at the top and

00:15:09,119 --> 00:15:13,309
you also just set an environment

00:15:10,889 --> 00:15:15,269
variable to tell it what folder that is

00:15:13,309 --> 00:15:16,919
it's the most that's just really really

00:15:15,269 --> 00:15:19,049
basic code there's not knowing anything

00:15:16,919 --> 00:15:20,819
particularly onerous and they writing

00:15:19,049 --> 00:15:22,679
out the file that's only once every 10

00:15:20,819 --> 00:15:25,109
seconds so it's not really adding any

00:15:22,679 --> 00:15:28,949
overhead and the great thing about this

00:15:25,109 --> 00:15:30,539
approach is it works so we've actually

00:15:28,949 --> 00:15:35,249
got it running in production now as

00:15:30,539 --> 00:15:38,009
evidenced by the pretty graph so what's

00:15:35,249 --> 00:15:39,720
what it is allowing us to do now is as

00:15:38,009 --> 00:15:41,339
we talked about earlier on is trying to

00:15:39,720 --> 00:15:44,669
get really deep down inside what what

00:15:41,339 --> 00:15:46,739
about more report jobs actually doing so

00:15:44,669 --> 00:15:48,269
well now we can see that all the jobs

00:15:46,739 --> 00:15:50,879
follow this sort of saying this set

00:15:48,269 --> 00:15:53,369
pattern so the blue stuff for this at

00:15:50,879 --> 00:15:56,369
the side that's a that's a phase called

00:15:53,369 --> 00:15:58,169
loading so very basically that's loading

00:15:56,369 --> 00:15:59,650
data out of the database so it's a load

00:15:58,169 --> 00:16:01,510
of reads

00:15:59,650 --> 00:16:03,550
and it's loading that data into memory

00:16:01,510 --> 00:16:05,529
you giving it to spark and then you have

00:16:03,550 --> 00:16:06,460
the second phase which is basically all

00:16:05,529 --> 00:16:08,440
the business logic

00:16:06,460 --> 00:16:11,560
so all those phases they're called stuff

00:16:08,440 --> 00:16:12,940
like projecting and linking and grouping

00:16:11,560 --> 00:16:16,540
which are just like business logic

00:16:12,940 --> 00:16:19,839
specific things so a bit up see exactly

00:16:16,540 --> 00:16:22,660
what's going on whereas previously we

00:16:19,839 --> 00:16:27,400
just had that which is obviously not

00:16:22,660 --> 00:16:29,110
much use so what we'll do now is with

00:16:27,400 --> 00:16:30,910
that new vision of what's actually

00:16:29,110 --> 00:16:33,970
happening we've been able to correlate

00:16:30,910 --> 00:16:35,260
that with resource utilization and get a

00:16:33,970 --> 00:16:37,210
better understanding of what's going on

00:16:35,260 --> 00:16:41,200
so you see in the top left that's that's

00:16:37,210 --> 00:16:42,880
our business logic level graph you can

00:16:41,200 --> 00:16:45,580
see when we're reading from the database

00:16:42,880 --> 00:16:49,779
doing the loading phase the graph here

00:16:45,580 --> 00:16:52,990
react gets correlates additionally this

00:16:49,779 --> 00:16:54,400
time correlates so you know when you

00:16:52,990 --> 00:16:57,460
read from a database it hits the disk

00:16:54,400 --> 00:17:01,180
info and then the business logic stuff

00:16:57,460 --> 00:17:02,860
that correlates with high CPU so we kind

00:17:01,180 --> 00:17:04,209
of we kind of had a feel that this is

00:17:02,860 --> 00:17:05,740
the sort of thing that was going on but

00:17:04,209 --> 00:17:07,329
now we can really confidently say oh

00:17:05,740 --> 00:17:08,980
yeah this phase is really CPU intensive

00:17:07,329 --> 00:17:12,579
and it occurs at this point and after

00:17:08,980 --> 00:17:14,140
listen before this so this is this

00:17:12,579 --> 00:17:18,189
knowledge is now starting to drive our

00:17:14,140 --> 00:17:20,459
behaviors so the the loading phase again

00:17:18,189 --> 00:17:23,920
was is quite variable in its performance

00:17:20,459 --> 00:17:27,370
so I'm here it's running around 12,000

00:17:23,920 --> 00:17:30,520
reads per second but we sometimes see

00:17:27,370 --> 00:17:32,020
around about 3,000 so we looked at that

00:17:30,520 --> 00:17:34,660
why that is we attributed it to the

00:17:32,020 --> 00:17:36,309
effect of the page cache which kind of

00:17:34,660 --> 00:17:39,790
led us on to thinking well should we

00:17:36,309 --> 00:17:41,530
then start considering buying some solid

00:17:39,790 --> 00:17:43,600
state read cash cards supporting the

00:17:41,530 --> 00:17:45,700
service and this kind of stuff and we've

00:17:43,600 --> 00:17:47,080
never been out of like gather that

00:17:45,700 --> 00:17:49,240
evidence before to actually inform that

00:17:47,080 --> 00:17:52,480
work so it start to drive platform

00:17:49,240 --> 00:17:53,590
improvements along those lines yep and

00:17:52,480 --> 00:17:56,530
that's what we really wanted to get

00:17:53,590 --> 00:17:58,870
across today is how worthwhile it is to

00:17:56,530 --> 00:18:01,030
get the real-time instrumentation deep

00:17:58,870 --> 00:18:02,920
into your applications disk is there's

00:18:01,030 --> 00:18:04,570
so much benefits just just by knowing

00:18:02,920 --> 00:18:06,670
what's going on just be able to see up

00:18:04,570 --> 00:18:08,590
on that wall boards every day and then

00:18:06,670 --> 00:18:10,540
yeah as Dungey said we're actually

00:18:08,590 --> 00:18:12,950
driving our decisions on how advanced

00:18:10,540 --> 00:18:15,139
this platform for going forward

00:18:12,950 --> 00:18:16,789
let's keep it is nothing's on

00:18:15,139 --> 00:18:19,190
instrumental it might be a bit difficult

00:18:16,789 --> 00:18:21,019
Apache spark looked pretty crazy black

00:18:19,190 --> 00:18:23,139
box when we first started working on it

00:18:21,019 --> 00:18:26,210
obviously as normal lots of iterations

00:18:23,139 --> 00:18:28,610
constantly improving that'll get you to

00:18:26,210 --> 00:18:30,440
some solutions that often really are

00:18:28,610 --> 00:18:33,320
quite simple and work really well for

00:18:30,440 --> 00:18:34,970
your use case and the last bits really

00:18:33,320 --> 00:18:36,860
just a shout out to Prometheus it's been

00:18:34,970 --> 00:18:39,889
great for me personally to learn it on

00:18:36,860 --> 00:18:41,330
this project and is pretty flexible as

00:18:39,889 --> 00:18:42,950
we just said you know it's come out for

00:18:41,330 --> 00:18:45,409
world of web api so everything's about

00:18:42,950 --> 00:18:48,529
much much quicker than our multi our

00:18:45,409 --> 00:18:52,340
spark jobs in some West cases but it

00:18:48,529 --> 00:18:54,020
works and it's great yeah yeah thanks

00:18:52,340 --> 00:19:00,740
for listening

00:18:54,020 --> 00:19:10,410
[Applause]

00:19:00,740 --> 00:19:12,510
alright question time did you run into

00:19:10,410 --> 00:19:15,060
performance problems with having lots

00:19:12,510 --> 00:19:19,290
and lots of files being read by the

00:19:15,060 --> 00:19:22,650
world's fastest collector um no we just

00:19:19,290 --> 00:19:24,060
delete them really quickly because it's

00:19:22,650 --> 00:19:26,130
just a minute it's just a nice simple

00:19:24,060 --> 00:19:28,410
calm calm job that checks for when they

00:19:26,130 --> 00:19:30,270
were last modified or accessed or

00:19:28,410 --> 00:19:32,700
created or something and gets rid of

00:19:30,270 --> 00:19:35,220
them after about an hour it seems to be

00:19:32,700 --> 00:19:39,540
working quite well rent to delete those

00:19:35,220 --> 00:19:40,410
when deep application restarts well see

00:19:39,540 --> 00:19:43,350
that's again where it comes a bit

00:19:40,410 --> 00:19:44,940
complicated because the master process

00:19:43,350 --> 00:19:47,700
where the spark drops come forms always

00:19:44,940 --> 00:19:49,320
running so it's kind of hard to tell

00:19:47,700 --> 00:19:51,000
exactly when the Python workers die

00:19:49,320 --> 00:19:52,950
hence why we sort of have the quite

00:19:51,000 --> 00:19:54,900
naive contact to clean them up

00:19:52,950 --> 00:19:56,640
afterwards yeah they just silently die

00:19:54,900 --> 00:19:59,040
so that the spark code we had on the

00:19:56,640 --> 00:20:00,420
screen before it's totally out of our

00:19:59,040 --> 00:20:01,740
hands they're just gonna die at some

00:20:00,420 --> 00:20:05,340
point there's nothing you can do about

00:20:01,740 --> 00:20:07,140
it continue a spark job or a job runs

00:20:05,340 --> 00:20:09,270
for an hour a job runs for an hour a dog

00:20:07,140 --> 00:20:12,270
runs for an hour a little bit of both so

00:20:09,270 --> 00:20:13,650
the kind of this stuff ray the front

00:20:12,270 --> 00:20:16,860
spark context stuff that runs

00:20:13,650 --> 00:20:18,680
indefinitely and the cluster the the jet

00:20:16,860 --> 00:20:22,470
cluster JVM is kind of run indefinitely

00:20:18,680 --> 00:20:24,540
but the jobs do want for an hour and

00:20:22,470 --> 00:20:26,520
then another one for an hour and also

00:20:24,540 --> 00:20:28,500
doing those jobs through a lot of gaps

00:20:26,520 --> 00:20:29,820
and that's where that's when spark jumps

00:20:28,500 --> 00:20:33,030
in and says yep you know I've been that

00:20:29,820 --> 00:20:34,350
work anymore which is very useful so

00:20:33,030 --> 00:20:36,090
you're gonna break rage if you're

00:20:34,350 --> 00:20:39,290
deleting those files we're gonna break

00:20:36,090 --> 00:20:45,360
race because the counters will go down

00:20:39,290 --> 00:20:50,940
yeah we've not seen it break rate I

00:20:45,360 --> 00:20:53,670
don't think maybe they're just using

00:20:50,940 --> 00:21:00,890
gauges no use it wheezing the counters

00:20:53,670 --> 00:21:04,860
and rates one more okay so great talk

00:21:00,890 --> 00:21:08,540
interesting insight how about your act

00:21:04,860 --> 00:21:08,540
did you rehearsed it a lot

00:21:08,789 --> 00:21:24,909
yeah yes yeah this is my first time

00:21:12,820 --> 00:21:28,320
public speaking so yeah we but I do have

00:21:24,909 --> 00:21:28,320
to sit next to him at work all the time

00:21:32,220 --> 00:21:39,850
how big was this project in terms of

00:21:34,900 --> 00:21:43,960
time and people maybe pain it was pretty

00:21:39,850 --> 00:21:47,080
big so it started around about May

00:21:43,960 --> 00:21:49,150
two years ago so what's up to 200 years

00:21:47,080 --> 00:21:51,850
something like that

00:21:49,150 --> 00:21:57,150
the team has probably engineers on it

00:21:51,850 --> 00:22:02,440
now in terms of kit the the production

00:21:57,150 --> 00:22:04,480
clusters run 18 physical servers in one

00:22:02,440 --> 00:22:05,980
datacenter and another 18 another

00:22:04,480 --> 00:22:08,140
datacenter then it's probably the same

00:22:05,980 --> 00:22:09,520
again of supporting servers and then we

00:22:08,140 --> 00:22:11,169
have all that copied again for a

00:22:09,520 --> 00:22:12,130
reference environment and then we have

00:22:11,169 --> 00:22:13,480
like test environments and dev

00:22:12,130 --> 00:22:21,669
environment so there's a lot of

00:22:13,480 --> 00:22:27,460
infrastructure as well so yeah oh the

00:22:21,669 --> 00:22:29,110
Prometheus part no no no it's a very

00:22:27,460 --> 00:22:32,770
long really I mean I wish probably

00:22:29,110 --> 00:22:34,870
working on it for three or four weeks to

00:22:32,770 --> 00:22:36,400
break the back of it and get it in there

00:22:34,870 --> 00:22:37,929
we're just been in training on it ever

00:22:36,400 --> 00:22:41,730
since something we started with three

00:22:37,929 --> 00:22:41,730
dashboards we probably got about 30 now

00:22:42,659 --> 00:22:54,520
any other questions yes come it works on

00:22:51,880 --> 00:22:57,610
so you know a lot about what's Python

00:22:54,520 --> 00:23:00,669
doing but you didn't get to money toward

00:22:57,610 --> 00:23:04,000
the Java part I guess did you try to dig

00:23:00,669 --> 00:23:07,179
into that yes we've we have got

00:23:04,000 --> 00:23:11,770
monitoring on the JVM so there's the gem

00:23:07,179 --> 00:23:12,820
xx bottom so we've deployed that but to

00:23:11,770 --> 00:23:15,490
be honest it doesn't really give us

00:23:12,820 --> 00:23:17,080
anything that's that useful it sort of

00:23:15,490 --> 00:23:21,250
says yeah lots of garbage collects

00:23:17,080 --> 00:23:23,410
happen and it uses some of its

00:23:21,250 --> 00:23:28,420
but it never in everyone tells you

00:23:23,410 --> 00:23:30,420
anything that's not useful so somewhat

00:23:28,420 --> 00:23:33,400
related to that question potentially um

00:23:30,420 --> 00:23:34,480
you're smart contexts how long are they

00:23:33,400 --> 00:23:36,700
actually running for it what's a longer

00:23:34,480 --> 00:23:39,600
situation your context is your context a

00:23:36,700 --> 00:23:45,010
context that's been allowed for um

00:23:39,600 --> 00:23:47,920
probably a week we run for a week cool

00:23:45,010 --> 00:23:50,590
yeah the context is we context the

00:23:47,920 --> 00:23:52,600
context to the context yeah we we face

00:23:50,590 --> 00:23:54,700
several times we're very mature don't

00:23:52,600 --> 00:23:57,550
Python Java based but our context sir

00:23:54,700 --> 00:23:58,930
multi tweak and very long charred

00:23:57,550 --> 00:24:01,540
durations yeah

00:23:58,930 --> 00:24:02,860
so it's summer touch it's a JVM

00:24:01,540 --> 00:24:04,240
monitoring piece that is that the bigger

00:24:02,860 --> 00:24:06,640
challenge as opposed to the business

00:24:04,240 --> 00:24:08,320
metrics piece yeah okay we used to spin

00:24:06,640 --> 00:24:10,600
up a new context for every job but

00:24:08,320 --> 00:24:17,350
there's an overhead with that so not

00:24:10,600 --> 00:24:19,240
doing that saves a lot of time all right

00:24:17,350 --> 00:24:21,880
I want you to know if you're so

00:24:19,240 --> 00:24:24,400
important to maintaining the react

00:24:21,880 --> 00:24:28,920
database and if yes now that partially

00:24:24,400 --> 00:24:31,840
said are you doing anything yeah so

00:24:28,920 --> 00:24:34,780
context those aren't aware the the

00:24:31,840 --> 00:24:37,480
company behind react called bash oh they

00:24:34,780 --> 00:24:41,380
went into administration last month I

00:24:37,480 --> 00:24:43,900
think so that's been interesting we're

00:24:41,380 --> 00:24:46,510
we're not involved in contributing it

00:24:43,900 --> 00:24:49,510
life infinity works on our client is

00:24:46,510 --> 00:24:51,610
however so they are kind of adopting it

00:24:49,510 --> 00:24:54,430
and they're working on some improvements

00:24:51,610 --> 00:24:56,410
which which may well see the light of

00:24:54,430 --> 00:24:57,820
day is a new version if all goes well

00:24:56,410 --> 00:25:01,950
okay thanks

00:24:57,820 --> 00:25:01,950
all right thank you very much

00:25:03,510 --> 00:25:16,200
[Music]

00:25:16,000 --> 00:25:19,339
you

00:25:16,200 --> 00:25:19,339

YouTube URL: https://www.youtube.com/watch?v=KMk1aJaAkhw


