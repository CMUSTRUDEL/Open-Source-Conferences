Title: PromCon 2017: Cortex: Prometheus as a Service, One Year On - Tom Wilkie
Publication date: 2017-09-01
Playlist: PromCon 2017
Description: 
	* Abstract:

At PromCon 2016, I presented "Project Frankenstein: A multitenant, horizontally scalable Prometheus as a service". It's now one year later, and lots has changed - not least the name!  This talk will discuss what we've learnt running a Prometheus service for the past year, the architectural changes we made from the original design, and the improvements we've made to the Cortex user experience.

* Speaker biography:

Until recently, Tom was a Software Engineer at Weaveworks, where he built Cortex. Previously he was at Google as a Site Reliability Manager for Google Analytics. Before that he was Founder, VP Eng and CTO at Acunu, and before that a Software Engineer at XenSource. In his spare time, Tom likes to make craft beer and build home automation systems.

* Slides:

https://promcon.io/2017-munich/slides/cortex-prometheus-as-a-service-one-year-on.pdf

* PromCon website:

https://promcon.io/
Captions: 
	00:00:00,380 --> 00:00:18,090
[Music]

00:00:13,340 --> 00:00:19,230
hello everybody welcome to day two how

00:00:18,090 --> 00:00:24,689
are you feeling after the beer last

00:00:19,230 --> 00:00:27,240
night so a priority had a few fights I

00:00:24,689 --> 00:00:29,039
was a - and so a few small notices there

00:00:27,240 --> 00:00:31,410
are still some slots for electing talks

00:00:29,039 --> 00:00:33,450
so just write your name down on the

00:00:31,410 --> 00:00:35,940
board outside above Ritchie's name

00:00:33,450 --> 00:00:37,590
because he's going to last you also if

00:00:35,940 --> 00:00:40,320
all speakers if you could send on our

00:00:37,590 --> 00:00:43,260
slides please so we can link to them

00:00:40,320 --> 00:00:45,149
from the schedule so now you're going to

00:00:43,260 --> 00:00:48,570
have Tom who is going to talk about

00:00:45,149 --> 00:01:01,309
cortex which is long-term storage -

00:00:48,570 --> 00:01:03,989
distributed radius yes yes okay good

00:01:01,309 --> 00:01:06,810
hello everyone thanks instruction brian

00:01:03,989 --> 00:01:08,299
i said i'm talk about context but first

00:01:06,810 --> 00:01:11,700
I'd like everyone to put their hand up

00:01:08,299 --> 00:01:12,900
everyone no excuses right and now if you

00:01:11,700 --> 00:01:16,799
weren't here last year you can put them

00:01:12,900 --> 00:01:19,409
down again if you weren't yeah okay not

00:01:16,799 --> 00:01:21,090
as many as I was hoping so this is going

00:01:19,409 --> 00:01:22,830
to be a bit of her what we taught them

00:01:21,090 --> 00:01:24,330
because it's going to be very

00:01:22,830 --> 00:01:26,369
referential to last year's talk and I'm

00:01:24,330 --> 00:01:28,439
kind of try and build on that last year

00:01:26,369 --> 00:01:30,229
I presented cortex which is a

00:01:28,439 --> 00:01:33,119
horizontally scalable distributed

00:01:30,229 --> 00:01:34,979
prometheus version of Prometheus

00:01:33,119 --> 00:01:37,409
basically Prometheus compatible

00:01:34,979 --> 00:01:39,420
monitoring system I guess you should all

00:01:37,409 --> 00:01:41,390
go and read that or watch that video so

00:01:39,420 --> 00:01:43,890
I'll give you about five minutes here

00:01:41,390 --> 00:01:47,399
okay so yeah about a year ago I had a

00:01:43,890 --> 00:01:49,079
different haircut I'll give you a brief

00:01:47,399 --> 00:01:50,040
overview of what we discussed last year

00:01:49,079 --> 00:01:51,299
and then I'm just going to build on it

00:01:50,040 --> 00:01:53,310
and give you my experience of running

00:01:51,299 --> 00:01:55,140
this system and improving the system for

00:01:53,310 --> 00:01:57,240
the last year so what is prometheus

00:01:55,140 --> 00:01:59,990
Prometheus is a natively multi-tenant

00:01:57,240 --> 00:02:02,250
where we Prometheus service where we

00:01:59,990 --> 00:02:05,610
isolate different customers in the same

00:02:02,250 --> 00:02:06,570
services we did this because a year ago

00:02:05,610 --> 00:02:08,970
I worked for a company called weave

00:02:06,570 --> 00:02:10,379
works which is sponsoring this event and

00:02:08,970 --> 00:02:12,330
we wanted to offer Prometheus as a

00:02:10,379 --> 00:02:13,790
service as part of our product which is

00:02:12,330 --> 00:02:16,010
called weave

00:02:13,790 --> 00:02:17,750
so we needed multi-tenancy we want him

00:02:16,010 --> 00:02:19,490
to do it in that in a single service not

00:02:17,750 --> 00:02:21,650
give like a container to each customer

00:02:19,490 --> 00:02:22,430
because we wanted it to be nice and easy

00:02:21,650 --> 00:02:24,170
to operate

00:02:22,430 --> 00:02:26,060
we wanted it to be more scalable and so

00:02:24,170 --> 00:02:27,200
on we had a bit of experience running a

00:02:26,060 --> 00:02:30,620
container per customer and it wasn't

00:02:27,200 --> 00:02:32,270
good one of the main motivations for

00:02:30,620 --> 00:02:34,100
cortex was making it cost-effective to

00:02:32,270 --> 00:02:36,260
run you know we've worked the business

00:02:34,100 --> 00:02:37,400
to make money and so therefore we had to

00:02:36,260 --> 00:02:39,140
you know sell this thing for more than

00:02:37,400 --> 00:02:42,170
it cost us to run it and you'll see that

00:02:39,140 --> 00:02:44,240
later in in my experience we also wanted

00:02:42,170 --> 00:02:45,950
to offer a different set of trade-offs

00:02:44,240 --> 00:02:48,380
around scalability in hey CheY and

00:02:45,950 --> 00:02:50,240
Prometheus is a very high-performance

00:02:48,380 --> 00:02:53,060
system you know and it scales vertically

00:02:50,240 --> 00:02:55,160
very well but you know some people have

00:02:53,060 --> 00:02:56,840
referenced yesterday the hey chase story

00:02:55,160 --> 00:02:58,130
if you run a pair of them and one fails

00:02:56,840 --> 00:02:59,180
you get gaps in grass and things like

00:02:58,130 --> 00:03:00,470
that and that wasn't something that we

00:02:59,180 --> 00:03:03,080
really wanted to do we wanted to give

00:03:00,470 --> 00:03:04,670
more of a kind of traditional H a story

00:03:03,080 --> 00:03:06,890
a traditional distributed system story

00:03:04,670 --> 00:03:10,280
and that someone comes to expect from

00:03:06,890 --> 00:03:12,140
from a service one of the nice things

00:03:10,280 --> 00:03:14,420
about cortex is we wanted to offer a

00:03:12,140 --> 00:03:16,100
virtually infinite retention and

00:03:14,420 --> 00:03:17,810
durability and now okay that's a big

00:03:16,100 --> 00:03:19,850
promise but effectively all we've done

00:03:17,810 --> 00:03:22,310
is is not make that our responsibility

00:03:19,850 --> 00:03:24,860
we've offloaded all of the storage to

00:03:22,310 --> 00:03:27,260
something like dynamodb or Google

00:03:24,860 --> 00:03:28,459
BigTable and then you know if you want

00:03:27,260 --> 00:03:30,350
to delete the data from there great you

00:03:28,459 --> 00:03:31,880
lose it but but it's not our problem and

00:03:30,350 --> 00:03:33,580
this is how you know it something like

00:03:31,880 --> 00:03:37,640
dynamodb you can actually get really

00:03:33,580 --> 00:03:39,200
basically infinite retention and finally

00:03:37,640 --> 00:03:40,190
yeah there was when you're building a

00:03:39,200 --> 00:03:41,720
distributed systems

00:03:40,190 --> 00:03:43,130
there's a bunch of opportunities for

00:03:41,720 --> 00:03:44,450
performance enhancement so you don't get

00:03:43,130 --> 00:03:46,010
when you've got a single binary running

00:03:44,450 --> 00:03:48,080
on a single machine and and the best

00:03:46,010 --> 00:03:50,570
example I've got of this is when you're

00:03:48,080 --> 00:03:52,400
loading a dashboard through cortex we

00:03:50,570 --> 00:03:54,709
power massively parallel eyes each

00:03:52,400 --> 00:03:56,420
individual query so I we massively

00:03:54,709 --> 00:03:58,130
parallel but we paralyze between

00:03:56,420 --> 00:03:59,810
different queries and therefore like

00:03:58,130 --> 00:04:01,850
your overall latency can load your

00:03:59,810 --> 00:04:05,240
dashboard can be better it can be worse

00:04:01,850 --> 00:04:06,350
as which we'll cover in a minute so I've

00:04:05,240 --> 00:04:07,760
got to be have an example of some

00:04:06,350 --> 00:04:10,459
diagrams but I'm going to dive into the

00:04:07,760 --> 00:04:12,770
bottom one here this was the

00:04:10,459 --> 00:04:16,160
architecture we presented at Hong Kong

00:04:12,770 --> 00:04:17,270
last year so we've got a normal laser

00:04:16,160 --> 00:04:20,540
pointer doesn't work on screen that's

00:04:17,270 --> 00:04:23,060
useful we've got a normal Prometheus up

00:04:20,540 --> 00:04:24,620
up in the top left corner and it's using

00:04:23,060 --> 00:04:26,630
the remote right API which i discussed

00:04:24,620 --> 00:04:29,570
yesterday to send data to

00:04:26,630 --> 00:04:30,980
to cortex so Prometheus is still

00:04:29,570 --> 00:04:32,660
responsible for doing service discovery

00:04:30,980 --> 00:04:34,640
it's still responsible for scraping your

00:04:32,660 --> 00:04:36,590
jobs still responsible for doing

00:04:34,640 --> 00:04:39,650
relabeling still responsible for doing

00:04:36,590 --> 00:04:41,120
recording rules at this stage and in a

00:04:39,650 --> 00:04:42,920
lot of deployments still responsible for

00:04:41,120 --> 00:04:44,450
doing alerts and then it would take all

00:04:42,920 --> 00:04:46,340
the day to take all the samples send

00:04:44,450 --> 00:04:47,720
them to cortex you know and you run an

00:04:46,340 --> 00:04:49,100
engine X that does some authentication

00:04:47,720 --> 00:04:50,990
or something in front of it which is not

00:04:49,100 --> 00:04:53,780
really part of cortex and then I've just

00:04:50,990 --> 00:05:01,210
noticed a typo and that's going to be in

00:04:53,780 --> 00:05:03,800
every slight because I build on this ok

00:05:01,210 --> 00:05:05,920
so it's something called the distributor

00:05:03,800 --> 00:05:08,240
which definitely should be distributor

00:05:05,920 --> 00:05:09,710
and the distributor's job is to

00:05:08,240 --> 00:05:11,540
basically implement the consistent

00:05:09,710 --> 00:05:14,390
hashing and the dynamo style replication

00:05:11,540 --> 00:05:15,620
so it will that the load balancing from

00:05:14,390 --> 00:05:17,300
the front end is a distributor at the

00:05:15,620 --> 00:05:19,550
time was just done by the q proxy sounds

00:05:17,300 --> 00:05:21,500
random but from the distributor down to

00:05:19,550 --> 00:05:24,020
the in gesture we avoid q proxy and we

00:05:21,500 --> 00:05:25,850
target specific and jester's so that we

00:05:24,020 --> 00:05:28,610
can give some locality to where the time

00:05:25,850 --> 00:05:30,170
series should live so the time series

00:05:28,610 --> 00:05:32,030
get you know we pick an in gesture to

00:05:30,170 --> 00:05:33,800
send or pick three and jester's to send

00:05:32,030 --> 00:05:34,880
the time series to we replicate them out

00:05:33,800 --> 00:05:36,890
to the time season and it's the in

00:05:34,880 --> 00:05:38,750
jesters job to build chunks in the same

00:05:36,890 --> 00:05:41,030
way cortex and we Prometheus builds

00:05:38,750 --> 00:05:44,960
trunks and periodically flush these out

00:05:41,030 --> 00:05:46,700
to s3 with a with an inverted memory an

00:05:44,960 --> 00:05:49,070
external memory inverted index in vitam

00:05:46,700 --> 00:05:51,500
ODB and some caching and then on top of

00:05:49,070 --> 00:05:54,050
all of this we picked console as kind of

00:05:51,500 --> 00:05:56,930
a place to store the distributed hash

00:05:54,050 --> 00:05:58,730
ring and all the tokens and and some

00:05:56,930 --> 00:06:00,200
service discovery day turns on that's

00:05:58,730 --> 00:06:01,520
not strictly necessary one of the things

00:06:00,200 --> 00:06:06,860
I want to do in the future is make that

00:06:01,520 --> 00:06:08,720
more optional so that's in home do that

00:06:06,860 --> 00:06:10,790
in five minutes that's my last year's

00:06:08,720 --> 00:06:13,070
40-minute talk and how we're going to

00:06:10,790 --> 00:06:14,840
talk about what I've learned in the last

00:06:13,070 --> 00:06:17,270
year I'm gonna try and do this roughly

00:06:14,840 --> 00:06:19,280
chronologically but I'm gonna also like

00:06:17,270 --> 00:06:21,530
focus on problems that I that we

00:06:19,280 --> 00:06:23,930
experience when we're running it so the

00:06:21,530 --> 00:06:25,670
first problem if anyone here works for

00:06:23,930 --> 00:06:26,930
Amazon I'm apologizing because I'm

00:06:25,670 --> 00:06:30,500
probably gonna be pretty pretty

00:06:26,930 --> 00:06:32,750
detrimental about DynamoDB we really

00:06:30,500 --> 00:06:35,540
struggled to saturate our DynamoDB

00:06:32,750 --> 00:06:38,210
throughput and for those that aren't

00:06:35,540 --> 00:06:39,980
familiar with DynamoDB DynamoDB you pay

00:06:38,210 --> 00:06:41,390
for provisions throughput you

00:06:39,980 --> 00:06:42,530
if you use you pay for provision through

00:06:41,390 --> 00:06:44,750
good so we came along and said you know

00:06:42,530 --> 00:06:46,730
we need 10,000 lights a second we pay

00:06:44,750 --> 00:06:49,850
for that and then we find out we get

00:06:46,730 --> 00:06:51,650
throttled when we try and do 1,000 okay

00:06:49,850 --> 00:06:53,240
so how crap you know and this makes the

00:06:51,650 --> 00:06:54,650
system the maths that I did at the

00:06:53,240 --> 00:06:56,140
beginning that makes it all invalid and

00:06:54,650 --> 00:06:58,060
makes it ten times more expensive Tron

00:06:56,140 --> 00:07:00,740
dynamodb was the most expensive

00:06:58,060 --> 00:07:04,190
operational cost of the system so why

00:07:00,740 --> 00:07:05,390
why was this you know this was you know

00:07:04,190 --> 00:07:07,610
something we scratched our heads about

00:07:05,390 --> 00:07:10,610
for a while Amazon give you some really

00:07:07,610 --> 00:07:12,710
basic graphs which show you know fifteen

00:07:10,610 --> 00:07:16,160
thousand provision throughput one and a

00:07:12,710 --> 00:07:17,810
half thousand actual throughput and ten

00:07:16,160 --> 00:07:20,360
thousand almost 10,000 workers ekend

00:07:17,810 --> 00:07:22,280
being throttled so it's like it turns

00:07:20,360 --> 00:07:24,410
out like this isn't necessarily Amazon's

00:07:22,280 --> 00:07:26,780
fault this is more likely the my use

00:07:24,410 --> 00:07:27,890
case my patterns of usage and so we dug

00:07:26,780 --> 00:07:31,430
into it we read a lot there was a

00:07:27,890 --> 00:07:33,500
fantastic blog post by that company that

00:07:31,430 --> 00:07:35,450
does the analytics like multiplexing in

00:07:33,500 --> 00:07:36,920
the States about exactly the same thing

00:07:35,450 --> 00:07:38,660
came out at the same time as I was doing

00:07:36,920 --> 00:07:39,800
this and it turns out one of the

00:07:38,660 --> 00:07:42,740
concepts you've got to understand and

00:07:39,800 --> 00:07:44,660
dynamodb is you take a table and it

00:07:42,740 --> 00:07:46,400
partitions it up into 10 gigabyte chunks

00:07:44,660 --> 00:07:47,630
and then it takes you you take your

00:07:46,400 --> 00:07:49,550
provision throughput and you divide that

00:07:47,630 --> 00:07:51,590
by all of your partitions in your table

00:07:49,550 --> 00:07:54,440
and so for those who are paying

00:07:51,590 --> 00:07:56,300
attention as your table gets bigger the

00:07:54,440 --> 00:07:59,150
provision throughput per partition gets

00:07:56,300 --> 00:08:01,220
smaller and if you don't have absolutely

00:07:59,150 --> 00:08:02,750
perfect load balancing you're gonna

00:08:01,220 --> 00:08:04,070
start you know if you're slightly hot

00:08:02,750 --> 00:08:04,970
spotting one partition you're going to

00:08:04,070 --> 00:08:06,710
start getting throttled on that

00:08:04,970 --> 00:08:08,960
partition which eventually will just

00:08:06,710 --> 00:08:11,120
chuckle everything then that that was

00:08:08,960 --> 00:08:12,380
the first theory that we had and so he

00:08:11,120 --> 00:08:14,510
thought well what's a quick way of

00:08:12,380 --> 00:08:15,800
solving that what we thought we would do

00:08:14,510 --> 00:08:17,510
is we would had a job which would

00:08:15,800 --> 00:08:20,420
periodically rotate the tables create a

00:08:17,510 --> 00:08:21,740
new table every week update the codes

00:08:20,420 --> 00:08:24,440
that knew to read and write from the

00:08:21,740 --> 00:08:25,700
different tables and if you if you're

00:08:24,440 --> 00:08:27,380
interested in the analysis and the

00:08:25,700 --> 00:08:29,660
actual code it's in this issue here on

00:08:27,380 --> 00:08:32,900
github but we added this job down here

00:08:29,660 --> 00:08:33,979
distributor the table manager record it

00:08:32,900 --> 00:08:35,900
and it's job is just to sit there

00:08:33,979 --> 00:08:37,640
periodically create tables periodically

00:08:35,900 --> 00:08:40,070
do things like scale up the provision

00:08:37,640 --> 00:08:41,599
throughput on tables and scale down the

00:08:40,070 --> 00:08:43,520
provision throughput on old tables and

00:08:41,599 --> 00:08:46,070
this works pretty well this actually got

00:08:43,520 --> 00:08:48,140
us through Christmas basically without

00:08:46,070 --> 00:08:49,790
the thing you know exploding and I was

00:08:48,140 --> 00:08:52,160
pretty happy but then in the beginning

00:08:49,790 --> 00:08:53,450
of the new year we realized it's still

00:08:52,160 --> 00:08:55,519
not very good

00:08:53,450 --> 00:08:58,760
so we had problem number two which is

00:08:55,519 --> 00:09:00,170
write throughput again it turns out well

00:08:58,760 --> 00:09:01,970
I'm going to discuss this for us before

00:09:00,170 --> 00:09:03,769
I go into the details we were getting

00:09:01,970 --> 00:09:05,060
about 30 percent write throughput even

00:09:03,769 --> 00:09:06,920
though we were getting fresh tables

00:09:05,060 --> 00:09:08,300
every year every week we're getting

00:09:06,920 --> 00:09:10,040
about 30 percent at the write throughput

00:09:08,300 --> 00:09:12,560
and so I really had to go look at the

00:09:10,040 --> 00:09:14,329
schema and figure out why my write load

00:09:12,560 --> 00:09:17,540
was not you know more uniformly

00:09:14,329 --> 00:09:18,920
distributed so we've got the schema I

00:09:17,540 --> 00:09:21,560
went with it for those familiar with

00:09:18,920 --> 00:09:23,149
DynamoDB or not familiar the hash key is

00:09:21,560 --> 00:09:24,500
the key that you write to DynamoDB that

00:09:23,149 --> 00:09:27,199
they used to do the distribution between

00:09:24,500 --> 00:09:29,690
partitions and the range key is kind of

00:09:27,199 --> 00:09:31,310
used within a partition to allow you to

00:09:29,690 --> 00:09:33,620
do range queries which cortex is built

00:09:31,310 --> 00:09:35,149
on the ability to do range groups so the

00:09:33,620 --> 00:09:37,010
initial design we've got a user ID

00:09:35,149 --> 00:09:39,699
because this is multi-tenant we've got

00:09:37,010 --> 00:09:42,620
an hour because we're partitioning the

00:09:39,699 --> 00:09:45,170
index by time slices because it has some

00:09:42,620 --> 00:09:46,490
nice properties when you're doing things

00:09:45,170 --> 00:09:48,620
like having jobs that come and go very

00:09:46,490 --> 00:09:49,820
quickly and then we added the metric

00:09:48,620 --> 00:09:51,260
name and that was the hash gain that was

00:09:49,820 --> 00:09:52,970
all we were distributing the data on and

00:09:51,260 --> 00:09:54,320
then in the range key we have the label

00:09:52,970 --> 00:09:55,910
name the label value in the trunk ID and

00:09:54,320 --> 00:09:59,990
of course for every time series we'd

00:09:55,910 --> 00:10:01,040
write kind of 8 to 10 range keys and but

00:09:59,990 --> 00:10:02,329
of course anyone looking at this will

00:10:01,040 --> 00:10:03,829
realize that you know anyone who's run

00:10:02,329 --> 00:10:05,180
Prometheus will know that for some

00:10:03,829 --> 00:10:07,579
metric names or I think well as you

00:10:05,180 --> 00:10:10,220
saying yesterday Brian that you know 50

00:10:07,579 --> 00:10:11,600
to 80% of your you know Prometheus usage

00:10:10,220 --> 00:10:13,790
is really taken up by a handful of

00:10:11,600 --> 00:10:15,920
metrics and this is definitely true for

00:10:13,790 --> 00:10:18,290
this schema so 50 to 80 percent of all

00:10:15,920 --> 00:10:20,810
the right lobe was going to you know one

00:10:18,290 --> 00:10:22,820
or a small number of partitions so the

00:10:20,810 --> 00:10:26,360
solution we came up with was to move the

00:10:22,820 --> 00:10:27,829
label name into the hash key and this

00:10:26,360 --> 00:10:29,839
meant this actually is still not perfect

00:10:27,829 --> 00:10:31,730
and it still suffers quite badly but

00:10:29,839 --> 00:10:34,820
it's good enough and it actually gets us

00:10:31,730 --> 00:10:37,010
to 99% of saturation so by moving the

00:10:34,820 --> 00:10:38,990
label name to the hash key one of the

00:10:37,010 --> 00:10:40,850
things you'll notice is that you can't

00:10:38,990 --> 00:10:43,310
now do queries which just involve the

00:10:40,850 --> 00:10:44,990
metric name because you need to be able

00:10:43,310 --> 00:10:47,000
to construct the hash key before you do

00:10:44,990 --> 00:10:49,550
your query so we then had to write a

00:10:47,000 --> 00:10:51,529
second set of hash keys which look a lot

00:10:49,550 --> 00:10:53,149
more like this but in this case we just

00:10:51,529 --> 00:10:55,670
omit the label names and label values

00:10:53,149 --> 00:10:58,160
and just index the chunk IDs and say

00:10:55,670 --> 00:11:00,560
this around this time we realized you

00:10:58,160 --> 00:11:02,990
know the first schema we built is not

00:11:00,560 --> 00:11:04,579
really going to fly in fact the second

00:11:02,990 --> 00:11:06,270
schema we vote didn't fly either and

00:11:04,579 --> 00:11:07,530
neither did the third one and so

00:11:06,270 --> 00:11:09,480
this point we're like okay we need like

00:11:07,530 --> 00:11:11,610
a schema abstraction layer so we've got

00:11:09,480 --> 00:11:13,320
nice interface we built like this

00:11:11,610 --> 00:11:15,120
composite schema implementation which

00:11:13,320 --> 00:11:17,010
new for given time ranges to consult

00:11:15,120 --> 00:11:19,170
this schema and to consult this schema

00:11:17,010 --> 00:11:23,010
and since then since we implemented that

00:11:19,170 --> 00:11:24,690
we're now on schema v8 I think since

00:11:23,010 --> 00:11:26,010
then like lots of tweaks you know one of

00:11:24,690 --> 00:11:28,200
the things you'll also notice that we

00:11:26,010 --> 00:11:29,730
discussed last year is that you still

00:11:28,200 --> 00:11:33,420
need to know the metric name when doing

00:11:29,730 --> 00:11:34,680
the query and that's you know at the

00:11:33,420 --> 00:11:37,050
time when I wrote cortex or when I

00:11:34,680 --> 00:11:38,370
design cortex I wasn't aware of queries

00:11:37,050 --> 00:11:40,910
which didn't involve the metric name

00:11:38,370 --> 00:11:43,380
that were particularly useful now I am

00:11:40,910 --> 00:11:44,550
so we've also added or Aaron who's

00:11:43,380 --> 00:11:46,740
sitting in the back somewhere added

00:11:44,550 --> 00:11:47,760
recently the ability to do queries that

00:11:46,740 --> 00:11:49,890
don't involve the metric name so that

00:11:47,760 --> 00:11:51,480
said the third hash key that gets

00:11:49,890 --> 00:11:53,400
written just containing the user ID in

00:11:51,480 --> 00:11:54,150
the day and then we do some different

00:11:53,400 --> 00:11:55,710
tricks there to make sure the

00:11:54,150 --> 00:12:01,590
cardinality of that rows not too large

00:11:55,710 --> 00:12:03,780
so yes that's about it for that this

00:12:01,590 --> 00:12:05,900
gives us you know we get 99% over the

00:12:03,780 --> 00:12:08,670
provision through port and dynamodb now

00:12:05,900 --> 00:12:11,790
problem number three queries of death

00:12:08,670 --> 00:12:13,710
Prometheus is not really doesn't really

00:12:11,790 --> 00:12:15,990
do much to protect itself you know if

00:12:13,710 --> 00:12:17,490
you if you ask it for you know ten years

00:12:15,990 --> 00:12:19,890
worth of data it will happily go away

00:12:17,490 --> 00:12:23,190
and calculate ten years worth of data if

00:12:19,890 --> 00:12:25,080
you send it a label name with you know

00:12:23,190 --> 00:12:26,670
with a megabyte of binary in it one

00:12:25,080 --> 00:12:29,160
maybe a label value with a megabyte it

00:12:26,670 --> 00:12:30,180
will consume that and so this was a

00:12:29,160 --> 00:12:32,190
problem for us because we're running

00:12:30,180 --> 00:12:33,930
this as a service and you know we did

00:12:32,190 --> 00:12:37,950
have a few guys try and find the

00:12:33,930 --> 00:12:40,590
boundaries of this service at the time

00:12:37,950 --> 00:12:42,810
you'll notice in the beginning I didn't

00:12:40,590 --> 00:12:44,310
I didn't really highlight it but the

00:12:42,810 --> 00:12:46,140
read path just went directly through the

00:12:44,310 --> 00:12:47,610
same binaries as the right path so the

00:12:46,140 --> 00:12:50,070
distributor implemented the prompt or

00:12:47,610 --> 00:12:51,750
query engine when you did reads they

00:12:50,070 --> 00:12:53,010
went to distributors they were going hit

00:12:51,750 --> 00:12:55,050
lien jesters and go and read from the

00:12:53,010 --> 00:12:56,580
trunk store and everyone was happy but

00:12:55,050 --> 00:12:59,220
if you manage to find a query of death

00:12:56,580 --> 00:13:01,650
that will cause a distributor to boom or

00:12:59,220 --> 00:13:02,970
cause something to you know crash then

00:13:01,650 --> 00:13:05,040
you'd actually take out the right path

00:13:02,970 --> 00:13:06,060
and we care a lot about making sure we

00:13:05,040 --> 00:13:07,950
get your data in and we store it

00:13:06,060 --> 00:13:09,690
successfully so what we did is we

00:13:07,950 --> 00:13:12,120
separate adapter just tribute at that's

00:13:09,690 --> 00:13:13,560
gonna really annoy me we we separated

00:13:12,120 --> 00:13:15,420
out the query path out from the right

00:13:13,560 --> 00:13:18,660
path this binary is actually identical

00:13:15,420 --> 00:13:20,130
to the distributor we just we just run

00:13:18,660 --> 00:13:21,570
another set of services with

00:13:20,130 --> 00:13:23,400
different slightly different flags and

00:13:21,570 --> 00:13:25,950
then at the very top we just route

00:13:23,400 --> 00:13:27,960
requests to certain paths to the queer

00:13:25,950 --> 00:13:28,890
and this also meant we could scale the

00:13:27,960 --> 00:13:32,730
two differently because there's very

00:13:28,890 --> 00:13:34,320
different requirements you know in QPS

00:13:32,730 --> 00:13:36,210
and latency between our rights which are

00:13:34,320 --> 00:13:38,100
very very predictable you know they grow

00:13:36,210 --> 00:13:40,020
linearly with more customers but they

00:13:38,100 --> 00:13:41,550
don't tend to you know change dianely or

00:13:40,020 --> 00:13:44,430
anything like that and the querier which

00:13:41,550 --> 00:13:46,500
basically only gets traffic when you

00:13:44,430 --> 00:13:47,610
know people are in the office and one of

00:13:46,500 --> 00:13:49,170
the other things is someone able in the

00:13:47,610 --> 00:13:51,120
future is ours to kind of elastically

00:13:49,170 --> 00:13:53,310
scale this in response to demand but

00:13:51,120 --> 00:13:57,330
we've not done that yet so yeah that was

00:13:53,310 --> 00:13:59,250
that was problem number two three I

00:13:57,330 --> 00:14:00,570
thought my numbering long hula recording

00:13:59,250 --> 00:14:03,000
wasn't alert so this was as I mentioned

00:14:00,570 --> 00:14:04,350
earlier when we first launched cortex we

00:14:03,000 --> 00:14:06,270
just completely ignored recording rules

00:14:04,350 --> 00:14:07,710
and alerts and we had them just done in

00:14:06,270 --> 00:14:09,480
the original Prometheus and send that

00:14:07,710 --> 00:14:12,330
the actual data to the recording will

00:14:09,480 --> 00:14:14,130
data to us and so this was a job this

00:14:12,330 --> 00:14:15,630
was a project that John oh who can be

00:14:14,130 --> 00:14:17,460
here sir this week but John I did it

00:14:15,630 --> 00:14:19,350
we've works and he added an extra job

00:14:17,460 --> 00:14:22,170
called the ruler and this would sit

00:14:19,350 --> 00:14:25,020
there and periodically do a bunch of

00:14:22,170 --> 00:14:26,670
queries read from the and write the

00:14:25,020 --> 00:14:29,100
results in jest and basically implement

00:14:26,670 --> 00:14:30,930
recording rules we did this we one of

00:14:29,100 --> 00:14:33,810
the decisions we took was to not have

00:14:30,930 --> 00:14:36,740
the ruler run its queries through the

00:14:33,810 --> 00:14:38,670
query because again the usage pattern

00:14:36,740 --> 00:14:40,260
between the query and the room looked

00:14:38,670 --> 00:14:42,450
very different the ruler is again a

00:14:40,260 --> 00:14:44,430
constant load something you can scale

00:14:42,450 --> 00:14:45,750
you know reasonably well whereas the

00:14:44,430 --> 00:14:47,220
query is very unpredictable something

00:14:45,750 --> 00:14:49,920
you need to scale up and down on or do

00:14:47,220 --> 00:14:52,740
on demand so yes so now it's kind of

00:14:49,920 --> 00:14:53,880
getting a bit a bit much right there's a

00:14:52,740 --> 00:14:56,310
lot of a lot of things but it gets

00:14:53,880 --> 00:14:58,680
simpler the next one oh not quite next

00:14:56,310 --> 00:15:00,750
one one after so problem number four we

00:14:58,680 --> 00:15:03,300
had this one was actually my favorite

00:15:00,750 --> 00:15:05,520
problem because it involved some real

00:15:03,300 --> 00:15:09,630
like real software engineering and real

00:15:05,520 --> 00:15:10,320
debugging we noticed that everyone else

00:15:09,630 --> 00:15:13,710
were phonographs

00:15:10,320 --> 00:15:15,240
we noticed that the 99th percentile

00:15:13,710 --> 00:15:18,050
latency on the distributor spelled

00:15:15,240 --> 00:15:21,240
correctly this time is 80 milliseconds

00:15:18,050 --> 00:15:23,820
whereas on the in gesture is eight okay

00:15:21,240 --> 00:15:25,350
so if you're familiar with dynamo style

00:15:23,820 --> 00:15:26,970
dynamos different to dynamodb by the way

00:15:25,350 --> 00:15:30,450
dynamo style consistency is where you

00:15:26,970 --> 00:15:32,220
write to three replicas for instance and

00:15:30,450 --> 00:15:32,910
then wait for the response from two and

00:15:32,220 --> 00:15:34,440
then

00:15:32,910 --> 00:15:35,820
the request so you're just waiting for a

00:15:34,440 --> 00:15:37,830
response from a quorum of the replicas

00:15:35,820 --> 00:15:40,890
so in that kind of situation you would

00:15:37,830 --> 00:15:42,630
actually expect the latency profile of

00:15:40,890 --> 00:15:44,250
distributor to look very similar to the

00:15:42,630 --> 00:15:45,930
in gesture if not better you know

00:15:44,250 --> 00:15:47,820
there's a chance where one slow ingest

00:15:45,930 --> 00:15:49,320
though which would appear here wouldn't

00:15:47,820 --> 00:15:50,910
affect the distributor because it would

00:15:49,320 --> 00:15:52,500
just ignore the slow and Jess in gesture

00:15:50,910 --> 00:15:54,390
effectively so this is kind of the

00:15:52,500 --> 00:15:56,340
opposite of what I was expecting and I

00:15:54,390 --> 00:15:59,700
decided to spend a week or two digging

00:15:56,340 --> 00:16:01,590
into this I used all sorts of tools one

00:15:59,700 --> 00:16:03,180
of the things I'm particularly a big fan

00:16:01,590 --> 00:16:05,070
of is things like Zipkin and distributed

00:16:03,180 --> 00:16:06,750
tracing and that was actually the tool

00:16:05,070 --> 00:16:08,640
that really showed me there were these

00:16:06,750 --> 00:16:09,930
big pauses in the distributor where

00:16:08,640 --> 00:16:10,920
would parse a request it would do a

00:16:09,930 --> 00:16:12,570
bunch of stuff and then I couldn't quite

00:16:10,920 --> 00:16:13,530
figure out what it was doing and it

00:16:12,570 --> 00:16:14,790
turned out was garbage collection

00:16:13,530 --> 00:16:18,870
because goes really good at garbage

00:16:14,790 --> 00:16:21,900
collection the thing that eventually I

00:16:18,870 --> 00:16:23,340
realized after looking finally looking

00:16:21,900 --> 00:16:25,950
into the compiler that generated

00:16:23,340 --> 00:16:27,960
protobuf code is that proto buffers ingo

00:16:25,950 --> 00:16:29,730
or the default protobuf a compiler

00:16:27,960 --> 00:16:30,690
actually relies on reflection like it

00:16:29,730 --> 00:16:32,490
generates some strux

00:16:30,690 --> 00:16:34,590
but the actual marshalling turned from

00:16:32,490 --> 00:16:36,480
those trucks is reflection based so the

00:16:34,590 --> 00:16:38,750
minute I moved away from that and to

00:16:36,480 --> 00:16:40,590
Senate called gogo prototype which is a

00:16:38,750 --> 00:16:42,900
protobuf compiler that actually

00:16:40,590 --> 00:16:43,830
generates real code and allows you to do

00:16:42,900 --> 00:16:46,800
things like and not have as many

00:16:43,830 --> 00:16:48,450
pointers not have as many pointers and

00:16:46,800 --> 00:16:51,180
add your own types in there to do things

00:16:48,450 --> 00:16:53,670
like zero copy we managed to get the

00:16:51,180 --> 00:16:56,730
latency from 80 to 100 milliseconds down

00:16:53,670 --> 00:16:58,440
to 25 and this looks a lot more like the

00:16:56,730 --> 00:17:00,120
latency profile I didn't quite believe

00:16:58,440 --> 00:17:01,380
you know this is getting quite late at

00:17:00,120 --> 00:17:03,900
night I didn't quite believe that I'd

00:17:01,380 --> 00:17:04,950
managed to reduce it by like 4x so I

00:17:03,900 --> 00:17:07,860
rolled back the change

00:17:04,950 --> 00:17:09,360
and I thought crap it really did it

00:17:07,860 --> 00:17:10,770
really did reduce it by 4x so then I

00:17:09,360 --> 00:17:12,600
rolled it you know vote it forward again

00:17:10,770 --> 00:17:13,880
and and wrote a nice blog post which you

00:17:12,600 --> 00:17:18,090
should check out

00:17:13,880 --> 00:17:19,770
so finally problem number 5 cost I

00:17:18,090 --> 00:17:21,450
alluded to this at the beginning the

00:17:19,770 --> 00:17:25,020
system cost is a lot more to run than we

00:17:21,450 --> 00:17:26,610
expected the mass was slightly off but

00:17:25,020 --> 00:17:28,230
also just generally I hadn't considered

00:17:26,610 --> 00:17:29,220
certain possibilities and one of the

00:17:28,230 --> 00:17:31,140
things you'll notice at the very

00:17:29,220 --> 00:17:33,810
beginning we were putting the chunks in

00:17:31,140 --> 00:17:36,150
s3 and we're indexing them in dynamo DB

00:17:33,810 --> 00:17:38,220
and s3 and DynamoDB have very different

00:17:36,150 --> 00:17:39,660
cost structures I don't really

00:17:38,220 --> 00:17:41,010
understand this at the time but once you

00:17:39,660 --> 00:17:42,270
kind of normalize them so you're looking

00:17:41,010 --> 00:17:44,430
at the same units you end up at the

00:17:42,270 --> 00:17:47,210
table that kind of looks like this which

00:17:44,430 --> 00:17:49,169
fits very pleasingly with these monitors

00:17:47,210 --> 00:17:52,950
dynamodb is an order of magnitude

00:17:49,169 --> 00:17:54,690
cheaper to do i ops to to do operations

00:17:52,950 --> 00:17:57,419
on but an order of magnitude more

00:17:54,690 --> 00:17:59,250
expensive to store data in and this

00:17:57,419 --> 00:18:00,960
can't be coincidence like surely Amazon

00:17:59,250 --> 00:18:03,090
have done this on purpose and if you

00:18:00,960 --> 00:18:04,740
plot this on a graph so you've got the

00:18:03,090 --> 00:18:06,960
size of the object you're storing and

00:18:04,740 --> 00:18:09,480
the cost of storing it you'll notice for

00:18:06,960 --> 00:18:11,190
small objects it's cheaper destroyed in

00:18:09,480 --> 00:18:13,409
DynamoDB and for large objects it's more

00:18:11,190 --> 00:18:16,020
expensive and then it's cheaper to put

00:18:13,409 --> 00:18:17,370
it in s3 and our objects were chunks

00:18:16,020 --> 00:18:20,309
they were one kilobyte in size they're

00:18:17,370 --> 00:18:21,899
like down here and so of course it makes

00:18:20,309 --> 00:18:24,779
a lot more sense to put them in dynamodb

00:18:21,899 --> 00:18:27,840
so we got rid of s3 we actually finally

00:18:24,779 --> 00:18:29,370
managed to delete a moving part put it

00:18:27,840 --> 00:18:31,710
on in dynamodb and we reduces the cost

00:18:29,370 --> 00:18:33,559
of running this thing by over 50% which

00:18:31,710 --> 00:18:36,299
is pretty good place work

00:18:33,559 --> 00:18:39,750
finally the hospital doesn't maybe again

00:18:36,299 --> 00:18:42,899
I was getting kind of fed up at this

00:18:39,750 --> 00:18:44,399
point at the same time I decided to

00:18:42,899 --> 00:18:47,220
leave we've works and go it on my own

00:18:44,399 --> 00:18:49,590
and I decided to reevaluate whether I

00:18:47,220 --> 00:18:50,820
wanted to use dynamodb and if you go and

00:18:49,590 --> 00:18:52,020
talk to the weaver works guys I'm sure

00:18:50,820 --> 00:18:53,850
they'll tell you a lot more stories

00:18:52,020 --> 00:18:55,770
about their worries with dynamodb but

00:18:53,850 --> 00:18:58,470
what I decided to do is move to BigTable

00:18:55,770 --> 00:19:02,700
so Google's Google clouds version of

00:18:58,470 --> 00:19:04,169
dynamodb effectively we did this it

00:19:02,700 --> 00:19:05,669
turned out was super easy there was

00:19:04,169 --> 00:19:07,020
already an abstraction layer inside

00:19:05,669 --> 00:19:09,210
cortex that you just had to implement

00:19:07,020 --> 00:19:11,460
and it took us a few days to implement

00:19:09,210 --> 00:19:13,860
and get it working in debug and we ended

00:19:11,460 --> 00:19:15,779
up with some really nice results firstly

00:19:13,860 --> 00:19:17,909
the number the amount of code you need

00:19:15,779 --> 00:19:19,590
to write to BigTable about four hundred

00:19:17,909 --> 00:19:21,870
lines of code versus the two thousand

00:19:19,590 --> 00:19:23,190
that it tooks writes dynamodb now that

00:19:21,870 --> 00:19:24,419
seems a bit strange but if you're going

00:19:23,190 --> 00:19:25,980
through the dynamodb code you'll see

00:19:24,419 --> 00:19:27,990
that there's like exponential back-off

00:19:25,980 --> 00:19:29,669
and there's all sorts of retry logic and

00:19:27,990 --> 00:19:31,289
there's extra monitoring because we had

00:19:29,669 --> 00:19:33,299
so much problems saturating the

00:19:31,289 --> 00:19:35,429
bandwidth and there's you know it's just

00:19:33,299 --> 00:19:37,890
a really hard API to work against

00:19:35,429 --> 00:19:40,350
BigTable on the other hand G RPC API

00:19:37,890 --> 00:19:42,330
it's all auto-generated which works

00:19:40,350 --> 00:19:43,770
really well I don't work for Google by

00:19:42,330 --> 00:19:47,429
the way like I just actually like

00:19:43,770 --> 00:19:51,390
BigTable write performance is very

00:19:47,429 --> 00:19:52,440
similar slightly wider spread but read

00:19:51,390 --> 00:19:54,240
performance was the other nice thing

00:19:52,440 --> 00:19:57,000
like the read performance of BigTable

00:19:54,240 --> 00:19:58,350
was significantly better and this is

00:19:57,000 --> 00:19:59,710
probably because my big table is over

00:19:58,350 --> 00:20:01,000
provisioned but all

00:19:59,710 --> 00:20:02,710
like the provisioning model between

00:20:01,000 --> 00:20:04,570
BigTable and dynamo DB is completely

00:20:02,710 --> 00:20:07,390
different in DynamoDB I had to

00:20:04,570 --> 00:20:09,700
statically assign you know how much read

00:20:07,390 --> 00:20:11,500
and write throughput I wanted BigTable I

00:20:09,700 --> 00:20:13,510
pick how many instances I want you know

00:20:11,500 --> 00:20:16,210
and and and it just shares the available

00:20:13,510 --> 00:20:18,279
CPU and memories to all of that and with

00:20:16,210 --> 00:20:22,480
all of this it's less than third of the

00:20:18,279 --> 00:20:27,899
price as well so you know Google should

00:20:22,480 --> 00:20:30,520
pay me for this so closing thoughts

00:20:27,899 --> 00:20:32,169
DynamoDB right throughput was like I

00:20:30,520 --> 00:20:33,850
literally spent for three months of my

00:20:32,169 --> 00:20:36,130
life getting to the stage where I could

00:20:33,850 --> 00:20:37,289
saturate DynamoDB and which is two or

00:20:36,130 --> 00:20:39,490
three months I'm never going to get back

00:20:37,289 --> 00:20:41,710
the recording wasn't Alerts was really

00:20:39,490 --> 00:20:43,120
cool when we added it the longtail

00:20:41,710 --> 00:20:44,350
that's actually I would really encourage

00:20:43,120 --> 00:20:46,390
you to go and read the blog post because

00:20:44,350 --> 00:20:47,980
using all sorts of tools it shows you

00:20:46,390 --> 00:20:50,020
how to use them and that was something I

00:20:47,980 --> 00:20:51,130
was really proud of and the cost work is

00:20:50,020 --> 00:20:54,279
something I probably should have done in

00:20:51,130 --> 00:20:55,990
the design and since then you know I've

00:20:54,279 --> 00:20:58,210
decided to add cost sections for my

00:20:55,990 --> 00:21:00,399
design documents and DynamoDB in

00:20:58,210 --> 00:21:03,220
BigTable was an interesting discovery so

00:21:00,399 --> 00:21:05,440
some extra data we've been running this

00:21:03,220 --> 00:21:08,860
thing for over twelve months now if we

00:21:05,440 --> 00:21:10,720
look at the availability we're not where

00:21:08,860 --> 00:21:12,190
we're you know ninety nine point eight

00:21:10,720 --> 00:21:16,510
ninety nine point nine percent available

00:21:12,190 --> 00:21:19,480
for for reads the main problems we've

00:21:16,510 --> 00:21:21,700
had here have been the outage that

00:21:19,480 --> 00:21:24,039
s3 had where you couldn't read or write

00:21:21,700 --> 00:21:25,960
mystery and so we had a hard dependency

00:21:24,039 --> 00:21:27,159
on s3 where we actually kind of didn't

00:21:25,960 --> 00:21:29,020
need to and we could have could have

00:21:27,159 --> 00:21:31,270
returned partially degraded results and

00:21:29,020 --> 00:21:33,850
that's really the only the only problem

00:21:31,270 --> 00:21:36,279
we had their durability we've lost a lot

00:21:33,850 --> 00:21:40,480
less than two days of data but it's hard

00:21:36,279 --> 00:21:42,100
to put a bound on it especially as we've

00:21:40,480 --> 00:21:45,130
had about three incidents where we've

00:21:42,100 --> 00:21:46,659
potentially lost data and the amount of

00:21:45,130 --> 00:21:48,549
data kept in the injustice is about

00:21:46,659 --> 00:21:50,260
twelve hours maybe a little bit more and

00:21:48,549 --> 00:21:51,370
because we've lost bits here and bits

00:21:50,260 --> 00:21:53,289
there it's hard for us to know exactly

00:21:51,370 --> 00:21:54,610
what data we've lost so this is the

00:21:53,289 --> 00:21:57,600
pessimistic estimate is we're about

00:21:54,610 --> 00:22:00,070
99.5% durable over the last year

00:21:57,600 --> 00:22:02,470
performance this is against BigTable

00:22:00,070 --> 00:22:04,960
sorry against dynamodb we've found that

00:22:02,470 --> 00:22:06,970
performance for writes is exceptional

00:22:04,960 --> 00:22:08,620
because it's just did memory performance

00:22:06,970 --> 00:22:10,510
for reads is two to three hundred

00:22:08,620 --> 00:22:12,100
milliseconds that's a bit optimistic

00:22:10,510 --> 00:22:13,210
that number maybe that one's against

00:22:12,100 --> 00:22:14,620
BigTable I have found the

00:22:13,210 --> 00:22:16,450
table reforms to be significant about

00:22:14,620 --> 00:22:18,250
the big problem at the read performance

00:22:16,450 --> 00:22:19,899
is it's highly variable you know if you

00:22:18,250 --> 00:22:22,299
do a query that involves a lot of time

00:22:19,899 --> 00:22:24,520
series it's slower than Prometheus if

00:22:22,299 --> 00:22:27,010
you do queries you know lots of queries

00:22:24,520 --> 00:22:28,510
in parallel that that involves fewer

00:22:27,010 --> 00:22:29,770
number of time series then it's faster

00:22:28,510 --> 00:22:32,440
than a single Prometheus because it's

00:22:29,770 --> 00:22:33,820
got more resources but yeah I mean I'm

00:22:32,440 --> 00:22:35,080
pretty happy with the performance it's

00:22:33,820 --> 00:22:40,600
nice and interactive and it works quite

00:22:35,080 --> 00:22:42,220
well so where are we going with cortex

00:22:40,600 --> 00:22:44,350
in the future now that I'm working on

00:22:42,220 --> 00:22:45,850
cortex full time bit of a change of

00:22:44,350 --> 00:22:47,230
direction from from what I was doing

00:22:45,850 --> 00:22:48,820
we've works one of the things I want to

00:22:47,230 --> 00:22:50,890
do is integrate it more closely with

00:22:48,820 --> 00:22:55,529
Prometheus been talking a lot with

00:22:50,890 --> 00:22:57,880
Fabian who can't see but and where is he

00:22:55,529 --> 00:23:00,490
and chop it back whose name sketching

00:22:57,880 --> 00:23:01,899
I'm really sorry talking a lot about

00:23:00,490 --> 00:23:03,820
doing direct chunk writes from from

00:23:01,899 --> 00:23:05,470
easiest straight into the trunk store so

00:23:03,820 --> 00:23:06,850
avoiding the hole and gesture this will

00:23:05,470 --> 00:23:07,899
make it a lot cheaper to run and will

00:23:06,850 --> 00:23:11,919
give people the kind of long-term

00:23:07,899 --> 00:23:13,210
storage they seem to want a bit more we

00:23:11,919 --> 00:23:14,830
want to do a second we want to separate

00:23:13,210 --> 00:23:17,010
out the ingest or index apparently the

00:23:14,830 --> 00:23:19,360
the in jesters have an in memory index

00:23:17,010 --> 00:23:21,100
that helps them answer queries against

00:23:19,360 --> 00:23:22,809
the last kind of twelve hours of data we

00:23:21,100 --> 00:23:24,220
want to move that out into a separate

00:23:22,809 --> 00:23:25,690
service and this will allow us to do a

00:23:24,220 --> 00:23:27,909
different kind of load balancing and

00:23:25,690 --> 00:23:29,470
distribution on that index as we do on

00:23:27,909 --> 00:23:31,870
the time series and so will generally

00:23:29,470 --> 00:23:33,970
allow us to do better load balancing I

00:23:31,870 --> 00:23:35,320
definitely want to use Prometheus TS DB

00:23:33,970 --> 00:23:38,559
for the in jesters I've got a kind of

00:23:35,320 --> 00:23:39,789
you know Prometheus one esque system

00:23:38,559 --> 00:23:41,230
that I've kind of hacked up and fought

00:23:39,789 --> 00:23:43,149
from there and I want to move it forward

00:23:41,230 --> 00:23:45,549
to the new thing I've already discussed

00:23:43,149 --> 00:23:47,500
moving to gossip for the ring storage

00:23:45,549 --> 00:23:49,149
and maybe adding a CD maybe looking at a

00:23:47,500 --> 00:23:51,580
little kV and stuff like that for an

00:23:49,149 --> 00:23:53,260
abstraction layer there and I did the

00:23:51,580 --> 00:23:56,380
same calculations that I did for s3 and

00:23:53,260 --> 00:23:58,899
DynamoDB for Google Cloud Storage which

00:23:56,380 --> 00:24:00,370
is their s3 competitor and BigTable and

00:23:58,899 --> 00:24:01,809
it turns out these answer is the

00:24:00,370 --> 00:24:03,190
opposite it turns out it's actually

00:24:01,809 --> 00:24:05,409
cheaper to put the chunks in cloud

00:24:03,190 --> 00:24:06,610
storage this time and so that's

00:24:05,409 --> 00:24:07,890
something I want to do you want to make

00:24:06,610 --> 00:24:09,820
the system also Cloud Storage has

00:24:07,890 --> 00:24:12,309
incredibly good latency I don't know how

00:24:09,820 --> 00:24:13,450
they do it but the the s3 latency for

00:24:12,309 --> 00:24:15,690
small objects is atrocious

00:24:13,450 --> 00:24:17,440
like you know 99% I was like a second

00:24:15,690 --> 00:24:21,220
counselor which doesn't have that is

00:24:17,440 --> 00:24:23,470
much much faster so really quickly one

00:24:21,220 --> 00:24:25,690
more thing as I've alluded to I left

00:24:23,470 --> 00:24:26,800
we've work so well I've left we've works

00:24:25,690 --> 00:24:28,810
at the end of June to focus on

00:24:26,800 --> 00:24:29,830
Prometheus and cortex development since

00:24:28,810 --> 00:24:32,560
then I've teamed up a chat with David

00:24:29,830 --> 00:24:33,700
who's in the corner over there and we're

00:24:32,560 --> 00:24:35,110
working together on some ideas around

00:24:33,700 --> 00:24:36,400
Prometheus and logging and tracing as

00:24:35,110 --> 00:24:38,440
I'm now like you know we're

00:24:36,400 --> 00:24:40,690
self-employed and we're looking for work

00:24:38,440 --> 00:24:42,100
this is seeing a little pitch you know

00:24:40,690 --> 00:24:43,630
if anyone wants some Prometheus hosting

00:24:42,100 --> 00:24:46,690
or consulting or training or support or

00:24:43,630 --> 00:24:48,790
anything you know just give us a ping

00:24:46,690 --> 00:24:50,200
we've registered a little website and

00:24:48,790 --> 00:24:51,940
stuff you should check it out I just

00:24:50,200 --> 00:24:53,740
want to give you a quick idea of some of

00:24:51,940 --> 00:24:58,060
our ideas most of this is complete demo

00:24:53,740 --> 00:25:00,580
work this works this is our new UI for

00:24:58,060 --> 00:25:03,190
cortex so you can do tab completion and

00:25:00,580 --> 00:25:06,040
all the usual things and the graphics

00:25:03,190 --> 00:25:07,120
awfully low resolution I apologize but

00:25:06,040 --> 00:25:08,800
that works really well but one of the

00:25:07,120 --> 00:25:09,580
things we've one of the ideas we've got

00:25:08,800 --> 00:25:11,650
that we've worked on that we've

00:25:09,580 --> 00:25:13,900
prototyped is the ability to use the

00:25:11,650 --> 00:25:15,340
cortex index I talked about separating

00:25:13,900 --> 00:25:17,800
out the in gesture index and the chunks

00:25:15,340 --> 00:25:20,260
door index use that also index log data

00:25:17,800 --> 00:25:21,640
so we've got a mode where you can switch

00:25:20,260 --> 00:25:24,670
to looking at log data and it preserves

00:25:21,640 --> 00:25:25,840
the selector you've used because it's

00:25:24,670 --> 00:25:27,280
just using the same indexing mechanism

00:25:25,840 --> 00:25:29,080
it gives you a stream of log entries

00:25:27,280 --> 00:25:30,340
instead and this allows you to chop and

00:25:29,080 --> 00:25:33,790
change between metrics and logs really

00:25:30,340 --> 00:25:34,870
quickly at least that's our ID and we

00:25:33,790 --> 00:25:36,460
want to do the same thing and this is

00:25:34,870 --> 00:25:37,960
really verging into the world of demo

00:25:36,460 --> 00:25:39,520
where now we want to do the same thing

00:25:37,960 --> 00:25:40,890
with tracing I've got a project that I

00:25:39,520 --> 00:25:43,300
was working on a while ago called loci

00:25:40,890 --> 00:25:45,130
which does kind of Prometheus style

00:25:43,300 --> 00:25:48,220
school based tracing and I want

00:25:45,130 --> 00:25:49,450
integrate that into here as well yeah

00:25:48,220 --> 00:25:51,310
basically zipping yeah

00:25:49,450 --> 00:25:53,590
it's the same idea at least different

00:25:51,310 --> 00:25:57,110
codes written go uses protobufs anyway

00:25:53,590 --> 00:26:00,970
that's it thank you

00:25:57,110 --> 00:26:00,970
[Applause]

00:26:02,060 --> 00:26:10,830
so thank you Tom

00:26:04,080 --> 00:26:13,460
questions five minutes hi two questions

00:26:10,830 --> 00:26:15,870
first of all is any of that open source

00:26:13,460 --> 00:26:17,820
all of cortex is open source

00:26:15,870 --> 00:26:19,590
could all of loci the trace and stuff is

00:26:17,820 --> 00:26:21,710
open source the logging stuff is too

00:26:19,590 --> 00:26:24,510
embarrassing to open source but will be

00:26:21,710 --> 00:26:26,790
and second question maybe I

00:26:24,510 --> 00:26:30,420
misunderstand but if you do beat queries

00:26:26,790 --> 00:26:34,260
like big requests at some point you'll

00:26:30,420 --> 00:26:37,200
have to get all all data back to one

00:26:34,260 --> 00:26:39,030
prometheus instance so are you is that

00:26:37,200 --> 00:26:43,110
true and are you so limited by the

00:26:39,030 --> 00:26:45,270
memory you have on the Prometheus that

00:26:43,110 --> 00:26:49,380
yeah from from which the request Square

00:26:45,270 --> 00:26:51,150
was issued or so right now the the query

00:26:49,380 --> 00:26:52,860
the query service in here isn't actually

00:26:51,150 --> 00:26:54,780
a Prometheus it's a customized service

00:26:52,860 --> 00:26:56,820
all right I uses a from easy experience

00:26:54,780 --> 00:26:57,930
and so right now yes it's true but one

00:26:56,820 --> 00:26:59,670
of the things I want to do in the future

00:26:57,930 --> 00:27:02,370
one of the potential query optimizations

00:26:59,670 --> 00:27:04,290
is slicing up and parallelizing big

00:27:02,370 --> 00:27:08,340
queries in the time dimension and with

00:27:04,290 --> 00:27:10,680
us or under aggregation I mean yeah I'm

00:27:08,340 --> 00:27:12,120
hoping in there's a lot of detail and a

00:27:10,680 --> 00:27:14,490
lot of trickiness and a lot of overlaps

00:27:12,120 --> 00:27:16,200
or so on but I'm hoping for instance if

00:27:14,490 --> 00:27:17,580
you do a month query I can bisect it in

00:27:16,200 --> 00:27:19,650
today queries and then do all the days

00:27:17,580 --> 00:27:21,120
in parallel and respond quicker and not

00:27:19,650 --> 00:27:22,740
be limited but right now you are limited

00:27:21,120 --> 00:27:25,080
right okay thanks so that not to be a

00:27:22,740 --> 00:27:26,460
massive practical problem but but yeah I

00:27:25,080 --> 00:27:27,570
want to do I want to do optimizations is

00:27:26,460 --> 00:27:31,640
why I think you can do in a distributed

00:27:27,570 --> 00:27:33,000
system that's harder to do in fluid okay

00:27:31,640 --> 00:27:38,550
next question

00:27:33,000 --> 00:27:40,770
hi so do you have any plans to to port

00:27:38,550 --> 00:27:42,540
the storage engine to not depend on

00:27:40,770 --> 00:27:44,790
cloud-based storage engines but

00:27:42,540 --> 00:27:46,740
something you can self host are you are

00:27:44,790 --> 00:27:48,300
you a plump as I was literally talking

00:27:46,740 --> 00:27:51,270
somebody about it yesterday there's an

00:27:48,300 --> 00:27:52,410
abstraction layer and I've kind of half

00:27:51,270 --> 00:27:55,950
hacked up of those in the works on

00:27:52,410 --> 00:27:56,790
Cassandra already I'm looking at I don't

00:27:55,950 --> 00:27:58,500
know other ones

00:27:56,790 --> 00:28:00,270
suggestions I'm working with another

00:27:58,500 --> 00:28:01,590
company that has their own close source

00:28:00,270 --> 00:28:04,290
one that they want to have a storage

00:28:01,590 --> 00:28:07,530
engine for though yeah there's a nice

00:28:04,290 --> 00:28:09,120
abstraction there yeah like my favorite

00:28:07,530 --> 00:28:11,820
example is cockroach TB because it's

00:28:09,120 --> 00:28:12,130
also a nice go based database yeah can

00:28:11,820 --> 00:28:15,730
do

00:28:12,130 --> 00:28:18,190
indexing and storage and cortex has a

00:28:15,730 --> 00:28:19,870
has like very loose requirements on its

00:28:18,190 --> 00:28:22,000
storage as long as you can like do range

00:28:19,870 --> 00:28:23,950
queries and get data from it so

00:28:22,000 --> 00:28:24,700
cockroach DB is like massive overkill

00:28:23,950 --> 00:28:26,920
for what we need

00:28:24,700 --> 00:28:29,170
yeah but yeah sure like it doesn't

00:28:26,920 --> 00:28:31,000
refraction layer it's really easy to to

00:28:29,170 --> 00:28:32,710
add as we just demonstrated four hundred

00:28:31,000 --> 00:28:34,020
lines of code talks to BigTable so nice

00:28:32,710 --> 00:28:37,950
okay so yeah

00:28:34,020 --> 00:28:37,950
Sneha if you get ready please

00:28:46,500 --> 00:28:55,120
this garlic's how much is a premiership

00:28:50,200 --> 00:28:57,850
still it's a good question it uses a lot

00:28:55,120 --> 00:28:59,920
of Prometheus it uses the query engine

00:28:57,850 --> 00:29:01,930
completely unmodified and the chunk

00:28:59,920 --> 00:29:04,030
storage completely unmodified and more

00:29:01,930 --> 00:29:05,800
recently a lot of the api's internally

00:29:04,030 --> 00:29:07,450
within cortex have been ported up to

00:29:05,800 --> 00:29:09,100
Prometheus so the remote API is the

00:29:07,450 --> 00:29:12,220
remote right and the read API is used

00:29:09,100 --> 00:29:13,840
exactly as a proto buffers the API is

00:29:12,220 --> 00:29:17,320
completely the same uses the same API

00:29:13,840 --> 00:29:19,360
code other than that it's all custom so

00:29:17,320 --> 00:29:21,640
it you know there's a lot of custom code

00:29:19,360 --> 00:29:23,470
there I mean if you look in the cortex

00:29:21,640 --> 00:29:25,420
repo it's all custom code because we

00:29:23,470 --> 00:29:27,070
just vendor in Prometheus and I I think

00:29:25,420 --> 00:29:28,630
I should have a line of code count but

00:29:27,070 --> 00:29:31,710
it's you know it's probably like 20,000

00:29:28,630 --> 00:29:36,310
lines of code there's a lot thank you

00:29:31,710 --> 00:29:38,760
any more questions cool thank you very

00:29:36,310 --> 00:29:38,760
much

00:29:39,020 --> 00:29:51,720
[Music]

00:29:51,530 --> 00:29:54,859
you

00:29:51,720 --> 00:29:54,859

YouTube URL: https://www.youtube.com/watch?v=_8DmPW4iQBQ


