Title: PromCon 2017: Analyze Prometheus Metrics like a Data Scientist - Georg Öttl
Publication date: 2017-09-01
Playlist: PromCon 2017
Description: 
	* Abstract:

Gathering software metrics with Prometheus is great and easy. However, at some point there are too many timeseries to craft handwritten rule based alert systems. In this talk I will show how to export data from the Prometheus HTTP API, show how and what to analyze with open-source tools like R, Python SciPi and describe why DevOps and Whitebox Monitoring fits so great here. As an outlook I will show how to integrate/export timeseries to machine learning services.

* Speaker biography:

Georg Öttl is an IT professional with an agile software development portfolio and 15+ years of professional experience. His focus so far was on Software Development, Knowledge Discovery/Data Science Services and IT-Security. He is  a Full Stack developer, DevOps enthusiast and Continuous Delivery specialist.

* Slides:

* PromCon website:

https://promcon.io/
Captions: 
	00:00:00,380 --> 00:00:16,920
[Music]

00:00:12,950 --> 00:00:19,619
so next time we'll have George fo he'll

00:00:16,920 --> 00:00:21,660
talk about analyzing metrics like a data

00:00:19,619 --> 00:00:23,070
scientist and he'll be pleased to know

00:00:21,660 --> 00:00:26,519
that somewhere in our bucket list

00:00:23,070 --> 00:00:29,820
there's an our exporter from what we

00:00:26,519 --> 00:00:34,050
have in privy to our own assets hello

00:00:29,820 --> 00:00:35,850
everybody my name is gear acquittal and

00:00:34,050 --> 00:00:38,670
that start was to talk analyze

00:00:35,850 --> 00:00:42,210
Prometheus metrics like like a data

00:00:38,670 --> 00:00:45,780
scientist to my person itself so to put

00:00:42,210 --> 00:00:48,840
everything I say into context me I'm an

00:00:45,780 --> 00:00:51,480
enterprise software developer having a

00:00:48,840 --> 00:00:53,010
background in data science services and

00:00:51,480 --> 00:00:56,460
thinking around the last two years

00:00:53,010 --> 00:00:58,800
around the topic of death of deaf

00:00:56,460 --> 00:01:01,590
development and DevOps and Holmes know

00:00:58,800 --> 00:01:04,920
basically I'm a developer who likes math

00:01:01,590 --> 00:01:08,159
and I'm the second guy here will go to

00:01:04,920 --> 00:01:09,869
paternity leave in a few days so my

00:01:08,159 --> 00:01:12,390
employee will be a eight month old

00:01:09,869 --> 00:01:15,270
toddler and my daughter and next

00:01:12,390 --> 00:01:23,009
starting next week I would say good the

00:01:15,270 --> 00:01:24,930
objective of this talk is I wanna I

00:01:23,009 --> 00:01:27,900
wanted in the beginning pushing the

00:01:24,930 --> 00:01:31,640
limits of trauma tours and I wanted to

00:01:27,900 --> 00:01:34,380
know their models the alerting models

00:01:31,640 --> 00:01:35,700
the predictions I had in my dashboards

00:01:34,380 --> 00:01:38,700
are the good are not good

00:01:35,700 --> 00:01:42,090
is there a way to make it better than I

00:01:38,700 --> 00:01:46,049
have done so far and according to my

00:01:42,090 --> 00:01:49,170
background I actually took the chance

00:01:46,049 --> 00:01:51,899
and tried to use some data science

00:01:49,170 --> 00:01:55,380
methods and open source data science

00:01:51,899 --> 00:01:58,079
tools to improve and this was my goal

00:01:55,380 --> 00:02:00,840
and my aim to improve our rules and our

00:01:58,079 --> 00:02:06,930
model I have and the prediction they

00:02:00,840 --> 00:02:09,830
have and like Prometheus I wanted to

00:02:06,930 --> 00:02:13,110
bring a bit more light into the dark

00:02:09,830 --> 00:02:15,240
I wanted to bring I wanted to bring more

00:02:13,110 --> 00:02:18,590
light into the dark of what's going on

00:02:15,240 --> 00:02:21,750
in the software we were monitoring and

00:02:18,590 --> 00:02:24,950
this was my objective when I started

00:02:21,750 --> 00:02:27,630
with all this work at the beginning

00:02:24,950 --> 00:02:30,090
inert spoiler

00:02:27,630 --> 00:02:32,700
this is about the topic where you could

00:02:30,090 --> 00:02:35,070
drift off into official intelligence and

00:02:32,700 --> 00:02:36,600
machine learning and so on this is the

00:02:35,070 --> 00:02:39,300
warning should I do

00:02:36,600 --> 00:02:43,580
use advanced mathematics and should I

00:02:39,300 --> 00:02:46,410
use advanced methods to analyze the data

00:02:43,580 --> 00:02:48,450
from either this gathering or not it's a

00:02:46,410 --> 00:02:50,880
rule of thumb if you have an system that

00:02:48,450 --> 00:02:53,370
is doing well you're getting alerts you

00:02:50,880 --> 00:02:55,830
don't get awake at night without no

00:02:53,370 --> 00:02:58,140
reasons then keep it as it is stay with

00:02:55,830 --> 00:02:59,760
the rule-based systems on the other hand

00:02:58,140 --> 00:03:01,860
if you're not satisfied with your alerts

00:02:59,760 --> 00:03:04,110
your rules and your predictions data

00:03:01,860 --> 00:03:05,760
science might give you some insight into

00:03:04,110 --> 00:03:08,010
how your data is structured and what

00:03:05,760 --> 00:03:09,990
hidden patterns are in the data that you

00:03:08,010 --> 00:03:12,140
could use and could exploit to improve

00:03:09,990 --> 00:03:16,769
your your alerting and your data model

00:03:12,140 --> 00:03:19,110
so big thing where do you want you to be

00:03:16,769 --> 00:03:21,750
when you're of where was I or where I

00:03:19,110 --> 00:03:24,120
expect somebody to be when you start to

00:03:21,750 --> 00:03:27,660
think about doing further analysis of

00:03:24,120 --> 00:03:31,800
the data Prometheus has gathered you

00:03:27,660 --> 00:03:34,560
already I'm sorry I'm but walking around

00:03:31,800 --> 00:03:35,550
them and so but what do you have when

00:03:34,560 --> 00:03:38,280
you start you have the great

00:03:35,550 --> 00:03:41,489
architecture and and the great quality

00:03:38,280 --> 00:03:43,110
of data promises is providing to you you

00:03:41,489 --> 00:03:45,540
have from a data scientists point of

00:03:43,110 --> 00:03:47,340
view only in America data now yeah

00:03:45,540 --> 00:03:49,230
you're happy you can apply functions on

00:03:47,340 --> 00:03:51,360
it these functions are good and help you

00:03:49,230 --> 00:03:53,670
to create your alerts and your alert

00:03:51,360 --> 00:03:57,299
system you have is easy and fast to

00:03:53,670 --> 00:03:59,640
navigate from ql language that you

00:03:57,299 --> 00:04:01,590
allows you to inspect subsets of the

00:03:59,640 --> 00:04:03,690
data you have you have to alert if the

00:04:01,590 --> 00:04:05,790
room model plus you have this great

00:04:03,690 --> 00:04:09,030
histogram visualizations of CRO fauna

00:04:05,790 --> 00:04:12,299
and the great visualizations crow fauna

00:04:09,030 --> 00:04:14,100
is providing to you one of the biggest

00:04:12,299 --> 00:04:16,470
things from my point of view is the

00:04:14,100 --> 00:04:20,030
introduction of these heat panels and

00:04:16,470 --> 00:04:23,110
this rolling histograms over time with

00:04:20,030 --> 00:04:26,169
gravano 4.3 which already allow

00:04:23,110 --> 00:04:28,120
you to get mints out inside how the the

00:04:26,169 --> 00:04:31,750
data distribution how the data behaves

00:04:28,120 --> 00:04:33,129
over time so I want you to start if you

00:04:31,750 --> 00:04:34,810
have running system you have exploited

00:04:33,129 --> 00:04:42,370
all the possibilities there are and

00:04:34,810 --> 00:04:43,810
you're looking for I cannot stop and

00:04:42,370 --> 00:04:46,030
you're looking and you're looking for

00:04:43,810 --> 00:04:52,750
further models to improve your some ways

00:04:46,030 --> 00:04:54,430
to improve your models no good this next

00:04:52,750 --> 00:04:55,870
few minutes will be structured as

00:04:54,430 --> 00:04:58,180
follows I will show you how to get the

00:04:55,870 --> 00:05:01,779
data out of amazing promises into a

00:04:58,180 --> 00:05:05,229
format that is used for for for data

00:05:01,779 --> 00:05:07,629
science tools and afterwards I will show

00:05:05,229 --> 00:05:09,610
you a few examples from my from my

00:05:07,629 --> 00:05:11,409
personal experience that allowed me to

00:05:09,610 --> 00:05:18,189
improve the model the rule based model

00:05:11,409 --> 00:05:22,060
for a thing that is in place no good but

00:05:18,189 --> 00:05:24,189
to abort you want it bought the raw

00:05:22,060 --> 00:05:26,620
matrix data and with no functions

00:05:24,189 --> 00:05:28,599
applied to it and you wanna exploit as

00:05:26,620 --> 00:05:30,339
much as possible without putting too

00:05:28,599 --> 00:05:31,839
much load on for meteors running so if

00:05:30,339 --> 00:05:35,110
you are querying and fetching a data

00:05:31,839 --> 00:05:37,930
from a live system you could actually if

00:05:35,110 --> 00:05:40,870
you grab all the metrics we are the the

00:05:37,930 --> 00:05:45,009
API Pro meters can run into a time out

00:05:40,870 --> 00:05:47,319
or just just be get killed the two ways

00:05:45,009 --> 00:05:50,770
to get data out of Prometheus the one is

00:05:47,319 --> 00:05:52,330
using HTTP API and the other one is with

00:05:50,770 --> 00:05:56,229
this appalling mechanism this is for

00:05:52,330 --> 00:05:58,029
Ravana is using and it's very good

00:05:56,229 --> 00:06:00,279
interface to do some kind of exploratory

00:05:58,029 --> 00:06:02,379
data analysis the second way to get data

00:06:00,279 --> 00:06:05,050
out of of promises is the remote API

00:06:02,379 --> 00:06:08,080
which actually pushes data to an HTTP

00:06:05,050 --> 00:06:10,539
endpoint which might be very well suited

00:06:08,080 --> 00:06:13,240
for streaming analysis the further

00:06:10,539 --> 00:06:15,939
talked about only at the further I will

00:06:13,240 --> 00:06:16,979
only talk about HTTP API and if the

00:06:15,939 --> 00:06:19,930
remote API

00:06:16,979 --> 00:06:22,060
it is so everybody who has used

00:06:19,930 --> 00:06:25,210
co-founder of the katana people will

00:06:22,060 --> 00:06:27,550
know this endpoint so Prometheus has

00:06:25,210 --> 00:06:31,810
this API where you can do created by

00:06:27,550 --> 00:06:37,050
range so you want to get as you can see

00:06:31,810 --> 00:06:37,050
here you want to get from from users API

00:06:37,150 --> 00:06:44,830
basically all all metrics having any

00:06:42,520 --> 00:06:47,100
name you want to get them and then you

00:06:44,830 --> 00:06:52,080
want to convert it to a format that is

00:06:47,100 --> 00:06:56,650
actually useful for data science no

00:06:52,080 --> 00:06:59,320
nothing big the query range itself is an

00:06:56,650 --> 00:07:06,540
HTTP method has like the query start and

00:06:59,320 --> 00:07:09,040
step step is somehow how often data is

00:07:06,540 --> 00:07:12,520
unclear it and this response you get you

00:07:09,040 --> 00:07:17,290
see at the bottom a simple JSON file for

00:07:12,520 --> 00:07:18,940
the use case of use case of of statistic

00:07:17,290 --> 00:07:21,370
analysis we are only interested here and

00:07:18,940 --> 00:07:23,050
the result type matrix or other types

00:07:21,370 --> 00:07:27,070
are not interesting for this use case

00:07:23,050 --> 00:07:29,950
and you get a list of metrics back with

00:07:27,070 --> 00:07:32,230
annotations and so on yeah this is a

00:07:29,950 --> 00:07:37,660
good format it's a JSON format but it's

00:07:32,230 --> 00:07:40,150
not easy to use in in a way for external

00:07:37,660 --> 00:07:42,130
open source data science tools now

00:07:40,150 --> 00:07:43,900
believe it or not one of the best

00:07:42,130 --> 00:07:46,710
moments to have there is a comma

00:07:43,900 --> 00:07:49,150
separated value file if the tabular data

00:07:46,710 --> 00:07:53,740
from my personal experience over the

00:07:49,150 --> 00:07:56,290
last few months this basically is it so

00:07:53,740 --> 00:07:59,500
you have a time column where you store

00:07:56,290 --> 00:08:01,480
the timestamps then you have column for

00:07:59,500 --> 00:08:05,170
each metric where each metric is

00:08:01,480 --> 00:08:07,450
identified by a name set of labels and

00:08:05,170 --> 00:08:12,370
on the first roll you have an ID which

00:08:07,450 --> 00:08:17,380
somehow identifies in your identifiers

00:08:12,370 --> 00:08:22,360
in your data what time series what the

00:08:17,380 --> 00:08:24,610
metric is not the easiest way to get

00:08:22,360 --> 00:08:27,040
this data out of Prometheus is to use

00:08:24,610 --> 00:08:30,010
Ravana yeah don't believe it or believe

00:08:27,040 --> 00:08:33,400
it or not in a graph on a dashboard you

00:08:30,010 --> 00:08:35,140
can create a query display the

00:08:33,400 --> 00:08:38,050
visualization and then you can export it

00:08:35,140 --> 00:08:41,169
to a comma separated value file so you

00:08:38,050 --> 00:08:43,450
could order use Python like I did and do

00:08:41,169 --> 00:08:46,030
some kind of normalization stuff but

00:08:43,450 --> 00:08:47,830
crow fauna would be for for the beginner

00:08:46,030 --> 00:08:49,870
first entry points to get to a

00:08:47,830 --> 00:08:53,980
comma-separated value file that you can

00:08:49,870 --> 00:08:57,430
actually then input to our Python side

00:08:53,980 --> 00:09:03,340
PI or MATLAB if you want to have a paid

00:08:57,430 --> 00:09:04,870
solution good yeah I jumped the gun so

00:09:03,340 --> 00:09:07,210
the easiest way to export this is crow

00:09:04,870 --> 00:09:10,270
fauna and the second one is is Python

00:09:07,210 --> 00:09:12,910
there is an entry from from O'Brien

00:09:10,270 --> 00:09:15,340
which actually shows you in a simple

00:09:12,910 --> 00:09:17,560
Python hotel to access the API you can

00:09:15,340 --> 00:09:21,450
use it as a starting point and convert

00:09:17,560 --> 00:09:23,220
data from the HTTP API to your liking

00:09:21,450 --> 00:09:25,660
[Music]

00:09:23,220 --> 00:09:29,220
one thing when you export the data is

00:09:25,660 --> 00:09:31,780
you soon getting into position - if you

00:09:29,220 --> 00:09:35,020
export or matrix into a comma separated

00:09:31,780 --> 00:09:39,010
value file I assume get to a place where

00:09:35,020 --> 00:09:40,750
the CSV files become very large so from

00:09:39,010 --> 00:09:44,740
a data scientist point of view reduced

00:09:40,750 --> 00:09:47,440
and the data you want to export to

00:09:44,740 --> 00:09:49,810
really the one that you need this query

00:09:47,440 --> 00:09:55,420
here is a very simple query which

00:09:49,810 --> 00:10:01,360
basically export or or data there is for

00:09:55,420 --> 00:10:02,830
matrix there are no another tip when you

00:10:01,360 --> 00:10:05,740
are then later on losing actually

00:10:02,830 --> 00:10:07,570
learning technologies you we need for

00:10:05,740 --> 00:10:10,960
supervised learning you will need some

00:10:07,570 --> 00:10:12,550
kind of annotations of your matrix there

00:10:10,960 --> 00:10:15,370
lurtz that are created by an ongoing

00:10:12,550 --> 00:10:17,860
system are already some kind of

00:10:15,370 --> 00:10:21,610
annotation to your data you have and you

00:10:17,860 --> 00:10:25,890
can abuse it to create your initial set

00:10:21,610 --> 00:10:29,790
of labored training and matrix data yeah

00:10:25,890 --> 00:10:35,410
another thing we heard it before and the

00:10:29,790 --> 00:10:39,250
example of dupe is only using couch

00:10:35,410 --> 00:10:41,590
values none when you exported data and

00:10:39,250 --> 00:10:43,510
you want to further processes was with

00:10:41,590 --> 00:10:46,750
with data science tool I'm gonna analyze

00:10:43,510 --> 00:10:48,670
want to analyze the math and statistics

00:10:46,750 --> 00:10:51,640
behind it when you do some kind of

00:10:48,670 --> 00:10:54,370
descriptive statistic count hours are

00:10:51,640 --> 00:10:57,160
good however you pronounce that counters

00:10:54,370 --> 00:10:59,130
are not that good counters are only only

00:10:57,160 --> 00:11:01,620
now they don't repeat themselves

00:10:59,130 --> 00:11:03,180
best thing here is to do some kind of

00:11:01,620 --> 00:11:07,040
people your post-processing

00:11:03,180 --> 00:11:09,900
like the guys before did so you actually

00:11:07,040 --> 00:11:15,030
convert this counter into a tariff in

00:11:09,900 --> 00:11:16,290
some way no so are different you could

00:11:15,030 --> 00:11:17,640
do it in premises

00:11:16,290 --> 00:11:21,300
the problem with per method is if you

00:11:17,640 --> 00:11:23,820
apply the rate function to a time series

00:11:21,300 --> 00:11:27,540
you lose the name of it now then you

00:11:23,820 --> 00:11:29,220
have to figure out what time series of a

00:11:27,540 --> 00:11:32,160
metric you you select the what name it

00:11:29,220 --> 00:11:34,980
was so this is a complex not complex but

00:11:32,160 --> 00:11:36,690
is like something I I didn't do so

00:11:34,980 --> 00:11:39,510
actually use the functions provided by

00:11:36,690 --> 00:11:44,670
your tools to convert the counter to an

00:11:39,510 --> 00:11:46,380
to a golf good examples now some

00:11:44,670 --> 00:11:51,330
complete examples I'm having the data in

00:11:46,380 --> 00:11:53,460
hand no I have exported it I I'm at the

00:11:51,330 --> 00:11:54,840
limits of what I can do is promise ice

00:11:53,460 --> 00:11:56,910
and I want to get some additional

00:11:54,840 --> 00:11:58,890
insights into the data I want to exploit

00:11:56,910 --> 00:12:01,170
it I'm gonna have some descriptive

00:11:58,890 --> 00:12:03,450
statistics and now I will show you at

00:12:01,170 --> 00:12:06,720
least one or two examples how this

00:12:03,450 --> 00:12:08,850
exploitation of statistic facts can help

00:12:06,720 --> 00:12:12,810
you to improve your rules and your

00:12:08,850 --> 00:12:14,940
predictions you have the first one these

00:12:12,810 --> 00:12:18,840
are artificial examples saw but not

00:12:14,940 --> 00:12:22,470
fully based on real data but here I

00:12:18,840 --> 00:12:25,380
actually state I can predict the latency

00:12:22,470 --> 00:12:27,090
of HTTP requests over time there is a

00:12:25,380 --> 00:12:28,470
function in promises which already does

00:12:27,090 --> 00:12:31,710
some kind of linear prediction

00:12:28,470 --> 00:12:35,130
it's called predict linear you can you

00:12:31,710 --> 00:12:36,480
can apply it to any metric you have no

00:12:35,130 --> 00:12:40,640
matter if it's reasonable or not

00:12:36,480 --> 00:12:44,400
no and fermius does not provide you any

00:12:40,640 --> 00:12:45,900
insight if it works a lot other than you

00:12:44,400 --> 00:12:51,330
do it by trial and error so this example

00:12:45,900 --> 00:12:53,880
is about can I use tennis can I use the

00:12:51,330 --> 00:12:55,440
predict linear function and cannot be

00:12:53,880 --> 00:12:57,420
sure without trial and error that this

00:12:55,440 --> 00:12:59,610
works and in my boundary does it work

00:12:57,420 --> 00:13:03,360
somewhat why not no so basically

00:12:59,610 --> 00:13:07,560
exporting the data in a CSV file then

00:13:03,360 --> 00:13:10,800
using for example R here I have an

00:13:07,560 --> 00:13:11,950
export of an R our workflow which I most

00:13:10,800 --> 00:13:14,980
of news when

00:13:11,950 --> 00:13:16,149
Houston last week's months when actually

00:13:14,980 --> 00:13:18,670
want to know if the linear model

00:13:16,149 --> 00:13:20,860
prediction is reasonable or not so you

00:13:18,670 --> 00:13:25,029
have the data you export it in the first

00:13:20,860 --> 00:13:27,010
row you see here this LM here is the our

00:13:25,029 --> 00:13:29,050
a function that actually does the same

00:13:27,010 --> 00:13:31,660
like the predict linear function it

00:13:29,050 --> 00:13:33,670
trains a linear regression models plus

00:13:31,660 --> 00:13:36,130
and that's not the interesting thing

00:13:33,670 --> 00:13:39,190
plus it gives you some insight how well

00:13:36,130 --> 00:13:40,510
this prediction model fits your data no

00:13:39,190 --> 00:13:42,820
there's a simple function which is

00:13:40,510 --> 00:13:44,920
called summary linear model number one

00:13:42,820 --> 00:13:48,279
which was here 10 right over there and

00:13:44,920 --> 00:13:51,130
then you get the list of statistical

00:13:48,279 --> 00:13:53,920
properties of the data and your model

00:13:51,130 --> 00:13:55,209
and if you are a skilled data scientist

00:13:53,920 --> 00:13:58,029
you can see it one can see if it's

00:13:55,209 --> 00:14:01,269
working or not know in this first

00:13:58,029 --> 00:14:03,730
example I I tried to predict the

00:14:01,269 --> 00:14:07,329
duration of an HTTP request based on the

00:14:03,730 --> 00:14:10,540
time which is yeah Captain Obvious

00:14:07,329 --> 00:14:11,950
dating not really working well so nobody

00:14:10,540 --> 00:14:14,260
would I wouldn't have expected that this

00:14:11,950 --> 00:14:16,180
works no good and then you get a lot of

00:14:14,260 --> 00:14:18,750
interesting graphs you can interpret it

00:14:16,180 --> 00:14:23,699
or not blah blah blah

00:14:18,750 --> 00:14:26,050
here for example is the QQ plot which

00:14:23,699 --> 00:14:29,589
probably I might come back to the

00:14:26,050 --> 00:14:32,980
question how to to size the pins of an

00:14:29,589 --> 00:14:37,170
histogram so if this this diagram for

00:14:32,980 --> 00:14:40,209
example shows you if I can show you if

00:14:37,170 --> 00:14:42,250
if the data is normal it is to build it

00:14:40,209 --> 00:14:44,610
or not why do you need that if it's

00:14:42,250 --> 00:14:48,490
normally distributed then you actually

00:14:44,610 --> 00:14:50,529
have specific rules how to to size your

00:14:48,490 --> 00:14:52,510
pins of a histogram now you don't need

00:14:50,529 --> 00:14:54,459
to guess you don't need to try an error

00:14:52,510 --> 00:14:56,740
so if you have normally distributed data

00:14:54,459 --> 00:14:59,769
you can use this information to actually

00:14:56,740 --> 00:15:03,970
use the best possible pin size for your

00:14:59,769 --> 00:15:05,620
Instagram or not yeah excellent for

00:15:03,970 --> 00:15:08,529
example if somebody know this program

00:15:05,620 --> 00:15:10,300
when you propped in excellent and and an

00:15:08,529 --> 00:15:12,519
histogram it assumes that the data is

00:15:10,300 --> 00:15:14,709
normally distributed and then the number

00:15:12,519 --> 00:15:17,670
of pins is the square root of n samples

00:15:14,709 --> 00:15:20,050
you have this is how Excel doesn't know

00:15:17,670 --> 00:15:21,910
but there are lot insights you can get

00:15:20,050 --> 00:15:24,110
out of this data and a lot of ideas now

00:15:21,910 --> 00:15:29,329
so basically he

00:15:24,110 --> 00:15:31,899
is a plot which is residuals versus

00:15:29,329 --> 00:15:35,899
leverage you can identify outliers

00:15:31,899 --> 00:15:37,910
outliers are important a bet for the

00:15:35,899 --> 00:15:40,310
predictive linear model training because

00:15:37,910 --> 00:15:41,870
actually there are states which normally

00:15:40,310 --> 00:15:43,790
don't happen so if you want to train a

00:15:41,870 --> 00:15:46,190
predictive linear model you want to

00:15:43,790 --> 00:15:50,180
remove this outliers if you actually

00:15:46,190 --> 00:15:51,800
want to detect errors in the errors in

00:15:50,180 --> 00:15:54,260
the application then actually you want

00:15:51,800 --> 00:15:58,220
to search for this outlier so this is in

00:15:54,260 --> 00:15:59,380
a nutshell is it good how much time do I

00:15:58,220 --> 00:16:04,070
have left

00:15:59,380 --> 00:16:06,200
what okay good um so here okay

00:16:04,070 --> 00:16:09,890
my first type of thesis didn't hold no I

00:16:06,200 --> 00:16:12,980
cannot apply the linear model based on

00:16:09,890 --> 00:16:14,980
time over-over latency no good my next

00:16:12,980 --> 00:16:18,470
study would be in hell what about

00:16:14,980 --> 00:16:21,680
finding a correlation between the number

00:16:18,470 --> 00:16:25,519
of requests and the duration more much

00:16:21,680 --> 00:16:28,070
more reasonable hypothesis now and as

00:16:25,519 --> 00:16:30,290
you will see we summarized it you can

00:16:28,070 --> 00:16:33,110
see here the F value is high which is a

00:16:30,290 --> 00:16:37,820
good sign no and we can't go back look

00:16:33,110 --> 00:16:40,339
across the pots again and in the end you

00:16:37,820 --> 00:16:43,370
will see here I'm here is important

00:16:40,339 --> 00:16:46,250
thing a scale on the y axis now it's the

00:16:43,370 --> 00:16:49,339
other it's it's not it's not normally

00:16:46,250 --> 00:16:52,610
distributed but it's it's less cute than

00:16:49,339 --> 00:16:55,550
it was in the first example here so and

00:16:52,610 --> 00:16:56,990
I also see some kind of range where

00:16:55,550 --> 00:16:59,660
actually it behaves like a normal

00:16:56,990 --> 00:17:03,140
distribution so I can see the boundaries

00:16:59,660 --> 00:17:05,240
where it is sensible to train and train

00:17:03,140 --> 00:17:07,100
the model and I can filter out which is

00:17:05,240 --> 00:17:10,370
also a quite important point I can

00:17:07,100 --> 00:17:12,770
filter out in my room models all points

00:17:10,370 --> 00:17:15,049
all measurements that are put above or

00:17:12,770 --> 00:17:17,240
below a threshold before I start to

00:17:15,049 --> 00:17:20,839
train a linear model no so you can use

00:17:17,240 --> 00:17:22,549
these insights to optimize to optimize

00:17:20,839 --> 00:17:24,650
the prediction of the linear model and

00:17:22,549 --> 00:17:27,919
use these insights from these graphs

00:17:24,650 --> 00:17:30,200
here to cut off data that you don't want

00:17:27,919 --> 00:17:33,050
to have and to have stable predictions

00:17:30,200 --> 00:17:35,890
of whatever usage you are you up to and

00:17:33,050 --> 00:17:42,370
so on and so on yeah good

00:17:35,890 --> 00:17:45,760
so second example I brought here is in

00:17:42,370 --> 00:17:47,770
the case you have a lot of metrics if

00:17:45,760 --> 00:17:51,280
you see it where as something you start

00:17:47,770 --> 00:17:53,530
with 800 or 900 and you wanna you want

00:17:51,280 --> 00:17:56,410
to ask yourself earlier some metrics

00:17:53,530 --> 00:17:58,299
India that was you to predict whatever

00:17:56,410 --> 00:18:00,400
incident whatever state of the

00:17:58,299 --> 00:18:04,059
application I have other ones they're

00:18:00,400 --> 00:18:06,130
good or not the mythology I'm borrowing

00:18:04,059 --> 00:18:08,020
here from for machine learning is called

00:18:06,130 --> 00:18:12,400
feature selection which would typically

00:18:08,020 --> 00:18:15,760
do before you start to train a model and

00:18:12,400 --> 00:18:18,700
if I actually do machine learning so I

00:18:15,760 --> 00:18:20,740
was this method and that methodology you

00:18:18,700 --> 00:18:23,590
actually ask you the question which of

00:18:20,740 --> 00:18:27,970
my metrics in this case the most

00:18:23,590 --> 00:18:29,950
predictive metrics having at most the

00:18:27,970 --> 00:18:34,270
best metrics to predict a certain state

00:18:29,950 --> 00:18:38,620
no how did I choose this method here I'm

00:18:34,270 --> 00:18:40,840
in the sci-fi world type I is if we

00:18:38,620 --> 00:18:43,600
don't know it sci-fi is a machine

00:18:40,840 --> 00:18:45,490
learning toolkit from Titan implemented

00:18:43,600 --> 00:18:48,630
now you have an overview of kind what

00:18:45,490 --> 00:18:51,850
what what thing you want to do you can

00:18:48,630 --> 00:18:54,790
go down here and if you read this

00:18:51,850 --> 00:18:57,100
detailed you will end up like I did in a

00:18:54,790 --> 00:19:00,610
linear regression and feature selection

00:18:57,100 --> 00:19:01,600
method nope good then you have to do

00:19:00,610 --> 00:19:05,950
some things

00:19:01,600 --> 00:19:08,049
remove seasonality from the data bring

00:19:05,950 --> 00:19:10,750
the data in a format that it's good for

00:19:08,049 --> 00:19:15,370
for this analysis and then in the end

00:19:10,750 --> 00:19:18,250
this is the the meat of it you actually

00:19:15,370 --> 00:19:20,580
have your data are you train wreck here

00:19:18,250 --> 00:19:25,000
train wreck us we feature elimination

00:19:20,580 --> 00:19:27,070
and I get back the information what of

00:19:25,000 --> 00:19:29,799
the features a lot of the metrics are

00:19:27,070 --> 00:19:33,100
used has the highest relevance to detect

00:19:29,799 --> 00:19:37,360
or identify the alert I have I have

00:19:33,100 --> 00:19:40,020
before used for the training now I have

00:19:37,360 --> 00:19:43,419
this insight from a practical example

00:19:40,020 --> 00:19:46,480
this could reduce the number of metrics

00:19:43,419 --> 00:19:48,150
that you're looking at from 800,000

00:19:46,480 --> 00:19:51,300
20,000 to

00:19:48,150 --> 00:19:55,020
10 or 20 so it reduces the complexity

00:19:51,300 --> 00:19:57,600
and now you can actually have a look at

00:19:55,020 --> 00:20:00,180
the remaining 20 or 30 metrics and

00:19:57,600 --> 00:20:01,920
decide for your own if you want to

00:20:00,180 --> 00:20:04,770
change the model you have and use this

00:20:01,920 --> 00:20:09,000
better prediction data to alter your

00:20:04,770 --> 00:20:10,620
model or not then there are a lot of

00:20:09,000 --> 00:20:14,760
libraries around the one which is stuck

00:20:10,620 --> 00:20:17,190
into my head and which I was using some

00:20:14,760 --> 00:20:19,790
time is like this to use fresh library

00:20:17,190 --> 00:20:22,950
which does what I did here before

00:20:19,790 --> 00:20:28,260
automatically you send to it more or

00:20:22,950 --> 00:20:30,930
less in the proper format a lot of time

00:20:28,260 --> 00:20:34,160
serious you give it some kind of label

00:20:30,930 --> 00:20:39,020
annotation to it and then actually it

00:20:34,160 --> 00:20:42,900
selects the best features for you it

00:20:39,020 --> 00:20:44,520
takes into account if the features

00:20:42,900 --> 00:20:48,960
metrics are dependent on each other

00:20:44,520 --> 00:20:54,810
which is somehow the answer to the

00:20:48,960 --> 00:20:56,250
question did the developers expose the

00:20:54,810 --> 00:20:58,280
metrics so that there are no overlaps

00:20:56,250 --> 00:21:01,110
between the information there is and

00:20:58,280 --> 00:21:04,620
actually what it also does is like if

00:21:01,110 --> 00:21:07,680
you have 800 metrics it extracts and

00:21:04,620 --> 00:21:09,930
creates twenty thirty thousand metrics

00:21:07,680 --> 00:21:14,670
using different mathematical functions

00:21:09,930 --> 00:21:16,950
and tries to see and automatically tests

00:21:14,670 --> 00:21:20,490
if these functions are also better

00:21:16,950 --> 00:21:23,820
predictors for for for a thing you want

00:21:20,490 --> 00:21:25,410
to detect or not give it a try you will

00:21:23,820 --> 00:21:28,440
see it's interesting and it will give

00:21:25,410 --> 00:21:30,750
you some insight into probably missing

00:21:28,440 --> 00:21:34,250
methods in in Prometheus that you might

00:21:30,750 --> 00:21:37,800
need to implement or the similar

00:21:34,250 --> 00:21:39,990
basically the the the mantra that I was

00:21:37,800 --> 00:21:42,960
following over here is I'm created their

00:21:39,990 --> 00:21:45,890
hypotheses about your system and my

00:21:42,960 --> 00:21:47,910
metrics how I could improve it

00:21:45,890 --> 00:21:51,930
converting the metrics to the right

00:21:47,910 --> 00:21:54,510
format you use some some methods from

00:21:51,930 --> 00:21:56,970
our and scifi that you think might help

00:21:54,510 --> 00:21:58,650
you and then which is not the most

00:21:56,970 --> 00:22:01,860
important part

00:21:58,650 --> 00:22:05,640
take the results and that the insights

00:22:01,860 --> 00:22:07,380
about how data is structured to years

00:22:05,640 --> 00:22:09,960
dashboards and your alerts this can

00:22:07,380 --> 00:22:12,150
reduce the number of alerts this could

00:22:09,960 --> 00:22:15,630
increase the accuracy the precision the

00:22:12,150 --> 00:22:18,870
recall of your alerts this could reduce

00:22:15,630 --> 00:22:21,240
the size of your dashboards so it

00:22:18,870 --> 00:22:24,000
actually is a very good tool to reduce

00:22:21,240 --> 00:22:27,450
the complexity to detect errors in your

00:22:24,000 --> 00:22:30,810
domain domain is the software the micro

00:22:27,450 --> 00:22:35,670
service whatever you are using for my

00:22:30,810 --> 00:22:37,230
personal perspective on musclemen

00:22:35,670 --> 00:22:39,180
lessons I've learned using all this

00:22:37,230 --> 00:22:43,110
machine learning and data science stuff

00:22:39,180 --> 00:22:45,300
it's like this descriptive statistics

00:22:43,110 --> 00:22:49,170
that you can get out of data after

00:22:45,300 --> 00:22:52,140
others they actually help you to improve

00:22:49,170 --> 00:22:55,740
your model you're using for alerting and

00:22:52,140 --> 00:22:59,520
in prediction and it's the choice of you

00:22:55,740 --> 00:23:01,760
as a developer or of operations guide to

00:22:59,520 --> 00:23:05,340
correct this card or handle data

00:23:01,760 --> 00:23:07,050
differently or not my main use case my

00:23:05,340 --> 00:23:09,270
day-to-day use case I really like and

00:23:07,050 --> 00:23:12,390
I'm using and I would really is easy to

00:23:09,270 --> 00:23:15,390
implement this is giving an answer to

00:23:12,390 --> 00:23:18,510
may I use predict linear function for my

00:23:15,390 --> 00:23:20,160
metric or not yeah there is a simple

00:23:18,510 --> 00:23:22,590
answer to that you don't need any trial

00:23:20,160 --> 00:23:24,900
and error yeah you can honor it by

00:23:22,590 --> 00:23:30,780
looking at the descriptive statistics of

00:23:24,900 --> 00:23:32,940
your data and personally I had no reason

00:23:30,780 --> 00:23:39,300
yet to do online machine learning

00:23:32,940 --> 00:23:43,050
artificial intelligence and other CPU

00:23:39,300 --> 00:23:47,090
Ram resource usage intense things I was

00:23:43,050 --> 00:23:49,590
always good identifying some kind of

00:23:47,090 --> 00:23:51,660
statistical properties of the data I

00:23:49,590 --> 00:23:54,690
have been feeding back this knowledge to

00:23:51,660 --> 00:23:56,700
the to the model I'm using and keeping

00:23:54,690 --> 00:24:00,860
the resource consumption for this reason

00:23:56,700 --> 00:24:04,820
as long as possible no skips it's cheap

00:24:00,860 --> 00:24:07,890
yeah that's it basically a summary of my

00:24:04,820 --> 00:24:11,310
experience using machine learning and

00:24:07,890 --> 00:24:12,029
data science to improve my Prometheus

00:24:11,310 --> 00:24:15,690
model and

00:24:12,029 --> 00:24:34,289
the predictions that are possible any

00:24:15,690 --> 00:24:36,570
questions so how do you feedback this

00:24:34,289 --> 00:24:39,059
data you you perform some analysis and

00:24:36,570 --> 00:24:42,269
then you guards as well as so how do you

00:24:39,059 --> 00:24:44,070
send it back to our manager or like how

00:24:42,269 --> 00:24:46,559
do you use this data how to use this

00:24:44,070 --> 00:24:48,779
analysis I using effects to figure out

00:24:46,559 --> 00:24:52,019
about the data to actually improve my

00:24:48,779 --> 00:24:53,639
hand my rules that I wrote and to change

00:24:52,019 --> 00:24:57,179
the data that is displayed on my

00:24:53,639 --> 00:25:00,899
dashboards so this increases the

00:24:57,179 --> 00:25:03,210
accuracy of my my my alerts for example

00:25:00,899 --> 00:25:05,219
and it improves the what I can see on

00:25:03,210 --> 00:25:07,950
the dashboard yeah so it's not

00:25:05,219 --> 00:25:09,960
automatically you could do it this is

00:25:07,950 --> 00:25:11,969
all the reason why I'm here now so if

00:25:09,960 --> 00:25:19,889
somebody is interested now probably

00:25:11,969 --> 00:25:21,659
afterwards have a coffee yeah oh yeah

00:25:19,889 --> 00:25:23,729
all the point of order if you could

00:25:21,659 --> 00:25:24,659
stand up when asking questions then

00:25:23,729 --> 00:25:28,710
people can see you

00:25:24,659 --> 00:25:29,879
thank you yeah excellent works I was

00:25:28,710 --> 00:25:32,879
wondering you mentioned that you use

00:25:29,879 --> 00:25:34,649
Python or R what's the library that you

00:25:32,879 --> 00:25:36,570
use to pump it into let's say a data

00:25:34,649 --> 00:25:38,399
frame is that your own library you are

00:25:36,570 --> 00:25:46,499
using and Richard also mentioned there's

00:25:38,399 --> 00:25:48,539
an R library for how did you pump the

00:25:46,499 --> 00:25:50,460
data into the data frame or I wrote a

00:25:48,539 --> 00:25:52,409
python script it reads the data

00:25:50,460 --> 00:25:55,859
normalize it like this is in the blog

00:25:52,409 --> 00:25:57,570
entry of Brian he is just writing it - I

00:25:55,859 --> 00:25:58,830
don't know where but then you need to

00:25:57,570 --> 00:26:01,169
have the data at hand and then you

00:25:58,830 --> 00:26:02,849
create dynamic Elliot elephant

00:26:01,169 --> 00:26:05,070
okay and this data frame can be backed

00:26:02,849 --> 00:26:06,960
by a file or HBase or whatever yeah you

00:26:05,070 --> 00:26:09,330
know data frames are really really

00:26:06,960 --> 00:26:11,339
powerful in where they store the data

00:26:09,330 --> 00:26:14,129
and in the end right so I noticed in

00:26:11,339 --> 00:26:16,169
your other chart that you use Lhasa so

00:26:14,129 --> 00:26:18,179
lot so when you regularize it automatic

00:26:16,169 --> 00:26:20,639
does the best prediction but you use ts

00:26:18,179 --> 00:26:22,139
fresh to find the best predictors so T

00:26:20,639 --> 00:26:24,500
is fresh is some kind of convenient

00:26:22,139 --> 00:26:26,990
libraries which parallel

00:26:24,500 --> 00:26:29,540
this whole stuff and does automatic

00:26:26,990 --> 00:26:32,060
feature extraction from the initial

00:26:29,540 --> 00:26:33,800
feature sets matrix it I have also able

00:26:32,060 --> 00:26:34,940
to switch between the two domains but

00:26:33,800 --> 00:26:37,040
which is does some kind of feature

00:26:34,940 --> 00:26:39,230
extraction and then reduces the features

00:26:37,040 --> 00:26:41,300
again so it's a convenient library that

00:26:39,230 --> 00:26:42,860
has a lot of things that most time data

00:26:41,300 --> 00:26:46,220
scientists have to do on a day to day

00:26:42,860 --> 00:26:47,660
basis it's using air mass and air are

00:26:46,220 --> 00:26:49,730
those times very slice

00:26:47,660 --> 00:26:56,990
it uses internally the whole range of

00:26:49,730 --> 00:27:00,500
soil hi so I just had a question that I

00:26:56,990 --> 00:27:04,790
see that you use a lot of the linear

00:27:00,500 --> 00:27:07,760
models but you also see some if you

00:27:04,790 --> 00:27:11,720
tried if you see some metrics which

00:27:07,760 --> 00:27:16,490
better fit that other models yeah some

00:27:11,720 --> 00:27:19,010
examples then it's not linear than it

00:27:16,490 --> 00:27:21,200
and not linear regression model it's not

00:27:19,010 --> 00:27:24,380
implemented infamous resna and then you

00:27:21,200 --> 00:27:26,570
can think of if it's nonlinear now then

00:27:24,380 --> 00:27:30,320
you have to use another model to train

00:27:26,570 --> 00:27:33,350
it now and and yeah and you have to hold

00:27:30,320 --> 00:27:36,020
a range of regression models there are

00:27:33,350 --> 00:27:38,900
implemented is that other metrics which

00:27:36,020 --> 00:27:42,740
you already saw which do not follow a

00:27:38,900 --> 00:27:44,990
linear model the most matrix I looked

00:27:42,740 --> 00:27:47,420
into it so far only follow the linear

00:27:44,990 --> 00:27:50,240
model within a certain range of data no

00:27:47,420 --> 00:27:53,210
yeah yeah so you have to cut it to to

00:27:50,240 --> 00:27:57,350
one end to make reliable predictions at

00:27:53,210 --> 00:27:59,840
all and in production or in life systems

00:27:57,350 --> 00:28:03,700
using nonlinear models I never came to

00:27:59,840 --> 00:28:03,700
this point okay thank you okay

00:28:04,810 --> 00:28:08,500
any more questions

00:28:14,480 --> 00:28:23,830
oh yes there we go

00:28:24,640 --> 00:28:41,270
is it blinking hello oh cool thank you

00:28:38,060 --> 00:28:42,650
so that's a super fascinating it's it's

00:28:41,270 --> 00:28:44,210
very insightful the thing I'm quite

00:28:42,650 --> 00:28:47,270
interested in is given that you know

00:28:44,210 --> 00:28:49,730
have a model these unique are the you

00:28:47,270 --> 00:28:51,080
see a possibility for using evolutionary

00:28:49,730 --> 00:28:54,050
techniques reinforcement learning

00:28:51,080 --> 00:28:55,910
robotic evolution etc to better evolve

00:28:54,050 --> 00:28:58,730
the model that you currently have yes

00:28:55,910 --> 00:29:01,880
yes for sure for example that one arm

00:28:58,730 --> 00:29:03,230
that the example I gave like extending

00:29:01,880 --> 00:29:05,600
the training set or having an initial

00:29:03,230 --> 00:29:09,110
training set by using their words you

00:29:05,600 --> 00:29:11,300
have no this is like could be and a

00:29:09,110 --> 00:29:13,700
pointer to a method how to extend your

00:29:11,300 --> 00:29:16,460
training data with onboard methods from

00:29:13,700 --> 00:29:18,440
from a listener so I'm having a thousand

00:29:16,460 --> 00:29:18,790
ideas how to do that but I don't have

00:29:18,440 --> 00:29:21,900
the time

00:29:18,790 --> 00:29:34,659
[Laughter]

00:29:21,900 --> 00:29:34,840
[Music]

00:29:34,659 --> 00:29:37,980
you

00:29:34,840 --> 00:29:37,980

YouTube URL: https://www.youtube.com/watch?v=aUOgPdaXOwQ


