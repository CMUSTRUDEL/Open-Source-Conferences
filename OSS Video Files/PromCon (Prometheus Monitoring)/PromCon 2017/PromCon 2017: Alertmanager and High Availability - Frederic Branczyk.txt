Title: PromCon 2017: Alertmanager and High Availability - Frederic Branczyk
Publication date: 2017-09-04
Playlist: PromCon 2017
Description: 
	* Abstract:

The Alertmanager is a highly critical component in the monitoring pipeline.  Operators must trust it to be a reliable component. Thus in the 0.5 release of the Alertmanager a high availability mode has been implemented. This talk will go over some implementation details of the high availability mode as well as highlight what this means for operators using and running the Alertmanager.

* Speaker biography:

Frederic Branczyk is an engineer at CoreOS, where he contributes to Prometheus and Kubernetes to build state-of-the-art modern infrastructure and monitoring tools. Frederic discovered his interest in monitoring tools and distributed systems in his previous jobs, where he used machine learning to detect anomalies indicating intrusion attempts. He also worked on projects involving secrets management for distributed applications to build sane and stable infrastructure.

* Slides:

* PromCon website:

https://promcon.io/
Captions: 
	00:00:00,380 --> 00:00:15,660
[Music]

00:00:13,920 --> 00:00:18,920
my name is ben kochi I'm gonna MC this

00:00:15,660 --> 00:00:21,210
afternoon and our first speaker is

00:00:18,920 --> 00:00:23,760
Frederick Brent chick from core OS and

00:00:21,210 --> 00:00:26,820
he's going to talk about alert manager

00:00:23,760 --> 00:00:28,830
and high availability he's got a

00:00:26,820 --> 00:00:31,349
background in distributed systems and

00:00:28,830 --> 00:00:33,030
rely really has some experience and

00:00:31,349 --> 00:00:34,980
reliability so it should be a great

00:00:33,030 --> 00:00:36,690
great talk about the alert manager

00:00:34,980 --> 00:00:39,540
thanks Ben for the introduction

00:00:36,690 --> 00:00:41,070
so yeah I'll be talking about alert

00:00:39,540 --> 00:00:43,440
manager and high availability we've

00:00:41,070 --> 00:00:46,079
actually already heard about the feature

00:00:43,440 --> 00:00:48,480
itself a couple times today and today I

00:00:46,079 --> 00:00:51,059
want to bring some light into the

00:00:48,480 --> 00:00:52,469
darkness of this dark magic how it

00:00:51,059 --> 00:00:54,390
actually works and what that means for

00:00:52,469 --> 00:00:58,320
us running alert manager and veil

00:00:54,390 --> 00:01:00,180
abilities yeah I'm an engineer at chorus

00:00:58,320 --> 00:01:01,829
and I work on upstream prometheus alert

00:01:00,180 --> 00:01:04,430
manager and kubernetes and like

00:01:01,829 --> 00:01:06,869
everything that connects the two worlds

00:01:04,430 --> 00:01:10,439
so yeah

00:01:06,869 --> 00:01:12,840
which is which brings me to something

00:01:10,439 --> 00:01:16,020
that I want to bring up front

00:01:12,840 --> 00:01:18,479
why does Korres invest in Prometheus and

00:01:16,020 --> 00:01:20,970
the community that's myth because of our

00:01:18,479 --> 00:01:24,570
enterprise coronaries offering tech

00:01:20,970 --> 00:01:27,119
tonic which basically has Prometheus

00:01:24,570 --> 00:01:29,369
monitoring fully automated so that you

00:01:27,119 --> 00:01:33,890
can focus on building your product

00:01:29,369 --> 00:01:36,360
rather than monitoring infrastructure so

00:01:33,890 --> 00:01:38,310
what will I be talking about today in

00:01:36,360 --> 00:01:42,090
terms of high availability so I want to

00:01:38,310 --> 00:01:45,810
talk about how Prometheus how we get

00:01:42,090 --> 00:01:48,630
from Prometheus generating alerts to

00:01:45,810 --> 00:01:50,399
alert manager receiving those then the

00:01:48,630 --> 00:01:53,520
high availability contract between those

00:01:50,399 --> 00:01:56,969
two applications then how that how that

00:01:53,520 --> 00:01:58,979
actually works and what that means in

00:01:56,969 --> 00:02:02,689
terms of implications for us running the

00:01:58,979 --> 00:02:05,549
alert manager and high availability so

00:02:02,689 --> 00:02:08,840
we've already heard this somewhat and

00:02:05,549 --> 00:02:11,520
there's also a non-exhaustive list but

00:02:08,840 --> 00:02:12,100
the main features of alert manager is

00:02:11,520 --> 00:02:15,520
that

00:02:12,100 --> 00:02:18,820
it receives and groups alerts into a

00:02:15,520 --> 00:02:21,090
meaningful message or notification it

00:02:18,820 --> 00:02:24,760
deduplicate those so that we don't get

00:02:21,090 --> 00:02:27,010
notified every 30 seconds or however

00:02:24,760 --> 00:02:29,290
often Prometheus sends those alerts to

00:02:27,010 --> 00:02:31,540
alert monitor but in a reasonable time

00:02:29,290 --> 00:02:33,550
span so that we can actually fix these

00:02:31,540 --> 00:02:38,500
problems that are occur rather than just

00:02:33,550 --> 00:02:40,720
get alert fatigue and then it has

00:02:38,500 --> 00:02:43,860
routing so that we can say for this

00:02:40,720 --> 00:02:46,270
particular service we want this team to

00:02:43,860 --> 00:02:49,380
receive for the notification on slack or

00:02:46,270 --> 00:02:52,930
be a pager duty or all the other

00:02:49,380 --> 00:02:55,750
integrations that alert manager has

00:02:52,930 --> 00:02:57,970
built in and beyond that the alert

00:02:55,750 --> 00:03:00,090
manager also has silencing so when if we

00:02:57,970 --> 00:03:02,560
know that there is a maintenance window

00:03:00,090 --> 00:03:06,250
where we know our application is going

00:03:02,560 --> 00:03:08,590
to behave in a different way or there

00:03:06,250 --> 00:03:12,270
are all kinds of situations where you

00:03:08,590 --> 00:03:15,360
might want to mute an alert firing then

00:03:12,270 --> 00:03:19,140
that's where you want to use silencing

00:03:15,360 --> 00:03:21,580
but in terms of the high availability

00:03:19,140 --> 00:03:24,100
contract between Prometheus and alert

00:03:21,580 --> 00:03:26,290
manager that's this is like the the

00:03:24,100 --> 00:03:27,910
architecture diagram for Prometheus

00:03:26,290 --> 00:03:30,160
you've probably seen this if you've

00:03:27,910 --> 00:03:33,340
worked with Prometheus but what we want

00:03:30,160 --> 00:03:37,240
to focus on today is how we get from

00:03:33,340 --> 00:03:40,170
prompt QL generating our alerts and

00:03:37,240 --> 00:03:44,400
alert manager receiving those and then

00:03:40,170 --> 00:03:49,270
what that means for high availability so

00:03:44,400 --> 00:03:51,400
if you've seen or if you've seen how

00:03:49,270 --> 00:03:53,620
Prometheus actually generates alerts so

00:03:51,400 --> 00:03:55,930
we have a an alerting rule and that gets

00:03:53,620 --> 00:03:57,490
evaluated every 30 seconds or so and

00:03:55,930 --> 00:04:01,240
whenever that alerting rule triggers

00:03:57,490 --> 00:04:03,730
Prometheus sends an alert to the alert

00:04:01,240 --> 00:04:06,460
manager and if we have multiple

00:04:03,730 --> 00:04:08,920
Prometheus instances that they're

00:04:06,460 --> 00:04:11,830
supposed to actually generate the same

00:04:08,920 --> 00:04:14,080
alerts and fire both fire the same alert

00:04:11,830 --> 00:04:15,910
at the alert manager which is why the

00:04:14,080 --> 00:04:18,790
alert manager needs to detail duplicate

00:04:15,910 --> 00:04:23,680
those and then because we get all of

00:04:18,790 --> 00:04:25,540
these alerts from Prometheus possibly

00:04:23,680 --> 00:04:25,670
about the same service we actually want

00:04:25,540 --> 00:04:27,680
to

00:04:25,670 --> 00:04:30,530
get a meaningful notification and not

00:04:27,680 --> 00:04:32,090
we're probably not interested in every

00:04:30,530 --> 00:04:34,880
single alert but we want to have an

00:04:32,090 --> 00:04:36,890
overview so this could be an example

00:04:34,880 --> 00:04:40,400
notification that you could get through

00:04:36,890 --> 00:04:42,730
the alert manager so boiled down to the

00:04:40,400 --> 00:04:48,770
point is that alert manager reliably

00:04:42,730 --> 00:04:50,330
groups and sends notifications now let's

00:04:48,770 --> 00:04:54,320
see what that means for high

00:04:50,330 --> 00:04:58,070
availability or for that I first want to

00:04:54,320 --> 00:05:00,080
show the general idea why why we need an

00:04:58,070 --> 00:05:03,230
high availability mode in the alert

00:05:00,080 --> 00:05:05,990
manager so let's say you have some kind

00:05:03,230 --> 00:05:08,540
of a project and you start building your

00:05:05,990 --> 00:05:11,000
services and you build service one two

00:05:08,540 --> 00:05:12,410
and three and maybe you start making

00:05:11,000 --> 00:05:14,510
some money with that so you want to

00:05:12,410 --> 00:05:17,870
actually monitor what you make what you

00:05:14,510 --> 00:05:22,040
are producing here to have some kind of

00:05:17,870 --> 00:05:24,680
guarantee that your users are going to

00:05:22,040 --> 00:05:27,530
be able to access your application and

00:05:24,680 --> 00:05:28,820
then for you to actually get notified if

00:05:27,530 --> 00:05:31,130
something is wrong you want the alert

00:05:28,820 --> 00:05:34,900
manager because that actually sends the

00:05:31,130 --> 00:05:38,270
notification to pager due to you know

00:05:34,900 --> 00:05:39,920
but your project is actually really

00:05:38,270 --> 00:05:41,720
successful so you scale out your

00:05:39,920 --> 00:05:45,230
infrastructure and make it highly

00:05:41,720 --> 00:05:47,540
available but suddenly all your

00:05:45,230 --> 00:05:49,300
infrastructure for your user based

00:05:47,540 --> 00:05:51,740
application is highly available and

00:05:49,300 --> 00:05:56,060
everything's great but now your

00:05:51,740 --> 00:05:57,740
monitoring is actually not so you add an

00:05:56,060 --> 00:06:00,470
additional Prometheus instance to that

00:05:57,740 --> 00:06:03,110
which all scrape the same targets so

00:06:00,470 --> 00:06:05,770
they produce the same alerts and they

00:06:03,110 --> 00:06:09,320
both fire against the same alert manager

00:06:05,770 --> 00:06:11,180
but now we can already see we again have

00:06:09,320 --> 00:06:15,440
another single point of failure here

00:06:11,180 --> 00:06:17,420
which is the alert manager and so we

00:06:15,440 --> 00:06:20,030
create another instance of alert manager

00:06:17,420 --> 00:06:21,970
but now the problem is because single

00:06:20,030 --> 00:06:25,310
instances of alert manager are

00:06:21,970 --> 00:06:26,900
deduplicating those alerts that our

00:06:25,310 --> 00:06:29,150
Prometheus instances are firing

00:06:26,900 --> 00:06:31,240
if the alert managers don't know about

00:06:29,150 --> 00:06:35,750
each other then we're still going to get

00:06:31,240 --> 00:06:38,340
duplicated notifications so what was

00:06:35,750 --> 00:06:41,250
implemented to do this as

00:06:38,340 --> 00:06:42,990
gossip protocol for the alert managers

00:06:41,250 --> 00:06:45,150
so they can share some state about

00:06:42,990 --> 00:06:50,540
notifications that have already been

00:06:45,150 --> 00:06:55,260
sent and that's really the key here to

00:06:50,540 --> 00:06:58,290
the high availability implementation so

00:06:55,260 --> 00:07:01,230
now we can first take a step back and

00:06:58,290 --> 00:07:03,800
realize why alert manager and primitives

00:07:01,230 --> 00:07:06,930
are actually two separate components so

00:07:03,800 --> 00:07:09,720
why this is really great is that we can

00:07:06,930 --> 00:07:12,930
keep Prometheus to really be really

00:07:09,720 --> 00:07:14,610
simple it's not simple but like to be

00:07:12,930 --> 00:07:16,710
very focused about it's a time series

00:07:14,610 --> 00:07:20,250
database and we can query it and I

00:07:16,710 --> 00:07:22,800
produces alerts and that allows us as

00:07:20,250 --> 00:07:24,540
we've seen today already that Promethea

00:07:22,800 --> 00:07:26,130
is high availability is just a matter of

00:07:24,540 --> 00:07:28,530
spinning up another instance of

00:07:26,130 --> 00:07:30,120
Prometheus which scrapes the same same

00:07:28,530 --> 00:07:33,900
targets therefore produces the same

00:07:30,120 --> 00:07:35,280
alerts and otherwise if let's say

00:07:33,900 --> 00:07:37,560
Prometheus and other manager

00:07:35,280 --> 00:07:40,710
functionality would be in the same

00:07:37,560 --> 00:07:44,430
application then there would have to be

00:07:40,710 --> 00:07:46,620
this type of information sharing state

00:07:44,430 --> 00:07:48,990
sharing between those instances and we

00:07:46,620 --> 00:07:51,750
want to keep that which is itself a

00:07:48,990 --> 00:07:56,760
complicated problem into the alert

00:07:51,750 --> 00:08:01,010
manager so an example alerting rule that

00:07:56,760 --> 00:08:03,990
we could have for example for Etsy D

00:08:01,010 --> 00:08:05,669
which is a distributed key-value store

00:08:03,990 --> 00:08:09,060
in case you're not familiar with it

00:08:05,669 --> 00:08:11,280
so an Etsy D you have the notion of a

00:08:09,060 --> 00:08:13,919
leader so whenever an Etsy D cluster

00:08:11,280 --> 00:08:15,780
doesn't have a leader you can can

00:08:13,919 --> 00:08:19,800
perform rights to it because that's how

00:08:15,780 --> 00:08:22,740
the RAF protocol ensures consistency and

00:08:19,800 --> 00:08:24,300
that means if the leader is not present

00:08:22,740 --> 00:08:27,300
or no leader is present for the entire

00:08:24,300 --> 00:08:28,020
cluster then that could be could turn

00:08:27,300 --> 00:08:31,020
out to be bad

00:08:28,020 --> 00:08:34,380
so this basically the alerting rule that

00:08:31,020 --> 00:08:36,599
would detect that and just to make sure

00:08:34,380 --> 00:08:38,339
that we all understand that prometheus

00:08:36,599 --> 00:08:40,620
alerting is not there's nothing magic

00:08:38,339 --> 00:08:42,450
basically you have all your rules that

00:08:40,620 --> 00:08:44,780
you have a new rule files and Prometheus

00:08:42,450 --> 00:08:48,780
loads those and then every evaluation

00:08:44,780 --> 00:08:50,440
interval Prometheus just executes those

00:08:48,780 --> 00:08:52,750
and if they trigger if

00:08:50,440 --> 00:08:56,339
an alert against the configured alert

00:08:52,750 --> 00:09:00,370
manager instances so there's really no

00:08:56,339 --> 00:09:04,389
dark magic involved here and this is a

00:09:00,370 --> 00:09:06,250
simple alert manager configuration where

00:09:04,389 --> 00:09:09,910
we just have one route and we group

00:09:06,250 --> 00:09:12,639
everything by the job label and just

00:09:09,910 --> 00:09:15,939
send that to a web book configuration

00:09:12,639 --> 00:09:22,060
and whenever we don't so we have a

00:09:15,939 --> 00:09:25,089
resolve time out of five minutes and now

00:09:22,060 --> 00:09:28,060
when an alert actually hits the alert

00:09:25,089 --> 00:09:29,920
manager that alert goes through a

00:09:28,060 --> 00:09:33,639
notification pipeline so that basically

00:09:29,920 --> 00:09:35,529
is the sequence of these components so

00:09:33,639 --> 00:09:38,410
first of all the alert manager checks

00:09:35,529 --> 00:09:40,569
has this alert been silenced so it

00:09:38,410 --> 00:09:43,629
checks its silence database there's any

00:09:40,569 --> 00:09:45,939
silence match if no then we continue and

00:09:43,629 --> 00:09:47,079
the next step is the weight step and

00:09:45,939 --> 00:09:51,310
this is very important for the high

00:09:47,079 --> 00:09:55,269
availability functionality which we'll

00:09:51,310 --> 00:09:57,209
go into a bit later and then the alert

00:09:55,269 --> 00:09:59,350
manager checks whether this particular

00:09:57,209 --> 00:10:02,050
alert has already been notified about

00:09:59,350 --> 00:10:04,300
and that's the deduplication step if it

00:10:02,050 --> 00:10:07,230
hasn't then we go on to the next step

00:10:04,300 --> 00:10:10,810
which is which actually sends the

00:10:07,230 --> 00:10:12,939
notification out and once it's not

00:10:10,810 --> 00:10:14,980
successfully sent the notification it

00:10:12,939 --> 00:10:20,439
will gossip that it has successfully

00:10:14,980 --> 00:10:22,990
done so and now we can talk about what

00:10:20,439 --> 00:10:24,490
their lark manager actually gossips so

00:10:22,990 --> 00:10:26,380
what we just talked about is that the

00:10:24,490 --> 00:10:31,899
alert manager gossips their send

00:10:26,380 --> 00:10:34,540
notifications Rahman as there's often

00:10:31,899 --> 00:10:36,370
the question asked why we don't gossip

00:10:34,540 --> 00:10:38,680
all the received alerts that all the

00:10:36,370 --> 00:10:40,149
alert managers get oh that's a simple

00:10:38,680 --> 00:10:43,209
question because that simple answer

00:10:40,149 --> 00:10:45,880
because there would be that the sheer

00:10:43,209 --> 00:10:47,470
volume of the alerts received by all our

00:10:45,880 --> 00:10:49,720
managers would be just too much

00:10:47,470 --> 00:10:51,399
information shared and the actual

00:10:49,720 --> 00:10:54,550
information that we need is whether this

00:10:51,399 --> 00:10:57,699
alert has been notified about and then

00:10:54,550 --> 00:10:59,889
additionally so that all the alert

00:10:57,699 --> 00:11:02,140
managers have the same silences also the

00:10:59,889 --> 00:11:04,670
silences are

00:11:02,140 --> 00:11:09,470
so bite but how do we actually do this

00:11:04,670 --> 00:11:11,800
and we do do it with CR DTS and we did

00:11:09,470 --> 00:11:16,970
that because it really well fits this

00:11:11,800 --> 00:11:18,860
particular use case because we want an

00:11:16,970 --> 00:11:21,050
an available system so in terms of

00:11:18,860 --> 00:11:23,090
distributed systems theory the cap

00:11:21,050 --> 00:11:25,940
theorem this is an AP system that we

00:11:23,090 --> 00:11:28,430
want to build and that's what CRD tees

00:11:25,940 --> 00:11:30,860
are really good for but this is the

00:11:28,430 --> 00:11:33,950
theory how did we actually do this we

00:11:30,860 --> 00:11:37,280
use the library called mesh by weave

00:11:33,950 --> 00:11:39,350
works and that basically gives us most

00:11:37,280 --> 00:11:43,700
of the infrastructure to build this and

00:11:39,350 --> 00:11:50,990
we only had to design the CR DT that is

00:11:43,700 --> 00:11:53,750
actually gossiped so it gives us an

00:11:50,990 --> 00:11:58,070
eventually consistent system where like

00:11:53,750 --> 00:11:58,910
last writer winds elements it that's our

00:11:58,070 --> 00:12:02,210
choosing

00:11:58,910 --> 00:12:05,330
that we built the data structure like

00:12:02,210 --> 00:12:08,030
that and basically what we're doing is

00:12:05,330 --> 00:12:09,950
we have a log of records that are

00:12:08,030 --> 00:12:13,600
time-stamped and whenever the there's a

00:12:09,950 --> 00:12:17,450
larger time step then we overwrite that

00:12:13,600 --> 00:12:33,800
particular record and how we identify

00:12:17,450 --> 00:12:35,660
those is with a unique ID just so you

00:12:33,800 --> 00:12:37,550
might ask well why didn't we take a

00:12:35,660 --> 00:12:40,180
distributed system that already existed

00:12:37,550 --> 00:12:43,580
such as Etsy D which is like a

00:12:40,180 --> 00:12:46,520
reasonably like really well developed

00:12:43,580 --> 00:12:50,450
uh-huh database and it's been battle

00:12:46,520 --> 00:12:52,580
tested it works really well well one one

00:12:50,450 --> 00:12:54,740
point would be the roulette manager is a

00:12:52,580 --> 00:12:57,200
single self-contained binary you can

00:12:54,740 --> 00:13:00,920
just you just need to drop that binary

00:12:57,200 --> 00:13:03,290
on your server and you can run it so we

00:13:00,920 --> 00:13:05,420
have if we then also had EDD in there

00:13:03,290 --> 00:13:08,540
then we also would have to have a

00:13:05,420 --> 00:13:11,420
connection to add CD and that brings in

00:13:08,540 --> 00:13:13,970
a lot of baggage but most importantly is

00:13:11,420 --> 00:13:14,870
we want to build an AP system whereas at

00:13:13,970 --> 00:13:17,630
CD and

00:13:14,870 --> 00:13:21,980
no leader as we saw earlier will block

00:13:17,630 --> 00:13:23,210
right so in that case we wouldn't get a

00:13:21,980 --> 00:13:27,410
notification

00:13:23,210 --> 00:13:29,210
whereas in ap case we potentially get

00:13:27,410 --> 00:13:34,460
multiple notifications but that's better

00:13:29,210 --> 00:13:38,180
than none for hope for reliability so

00:13:34,460 --> 00:13:40,730
let's go into how this actually this

00:13:38,180 --> 00:13:46,610
merge process works or for different

00:13:40,730 --> 00:13:49,460
scenarios so when we create a silence we

00:13:46,610 --> 00:13:53,210
have multiple instances of alert manager

00:13:49,460 --> 00:13:54,950
that register 0 and 1 and they both have

00:13:53,210 --> 00:13:57,950
already some records in their database

00:13:54,950 --> 00:14:01,880
and now I go ahead and hit create

00:13:57,950 --> 00:14:04,040
endpoint for the silence and the alert

00:14:01,880 --> 00:14:09,440
manager 0 immediately writes that into

00:14:04,040 --> 00:14:11,510
its database and produces a delta of

00:14:09,440 --> 00:14:15,380
what it's just written to the database

00:14:11,510 --> 00:14:18,260
and then it gossips that Delta to all

00:14:15,380 --> 00:14:20,209
other alert manager instances and they

00:14:18,260 --> 00:14:24,260
then take that Delta and merge it with

00:14:20,209 --> 00:14:27,920
their database and eventually then both

00:14:24,260 --> 00:14:31,430
instances have the same database and

00:14:27,920 --> 00:14:33,260
then periodically they sync so that you

00:14:31,430 --> 00:14:37,190
can make sure that at least periodically

00:14:33,260 --> 00:14:38,839
the database is always in sync and if we

00:14:37,190 --> 00:14:41,480
update silences let's say we already

00:14:38,839 --> 00:14:43,370
have two records in our database then

00:14:41,480 --> 00:14:47,230
and I want to update that particular

00:14:43,370 --> 00:14:50,480
silence then I just override that

00:14:47,230 --> 00:14:52,910
particular field in the database and the

00:14:50,480 --> 00:14:56,810
database again returns the data for me

00:14:52,910 --> 00:14:58,670
and I just gossip the Delta to the other

00:14:56,810 --> 00:15:01,820
alert manager instances and they merge

00:14:58,670 --> 00:15:04,940
it into their state so that's not really

00:15:01,820 --> 00:15:10,420
that complicated because for silences we

00:15:04,940 --> 00:15:13,270
can just generate a UUID whereas for

00:15:10,420 --> 00:15:15,920
notifications that's a bit more tricky

00:15:13,270 --> 00:15:21,860
so let's look at how this works for

00:15:15,920 --> 00:15:24,650
notifications first let's see why the

00:15:21,860 --> 00:15:27,720
notifications or why this is why it's

00:15:24,650 --> 00:15:29,520
really important for us to

00:15:27,720 --> 00:15:31,590
gossip them in this way so we have a

00:15:29,520 --> 00:15:34,940
Prometheus instance which fires alerts

00:15:31,590 --> 00:15:38,040
against to alert manager instances and

00:15:34,940 --> 00:15:40,830
here comes exactly this wait step into

00:15:38,040 --> 00:15:43,050
play that I mentioned earlier so alert

00:15:40,830 --> 00:15:44,670
manager zero waits for zero seconds and

00:15:43,050 --> 00:15:47,220
then continues in the notification

00:15:44,670 --> 00:15:51,240
pipeline whereas lurk manager one waits

00:15:47,220 --> 00:15:54,870
for five seconds and well alert manager

00:15:51,240 --> 00:15:57,120
one is waiting alert manager zero goes

00:15:54,870 --> 00:16:00,990
through the steps and deduplicate and

00:15:57,120 --> 00:16:05,970
sends the notification out and then

00:16:00,990 --> 00:16:08,330
gossips the state and after a while the

00:16:05,970 --> 00:16:11,160
alert manager one after five seconds

00:16:08,330 --> 00:16:13,410
we'll continue this but has already

00:16:11,160 --> 00:16:15,630
received this gossip data so when it

00:16:13,410 --> 00:16:17,220
reaches it's the duplication step that

00:16:15,630 --> 00:16:19,590
will not send out this notification

00:16:17,220 --> 00:16:21,870
because it has been informed that this

00:16:19,590 --> 00:16:24,480
notification has already been notified

00:16:21,870 --> 00:16:27,060
about now

00:16:24,480 --> 00:16:31,850
here comes exactly into play why it's

00:16:27,060 --> 00:16:35,490
important that we have an AP system so

00:16:31,850 --> 00:16:39,080
if we were to block rights at all times

00:16:35,490 --> 00:16:42,060
in a CP system then we couldn't do

00:16:39,080 --> 00:16:43,350
couldn't handle this failure where we

00:16:42,060 --> 00:16:45,270
have like a network partition between

00:16:43,350 --> 00:16:49,110
the two alert manager instances in this

00:16:45,270 --> 00:16:52,710
case where the gossiping cannot take

00:16:49,110 --> 00:16:56,040
place and that will in its worst case

00:16:52,710 --> 00:16:57,570
give you two notifications which is much

00:16:56,040 --> 00:17:03,589
better than getting no notification

00:16:57,570 --> 00:17:06,990
about your system not being up and the

00:17:03,589 --> 00:17:08,699
merge operations basically work the same

00:17:06,990 --> 00:17:12,480
way except that there's different data

00:17:08,699 --> 00:17:15,630
in here so instead of the matcher for

00:17:12,480 --> 00:17:18,240
the silences we have the alerts that

00:17:15,630 --> 00:17:22,050
have been resolved that and those that

00:17:18,240 --> 00:17:23,730
have been notified about before and when

00:17:22,050 --> 00:17:27,630
an alert fires it goes through all of

00:17:23,730 --> 00:17:30,690
this pipeline and there is a new alert

00:17:27,630 --> 00:17:32,520
we actually know new notification then

00:17:30,690 --> 00:17:35,100
we write it to the database and we do

00:17:32,520 --> 00:17:36,540
the same thing where we gossip the Delta

00:17:35,100 --> 00:17:39,050
and then merge it into the other

00:17:36,540 --> 00:17:39,050
databases

00:17:40,250 --> 00:17:44,990
as I said already earlier we can't just

00:17:42,830 --> 00:17:47,240
generate a unique ID for all these

00:17:44,990 --> 00:17:49,040
notifications otherwise we would again

00:17:47,240 --> 00:17:50,980
get duplicate notifications for

00:17:49,040 --> 00:17:53,600
everything so we need to somehow

00:17:50,980 --> 00:17:57,560
generate a key that is always the same

00:17:53,600 --> 00:17:59,990
for the same set of alerts for the same

00:17:57,560 --> 00:18:02,300
route so that's exactly but what's

00:17:59,990 --> 00:18:05,030
happening here when the alert goes

00:18:02,300 --> 00:18:08,000
through the alert routing tree it

00:18:05,030 --> 00:18:09,800
collects all the values for the labels

00:18:08,000 --> 00:18:13,210
that have been matched against and those

00:18:09,800 --> 00:18:15,140
are then the routes are export and

00:18:13,210 --> 00:18:18,770
concatenated with the receiver because

00:18:15,140 --> 00:18:23,630
technically you could receive those with

00:18:18,770 --> 00:18:26,150
multiple receivers so now we saw how

00:18:23,630 --> 00:18:33,800
that works in theory let's check that

00:18:26,150 --> 00:18:39,770
that actually works can everybody see

00:18:33,800 --> 00:18:42,290
that okay so I'm just starting a couple

00:18:39,770 --> 00:18:45,290
of alert manager instances three here

00:18:42,290 --> 00:18:47,990
actually and I example webhook

00:18:45,290 --> 00:18:50,360
application which will when we send a

00:18:47,990 --> 00:18:54,350
notification just print it to standard

00:18:50,360 --> 00:18:57,220
out so I have a simple script that will

00:18:54,350 --> 00:19:01,040
just send an alert payload to all my

00:18:57,220 --> 00:19:03,640
alert manager instances and as we

00:19:01,040 --> 00:19:06,740
already saw above all those instances

00:19:03,640 --> 00:19:09,790
put some debugging debugging output in

00:19:06,740 --> 00:19:15,560
there that they've received alerts so

00:19:09,790 --> 00:19:18,550
have a look at the UI we can see that

00:19:15,560 --> 00:19:29,390
indeed a couple of alerts have been

00:19:18,550 --> 00:19:33,290
received by all of our instances but the

00:19:29,390 --> 00:19:36,080
interesting thing now is whether we have

00:19:33,290 --> 00:19:40,970
only received one notification here so

00:19:36,080 --> 00:19:44,600
let's look at the output and indeed the

00:19:40,970 --> 00:19:48,200
webhook only gives us one notification

00:19:44,600 --> 00:19:49,730
blob so our high availability here has

00:19:48,200 --> 00:19:52,220
worked we've only received one

00:19:49,730 --> 00:19:53,840
notification instead of three which

00:19:52,220 --> 00:19:54,200
would would have been the case if they

00:19:53,840 --> 00:19:59,090
were not

00:19:54,200 --> 00:20:01,340
connected and just as just to show that

00:19:59,090 --> 00:20:21,580
that also works let's also create a

00:20:01,340 --> 00:20:24,800
silence so I created the silence on the

00:20:21,580 --> 00:20:30,200
instance with the port 95 at the end and

00:20:24,800 --> 00:20:33,470
we can check whether the silence also

00:20:30,200 --> 00:20:43,040
exists for the other instances so for

00:20:33,470 --> 00:20:45,470
this one and for this one yeah so we

00:20:43,040 --> 00:20:47,540
have successfully gossiped all our state

00:20:45,470 --> 00:20:57,800
and we only receive one notification so

00:20:47,540 --> 00:20:59,990
high availability works if you have any

00:20:57,800 --> 00:21:14,210
more questions feel free to ask

00:20:59,990 --> 00:21:18,170
hey there's lots all right so I have one

00:21:14,210 --> 00:21:20,330
question in your stop of your expansion

00:21:18,170 --> 00:21:24,650
about the keys and how you merge them

00:21:20,330 --> 00:21:28,040
mentioned time stomp so I might crack

00:21:24,650 --> 00:21:29,420
that to use that local time stone to do

00:21:28,040 --> 00:21:31,820
the last part rain and so on and

00:21:29,420 --> 00:21:33,680
question is do you consider using vector

00:21:31,820 --> 00:21:35,030
clocks or stuff like that because time

00:21:33,680 --> 00:21:38,020
stuff you know yeah

00:21:35,030 --> 00:21:41,150
yes actually we we did think about that

00:21:38,020 --> 00:21:43,790
but so far we haven't actually had any

00:21:41,150 --> 00:21:45,170
problems with this so this was just a

00:21:43,790 --> 00:21:47,510
simple solution that we started

00:21:45,170 --> 00:21:49,400
implementing in the beginning but if

00:21:47,510 --> 00:21:51,440
there's an actual necessity for it like

00:21:49,400 --> 00:21:54,470
people actually have problems then

00:21:51,440 --> 00:21:57,710
that's certainly a possibility to use

00:21:54,470 --> 00:22:00,980
the thing is that we also use waiting

00:21:57,710 --> 00:22:03,260
for with the times right so we can be

00:22:00,980 --> 00:22:04,590
even if the clocks are not totally in

00:22:03,260 --> 00:22:08,190
soon it's

00:22:04,590 --> 00:22:10,650
we can in a reasonably reasonable range

00:22:08,190 --> 00:22:12,660
expected that that will still work every

00:22:10,650 --> 00:22:15,120
time

00:22:12,660 --> 00:22:18,120
just a quick work yes for the way I

00:22:15,120 --> 00:22:21,480
agree for the max and identification

00:22:18,120 --> 00:22:23,940
zone but for merging data if like one

00:22:21,480 --> 00:22:25,590
box is one hour late because of you know

00:22:23,940 --> 00:22:27,450
summertime and stuff it might actually

00:22:25,590 --> 00:22:30,330
merge the data in the other way wrong

00:22:27,450 --> 00:22:34,740
yeah that's totally true we should

00:22:30,330 --> 00:22:36,450
probably use it yeah I just checked I am

00:22:34,740 --> 00:22:38,669
actually just downloading the newest

00:22:36,450 --> 00:22:41,850
version from github but the last version

00:22:38,669 --> 00:22:43,980
I have doesn't work on ipv6 with high

00:22:41,850 --> 00:22:46,140
high very high values emo doesn't work

00:22:43,980 --> 00:22:49,409
is this something anyone but me cares

00:22:46,140 --> 00:22:53,030
about I'm actually not actually sure

00:22:49,409 --> 00:22:57,090
there if there's an issue for it similar

00:22:53,030 --> 00:22:59,280
yeah so the problem is that that doesn't

00:22:57,090 --> 00:23:02,520
really depend on us it depends on the

00:22:59,280 --> 00:23:06,450
mesh library yeah I'm not sure if it

00:23:02,520 --> 00:23:09,090
already has it implemented but once the

00:23:06,450 --> 00:23:10,919
mesh library implements ipv6 support we

00:23:09,090 --> 00:23:12,630
just need to update the dependency and

00:23:10,919 --> 00:23:27,169
it should work okay so I will try to

00:23:12,630 --> 00:23:29,970
file more effusive yeah so talk about

00:23:27,169 --> 00:23:33,030
event correlation from SNMP traps from

00:23:29,970 --> 00:23:34,740
event critical messages and even from

00:23:33,030 --> 00:23:36,720
this morning's talk about getting

00:23:34,740 --> 00:23:40,260
analytics into this and maybe

00:23:36,720 --> 00:23:41,909
correlating all these different not only

00:23:40,260 --> 00:23:43,409
I guess you're doing synchronous polling

00:23:41,909 --> 00:23:46,950
and thresholding but also asynchronous

00:23:43,409 --> 00:23:48,750
events coming in I'm not sure I

00:23:46,950 --> 00:23:50,520
understand the question can you so the

00:23:48,750 --> 00:23:52,260
question is if you have a mercifully

00:23:50,520 --> 00:23:54,419
elsewhere she tries to do analysis on

00:23:52,260 --> 00:23:56,159
them the general answer is put that out

00:23:54,419 --> 00:23:56,909
into some water system like or and duty

00:23:56,159 --> 00:24:00,470
analysis there

00:23:56,909 --> 00:24:02,760
that's the generalizes as with radius

00:24:00,470 --> 00:24:08,039
because that's a different and more

00:24:02,760 --> 00:24:12,350
complicated space there's there and

00:24:08,039 --> 00:24:12,350
there and back there as well

00:24:14,959 --> 00:24:22,829
hello hi there um I'm just curious why

00:24:19,619 --> 00:24:25,679
you did the gossip wait times instead of

00:24:22,829 --> 00:24:28,349
like consistent hash or something um

00:24:25,679 --> 00:24:30,809
actually I think that question would be

00:24:28,349 --> 00:24:33,149
I would give off to Fabian I think it

00:24:30,809 --> 00:24:35,279
was just a design decision and it turned

00:24:33,149 --> 00:24:39,059
out - yeah it turned off how to work

00:24:35,279 --> 00:24:42,139
really well so if it doesn't work for

00:24:39,059 --> 00:24:43,979
anyone then we should explore different

00:24:42,139 --> 00:24:49,139
implementations but so far it hasn't

00:24:43,979 --> 00:24:51,149
created any problems so if you use

00:24:49,139 --> 00:24:53,249
colors and hashing right and just you

00:24:51,149 --> 00:24:56,249
expect to appear to actually send this

00:24:53,249 --> 00:24:58,019
out and it just doesn't do it you got

00:24:56,249 --> 00:24:59,519
hot yeah I'm doing it isin right so this

00:24:58,019 --> 00:25:01,440
wait it's just an easy mechanism for

00:24:59,519 --> 00:25:03,539
everybody to always send it out and just

00:25:01,440 --> 00:25:05,219
like you duplicate that by just waiting

00:25:03,539 --> 00:25:07,499
for a bit and how do you so when you

00:25:05,219 --> 00:25:10,669
remove people from the list of gossip

00:25:07,499 --> 00:25:12,959
peers whenever mesh does which is never

00:25:10,669 --> 00:25:15,269
it does I'm pretty sure only if you

00:25:12,959 --> 00:25:21,089
restart them all no no I'm going to show

00:25:15,269 --> 00:25:22,949
this because that's not safe then like

00:25:21,089 --> 00:25:24,509
make sure do you reset the wait time so

00:25:22,949 --> 00:25:28,259
Ursula

00:25:24,509 --> 00:25:29,569
maybe I just hit the edge case but there

00:25:28,259 --> 00:25:31,769
was also an issue where actually

00:25:29,569 --> 00:25:34,289
depending if you have different ports if

00:25:31,769 --> 00:25:35,879
everybody joins but if the initial deal

00:25:34,289 --> 00:25:39,299
goes away everybody leaves it closer

00:25:35,879 --> 00:25:42,379
yeah I'm there some mesh thing set up

00:25:39,299 --> 00:25:45,899
certainly we need to figure out I guess

00:25:42,379 --> 00:25:47,789
yeah yeah okay always remember only

00:25:45,899 --> 00:25:49,369
there's a couple of years to make ipv6

00:25:47,789 --> 00:25:52,109
work but it probably doesn't work here

00:25:49,369 --> 00:25:53,219
so yeah and if you if you're thinkin at

00:25:52,109 --> 00:25:54,899
this I didn't know what that sort of

00:25:53,219 --> 00:25:57,229
Spanish you and we'll check it cool

00:25:54,899 --> 00:25:57,229
thank you

00:25:57,989 --> 00:26:04,229
you mentioned the ordering of the alert

00:26:01,289 --> 00:26:05,879
managers who sets that order is

00:26:04,229 --> 00:26:07,889
Prometheus saying you were the first in

00:26:05,879 --> 00:26:09,899
my list you the first - or other eleven

00:26:07,889 --> 00:26:11,249
are just making that out in their own

00:26:09,899 --> 00:26:13,079
yeah actually so that's something that

00:26:11,249 --> 00:26:16,069
we also take from the mesh library so we

00:26:13,079 --> 00:26:20,849
basically take all the numbers and the

00:26:16,069 --> 00:26:23,999
order that we get there is the position

00:26:20,849 --> 00:26:25,649
times five seconds that you wait so it's

00:26:23,999 --> 00:26:26,099
completely internal in the alone under

00:26:25,649 --> 00:26:28,579
cluster

00:26:26,099 --> 00:26:28,579
okay thank you

00:26:28,810 --> 00:26:37,400
hello Frederick okay this is the last

00:26:35,270 --> 00:26:40,460
question

00:26:37,400 --> 00:26:42,770
so you said we actually sync the

00:26:40,460 --> 00:26:45,110
silences every periodically rate do we

00:26:42,770 --> 00:26:48,110
also sync the notification log yes so

00:26:45,110 --> 00:26:50,600
that happens for for all the state that

00:26:48,110 --> 00:26:52,040
is shared for the the notification log

00:26:50,600 --> 00:26:56,020
can get really big great

00:26:52,040 --> 00:26:59,320
yes but it's garbage collected after a

00:26:56,020 --> 00:26:59,320
certain time

00:27:01,570 --> 00:27:05,020
all right thank you

00:27:08,550 --> 00:27:21,240
[Music]

00:27:21,040 --> 00:27:24,380
you

00:27:21,240 --> 00:27:24,380

YouTube URL: https://www.youtube.com/watch?v=i6Hmc0bKHAY


