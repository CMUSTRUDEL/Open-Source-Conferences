Title: PromCon 2017: Storing 16 Bytes at Scale - Fabian Reinartz
Publication date: 2017-09-01
Playlist: PromCon 2017
Description: 
	* Abstract:

From the beginning, Prometheus was built as a monitoring system with Cloud Native environments in mind. Orchestration systems such as Kubernetes are rapidly gaining traction and unlock features of highly dynamic environments, such as frequent rolling updates and auto-scaling, for everyone. This inevitably puts new strains on Prometheus as well.

In this talk we explore what these challenges are and how we are addressing them by building a new storage layer from the ground up. The new design provides efficient indexing techniques that gracefully handle high turnover rates of monitoring targets and provide consistent query performance. At the same time, it significantly reduces resource requirements and paves the way for advanced features like hot backups and dynamic retention policies.

* Speaker biography:

Fabian is a Prometheus core developer and software engineer at CoreOS, where he works on the Prometheus open source eco-system and its integration with Kubernetes. Previously, he was a production engineer at SoundCloud.

* Slides:

https://promcon.io/2017-munich/slides/storing-16-bytes-at-scale.pdf

* PromCon website:

https://promcon.io/
Captions: 
	00:00:00,380 --> 00:00:16,840
[Music]

00:00:13,160 --> 00:00:19,960
so now a man who needs no introduction

00:00:16,840 --> 00:00:19,960
[Music]

00:00:23,869 --> 00:00:31,529
thank you so I'm huggin because I didn't

00:00:29,519 --> 00:00:34,050
tell you and so I'm going to talk about

00:00:31,529 --> 00:00:35,760
storing 16 by that scale um if you've

00:00:34,050 --> 00:00:37,739
looked at the schedule realize that this

00:00:35,760 --> 00:00:40,379
is sort of like starting off the chorus

00:00:37,739 --> 00:00:41,940
track and this is because Korres does a

00:00:40,379 --> 00:00:44,940
bunch of work on from misses upstream

00:00:41,940 --> 00:00:46,530
and after short break we are now hiring

00:00:44,940 --> 00:00:48,000
people again so if you're interested in

00:00:46,530 --> 00:00:49,710
actually working on prometheus upstream

00:00:48,000 --> 00:00:52,079
and also integrating from easiest with

00:00:49,710 --> 00:00:56,309
kubernetes and also tectonic opposed I'm

00:00:52,079 --> 00:01:00,239
just visit our careers page and apply so

00:00:56,309 --> 00:01:02,270
we slip out of the way let's start so we

00:01:00,239 --> 00:01:05,250
try to start sixteen by that scale and

00:01:02,270 --> 00:01:07,409
why do we try that and we want to start

00:01:05,250 --> 00:01:09,450
time series and time spheres happen to

00:01:07,409 --> 00:01:11,610
be composed out of samples and simple

00:01:09,450 --> 00:01:13,710
just like streams of values and each

00:01:11,610 --> 00:01:18,119
value has a time so I've attached and

00:01:13,710 --> 00:01:19,170
it's basically the same problem just

00:01:18,119 --> 00:01:24,780
that we don't have a single time series

00:01:19,170 --> 00:01:26,549
but a lot of time series and to do this

00:01:24,780 --> 00:01:27,900
now we somehow I have to also sort of

00:01:26,549 --> 00:01:29,759
determine like what is the time series

00:01:27,900 --> 00:01:31,650
right we need a way to address a single

00:01:29,759 --> 00:01:33,060
series or a set of series and in

00:01:31,650 --> 00:01:36,240
Prometheus you probably know the data

00:01:33,060 --> 00:01:37,920
model we have metric names these are

00:01:36,240 --> 00:01:40,259
meant to sort of give a semantic meaning

00:01:37,920 --> 00:01:43,649
to the values that are in your time

00:01:40,259 --> 00:01:47,009
series and then you can sort of fan out

00:01:43,649 --> 00:01:50,040
or split up this metric using labels so

00:01:47,009 --> 00:01:51,899
a metric name plus a set of labels and a

00:01:50,040 --> 00:01:54,630
unique set of values for these navels is

00:01:51,899 --> 00:01:57,149
a single time series and this allows

00:01:54,630 --> 00:01:59,460
them to sort of partition this metric

00:01:57,149 --> 00:02:03,299
space and gain insight around different

00:01:59,460 --> 00:02:06,299
dimensions and to now select or query

00:02:03,299 --> 00:02:08,160
different time series you can just

00:02:06,299 --> 00:02:09,179
select the metric name which gets you

00:02:08,160 --> 00:02:11,480
all the time series that have this

00:02:09,179 --> 00:02:13,370
metric name but it can also add

00:02:11,480 --> 00:02:16,129
they limit us like method equals gets

00:02:13,370 --> 00:02:18,379
and they also recognize us are just a

00:02:16,129 --> 00:02:22,150
really powerful way to select across all

00:02:18,379 --> 00:02:22,150
these series in a multi-dimensional way

00:02:22,510 --> 00:02:26,659
and of course there's a sort of second

00:02:24,830 --> 00:02:28,069
dimension to it which is time and so we

00:02:26,659 --> 00:02:30,379
have this two dimensional plane of

00:02:28,069 --> 00:02:32,659
series in the vertical axis and time in

00:02:30,379 --> 00:02:34,670
the horizontal one and for the queer is

00:02:32,659 --> 00:02:37,250
something we a select series but also we

00:02:34,670 --> 00:02:39,769
constrain these series of a certain time

00:02:37,250 --> 00:02:41,450
range and this can be pretty much

00:02:39,769 --> 00:02:43,430
anything right we can just sort of curvy

00:02:41,450 --> 00:02:46,670
all series for the last minute or just

00:02:43,430 --> 00:02:48,440
the last sample we can just curry a

00:02:46,670 --> 00:02:50,599
single series or set of series for the

00:02:48,440 --> 00:02:54,890
entire time range we know or any

00:02:50,599 --> 00:02:57,109
rectangle on this plane reading and

00:02:54,890 --> 00:02:58,310
that's quite like a lot of degrees of

00:02:57,109 --> 00:02:59,690
freedom right like we have to basically

00:02:58,310 --> 00:03:03,440
support any query pattern you can

00:02:59,690 --> 00:03:05,720
imagine on this on this plane whereas

00:03:03,440 --> 00:03:07,760
actually adding data is pretty simple

00:03:05,720 --> 00:03:09,319
because we just add data at the

00:03:07,760 --> 00:03:11,409
rightmost site here right we just add

00:03:09,319 --> 00:03:16,549
single samples so every time series

00:03:11,409 --> 00:03:18,079
pretty much periodically and so no you

00:03:16,549 --> 00:03:20,030
got to put a stop watch for this and

00:03:18,079 --> 00:03:21,889
before we start you have to serve figure

00:03:20,030 --> 00:03:24,230
out like what do we actually aiming for

00:03:21,889 --> 00:03:26,299
right what's our order of scale and when

00:03:24,230 --> 00:03:29,030
saying you want to store these 16 bytes

00:03:26,299 --> 00:03:31,549
at scale and I can get pretty good

00:03:29,030 --> 00:03:32,840
measure is and taking like five minute

00:03:31,549 --> 00:03:34,700
time series that's equivalent to about

00:03:32,840 --> 00:03:37,400
like two thousand to fifteen thousand

00:03:34,700 --> 00:03:39,709
microservice instances depending on how

00:03:37,400 --> 00:03:42,079
many series ejects poses and then

00:03:39,709 --> 00:03:44,930
scraping these every 30 seconds and with

00:03:42,079 --> 00:03:46,370
more attention this gives us about six

00:03:44,930 --> 00:03:49,760
thousand ten per second we actually have

00:03:46,370 --> 00:03:51,889
to possess to disk and in tone that's

00:03:49,760 --> 00:03:56,000
forty four months four is four hundred

00:03:51,889 --> 00:03:58,160
thirty two billion samples and samples

00:03:56,000 --> 00:03:59,989
of times then pushes eight bytes and a

00:03:58,160 --> 00:04:02,049
value which is also eight bytes thereby

00:03:59,989 --> 00:04:04,190
we have to store sixteen bytes at scale

00:04:02,049 --> 00:04:06,470
and if you do the math just the raw data

00:04:04,190 --> 00:04:10,160
we have to store and for one month's is

00:04:06,470 --> 00:04:12,410
having terabytes that seems pretty okay

00:04:10,160 --> 00:04:13,910
right like it's a pretty incredible fan

00:04:12,410 --> 00:04:15,769
in fact or like two thousand fifteen

00:04:13,910 --> 00:04:20,390
thousand sort of processes you can

00:04:15,769 --> 00:04:22,219
monitor with a single media server but

00:04:20,390 --> 00:04:23,900
because we are on this two-dimensional

00:04:22,219 --> 00:04:25,490
plane of series and time

00:04:23,900 --> 00:04:28,400
soon as I sort of tweak any of these

00:04:25,490 --> 00:04:32,060
base parameters this number goes up

00:04:28,400 --> 00:04:34,340
right so six months is 42 terabytes it's

00:04:32,060 --> 00:04:36,080
obviously possible but yeah it's kind of

00:04:34,340 --> 00:04:37,580
yeah and then you might add a few more

00:04:36,080 --> 00:04:39,770
targets and this number goes up again

00:04:37,580 --> 00:04:43,160
you might reduce experiment a little bit

00:04:39,770 --> 00:04:44,840
and just grows and grows and grows so

00:04:43,160 --> 00:04:47,360
natural choice would be okay maybe can

00:04:44,840 --> 00:04:48,470
do some sort of compression to make this

00:04:47,360 --> 00:04:50,180
a bit smaller

00:04:48,470 --> 00:04:51,860
especially because series are quite

00:04:50,180 --> 00:04:54,020
repetitive so a lot of series do not

00:04:51,860 --> 00:04:55,160
change all that much like I think the

00:04:54,020 --> 00:04:56,919
estimate is like forty to sixty percent

00:04:55,160 --> 00:05:00,830
of your series actually constant um

00:04:56,919 --> 00:05:03,340
within next certain bounds and of course

00:05:00,830 --> 00:05:07,220
if things don't change these are so

00:05:03,340 --> 00:05:08,539
candidates for compression and compress

00:05:07,220 --> 00:05:11,720
sample to be surplus time stems because

00:05:08,539 --> 00:05:13,310
it's pretty much easier we scrape every

00:05:11,720 --> 00:05:16,720
30 seconds and this means that these

00:05:13,310 --> 00:05:19,669
timestamps in the UNIX presentation here

00:05:16,720 --> 00:05:22,820
have a pretty equal distance between

00:05:19,669 --> 00:05:24,710
each other and if you just stuff wipe

00:05:22,820 --> 00:05:26,360
this down you have the first time same

00:05:24,710 --> 00:05:28,220
and then you just add sorry and surly

00:05:26,360 --> 00:05:31,460
and this length of an outlier was like

00:05:28,220 --> 00:05:32,960
nine and another 30 and but these normal

00:05:31,460 --> 00:05:34,849
self needs more than the initial numbers

00:05:32,960 --> 00:05:37,280
so there must be some compression

00:05:34,849 --> 00:05:38,780
potential there and of course you can

00:05:37,280 --> 00:05:42,349
also do this on disk right you can like

00:05:38,780 --> 00:05:46,820
restore 30 in less bytes on disk and

00:05:42,349 --> 00:05:48,530
then these whole UNIX timestamps but you

00:05:46,820 --> 00:05:49,909
can do even better right because even

00:05:48,530 --> 00:05:53,690
these dead just don't change that much

00:05:49,909 --> 00:05:57,080
so you take the dealt off the Delta get

00:05:53,690 --> 00:05:58,639
even smaller values and in most cases

00:05:57,080 --> 00:05:59,150
actually these status of Delta's will be

00:05:58,639 --> 00:06:01,669
00:05:59,150 --> 00:06:05,210
like almost like 99.9 percent of the

00:06:01,669 --> 00:06:07,130
time and this of course is like if you

00:06:05,210 --> 00:06:08,990
have some sort of repetitive wave that's

00:06:07,130 --> 00:06:10,610
for almost every sample the same you can

00:06:08,990 --> 00:06:13,099
just basically educate a single bit for

00:06:10,610 --> 00:06:15,409
that and in the general case we can

00:06:13,099 --> 00:06:17,870
thereby with some tricks

00:06:15,409 --> 00:06:22,099
generally store timestamps in the single

00:06:17,870 --> 00:06:23,690
bit ended our values and that's a bit

00:06:22,099 --> 00:06:26,560
more complicated because we basically

00:06:23,690 --> 00:06:28,849
allow you to do any float64 value right

00:06:26,560 --> 00:06:31,270
and luckily there was some prior

00:06:28,849 --> 00:06:33,380
research based on which then Facebook

00:06:31,270 --> 00:06:35,270
developed the compression mechanisms for

00:06:33,380 --> 00:06:37,280
their own times use database

00:06:35,270 --> 00:06:40,250
which you can find read about in the

00:06:37,280 --> 00:06:42,590
link below and it's based on taking the

00:06:40,250 --> 00:06:45,260
good representation of your float64

00:06:42,590 --> 00:06:47,120
values and if you look at that

00:06:45,260 --> 00:06:48,470
you see kind of similarities so here we

00:06:47,120 --> 00:06:50,240
have a few decimal numbers and you see

00:06:48,470 --> 00:06:53,810
that that representation there and there

00:06:50,240 --> 00:06:55,640
are a lot of zeros and that even the the

00:06:53,810 --> 00:06:58,310
the head of the of these numbers is

00:06:55,640 --> 00:07:01,280
pretty much the same so if you XOR like

00:06:58,310 --> 00:07:02,990
two similar things and all these same

00:07:01,280 --> 00:07:06,260
regions cancel out and you get even more

00:07:02,990 --> 00:07:08,060
zeros and the only sort of meaningful

00:07:06,260 --> 00:07:09,410
information to expect out that is in

00:07:08,060 --> 00:07:11,810
these boxes yeah that's the actual

00:07:09,410 --> 00:07:13,520
difference so similar it with the time

00:07:11,810 --> 00:07:15,260
stems we just like take the first value

00:07:13,520 --> 00:07:17,410
and then sort of derive the second value

00:07:15,260 --> 00:07:20,090
based on and just storing the difference

00:07:17,410 --> 00:07:22,580
we can also just thought this XOR

00:07:20,090 --> 00:07:24,410
difference and then just also store the

00:07:22,580 --> 00:07:26,720
name of trading and eating zeros for

00:07:24,410 --> 00:07:28,310
each sample value and they have a

00:07:26,720 --> 00:07:30,440
compress these values as well and this

00:07:28,310 --> 00:07:33,320
works pretty good for decimal values as

00:07:30,440 --> 00:07:38,270
well as for values with fractions at the

00:07:33,320 --> 00:07:40,040
end and in polo this gets you from raw

00:07:38,270 --> 00:07:43,310
16 bytes per sample to a compressed

00:07:40,040 --> 00:07:45,860
average of 127 bytes that's a pretty

00:07:43,310 --> 00:07:47,630
pure saving about 12x and we realize

00:07:45,860 --> 00:07:49,370
this pretty early on right and so

00:07:47,630 --> 00:07:51,170
Prometheus one comes with a compression

00:07:49,370 --> 00:07:53,240
scheme which is not quite as good and

00:07:51,170 --> 00:07:55,310
but then you're on after reading this

00:07:53,240 --> 00:07:57,170
paper implemented this exact mechanism

00:07:55,310 --> 00:08:00,650
for Prometheus 1 as well and you can

00:07:57,170 --> 00:08:02,540
sort of flip it on and they nest Damien

00:08:00,650 --> 00:08:04,180
who is just someone who used to look at

00:08:02,540 --> 00:08:06,020
poking calm and she's just like

00:08:04,180 --> 00:08:08,840
implements a lot of really cool papers

00:08:06,020 --> 00:08:11,000
and go and it's pretty good at it and he

00:08:08,840 --> 00:08:13,070
did the same for this question reckon

00:08:11,000 --> 00:08:15,770
ISM but also Ellison assembly in between

00:08:13,070 --> 00:08:16,460
so this is more efficient so we kind of

00:08:15,770 --> 00:08:18,830
like went

00:08:16,460 --> 00:08:21,200
using his library for Prometheus to and

00:08:18,830 --> 00:08:22,580
now the total compression sort of

00:08:21,200 --> 00:08:24,290
accounts for like three percent of our

00:08:22,580 --> 00:08:28,000
total CPU time so we get this

00:08:24,290 --> 00:08:28,000
compression literally for free

00:08:28,639 --> 00:08:33,140
and this turns our seven terabytes into

00:08:31,219 --> 00:08:35,349
a 0.8 air rights and it's a constant

00:08:33,140 --> 00:08:37,760
improvement right um it's not like

00:08:35,349 --> 00:08:39,500
skating a thing per se but it's pretty

00:08:37,760 --> 00:08:42,800
important you gotta take it buys you so

00:08:39,500 --> 00:08:44,300
much and this turns our problem of

00:08:42,800 --> 00:08:46,690
florrum sixteen by that scale into is

00:08:44,300 --> 00:08:51,920
flowing eight 1.37 bytes at scale and

00:08:46,690 --> 00:08:55,100
which is a smaller problem so how did

00:08:51,920 --> 00:08:55,490
promises one do this well we'll figure

00:08:55,100 --> 00:08:57,769
out that

00:08:55,490 --> 00:09:00,079
and this compression is based on the

00:08:57,769 --> 00:09:01,910
previous sample all the time so to

00:09:00,079 --> 00:09:03,860
actually get to sample and you have to

00:09:01,910 --> 00:09:06,230
know sample n minus one so you can only

00:09:03,860 --> 00:09:08,630
serve stream for what three compressed

00:09:06,230 --> 00:09:09,920
stream of samples and if you want to

00:09:08,630 --> 00:09:11,540
access the last step if you don't really

00:09:09,920 --> 00:09:13,610
want to really start at the beginning of

00:09:11,540 --> 00:09:16,279
time to like read all potentially

00:09:13,610 --> 00:09:18,140
millions of samples for series and but

00:09:16,279 --> 00:09:21,140
instead you want to be able to jump at

00:09:18,140 --> 00:09:23,209
least and somewhat in between and that's

00:09:21,140 --> 00:09:25,730
why we have chunks and showings that are

00:09:23,209 --> 00:09:28,519
funding up a bunch of samples like 100

00:09:25,730 --> 00:09:30,470
to 1,000 maybe and then you can jump

00:09:28,519 --> 00:09:32,600
around in a serious to certain chunk and

00:09:30,470 --> 00:09:34,779
then just decompress and this trying to

00:09:32,600 --> 00:09:37,160
get to the value you actually care about

00:09:34,779 --> 00:09:40,850
and it promises one we have for each

00:09:37,160 --> 00:09:42,709
series bonfire and you want figure like

00:09:40,850 --> 00:09:45,620
if your five minute series that's five

00:09:42,709 --> 00:09:47,570
million files and it sounds like a lot

00:09:45,620 --> 00:09:49,820
but like modified systems actually can

00:09:47,570 --> 00:09:52,880
use us pretty well and that's what

00:09:49,820 --> 00:09:54,890
missus one works but you're not to

00:09:52,880 --> 00:09:58,459
figure that if we sort of append new

00:09:54,890 --> 00:10:01,040
samples vertically in this in this graph

00:09:58,459 --> 00:10:02,870
via that we would have to basically go

00:10:01,040 --> 00:10:04,160
around to every series file and at one

00:10:02,870 --> 00:10:06,230
point three seven bytes on average um

00:10:04,160 --> 00:10:07,070
and that's like a few hundred thousand

00:10:06,230 --> 00:10:09,410
times per second

00:10:07,070 --> 00:10:10,910
and and of course I'm spinning just

00:10:09,410 --> 00:10:12,800
that's not gonna work at all and but

00:10:10,910 --> 00:10:15,170
you've known as these if you could get

00:10:12,800 --> 00:10:17,300
this report you would probably like

00:10:15,170 --> 00:10:20,060
Friday's use in like a day or two just

00:10:17,300 --> 00:10:21,350
because as these were out if we just

00:10:20,060 --> 00:10:25,790
make too many random it's more

00:10:21,350 --> 00:10:27,740
modifications and so to provide this we

00:10:25,790 --> 00:10:29,420
just sort of keep the most recent chunk

00:10:27,740 --> 00:10:31,730
which way is your adding data to in

00:10:29,420 --> 00:10:33,260
memory and only one such fungus will you

00:10:31,730 --> 00:10:36,410
complete it I mean don't add any more

00:10:33,260 --> 00:10:38,209
data we just flush it to disk and even

00:10:36,410 --> 00:10:39,350
if you do this sort of betting you're

00:10:38,209 --> 00:10:40,730
still probably flashing like a few

00:10:39,350 --> 00:10:43,580
thousand times per second dude

00:10:40,730 --> 00:10:48,350
um but you definitely get much higher

00:10:43,580 --> 00:10:50,570
throughput this works all great

00:10:48,350 --> 00:10:52,370
and now sort of the enemy of Prometheus

00:10:50,570 --> 00:10:55,700
comes in for like the last year which is

00:10:52,370 --> 00:10:59,960
churn and churn was basically brought on

00:10:55,700 --> 00:11:01,040
to things like kubernetes just our

00:10:59,960 --> 00:11:03,620
environments are becoming more dynamic

00:11:01,040 --> 00:11:06,080
right and we get all these like cool

00:11:03,620 --> 00:11:07,670
things like I don't know all the scaling

00:11:06,080 --> 00:11:09,050
and rolling deployments like every

00:11:07,670 --> 00:11:10,340
commit to mass that you can just like

00:11:09,050 --> 00:11:13,520
vote it up a new version to enter a

00:11:10,340 --> 00:11:17,300
cluster and your rotten teapots and each

00:11:13,520 --> 00:11:18,860
pop gets new name and with each new name

00:11:17,300 --> 00:11:23,090
of your instance you also get a new set

00:11:18,860 --> 00:11:24,740
of metrics so metrics and these time

00:11:23,090 --> 00:11:27,170
series like tend to become a lot shorter

00:11:24,740 --> 00:11:33,440
because just instances don't live as

00:11:27,170 --> 00:11:36,800
long but in return you get more them so

00:11:33,440 --> 00:11:38,960
now we still have 5 million active time

00:11:36,800 --> 00:11:40,700
series right we didn't like like make

00:11:38,960 --> 00:11:42,680
our infrastructure bigger at all it's

00:11:40,700 --> 00:11:44,960
just more dynamic and so over time we

00:11:42,680 --> 00:11:46,460
accumulate more time series in total but

00:11:44,960 --> 00:11:49,310
the total super did not be change all

00:11:46,460 --> 00:11:51,560
that much um but now we go from like 5

00:11:49,310 --> 00:11:53,570
million fires to 150 million files for

00:11:51,560 --> 00:11:55,370
more months and this just grows more and

00:11:53,570 --> 00:11:57,590
more and more as we increase retention

00:11:55,370 --> 00:12:01,670
time and of course in the future this

00:11:57,590 --> 00:12:03,110
will dislike dynamic sort of aspect of

00:12:01,670 --> 00:12:05,920
infrastructure will just become even

00:12:03,110 --> 00:12:07,790
more obvious and this sort of this

00:12:05,920 --> 00:12:09,980
problem just become an even steeper

00:12:07,790 --> 00:12:11,450
increase right so you might have like

00:12:09,980 --> 00:12:14,420
billions and billions of series for

00:12:11,450 --> 00:12:16,340
single months and that's sort of killing

00:12:14,420 --> 00:12:18,470
for me this one as you might have saw

00:12:16,340 --> 00:12:22,850
encountered if you're running in one of

00:12:18,470 --> 00:12:24,410
these environments and like one example

00:12:22,850 --> 00:12:25,760
problem here is let's say you want to

00:12:24,410 --> 00:12:27,350
probably something just like before we

00:12:25,760 --> 00:12:30,320
can pick any rectangle in our

00:12:27,350 --> 00:12:34,040
two-dimensional space and if you just

00:12:30,320 --> 00:12:36,110
select a bunch of series you get all the

00:12:34,040 --> 00:12:38,270
series as though it has four these

00:12:36,110 --> 00:12:40,490
selectors you provided but this does not

00:12:38,270 --> 00:12:42,110
necessarily mean anymore that there's

00:12:40,490 --> 00:12:43,550
actual data in the time window you

00:12:42,110 --> 00:12:47,300
selected right because it's just like a

00:12:43,550 --> 00:12:48,590
lot more sparse and it was before and

00:12:47,300 --> 00:12:50,510
this can like become real expensive

00:12:48,590 --> 00:12:52,010
occurring when actually you have data

00:12:50,510 --> 00:12:52,510
for like one or two years but I actually

00:12:52,010 --> 00:12:59,590
could

00:12:52,510 --> 00:13:01,510
like 100,000 so how can we dress this

00:12:59,590 --> 00:13:03,340
and I mean one opportunity just left

00:13:01,510 --> 00:13:06,430
store for every series which time when

00:13:03,340 --> 00:13:08,530
it covers sure but then now you have to

00:13:06,430 --> 00:13:11,110
like sort of index intervals of series

00:13:08,530 --> 00:13:12,970
and then you don't know all that well

00:13:11,110 --> 00:13:14,410
when it's use actually stops it's not a

00:13:12,970 --> 00:13:14,950
pretty good problem and Brian we'll talk

00:13:14,410 --> 00:13:17,410
about it later

00:13:14,950 --> 00:13:18,640
um and then it's super actually come

00:13:17,410 --> 00:13:20,050
back again right now you have to like

00:13:18,640 --> 00:13:21,460
store lists of intervals for every

00:13:20,050 --> 00:13:23,920
series and it gets like really complex

00:13:21,460 --> 00:13:25,780
so index is kind of stuff and it's

00:13:23,920 --> 00:13:26,280
really annoying and just even think

00:13:25,780 --> 00:13:29,200
about it

00:13:26,280 --> 00:13:32,050
so um like I said the solution is just

00:13:29,200 --> 00:13:34,810
okay and if our series grow over time

00:13:32,050 --> 00:13:37,390
and maybe just stick lets constrain the

00:13:34,810 --> 00:13:41,020
time and we divide our time dimension

00:13:37,390 --> 00:13:42,280
into these blocks um and we treat each

00:13:41,020 --> 00:13:44,020
of these blocks as a single database

00:13:42,280 --> 00:13:46,330
effectively like it sometimes use

00:13:44,020 --> 00:13:47,740
database and by pounding the time we

00:13:46,330 --> 00:13:49,990
also bound the number of series we can

00:13:47,740 --> 00:13:51,850
have right it's like within these blocks

00:13:49,990 --> 00:13:53,890
roughly equivalent to the actual amount

00:13:51,850 --> 00:13:56,980
of active times news we have maybe the

00:13:53,890 --> 00:13:59,650
factor of two and if everything worked

00:13:56,980 --> 00:14:01,090
before if you can constrain this amount

00:13:59,650 --> 00:14:03,730
of time series and this probably is

00:14:01,090 --> 00:14:05,230
going to work as well and of course now

00:14:03,730 --> 00:14:08,050
if you have all these small databases if

00:14:05,230 --> 00:14:09,430
you want to query something well yeah we

00:14:08,050 --> 00:14:12,040
still have to fan out right into happy

00:14:09,430 --> 00:14:13,870
block that matters for a curry and then

00:14:12,040 --> 00:14:15,310
get these partial results and merge them

00:14:13,870 --> 00:14:19,180
back together so there's certainly cost

00:14:15,310 --> 00:14:21,100
of that but in return whenever Granick

00:14:19,180 --> 00:14:22,990
hurry you only have to pick those blocks

00:14:21,100 --> 00:14:25,360
that I actually was in the range of your

00:14:22,990 --> 00:14:27,130
crow itself and the a by without

00:14:25,360 --> 00:14:29,230
actually indexing which time when does

00:14:27,130 --> 00:14:30,910
the series covers and you basically get

00:14:29,230 --> 00:14:32,770
the same properties of cutting down all

00:14:30,910 --> 00:14:37,470
the series set of Menna for my result

00:14:32,770 --> 00:14:40,030
set at least sort of in a modest way and

00:14:37,470 --> 00:14:42,130
and of course if you like have a long

00:14:40,030 --> 00:14:43,540
time range and at some point there is a

00:14:42,130 --> 00:14:45,280
lot of blocks right but if all these

00:14:43,540 --> 00:14:47,020
blocks have a time range of two hours

00:14:45,280 --> 00:14:48,790
and if you want to query data for like

00:14:47,020 --> 00:14:50,770
one months it's just like a lot of

00:14:48,790 --> 00:14:54,370
murder and has to happen from all these

00:14:50,770 --> 00:14:55,810
different blocks and therefore as time

00:14:54,370 --> 00:14:58,180
goes on we take a bunch of blocks and

00:14:55,810 --> 00:15:00,550
compact them into larger blocks um and

00:14:58,180 --> 00:15:04,180
this is quite sort of mapping world oh I

00:15:00,550 --> 00:15:05,889
should worry pattern because as we call

00:15:04,180 --> 00:15:08,199
the data the curves tend to cover

00:15:05,889 --> 00:15:10,420
penguins whereas I might worry like 100

00:15:08,199 --> 00:15:12,489
data in the last 30 minutes it's quite

00:15:10,420 --> 00:15:14,049
unlikely that Kirby actually one minutes

00:15:12,489 --> 00:15:17,290
from nine months ago I'm probably gonna

00:15:14,049 --> 00:15:21,989
look at the big picture of I don't last

00:15:17,290 --> 00:15:24,040
and that's generally for example and

00:15:21,989 --> 00:15:27,369
this block place design also like

00:15:24,040 --> 00:15:28,779
something incredibly easy and because if

00:15:27,369 --> 00:15:30,129
you want to read it lead or Taylor we

00:15:28,779 --> 00:15:32,079
just wait after the block sort of has

00:15:30,129 --> 00:15:34,179
travel past the retention boundary and I

00:15:32,079 --> 00:15:35,259
me just like delete it it's like one

00:15:34,179 --> 00:15:37,149
directory um it's basically

00:15:35,259 --> 00:15:38,980
instantaneous and that's quite different

00:15:37,149 --> 00:15:40,600
from promises Ron which had to sort of

00:15:38,980 --> 00:15:42,399
traverse all these individual series

00:15:40,600 --> 00:15:43,989
files look into the chance that were in

00:15:42,399 --> 00:15:46,480
there figure out that this chunk

00:15:43,989 --> 00:15:48,549
actually still was in our attention time

00:15:46,480 --> 00:15:51,459
and if not we bought the entire file to

00:15:48,549 --> 00:15:53,230
disk and do this and if you have serious

00:15:51,459 --> 00:15:55,359
turn right if it is like for hundreds of

00:15:53,230 --> 00:15:58,089
millions of files and it's naturally and

00:15:55,359 --> 00:15:59,290
took quite a tall and also contribute to

00:15:58,089 --> 00:16:04,199
the fact that yes these are sort of

00:15:59,290 --> 00:16:07,059
fright while they're quickly and so I

00:16:04,199 --> 00:16:11,290
would appear see for that if you like a

00:16:07,059 --> 00:16:13,119
were confusing and because it's kind of

00:16:11,290 --> 00:16:16,449
cool I wrote this like twenty page or

00:16:13,119 --> 00:16:18,220
post about it and it's a moment if I can

00:16:16,449 --> 00:16:22,869
use and of course people on I can use a

00:16:18,220 --> 00:16:24,639
lot smaller than anybody yet so yeah oh

00:16:22,869 --> 00:16:26,529
my god so damn that's basically in its

00:16:24,639 --> 00:16:31,600
empty time series must be in a b-plus

00:16:26,529 --> 00:16:34,389
degree okay there must be something

00:16:31,600 --> 00:16:36,069
that's not totally wrong and that's

00:16:34,389 --> 00:16:37,660
because well it has quite a few

00:16:36,069 --> 00:16:40,119
properties of Nellis entry right you

00:16:37,660 --> 00:16:41,949
have these compactions going on and use

00:16:40,119 --> 00:16:44,739
of you used to this get I get the

00:16:41,949 --> 00:16:46,209
curries but it's not like this

00:16:44,739 --> 00:16:49,029
hierarchical pattern that you have never

00:16:46,209 --> 00:16:52,959
samples usually and it's if you look at

00:16:49,029 --> 00:16:54,519
it this is the tree right and if you

00:16:52,959 --> 00:16:55,839
have a bit lastly somehow like the

00:16:54,519 --> 00:16:57,069
leaves will chance right because you

00:16:55,839 --> 00:16:58,779
still on this compression and in the

00:16:57,069 --> 00:17:00,399
leaf you have each chunk I wish some I

00:16:58,779 --> 00:17:01,869
have to get to the shank and you

00:17:00,399 --> 00:17:06,419
probably don't have a tree because you

00:17:01,869 --> 00:17:10,179
want to serve Travelocity um cheaply

00:17:06,419 --> 00:17:11,919
yeah oh sorry and pretty much like

00:17:10,179 --> 00:17:14,049
depending on how you model this whole

00:17:11,919 --> 00:17:15,939
thing as its implemented it is a tree

00:17:14,049 --> 00:17:19,829
structure if you want to consider this

00:17:15,939 --> 00:17:22,620
way right and so yeah it's this whole

00:17:19,829 --> 00:17:25,799
concept is something between LSM and the

00:17:22,620 --> 00:17:28,199
tree but it works and that's sort of the

00:17:25,799 --> 00:17:29,580
only thing amenity and compaction just

00:17:28,199 --> 00:17:36,330
is equivalent to sort of restructuring

00:17:29,580 --> 00:17:38,760
this tree a bit so so how do we actually

00:17:36,330 --> 00:17:42,779
write their disk each of these blocks

00:17:38,760 --> 00:17:44,519
here has a directory and similar to

00:17:42,779 --> 00:17:46,470
Prometheus one and we don't want to

00:17:44,519 --> 00:17:47,580
write like every cent to disk like as it

00:17:46,470 --> 00:17:50,039
comes in because it's just too expensive

00:17:47,580 --> 00:17:51,240
we have to better step up and he follows

00:17:50,039 --> 00:17:53,070
a similar model right like the most

00:17:51,240 --> 00:17:54,330
recent data we keep it in memory or just

00:17:53,070 --> 00:17:57,059
take to keep and have blocks in memory

00:17:54,330 --> 00:17:58,740
and not just chunk of ice per series and

00:17:57,059 --> 00:18:00,240
as these blocks sort of get completed

00:17:58,740 --> 00:18:02,220
and don't receive any more data just

00:18:00,240 --> 00:18:04,200
write enter disk and this happens in a

00:18:02,220 --> 00:18:06,690
custom format so it's there's an index

00:18:04,200 --> 00:18:09,059
file and a bunch of junk files but it's

00:18:06,690 --> 00:18:13,860
not millions of files but just per block

00:18:09,059 --> 00:18:16,080
something between 2 and 25 usually and

00:18:13,860 --> 00:18:18,149
these are then for the persistent data

00:18:16,080 --> 00:18:20,549
they are just a mapped and so we can

00:18:18,149 --> 00:18:22,080
solve accesses data transparently and

00:18:20,549 --> 00:18:23,909
the operating system will take care for

00:18:22,080 --> 00:18:25,409
us of actually caching certain

00:18:23,909 --> 00:18:27,720
frequently accessed pieces of it and

00:18:25,409 --> 00:18:29,730
just evicting other suffer memory and

00:18:27,720 --> 00:18:32,220
this sort of saves us from all the

00:18:29,730 --> 00:18:34,559
burden of managing memory which was why

00:18:32,220 --> 00:18:36,210
it is quite hard to get right and it's

00:18:34,559 --> 00:18:43,080
like a freaking point of traveling from

00:18:36,210 --> 00:18:45,330
easiest one so if you sort of paid

00:18:43,080 --> 00:18:48,750
attention you kind of realize that I

00:18:45,330 --> 00:18:51,720
said that you can pick any rectangle in

00:18:48,750 --> 00:18:53,789
this in this space for your queries and

00:18:51,720 --> 00:18:55,710
this also means that I can stick very a

00:18:53,789 --> 00:18:58,950
single series or a bunch of series for

00:18:55,710 --> 00:19:00,389
the entire time range so while this in

00:18:58,950 --> 00:19:03,269
general I was meant to cut out a lot of

00:19:00,389 --> 00:19:05,159
data for my queries and in the next case

00:19:03,269 --> 00:19:08,480
I still have to conserve it consider

00:19:05,159 --> 00:19:10,710
every single series that exists so

00:19:08,480 --> 00:19:12,659
theoretically speaking like in Big O

00:19:10,710 --> 00:19:14,279
notation this does not really buy us

00:19:12,659 --> 00:19:17,490
anything right everything is just as bad

00:19:14,279 --> 00:19:18,630
as it was which means basically we just

00:19:17,490 --> 00:19:20,669
need a better index that can actually

00:19:18,630 --> 00:19:24,960
handle hundreds of millions or even

00:19:20,669 --> 00:19:26,789
billions of series and so 617 posts and

00:19:24,960 --> 00:19:29,549
both sides of it

00:19:26,789 --> 00:19:31,169
and that's why we also have each of

00:19:29,549 --> 00:19:35,339
these databases have a separate index

00:19:31,169 --> 00:19:37,169
and actually quickly find serious and we

00:19:35,339 --> 00:19:38,909
borrow concepts from search engines

00:19:37,169 --> 00:19:41,820
basically what you can find it in a

00:19:38,909 --> 00:19:44,519
stick or Google and they have an

00:19:41,820 --> 00:19:45,929
inverted index and which basically just

00:19:44,519 --> 00:19:47,699
means if you have a document and you

00:19:45,929 --> 00:19:50,629
want to look up a document by searching

00:19:47,699 --> 00:19:52,799
for words have made a pn documents and

00:19:50,629 --> 00:19:54,149
you can do this efficiently right result

00:19:52,799 --> 00:19:57,509
reversing every single document index

00:19:54,149 --> 00:20:01,739
searching through it and it is like do a

00:19:57,509 --> 00:20:03,839
quick look up and to implement this we

00:20:01,739 --> 00:20:07,679
just give an ID to every serious was in

00:20:03,839 --> 00:20:09,569
a block and let's say it's five and then

00:20:07,679 --> 00:20:11,669
we consider the words of our document

00:20:09,569 --> 00:20:13,229
and the day will pass so the full

00:20:11,669 --> 00:20:17,369
combination of label name and have a

00:20:13,229 --> 00:20:19,079
value and for each of these labor pairs

00:20:17,369 --> 00:20:21,839
we maintained a solid list from labor

00:20:19,079 --> 00:20:24,599
power to all document IDs this labor

00:20:21,839 --> 00:20:27,029
pair the Pearson so now we're going to

00:20:24,599 --> 00:20:29,039
request our documents that have status

00:20:27,029 --> 00:20:30,899
equals 200 so all series that have this

00:20:29,039 --> 00:20:32,639
labor there I can just walk through this

00:20:30,899 --> 00:20:35,909
once and I got the result back right I

00:20:32,639 --> 00:20:37,109
don't have to traverse everything um and

00:20:35,909 --> 00:20:39,119
this approach also allows us to do

00:20:37,109 --> 00:20:40,709
really really efficient k way set

00:20:39,119 --> 00:20:44,609
operations which is usually merging an

00:20:40,709 --> 00:20:46,619
intersection and this usually occurs for

00:20:44,609 --> 00:20:48,599
example when you want to retrieve all

00:20:46,619 --> 00:20:51,089
series that have set us equals 200 and

00:20:48,599 --> 00:20:52,919
method equals get and so then retrieve

00:20:51,089 --> 00:20:55,159
all these series you just put a cursor

00:20:52,919 --> 00:20:57,779
at the beginning of these two lists and

00:20:55,159 --> 00:20:59,819
always advance a cursor that has the

00:20:57,779 --> 00:21:02,159
spot with the smaller value and as soon

00:20:59,819 --> 00:21:05,549
as the value of mesh we add this to our

00:21:02,159 --> 00:21:07,949
result set so after two and two and two

00:21:05,549 --> 00:21:10,859
matches right so this is our results and

00:21:07,949 --> 00:21:12,749
we just move on both of them and now

00:21:10,859 --> 00:21:14,969
three is more than five so if we advance

00:21:12,749 --> 00:21:17,819
this cursor until we have a match again

00:21:14,969 --> 00:21:20,999
and just continue this pattern until one

00:21:17,819 --> 00:21:24,179
of these lists is exhausted then we're

00:21:20,999 --> 00:21:27,059
done and it's of course incredibly you

00:21:24,179 --> 00:21:28,559
much more efficient and then traversing

00:21:27,059 --> 00:21:33,179
the entire set of series to find certain

00:21:28,559 --> 00:21:34,440
things and this also scares beyond just

00:21:33,179 --> 00:21:36,539
like to illustrate you can do the same

00:21:34,440 --> 00:21:38,879
for anything like any from any number of

00:21:36,539 --> 00:21:39,930
lists in fermitas this probably maxes

00:21:38,879 --> 00:21:43,080
out somewhere

00:21:39,930 --> 00:21:44,370
- well five medical so who murdered

00:21:43,080 --> 00:21:46,680
cetera which are important for our X

00:21:44,370 --> 00:21:48,780
measures which will then police tomorrow

00:21:46,680 --> 00:21:58,460
thereby also hopefully be a lot more

00:21:48,780 --> 00:21:58,460
efficient so I was fast benchmarks um

00:21:59,360 --> 00:22:04,650
yeah and so benchmarks are tough right

00:22:02,430 --> 00:22:06,810
and like historically benchmarking for

00:22:04,650 --> 00:22:09,390
us men get a big server and just look

00:22:06,810 --> 00:22:11,820
monitor binder stuff and then check

00:22:09,390 --> 00:22:14,040
numbers um it's kind of cool to get that

00:22:11,820 --> 00:22:16,200
high scores right like we can it's but

00:22:14,040 --> 00:22:18,900
it's easy to sort of is your benchmarks

00:22:16,200 --> 00:22:21,330
in favor of like increasing a certain

00:22:18,900 --> 00:22:22,740
metric so can I decided let's let's

00:22:21,330 --> 00:22:24,660
write something that's reproducible and

00:22:22,740 --> 00:22:26,700
that we can use to continuously develop

00:22:24,660 --> 00:22:28,020
the new storage layer and test whether

00:22:26,700 --> 00:22:30,600
we have iterations of actually improving

00:22:28,020 --> 00:22:33,030
on certain dimensions and we just picked

00:22:30,600 --> 00:22:35,910
issue the the source of our pain sort of

00:22:33,030 --> 00:22:38,460
disco Vanitas and we're starting a

00:22:35,910 --> 00:22:40,380
cluster and a Dipendra dedicated for

00:22:38,460 --> 00:22:42,660
missus notes where we can run one for

00:22:40,380 --> 00:22:44,910
missus over each and can sort of ABCDE

00:22:42,660 --> 00:22:46,320
test them and can also then compare

00:22:44,910 --> 00:22:49,080
different versions in the same

00:22:46,320 --> 00:22:50,730
environment doing the same thing and we

00:22:49,080 --> 00:22:52,920
deploy about eight on micro-service

00:22:50,730 --> 00:22:54,410
instances to it and more for these and

00:22:52,920 --> 00:22:57,390
also we cluster components themselves

00:22:54,410 --> 00:23:00,180
that's a bit like below the order of

00:22:57,390 --> 00:23:01,890
scale I have shown you before um but

00:23:00,180 --> 00:23:05,150
it's just more cost-effective thing

00:23:01,890 --> 00:23:07,680
right and this gets us to 100,000

00:23:05,150 --> 00:23:10,530
120,000 samples per second and over

00:23:07,680 --> 00:23:12,000
3,000 active time series there's not a

00:23:10,530 --> 00:23:14,310
few series that we actually so are

00:23:12,000 --> 00:23:16,140
shooting for but in return we really

00:23:14,310 --> 00:23:18,690
simulate this serious serious John very

00:23:16,140 --> 00:23:20,430
excessively so we spoke about 50% of our

00:23:18,690 --> 00:23:23,340
running applications every 10 minutes

00:23:20,430 --> 00:23:25,650
which means every 10 minutes our number

00:23:23,340 --> 00:23:28,830
of series in our entire database goes up

00:23:25,650 --> 00:23:30,300
at 50% and that's like a really

00:23:28,830 --> 00:23:33,210
straining interesting scenario that's

00:23:30,300 --> 00:23:34,470
probably like 10x and higher than you

00:23:33,210 --> 00:23:37,110
would actually encounter in the real

00:23:34,470 --> 00:23:39,180
world today and but it's a good way to

00:23:37,110 --> 00:23:43,520
test like this model that we developed

00:23:39,180 --> 00:23:43,520
actually stuff scares into the future so

00:23:43,610 --> 00:23:50,190
memory and we have four nines here and

00:23:47,010 --> 00:23:52,500
so we're running promises 1.5 it's a bit

00:23:50,190 --> 00:23:53,299
of no benchmark and from easiest to and

00:23:52,500 --> 00:23:54,860
then

00:23:53,299 --> 00:23:57,259
both versions beyond two instances each

00:23:54,860 --> 00:23:59,149
and and one of each are not to be

00:23:57,259 --> 00:24:01,009
encouraged and the other ones are so

00:23:59,149 --> 00:24:02,929
protruding which is receiving artificial

00:24:01,009 --> 00:24:04,429
acquiring traffic it's not super

00:24:02,929 --> 00:24:07,249
sophisticated but just like a pretty

00:24:04,429 --> 00:24:08,720
moderate to high load of complex careers

00:24:07,249 --> 00:24:11,600
like literally crying up to like

00:24:08,720 --> 00:24:13,399
hundreds of thousands of series so this

00:24:11,600 --> 00:24:16,309
is quite quite good to compare like

00:24:13,399 --> 00:24:17,869
what's what performs both congestion and

00:24:16,309 --> 00:24:20,720
like what's what's the data to actually

00:24:17,869 --> 00:24:23,029
also do crowing against it um and from

00:24:20,720 --> 00:24:25,129
top to bottom we see Prometheus 1.5

00:24:23,029 --> 00:24:27,139
being crude and then being unquiet and

00:24:25,129 --> 00:24:28,749
then the bottom we see Prometheus to be

00:24:27,139 --> 00:24:32,090
encouraged and encouraged respectively

00:24:28,749 --> 00:24:35,690
and you see some sort of notable savings

00:24:32,090 --> 00:24:37,519
I would say right Prometheus uses like

00:24:35,690 --> 00:24:39,889
2.5 gigabytes to actually handle this

00:24:37,519 --> 00:24:41,450
workload and a pretty consistent rate

00:24:39,889 --> 00:24:44,389
right like it goes up in the beginning

00:24:41,450 --> 00:24:46,519
and just stayed at this level and we as

00:24:44,389 --> 00:24:48,889
promises one sort of ramps up its memory

00:24:46,519 --> 00:24:50,749
consumption over time and then sort of

00:24:48,889 --> 00:24:53,779
steps out for a while and then suddenly

00:24:50,749 --> 00:24:55,220
bumps up again and that's when actually

00:24:53,779 --> 00:24:56,389
retention starts and we actually have to

00:24:55,220 --> 00:24:58,369
maintain the series we've written to

00:24:56,389 --> 00:24:59,830
disk you have to delete our data and we

00:24:58,369 --> 00:25:01,549
have two cycles for all these series

00:24:59,830 --> 00:25:03,499
basically all the time there can never

00:25:01,549 --> 00:25:06,259
reach the bottom we go to the top again

00:25:03,499 --> 00:25:08,359
because just take so long and this rams

00:25:06,259 --> 00:25:11,090
up memory usage like once more again

00:25:08,359 --> 00:25:14,299
right so um I think typically memory

00:25:11,090 --> 00:25:16,039
savings for promises to between 2 and 5

00:25:14,299 --> 00:25:18,549
X depending on the court environment

00:25:16,039 --> 00:25:18,549
you're operating in

00:25:26,030 --> 00:25:34,200
cpu same story I'm say motor for mrs.

00:25:30,210 --> 00:25:35,820
1.5 above and promises to below and here

00:25:34,200 --> 00:25:38,010
we can see that CPU usage dropped like

00:25:35,820 --> 00:25:40,080
massively and that's not just a storage

00:25:38,010 --> 00:25:42,990
right there's some optimizations the

00:25:40,080 --> 00:25:44,810
purpose of enforce by just integrating

00:25:42,990 --> 00:25:47,520
the new storage into the scrap Linea um

00:25:44,810 --> 00:25:49,580
but basically people XP on a thousand

00:25:47,520 --> 00:25:52,770
samples a second no around 20,000

00:25:49,580 --> 00:25:57,390
we barely using like have a CPU core and

00:25:52,770 --> 00:25:59,430
so this alternate realized that we are

00:25:57,390 --> 00:26:00,690
not really bound by number of samples

00:25:59,430 --> 00:26:02,550
you can ingest which is symmetric we

00:26:00,690 --> 00:26:04,710
usually used to sort of measure our

00:26:02,550 --> 00:26:06,090
performance in prometheus and it's

00:26:04,710 --> 00:26:08,160
really the total amount of series that

00:26:06,090 --> 00:26:10,200
matters right and we can you can crank

00:26:08,160 --> 00:26:12,780
up to probably one to five minutes and

00:26:10,200 --> 00:26:13,380
as a second and it's just gonna cost CPU

00:26:12,780 --> 00:26:16,410
about nothing else

00:26:13,380 --> 00:26:18,030
and and actually there's a separate is

00:26:16,410 --> 00:26:20,100
the benchmark just measuring like

00:26:18,030 --> 00:26:20,670
throughputs with USB itself is up for me

00:26:20,100 --> 00:26:23,400
she's attached

00:26:20,670 --> 00:26:25,620
and on my macbook this can go up to like

00:26:23,400 --> 00:26:27,260
12 minutes and there's a second and we

00:26:25,620 --> 00:26:30,930
doesn't matter it's it's not the

00:26:27,260 --> 00:26:32,580
relevant metric here but yeah for

00:26:30,930 --> 00:26:33,660
promises 1.5 is you that sort of the

00:26:32,580 --> 00:26:35,910
same pattern right if you have this

00:26:33,660 --> 00:26:37,320
memory it sort of rams up and then pumps

00:26:35,910 --> 00:26:42,750
up quickly again a sweet soft to

00:26:37,320 --> 00:26:46,140
attention this rights um I think not

00:26:42,750 --> 00:26:47,130
many people experience it but yeah if

00:26:46,140 --> 00:26:49,650
you have a heavy load of Prometheus

00:26:47,130 --> 00:26:51,900
server it can fry as the pretty much

00:26:49,650 --> 00:26:53,310
like at least that two or three times

00:26:51,900 --> 00:26:54,960
the speed that usually does right like

00:26:53,310 --> 00:26:57,720
after one year you might have to swap

00:26:54,960 --> 00:26:59,100
out a disk and what's the deck of

00:26:57,720 --> 00:27:01,400
backups in from easiest one that's kind

00:26:59,100 --> 00:27:01,400
of unfortunate

00:27:01,610 --> 00:27:06,720
so yeah yeah like saving like 98 to 99

00:27:05,220 --> 00:27:08,340
percent yeah it was quite surprising

00:27:06,720 --> 00:27:09,780
right this was not a metric we actually

00:27:08,340 --> 00:27:12,120
look at before or even try to optimize

00:27:09,780 --> 00:27:14,940
for and that you can see this pretty

00:27:12,120 --> 00:27:16,590
steady rate in Prometheus two and it's

00:27:14,940 --> 00:27:18,300
about like one megabyte continuously but

00:27:16,590 --> 00:27:19,770
is for the you right ahead lock which we

00:27:18,300 --> 00:27:22,320
used to actually and sure we don't lose

00:27:19,770 --> 00:27:23,670
any data and then you have these small

00:27:22,320 --> 00:27:25,560
spikes which are happening when we

00:27:23,670 --> 00:27:28,770
compact data or if we just like rewrite

00:27:25,560 --> 00:27:30,780
data into larger blocks um it seems

00:27:28,770 --> 00:27:33,710
expensive but as it turns out doesn't

00:27:30,780 --> 00:27:33,710
matter like at all

00:27:34,490 --> 00:27:41,509
on this exercise um yeah that's not fair

00:27:39,019 --> 00:27:43,190
and like both are using the Excel

00:27:41,509 --> 00:27:45,649
compression scheme and so this is

00:27:43,190 --> 00:27:47,509
already the sort of safe space inversion

00:27:45,649 --> 00:27:51,889
or space-saving version of premises one

00:27:47,509 --> 00:27:53,269
um sort of set the compression must be

00:27:51,889 --> 00:27:55,850
the same right using the same algorithm

00:27:53,269 --> 00:27:57,980
so this can't be it and this is probably

00:27:55,850 --> 00:28:01,309
just like due to the overhead we have

00:27:57,980 --> 00:28:04,460
per serious when storing storing gate

00:28:01,309 --> 00:28:06,710
and promises one yes and sort of custom

00:28:04,460 --> 00:28:08,090
disk format in promises tools like

00:28:06,710 --> 00:28:09,950
highly optimized and highly compressed

00:28:08,090 --> 00:28:12,619
and for example all strings are going to

00:28:09,950 --> 00:28:14,769
do duplicated um sort of like if you

00:28:12,619 --> 00:28:17,749
have like one miniseries that's like

00:28:14,769 --> 00:28:19,789
gigabytes or string get up and but

00:28:17,749 --> 00:28:28,190
effectively it's probably like 2,000 to

00:28:19,789 --> 00:28:31,220
4,000 UNIX things so oh yeah yeah yeah

00:28:28,190 --> 00:28:32,570
yeah yeah the above graph santa cruz is

00:28:31,220 --> 00:28:34,999
born in but it wants our produces two

00:28:32,570 --> 00:28:36,320
and we said to what are sort of

00:28:34,999 --> 00:28:38,690
diverging a bit that's probably just

00:28:36,320 --> 00:28:40,159
because of weird alignment of time when

00:28:38,690 --> 00:28:44,059
scraping so we get x select different

00:28:40,159 --> 00:28:45,139
compression ratios but yeah in general

00:28:44,059 --> 00:28:46,249
like for high series certain

00:28:45,139 --> 00:28:48,889
environments we can't Robbie's

00:28:46,249 --> 00:28:50,869
headquarters is impervious to in stable

00:28:48,889 --> 00:28:56,090
environments this is probably going to

00:28:50,869 --> 00:28:58,039
be a lot smaller the delta career agency

00:28:56,090 --> 00:29:01,340
and something that we never measured

00:28:58,039 --> 00:29:03,110
before this is as I said not super

00:29:01,340 --> 00:29:04,129
sophisticated and businesslike sort of

00:29:03,110 --> 00:29:05,600
hitting promises but like really

00:29:04,129 --> 00:29:08,720
expensive queries and next some cheaper

00:29:05,600 --> 00:29:11,539
ones just measuring yeah the 95th

00:29:08,720 --> 00:29:12,950
percentile latency and at what we can

00:29:11,539 --> 00:29:15,019
see quite nicely is that promise is one

00:29:12,950 --> 00:29:17,860
which is above sort of ramps up over

00:29:15,019 --> 00:29:20,600
time as we had more serious and then

00:29:17,860 --> 00:29:22,669
stabilizes and after attention kicks in

00:29:20,600 --> 00:29:24,769
and the amount of series we delete it's

00:29:22,669 --> 00:29:27,139
sort of in equilibrium is the amount of

00:29:24,769 --> 00:29:28,820
series we are adding in you and because

00:29:27,139 --> 00:29:30,649
it's quite busy already latencies are

00:29:28,820 --> 00:29:32,480
quite spiky

00:29:30,649 --> 00:29:33,919
where's policas two states please stay

00:29:32,480 --> 00:29:37,850
relax there are some spikes in there

00:29:33,919 --> 00:29:39,919
which might just be like the way we time

00:29:37,850 --> 00:29:42,139
queries as it's nice not super

00:29:39,919 --> 00:29:44,600
sophisticated but in general like we see

00:29:42,139 --> 00:29:47,390
the same pattern and we go to a certain

00:29:44,600 --> 00:29:49,580
state and level and we stay there

00:29:47,390 --> 00:29:51,040
and that's something that we see in all

00:29:49,580 --> 00:29:54,230
these girls basically right except for

00:29:51,040 --> 00:29:55,760
storage space of course um but I think

00:29:54,230 --> 00:29:57,679
that's the actual benefit of Prometheus

00:29:55,760 --> 00:29:59,150
- I've um

00:29:57,679 --> 00:30:00,350
you don't be care about a resource

00:29:59,150 --> 00:30:02,030
consumption like if you have a fan in

00:30:00,350 --> 00:30:03,410
factor of like ten thousand like ten

00:30:02,030 --> 00:30:05,510
thousand microservices being worthless

00:30:03,410 --> 00:30:07,220
like one promises ever you're probably

00:30:05,510 --> 00:30:10,730
happy relax panic fifty gigabytes on it

00:30:07,220 --> 00:30:12,020
but it must be predictable and if you

00:30:10,730 --> 00:30:15,110
have to go in and it was heavy skated

00:30:12,020 --> 00:30:16,700
you have to like some knobs and then it

00:30:15,110 --> 00:30:18,200
runs out of memory again and that's it

00:30:16,700 --> 00:30:20,690
so if the operation of pain you get and

00:30:18,200 --> 00:30:23,650
so this table behaved yes I think the

00:30:20,690 --> 00:30:25,790
main feature of missus to opening um

00:30:23,650 --> 00:30:27,830
because also all-star which flags are

00:30:25,790 --> 00:30:30,020
removed it's not just one left which is

00:30:27,830 --> 00:30:33,010
data directory and everything else is

00:30:30,020 --> 00:30:35,480
sort of just it is the way it is right

00:30:33,010 --> 00:30:37,220
retention as well yeah and they're like

00:30:35,480 --> 00:30:39,410
two flex for like debug purposes and

00:30:37,220 --> 00:30:40,760
just like to run benchmarks and buttons

00:30:39,410 --> 00:30:42,650
only you don't have to touch any knobs

00:30:40,760 --> 00:30:44,360
anymore for the store to select safe

00:30:42,650 --> 00:30:48,160
configuring a session or configure

00:30:44,360 --> 00:30:48,160
yourself just works away does

00:30:55,640 --> 00:31:04,290
so yeah how does it work and basically

00:31:01,230 --> 00:31:08,130
this development was 10% writing code

00:31:04,290 --> 00:31:12,060
and 90% staring at graphs this is like

00:31:08,130 --> 00:31:13,650
the profiling output of growth so you

00:31:12,060 --> 00:31:17,730
can either take memory profiles and

00:31:13,650 --> 00:31:20,130
locking profiles and location profiles

00:31:17,730 --> 00:31:21,690
and CPU profiles and basically as you

00:31:20,130 --> 00:31:23,250
optimize stuff you just have to like we

00:31:21,690 --> 00:31:25,470
throw those right because it's like we

00:31:23,250 --> 00:31:27,510
don't know stuff like how does the

00:31:25,470 --> 00:31:29,070
sounds hamper the second millions of

00:31:27,510 --> 00:31:31,470
series there's like so much potential to

00:31:29,070 --> 00:31:33,480
like you introduce performance

00:31:31,470 --> 00:31:34,770
bottlenecks and actually like make

00:31:33,480 --> 00:31:36,450
changes that actually cause recreations

00:31:34,770 --> 00:31:38,160
so that's what this reproduce the

00:31:36,450 --> 00:31:40,470
benchmark is also super cool because we

00:31:38,160 --> 00:31:41,850
could just pull these graphs and from

00:31:40,470 --> 00:31:44,900
these benchmarks and compare even

00:31:41,850 --> 00:31:47,430
different versions to each other and

00:31:44,900 --> 00:31:49,290
then you get that quite nice stuff done

00:31:47,430 --> 00:31:51,420
right and because go was like really

00:31:49,290 --> 00:31:52,740
really good at this profiling thing and

00:31:51,420 --> 00:31:54,630
it's also really valuable to be able to

00:31:52,740 --> 00:31:57,420
profile promises by monitoring

00:31:54,630 --> 00:31:59,400
Prometheus Prometheus and because here's

00:31:57,420 --> 00:32:00,990
one example that's fixed quite recently

00:31:59,400 --> 00:32:04,380
and compactions right they are they're

00:32:00,990 --> 00:32:07,320
quite expensive in a way and because you

00:32:04,380 --> 00:32:08,940
have to look at data from like huge

00:32:07,320 --> 00:32:11,820
blocks potentially and write bigger

00:32:08,940 --> 00:32:13,560
blocks out of it and this of course of

00:32:11,820 --> 00:32:15,000
course caused by excess you might expect

00:32:13,560 --> 00:32:17,790
if you don't sort of optimize all the

00:32:15,000 --> 00:32:19,800
way and and I personally don't know how

00:32:17,790 --> 00:32:21,090
I would have even detected those except

00:32:19,800 --> 00:32:22,710
for like the application crashing

00:32:21,090 --> 00:32:24,630
because our Prometheus actually

00:32:22,710 --> 00:32:28,050
monitoring them continuously and so this

00:32:24,630 --> 00:32:29,580
was like super helpful and yeah and then

00:32:28,050 --> 00:32:31,050
you can go and optimize the way this

00:32:29,580 --> 00:32:32,430
stuff and as you can see in the green

00:32:31,050 --> 00:32:36,540
graph which is like the latest beta

00:32:32,430 --> 00:32:42,990
these spices can be gone and this lot of

00:32:36,540 --> 00:32:45,090
work in this direction yeah of course on

00:32:42,990 --> 00:32:47,340
everything is great right like it's it's

00:32:45,090 --> 00:32:49,950
it's fairly new the whole storage system

00:32:47,340 --> 00:32:52,410
and there's a lot of different things to

00:32:49,950 --> 00:32:54,750
consider and so for example this

00:32:52,410 --> 00:32:57,390
benchmark was had a relatively small set

00:32:54,750 --> 00:32:59,070
of active series as well as forgot

00:32:57,390 --> 00:33:00,540
something reasons yeah and that's

00:32:59,070 --> 00:33:06,300
prometheus level it's currently running

00:33:00,540 --> 00:33:07,170
and it's collecting data from like four

00:33:06,300 --> 00:33:10,050
million series

00:33:07,170 --> 00:33:16,430
approximately from our like I think like

00:33:10,050 --> 00:33:16,430
about 2,000 of these fake microservices

00:33:18,560 --> 00:33:22,380
and you see this like small drops yeah

00:33:20,820 --> 00:33:24,300
and actually like how many samples are

00:33:22,380 --> 00:33:26,460
we getting and that's when we are

00:33:24,300 --> 00:33:27,690
starting a new block because we've

00:33:26,460 --> 00:33:31,140
basically reinitialize and in her

00:33:27,690 --> 00:33:32,250
database with over 4 million series and

00:33:31,140 --> 00:33:34,800
that's quite expensive operation we have

00:33:32,250 --> 00:33:36,570
to rebuild all these indices that's

00:33:34,800 --> 00:33:37,680
something like to optimize away and but

00:33:36,570 --> 00:33:39,540
in general it's already a lot more

00:33:37,680 --> 00:33:42,480
stable than promises one rupiah this at

00:33:39,540 --> 00:33:44,130
this stage um and yeah you're personally

00:33:42,480 --> 00:33:47,010
missing like one scrape per target and

00:33:44,130 --> 00:33:48,540
it's like a 10 second square interval if

00:33:47,010 --> 00:33:50,400
you have like 60 seconds is probably bad

00:33:48,540 --> 00:33:52,560
you not as well so that's something to

00:33:50,400 --> 00:33:55,860
improve on but probably nothing that's

00:33:52,560 --> 00:33:57,420
blocking a 2.0 release hopefully mentor

00:33:55,860 --> 00:33:59,880
Texas compare memory consumption for

00:33:57,420 --> 00:34:01,170
like without instance here and there's

00:33:59,880 --> 00:34:03,870
not been curried and you can see I

00:34:01,170 --> 00:34:05,460
cannot see the typical d-pad on here so

00:34:03,870 --> 00:34:07,350
for like 4 million series we use about

00:34:05,460 --> 00:34:13,080
between 10 gigabytes of memory and that

00:34:07,350 --> 00:34:14,370
peak time but like almost 18 and some of

00:34:13,080 --> 00:34:18,120
these spikes can probably be optimized

00:34:14,370 --> 00:34:20,250
the way but in general that's yeah it's

00:34:18,120 --> 00:34:22,200
like editing gigabytes maximum for like

00:34:20,250 --> 00:34:24,419
4 million series and and it's pretty

00:34:22,200 --> 00:34:34,250
stable so that's probably um I think

00:34:24,419 --> 00:34:34,250
it's good enough to panel so what else

00:34:34,520 --> 00:34:40,230
that goes and so you can now do backups

00:34:38,250 --> 00:34:42,300
and snapshots and thanks like two

00:34:40,230 --> 00:34:43,860
seconds and you know have a snapshot of

00:34:42,300 --> 00:34:46,770
your entire data and consume it off

00:34:43,860 --> 00:34:48,480
somewhere else and use it for other

00:34:46,770 --> 00:34:50,910
processing or just like actually restore

00:34:48,480 --> 00:34:52,980
from easiest service if the data got

00:34:50,910 --> 00:34:55,440
corrupted and this set up to in one

00:34:52,980 --> 00:34:57,330
picture and that's gotten and that's

00:34:55,440 --> 00:34:58,920
sort of one of the great side effects of

00:34:57,330 --> 00:35:01,830
writing this whole thing and because we

00:34:58,920 --> 00:35:03,300
gained a new PC developer so got home

00:35:01,830 --> 00:35:05,780
from India he is here today and will

00:35:03,300 --> 00:35:05,780
give a talk later

00:35:07,060 --> 00:35:12,360
[Applause]

00:35:09,950 --> 00:35:14,190
yeah implemented some cool stuff in

00:35:12,360 --> 00:35:15,750
pieces in general but especially 40s to

00:35:14,190 --> 00:35:18,900
be right he implemented these backups

00:35:15,750 --> 00:35:20,220
and he also implemented our deletion

00:35:18,900 --> 00:35:22,470
mechanism which wasn't wrong for a long

00:35:20,220 --> 00:35:23,970
time and which should hopefully at some

00:35:22,470 --> 00:35:25,890
point that I would like to dynamic

00:35:23,970 --> 00:35:27,090
retention policies like keeping some

00:35:25,890 --> 00:35:31,080
metrics for longer than others and

00:35:27,090 --> 00:35:33,720
that's like future work but yeah tcp of

00:35:31,080 --> 00:35:34,800
all gives us a lot of potential to like

00:35:33,720 --> 00:35:37,650
give it up all sort of different stuff

00:35:34,800 --> 00:35:41,730
in the future and can we talk about it a

00:35:37,650 --> 00:35:42,980
bit more later and yeah it's right out

00:35:41,730 --> 00:35:49,080
um

00:35:42,980 --> 00:35:53,960
beta one is crashing on set up so you

00:35:49,080 --> 00:35:53,960
gotta fix this and really spare that too

00:36:01,020 --> 00:36:05,850
[Applause]

00:36:03,590 --> 00:36:07,560
yeah and again if you're interested in

00:36:05,850 --> 00:36:10,350
working on these kind of things we are

00:36:07,560 --> 00:36:13,560
hiring and if you want to learn more

00:36:10,350 --> 00:36:15,330
there's my waited on blog post and the

00:36:13,560 --> 00:36:19,430
repositories also in the premises

00:36:15,330 --> 00:36:19,430
organization thank you

00:36:24,600 --> 00:36:38,470
who is questions okay first of all thank

00:36:37,510 --> 00:36:41,890
you very much for this very interesting

00:36:38,470 --> 00:36:43,540
talk and your great work a bunch of

00:36:41,890 --> 00:36:46,260
questions so maybe you have to interrupt

00:36:43,540 --> 00:36:49,180
me I don't know um if you go back to the

00:36:46,260 --> 00:36:51,070
is this still on yeah if you go back to

00:36:49,180 --> 00:36:54,369
the kind of the chunk format and you use

00:36:51,070 --> 00:36:58,180
stored deltas of deltas and do you at

00:36:54,369 --> 00:37:00,609
any point have like almost like a

00:36:58,180 --> 00:37:03,280
keyframe within the chunk that means you

00:37:00,609 --> 00:37:04,900
can avoid traversing the whole chunk if

00:37:03,280 --> 00:37:07,420
you need Aquarion and know basically

00:37:04,900 --> 00:37:09,130
times are all key frames so chunks I

00:37:07,420 --> 00:37:11,500
especially exactly for the purpose of

00:37:09,130 --> 00:37:15,340
text jumping into a series at certain

00:37:11,500 --> 00:37:17,950
points and then my second question was

00:37:15,340 --> 00:37:19,840
when you did the you have a graph where

00:37:17,950 --> 00:37:24,070
you show what the memory uses basically

00:37:19,840 --> 00:37:27,070
yeah and is that the they go heap size

00:37:24,070 --> 00:37:28,690
or is that the virtual segment size or

00:37:27,070 --> 00:37:30,640
resident section size so this is the

00:37:28,690 --> 00:37:33,130
heap size and if you look at the

00:37:30,640 --> 00:37:34,359
residents memory size and that's pretty

00:37:33,130 --> 00:37:36,130
much the same all right

00:37:34,359 --> 00:37:38,080
just that there you don't see the spikes

00:37:36,130 --> 00:37:39,670
as well because go keeps this memory

00:37:38,080 --> 00:37:41,530
sort of pending before giving back to

00:37:39,670 --> 00:37:44,050
the US or the US might not even take it

00:37:41,530 --> 00:37:45,609
back given it was told to do so so this

00:37:44,050 --> 00:37:47,710
just gives us a better idea of when is

00:37:45,609 --> 00:37:49,869
what happening but the actual memory

00:37:47,710 --> 00:37:52,180
concerns you need is about about the

00:37:49,869 --> 00:37:55,180
same and if you look at the total usage

00:37:52,180 --> 00:37:57,250
this just goes up until you run out of

00:37:55,180 --> 00:37:59,109
memory that's the end of stuff so the

00:37:57,250 --> 00:38:00,880
entire database is MF and as long as

00:37:59,109 --> 00:38:02,500
your free memory us but just like keep

00:38:00,880 --> 00:38:04,750
all this stuff in memory and we're just

00:38:02,500 --> 00:38:07,390
like purge it um as soon as any anything

00:38:04,750 --> 00:38:08,980
else needs more memory so this should

00:38:07,390 --> 00:38:10,810
make promises quite adaptive to actually

00:38:08,980 --> 00:38:12,550
and make as much as it came out of

00:38:10,810 --> 00:38:13,840
memory but just also scare it down again

00:38:12,550 --> 00:38:15,970
it sounds great

00:38:13,840 --> 00:38:17,770
so do you look at what the churn is and

00:38:15,970 --> 00:38:21,580
the page cache is there something that

00:38:17,770 --> 00:38:23,380
you've they use while developing no so

00:38:21,580 --> 00:38:25,510
there was no extensive testing on

00:38:23,380 --> 00:38:28,720
actually putting on a memory pressure

00:38:25,510 --> 00:38:29,920
etc just um well the next step to do as

00:38:28,720 --> 00:38:32,410
soon as we actually get reports on this

00:38:29,920 --> 00:38:34,660
being somewhat of a problem and I mean

00:38:32,410 --> 00:38:37,030
if we kind of seen it right like I push

00:38:34,660 --> 00:38:38,680
some of these media service to the

00:38:37,030 --> 00:38:40,780
boundary of how much memory the note has

00:38:38,680 --> 00:38:44,290
and Kirby supposed to be working so

00:38:40,780 --> 00:38:46,600
great okay you know there's one last

00:38:44,290 --> 00:38:48,190
it's quick I don't know if it's maybe

00:38:46,600 --> 00:38:50,170
too technical it's about to go runtime

00:38:48,190 --> 00:38:52,300
how it handles and map stuffs maybe if

00:38:50,170 --> 00:38:55,750
you it's not specific to this library

00:38:52,300 --> 00:38:58,780
but should I asking out we're not yeah

00:38:55,750 --> 00:39:01,900
okay so basically when you add map a

00:38:58,780 --> 00:39:04,690
file you hit the a map but the file is

00:39:01,900 --> 00:39:06,160
not in the page cache Linux will suspend

00:39:04,690 --> 00:39:10,030
your thread that you're calling from

00:39:06,160 --> 00:39:11,860
puts it into interruptible sleep which

00:39:10,030 --> 00:39:14,200
basically means that you starve the go

00:39:11,860 --> 00:39:15,670
scheduler of a thread yeah I think

00:39:14,200 --> 00:39:21,580
figure out what happens when what it

00:39:15,670 --> 00:39:23,020
does what do that works I put this

00:39:21,580 --> 00:39:24,880
information like quite a bit it's really

00:39:23,020 --> 00:39:26,470
kind of hard to find anything like

00:39:24,880 --> 00:39:30,430
specific about it it's not totally

00:39:26,470 --> 00:39:32,320
outdated um I think it relaxed some

00:39:30,430 --> 00:39:33,790
co-developers mention that it's there

00:39:32,320 --> 00:39:35,920
might be problems but I don't know like

00:39:33,790 --> 00:39:38,380
how accurate is it for all like reason

00:39:35,920 --> 00:39:42,370
this information is and I think it just

00:39:38,380 --> 00:39:43,990
really blocks but we never see like how

00:39:42,370 --> 00:39:45,910
this turns out right if it like 100

00:39:43,990 --> 00:39:48,850
problems you can just always swap the nm

00:39:45,910 --> 00:39:50,320
for the custom memory pooling and but so

00:39:48,850 --> 00:39:54,120
far this seems to work like really well

00:39:50,320 --> 00:39:59,410
so I wouldn't touch it on to the problem

00:39:54,120 --> 00:40:02,770
so two questions first when do you plan

00:39:59,410 --> 00:40:06,340
to release stable version when it's

00:40:02,770 --> 00:40:10,780
ready and the second question is if we

00:40:06,340 --> 00:40:13,120
start using it this better now do you

00:40:10,780 --> 00:40:16,600
anticipate any data loss or sign in

00:40:13,120 --> 00:40:19,000
Alexis at this stage yes so the start is

00:40:16,600 --> 00:40:20,560
pretty stable I think and we had some

00:40:19,000 --> 00:40:23,110
breaking changes in the storage format

00:40:20,560 --> 00:40:25,840
and for the Alpha releases but I think

00:40:23,110 --> 00:40:27,220
by now we are mostly done so no

00:40:25,840 --> 00:40:31,300
guarantees right but it should be pretty

00:40:27,220 --> 00:40:33,190
stable yeah what's mostly blocking to

00:40:31,300 --> 00:40:35,290
pono is just like other stuff you have

00:40:33,190 --> 00:40:36,580
to figure out like which API things

00:40:35,290 --> 00:40:38,500
won't we do we want to break like all

00:40:36,580 --> 00:40:39,700
these discussions and we really want to

00:40:38,500 --> 00:40:41,890
have version documentation on the

00:40:39,700 --> 00:40:43,780
website and before releasing to pono

00:40:41,890 --> 00:40:45,550
just because it's it's kind of this is

00:40:43,780 --> 00:40:46,920
one documentation which is like the

00:40:45,550 --> 00:40:48,150
latest release and

00:40:46,920 --> 00:40:57,750
that's up is not gonna work out with

00:40:48,150 --> 00:41:02,490
major versions thank you hello do you

00:40:57,750 --> 00:41:05,309
build time series labels index for each

00:41:02,490 --> 00:41:07,440
individual time range or it's a global

00:41:05,309 --> 00:41:08,579
index and it's it's local to a block

00:41:07,440 --> 00:41:10,319
like these blocks that levy that

00:41:08,579 --> 00:41:12,839
completely and close I said that data

00:41:10,319 --> 00:41:15,510
basis was known index on trance so you

00:41:12,839 --> 00:41:17,880
need so you have to rebuild it index

00:41:15,510 --> 00:41:19,890
when you're the data is come yes for

00:41:17,880 --> 00:41:20,640
each block yeah and it's pretty

00:41:19,890 --> 00:41:23,460
expensive

00:41:20,640 --> 00:41:26,010
um the indexed I think not so much to be

00:41:23,460 --> 00:41:27,690
honest so we are using this sort of

00:41:26,010 --> 00:41:29,730
search engine like inverted index

00:41:27,690 --> 00:41:31,290
technique right but if you look at that

00:41:29,730 --> 00:41:33,210
we chose we searched around that like

00:41:31,290 --> 00:41:35,040
there's infinite body of like

00:41:33,210 --> 00:41:36,569
optimizations and crazy algorithms and

00:41:35,040 --> 00:41:39,089
we don't really need that we just need

00:41:36,569 --> 00:41:41,609
the algorithmic sort of complexity of it

00:41:39,089 --> 00:41:44,130
because of our inverted indices are not

00:41:41,609 --> 00:41:45,660
that big just you don't have to compress

00:41:44,130 --> 00:41:47,369
them like phrase or something and

00:41:45,660 --> 00:41:50,339
there's not that expensive to rebuild

00:41:47,369 --> 00:41:53,309
and it just doesn't take off time just

00:41:50,339 --> 00:41:55,470
creating your CVS it's pretty fast um

00:41:53,309 --> 00:41:56,760
but if you do it like for me in times as

00:41:55,470 --> 00:42:00,650
fast as you can it's gonna take a few

00:41:56,760 --> 00:42:03,299
seconds how many this range do have and

00:42:00,650 --> 00:42:05,369
so the default is to start up is two

00:42:03,299 --> 00:42:06,900
hours which works well for like the

00:42:05,369 --> 00:42:09,510
typical center square intervals are like

00:42:06,900 --> 00:42:11,880
20 to 60 seconds basically it allows you

00:42:09,510 --> 00:42:15,119
to get like at least one full chunk to

00:42:11,880 --> 00:42:16,589
get the idea compression ratio and then

00:42:15,119 --> 00:42:17,970
we always take three of those and

00:42:16,589 --> 00:42:20,220
compact them into the larger blocks and

00:42:17,970 --> 00:42:22,710
we do this by default up to 10% of the

00:42:20,220 --> 00:42:25,680
retention time so that you most have

00:42:22,710 --> 00:42:28,890
like a 10% overhead of data as it sort

00:42:25,680 --> 00:42:31,190
of crosses a retention boundary and do

00:42:28,890 --> 00:42:35,880
you have any plans to support

00:42:31,190 --> 00:42:37,140
histograms compression and storing yes

00:42:35,880 --> 00:42:39,690
yes if you have anything about that

00:42:37,140 --> 00:42:40,950
quite a bit partially and there's

00:42:39,690 --> 00:42:42,750
definitely some really cool stuff to

00:42:40,950 --> 00:42:44,880
really I think and I just I think it's a

00:42:42,750 --> 00:42:47,250
fair bit outright first to open also

00:42:44,880 --> 00:42:49,380
stabilize but in the future I think this

00:42:47,250 --> 00:42:54,240
could be really cool to sort of go into

00:42:49,380 --> 00:42:56,970
okay thanks hello Lincoln last time as

00:42:54,240 --> 00:42:59,339
you told there is a one file per time

00:42:56,970 --> 00:43:00,150
series right so how this new storage

00:42:59,339 --> 00:43:02,910
model you are

00:43:00,150 --> 00:43:06,120
one file systems and so we have one

00:43:02,910 --> 00:43:08,430
index file um which is usually like if

00:43:06,120 --> 00:43:10,290
you are negative knows usually just a

00:43:08,430 --> 00:43:12,930
few few megabytes and for smaller

00:43:10,290 --> 00:43:15,810
servers and then we have multiple shrunk

00:43:12,930 --> 00:43:17,160
files and we just use 500 megabytes so

00:43:15,810 --> 00:43:18,840
we just write shines into these five

00:43:17,160 --> 00:43:20,070
sequentially and if you hit like five

00:43:18,840 --> 00:43:23,100
hundred megabytes which is not the next

00:43:20,070 --> 00:43:28,890
one and so yeah it's it's a couple of

00:43:23,100 --> 00:43:31,800
files basically I just wanted to ask if

00:43:28,890 --> 00:43:35,750
it starts using Prometheus now would you

00:43:31,800 --> 00:43:37,560
suggest to start right away with 201 or

00:43:35,750 --> 00:43:39,120
depends on the environment I would say

00:43:37,560 --> 00:43:41,010
and so I talked to someone who wants to

00:43:39,120 --> 00:43:43,290
sort of further Plaskett at the company

00:43:41,010 --> 00:43:45,030
but they have like a POC face for like

00:43:43,290 --> 00:43:46,910
one months and yeah I mean just go put

00:43:45,030 --> 00:43:51,650
open oh if you don't have any

00:43:46,910 --> 00:43:51,650
requirements on the crazy stability okay

00:43:55,100 --> 00:44:03,480
yeah hi so my question would be more

00:43:59,690 --> 00:44:08,400
development related so you shown that

00:44:03,480 --> 00:44:11,570
profiling to you were using I was going

00:44:08,400 --> 00:44:16,430
to ask you how difficult was it to use

00:44:11,570 --> 00:44:18,990
or whether you try to be joy differently

00:44:16,430 --> 00:44:20,640
it's it's super easy like it's like

00:44:18,990 --> 00:44:23,040
building go stuff right you don't get

00:44:20,640 --> 00:44:24,960
anything but you compile a binary and

00:44:23,040 --> 00:44:26,640
there are two options right because it's

00:44:24,960 --> 00:44:28,950
like profile process separately or I can

00:44:26,640 --> 00:44:30,960
just use the HTTP endpoint in paresis

00:44:28,950 --> 00:44:33,600
which can just fetch these profiles and

00:44:30,960 --> 00:44:35,010
it's really great I mean you can even

00:44:33,600 --> 00:44:38,280
filter these out right and I could just

00:44:35,010 --> 00:44:40,680
get more details expansions of certain

00:44:38,280 --> 00:44:42,870
subsets of the code and the only thing

00:44:40,680 --> 00:44:44,580
that's really annoying is that the

00:44:42,870 --> 00:44:46,680
allocations for example from the

00:44:44,580 --> 00:44:47,580
beginning of the process so your profile

00:44:46,680 --> 00:44:50,280
you get all their patients from

00:44:47,580 --> 00:44:52,170
beginning of time and so like very

00:44:50,280 --> 00:44:54,690
frequent and smaller locations can sort

00:44:52,170 --> 00:44:55,830
of shadow big spikes which is like in

00:44:54,690 --> 00:44:57,870
this compaction case what's really

00:44:55,830 --> 00:45:00,090
tricky to actually figure out what's

00:44:57,870 --> 00:45:01,740
causing these and and also just tells

00:45:00,090 --> 00:45:03,690
you in which function allocations

00:45:01,740 --> 00:45:05,910
happens but did not tell you which types

00:45:03,690 --> 00:45:07,410
actually graduated I don't know it was

00:45:05,910 --> 00:45:10,600
possible but it's negative kinda hard

00:45:07,410 --> 00:45:15,640
and so I was basically asking because

00:45:10,600 --> 00:45:18,490
maybe seeing what exactly makes a

00:45:15,640 --> 00:45:20,860
difference is some are simply using

00:45:18,490 --> 00:45:21,430
frame graphs and there's a really cool

00:45:20,860 --> 00:45:22,570
tool

00:45:21,430 --> 00:45:25,570
I just looked it up because I'm not a

00:45:22,570 --> 00:45:28,540
little perv then it's that's called go

00:45:25,570 --> 00:45:30,970
torch so maybe not for you but for other

00:45:28,540 --> 00:45:33,400
people developing a go it might be

00:45:30,970 --> 00:45:35,890
really useful to get to the really

00:45:33,400 --> 00:45:36,910
expensive operations yeah I've seen the

00:45:35,890 --> 00:45:38,500
through what I didn't actually try it

00:45:36,910 --> 00:45:40,870
out I probably should have and look

00:45:38,500 --> 00:45:43,660
pretty good so okay thanks

00:45:40,870 --> 00:45:46,330
I think there's some stuff coming in go

00:45:43,660 --> 00:45:47,920
Niners so I won nine which allow you to

00:45:46,330 --> 00:45:52,080
tag allocations or something like that

00:45:47,920 --> 00:45:55,180
to make this stuff easier as well um

00:45:52,080 --> 00:45:57,340
impressive work how do you envision

00:45:55,180 --> 00:46:03,130
updates from commit as one to permit us

00:45:57,340 --> 00:46:05,230
to offline conversion or simply use both

00:46:03,130 --> 00:46:06,820
storage engines for some time yeah so

00:46:05,230 --> 00:46:08,710
that these two options exactly right

00:46:06,820 --> 00:46:10,930
like on option one is we Connery's

00:46:08,710 --> 00:46:12,550
promises 1.8 was a remote we'd enabled

00:46:10,930 --> 00:46:14,830
and you just point to missus to to

00:46:12,550 --> 00:46:16,870
Prometheus one so we all the data that

00:46:14,830 --> 00:46:18,880
sits there as around and after retention

00:46:16,870 --> 00:46:20,350
times of past you just like can kill the

00:46:18,880 --> 00:46:22,210
old one that's probably gonna work quite

00:46:20,350 --> 00:46:24,250
well for like two to four weeks of

00:46:22,210 --> 00:46:26,590
retention time if you have like one or

00:46:24,250 --> 00:46:29,410
three years it's probably a bit tougher

00:46:26,590 --> 00:46:31,930
um like a conversion tool can be built

00:46:29,410 --> 00:46:33,970
and like the news throw it is basically

00:46:31,930 --> 00:46:35,980
just a library so can we leave this use

00:46:33,970 --> 00:46:39,550
it to do any custom tooling with it and

00:46:35,980 --> 00:46:41,350
the old storage is not that well

00:46:39,550 --> 00:46:43,270
suitable for that so it's not real easy

00:46:41,350 --> 00:46:45,220
to convert it to probably just have to

00:46:43,270 --> 00:46:47,410
read all data like Susan on the Curie

00:46:45,220 --> 00:46:49,690
interfaces and then just like write it

00:46:47,410 --> 00:46:52,810
again down somewhere and they're gonna

00:46:49,690 --> 00:46:54,640
be kind of slow but yeah if someone

00:46:52,810 --> 00:46:59,680
wants to build that probably be happy

00:46:54,640 --> 00:47:02,020
accepted I'm not gonna do it no it's a

00:46:59,680 --> 00:47:04,990
bit of work I'm happy to like guide

00:47:02,020 --> 00:47:13,060
anyone and but myself I think the time

00:47:04,990 --> 00:47:17,020
to actually write that hi I was looking

00:47:13,060 --> 00:47:20,420
at the way you are trying to optimize

00:47:17,020 --> 00:47:23,150
the storage and compressive

00:47:20,420 --> 00:47:25,130
thing in that time occurred to me that

00:47:23,150 --> 00:47:28,220
there are some projects for example I've

00:47:25,130 --> 00:47:29,510
read about RAM cloud maybe you've heard

00:47:28,220 --> 00:47:32,810
somebody felt about it

00:47:29,510 --> 00:47:35,450
it's a project from Stanford University

00:47:32,810 --> 00:47:37,760
it works on data centers with InfiniBand

00:47:35,450 --> 00:47:42,110
that you can create a distributed

00:47:37,760 --> 00:47:46,790
storage in RAM and I was wondering

00:47:42,110 --> 00:47:49,280
myself why people do not look up also

00:47:46,790 --> 00:47:51,940
this kind of because you are writing a

00:47:49,280 --> 00:47:55,190
very specific storage adapter and

00:47:51,940 --> 00:47:58,700
basically there is already open source

00:47:55,190 --> 00:48:00,770
Ram distributed storage then you might

00:47:58,700 --> 00:48:04,850
be able to use for that and it's fast

00:48:00,770 --> 00:48:08,750
and it already propagates what you read

00:48:04,850 --> 00:48:10,700
right in the round to disks and it makes

00:48:08,750 --> 00:48:13,460
it permanent so if you would probably

00:48:10,700 --> 00:48:18,700
lose your servers you would lose two

00:48:13,460 --> 00:48:22,280
seconds or less of writings it's just an

00:48:18,700 --> 00:48:24,800
thing that I don't see people interested

00:48:22,280 --> 00:48:26,540
in this kind of sounds interesting

00:48:24,800 --> 00:48:28,790
definitely I mean maybe that's sort of a

00:48:26,540 --> 00:48:31,330
good idea to investigate for like a

00:48:28,790 --> 00:48:34,070
remotes or disability our system

00:48:31,330 --> 00:48:35,540
promises local storage really like has

00:48:34,070 --> 00:48:37,160
to be focused on being like

00:48:35,540 --> 00:48:40,610
self-contained right and local and

00:48:37,160 --> 00:48:42,350
single no deployment and yes certainly

00:48:40,610 --> 00:48:43,340
like a lot of interesting stuff but will

00:48:42,350 --> 00:48:44,870
be anybody need something I

00:48:43,340 --> 00:48:47,360
self-contained it just like satisfies

00:48:44,870 --> 00:48:49,700
the performance requirements we have and

00:48:47,360 --> 00:48:53,060
yeah they're offering is an open source

00:48:49,700 --> 00:48:56,050
to solve this problem in this sort of

00:48:53,060 --> 00:48:59,420
performance requirements is quite sparse

00:48:56,050 --> 00:49:02,030
um thanks to the Prometheus team for all

00:48:59,420 --> 00:49:05,210
their work on this we're really excited

00:49:02,030 --> 00:49:07,640
by using prometheus 2.0 I have two marks

00:49:05,210 --> 00:49:10,010
on the benchmarking I don't know if you

00:49:07,640 --> 00:49:11,000
or maybe somebody else here might have

00:49:10,010 --> 00:49:12,230
time to look at this

00:49:11,000 --> 00:49:15,140
but be interesting to see a comparison

00:49:12,230 --> 00:49:17,600
between different file systems and how

00:49:15,140 --> 00:49:19,490
they perform and also with Prometheus

00:49:17,600 --> 00:49:21,590
being a cross-platform project and I'd

00:49:19,490 --> 00:49:23,630
also be interested if anyone's looked at

00:49:21,590 --> 00:49:25,880
how an mapping works on different

00:49:23,630 --> 00:49:27,710
platforms or Windows or FreeBSD for

00:49:25,880 --> 00:49:30,020
example versus Linux

00:49:27,710 --> 00:49:34,099
yes sir course on Windows um haven't

00:49:30,020 --> 00:49:35,990
tried it no I'm the map exist on Windows

00:49:34,099 --> 00:49:37,670
just like as it's not just one system

00:49:35,990 --> 00:49:39,410
car but like a bundle of them but it

00:49:37,670 --> 00:49:41,720
works I mean that's I just basically

00:49:39,410 --> 00:49:46,849
looked at how other databases are doing

00:49:41,720 --> 00:49:48,410
this program five systems I think it

00:49:46,849 --> 00:49:50,150
doesn't matter too much in our case here

00:49:48,410 --> 00:49:51,650
because before waiting is a lot of files

00:49:50,150 --> 00:49:54,319
and there might be differences in how

00:49:51,650 --> 00:49:55,849
the file systems handle it um but here

00:49:54,319 --> 00:49:58,099
just have a bunch of large fires which I

00:49:55,849 --> 00:50:00,349
like written completely sequentially and

00:49:58,099 --> 00:50:02,270
just once and so I think that of like

00:50:00,349 --> 00:50:04,250
work that file systems can do for you

00:50:02,270 --> 00:50:06,349
and does not necessarily make a

00:50:04,250 --> 00:50:11,359
difference here so but put me in this

00:50:06,349 --> 00:50:13,930
thing to see for sure any other

00:50:11,359 --> 00:50:13,930
questions okay

00:50:14,690 --> 00:50:32,570
well lunch starts in about two minutes

00:50:17,830 --> 00:50:32,760
[Music]

00:50:32,570 --> 00:50:35,899
you

00:50:32,760 --> 00:50:35,899

YouTube URL: https://www.youtube.com/watch?v=b_pEevMAC3I


