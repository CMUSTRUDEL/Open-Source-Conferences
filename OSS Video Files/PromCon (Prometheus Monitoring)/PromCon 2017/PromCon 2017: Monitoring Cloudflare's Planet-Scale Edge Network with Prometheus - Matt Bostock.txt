Title: PromCon 2017: Monitoring Cloudflare's Planet-Scale Edge Network with Prometheus - Matt Bostock
Publication date: 2017-09-01
Playlist: PromCon 2017
Description: 
	* Abstract:

Cloudflare operates a global anycast edge network serving content for 6 million web sites. This talk explains how we monitor our network, how we migrated from Nagios to Prometheus and the architecture we chose to provide maximum reliability for monitoring. We'll also discuss the impact of alert fatigue and how we reduced alert noise by analysing data, making alerts more actionable and alerting on symptoms rather than causes.

This talk will cover:

- The challenges of monitoring a high volume, anycast, edge network across 100+ locations
- The architecture we chose to maximise the reliability of our monitoring
- Why Prometheus excels as the new industry standard for modern monitoring
- Approaches for reducing alert noise and alert fatigue
- Triaging alerts into a ticket system
- Analysing past alert data for continuous improvement
- The pain points we endured
- Effecting change across engineering teams

* Speaker biography:

Matt is a Platform Operations engineer at Cloudflare, where he has spent the last year promoting a monitoring utopia. He was previously tech lead for the GOV.UK Infrastructure team and is a keen contributor to open source software. He also loves bacon, avocado, running, and the Oxford comma.

* Slides:

https://promcon.io/2017-munich/slides/monitoring-cloudflares-planet-scale-edge-network-with-prometheus.pdf

* PromCon website:

https://promcon.io/
Captions: 
	00:00:00,380 --> 00:00:14,490
[Music]

00:00:13,160 --> 00:00:17,430
morning everyone

00:00:14,490 --> 00:00:19,140
hi so my name is Matt Bostock I work at

00:00:17,430 --> 00:00:20,550
CloudFlare and this morning I want

00:00:19,140 --> 00:00:22,740
stalky talk to you about how we're

00:00:20,550 --> 00:00:28,560
monitoring car flares planet-scale edge

00:00:22,740 --> 00:00:29,880
network using Prometheus and so I work

00:00:28,560 --> 00:00:31,769
on the platform operations team at

00:00:29,880 --> 00:00:33,840
CloudFlare I'm going to be talking you

00:00:31,769 --> 00:00:35,340
through basically I'm over a year and a

00:00:33,840 --> 00:00:38,820
half's work of I'm integrating

00:00:35,340 --> 00:00:40,620
Prometheus within CloudFlare to monitor

00:00:38,820 --> 00:00:44,940
our edge network and also our internal

00:00:40,620 --> 00:00:47,550
services so we use Prometheus for

00:00:44,940 --> 00:00:49,710
monitoring so by monitoring I mean

00:00:47,550 --> 00:00:52,010
alerting on critical issues Incident

00:00:49,710 --> 00:00:54,270
Response and also post-mortem analysis

00:00:52,010 --> 00:00:55,770
Prometheus uses metrics for alerting

00:00:54,270 --> 00:00:57,870
which is really useful really flexible

00:00:55,770 --> 00:00:59,340
but we don't actually use Prometheus to

00:00:57,870 --> 00:01:01,739
store our application metrics in the

00:00:59,340 --> 00:01:04,019
long term so we set our data attention

00:01:01,739 --> 00:01:06,150
to 15 days which is just long enough for

00:01:04,019 --> 00:01:09,180
the monitoring use case and I'll explain

00:01:06,150 --> 00:01:13,170
a little bit more why we do that in just

00:01:09,180 --> 00:01:15,090
a moment so what does CloudFlare do so a

00:01:13,170 --> 00:01:16,799
lot of people know is there's a CDN but

00:01:15,090 --> 00:01:19,350
we also do a lot a lot more than that so

00:01:16,799 --> 00:01:20,610
we spot spots on lots of new protocols

00:01:19,350 --> 00:01:23,430
that allow you to speed up your websites

00:01:20,610 --> 00:01:25,170
for T less 1.3 HTTP 2 and we're also on

00:01:23,430 --> 00:01:27,330
the largest managed DNS providers in the

00:01:25,170 --> 00:01:29,490
world so I'm explaining all of this to

00:01:27,330 --> 00:01:31,170
give you an idea of what the scope is of

00:01:29,490 --> 00:01:34,200
what we actually have to monitor using

00:01:31,170 --> 00:01:36,000
using Prometheus so this is what our

00:01:34,200 --> 00:01:39,920
edge network looks like it's an anycast

00:01:36,000 --> 00:01:43,369
Network we have over 115 different

00:01:39,920 --> 00:01:45,899
points of presence around the world and

00:01:43,369 --> 00:01:48,600
we also serve around 6 million websites

00:01:45,899 --> 00:01:51,799
more than 6 million websites so we

00:01:48,600 --> 00:01:54,509
receive a lot of traffic we have a very

00:01:51,799 --> 00:01:56,939
widespread network and so there's a lot

00:01:54,509 --> 00:01:58,409
to monitor and this is what our

00:01:56,939 --> 00:02:00,689
Prometheus deployment looks like so we

00:01:58,409 --> 00:02:02,700
have a 185 from Easter service currently

00:02:00,689 --> 00:02:04,860
production we have 4 top level

00:02:02,700 --> 00:02:07,170
Prometheus servers that are collecting

00:02:04,860 --> 00:02:09,709
federated metrics we just about seven to

00:02:07,170 --> 00:02:12,780
two thousand samples per second maximum

00:02:09,709 --> 00:02:16,200
on any given Prometheus server and

00:02:12,780 --> 00:02:18,480
4.6 million time series is is that the

00:02:16,200 --> 00:02:20,850
most time series we have on any server

00:02:18,480 --> 00:02:23,550
and for that those 4.6 time million time

00:02:20,850 --> 00:02:25,710
series over 15 days that takes about 250

00:02:23,550 --> 00:02:30,170
gigabytes on disk and we're storing

00:02:25,710 --> 00:02:32,910
those metrics on rate and SSD storage so

00:02:30,170 --> 00:02:36,300
a little bit about our architecture so

00:02:32,910 --> 00:02:38,490
as this with so many points of presence

00:02:36,300 --> 00:02:40,800
every day Center is essentially

00:02:38,490 --> 00:02:44,400
configured in the same way we do routing

00:02:40,800 --> 00:02:46,890
via our anycast so the a user visiting a

00:02:44,400 --> 00:02:48,989
website will connect to the location

00:02:46,890 --> 00:02:49,860
closest to them and this is really

00:02:48,989 --> 00:02:51,600
important when you're thinking about how

00:02:49,860 --> 00:02:52,650
we deploy for meteors because a lot of

00:02:51,600 --> 00:02:54,090
these points that presents already

00:02:52,650 --> 00:02:57,239
configured identically which makes our

00:02:54,090 --> 00:02:58,800
deployment a lot simpler so these are

00:02:57,239 --> 00:03:00,480
the services we tend to run in each pop

00:02:58,800 --> 00:03:02,550
or at least some major services - via

00:03:00,480 --> 00:03:03,989
HTTP as I mentioned DNS we have a

00:03:02,550 --> 00:03:06,630
replicated key value store which my

00:03:03,989 --> 00:03:09,060
colleague Lorenz will talk about

00:03:06,630 --> 00:03:11,580
tomorrow and also things like agents for

00:03:09,060 --> 00:03:15,600
attack mitigation for blocking bad

00:03:11,580 --> 00:03:17,190
traffic so also in addition to the edge

00:03:15,600 --> 00:03:19,620
network we also have our core

00:03:17,190 --> 00:03:22,890
datacenters which receive things like

00:03:19,620 --> 00:03:25,980
customer logs customer analytics audit

00:03:22,890 --> 00:03:28,500
d-logs HTP areas from nginx

00:03:25,980 --> 00:03:30,239
application operational metrics and also

00:03:28,500 --> 00:03:32,670
we have a lot of internal api's which we

00:03:30,239 --> 00:03:35,820
need to monitor so there is a lot to

00:03:32,670 --> 00:03:37,860
monitor and also the services that we

00:03:35,820 --> 00:03:42,090
run support those internal services

00:03:37,860 --> 00:03:44,040
include marathon measles Kafka hpid HDFS

00:03:42,090 --> 00:03:45,660
elastic search and so there's a lot of

00:03:44,040 --> 00:03:49,980
exporters that we need to use to collect

00:03:45,660 --> 00:03:51,209
those metrics so those two main parts of

00:03:49,980 --> 00:03:52,890
Prometheus there's the metrics

00:03:51,209 --> 00:03:54,600
exposition format which I believe

00:03:52,890 --> 00:03:57,480
they're trying to make an IETF standard

00:03:54,600 --> 00:03:59,100
and the data model which is kind of part

00:03:57,480 --> 00:04:01,110
of that and also this is the Prometheus

00:03:59,100 --> 00:04:02,820
server itself so even if you're not

00:04:01,110 --> 00:04:04,230
using Prometheus server itself the

00:04:02,820 --> 00:04:06,120
metrics data model provided by

00:04:04,230 --> 00:04:07,799
Prometheus is very sane it's very well

00:04:06,120 --> 00:04:09,959
designed and we found it incredibly

00:04:07,799 --> 00:04:11,459
useful especially the fact that it's

00:04:09,959 --> 00:04:13,019
multi-dimensional so being able to slice

00:04:11,459 --> 00:04:16,470
metrics using labels

00:04:13,019 --> 00:04:18,000
it's very is a great advantage and so

00:04:16,470 --> 00:04:20,370
here for example this tells us the rate

00:04:18,000 --> 00:04:22,979
Health forgiven server as a percentage

00:04:20,370 --> 00:04:25,750
so it's a number of dis sack t'v divided

00:04:22,979 --> 00:04:27,940
by the total number of disks

00:04:25,750 --> 00:04:29,290
here we can figure out how many distinct

00:04:27,940 --> 00:04:31,120
kernel versions we have deployed across

00:04:29,290 --> 00:04:34,330
the whole fleet super useful French

00:04:31,120 --> 00:04:36,520
cheering that you you are using the same

00:04:34,330 --> 00:04:39,550
version everywhere and this is an

00:04:36,520 --> 00:04:41,620
approximation of a metric from IO start

00:04:39,550 --> 00:04:43,270
so this is the equivalent of our weight

00:04:41,620 --> 00:04:45,910
which is the time to sort of disagrees

00:04:43,270 --> 00:04:47,530
on average so just having the the

00:04:45,910 --> 00:04:49,780
metrics and parameters format allows us

00:04:47,530 --> 00:04:51,970
to do a larger scope grain gain a lot of

00:04:49,780 --> 00:04:54,940
good insight into how our systems are

00:04:51,970 --> 00:04:56,770
performing so also using metrics for

00:04:54,940 --> 00:05:00,280
alerting is it has been really flexible

00:04:56,770 --> 00:05:03,700
for us so for example here this query

00:05:00,280 --> 00:05:07,780
wall alerts whenever the number of HTTP

00:05:03,700 --> 00:05:10,900
5 500 errors goes over 0% forgiving

00:05:07,780 --> 00:05:12,340
service for example and I won't go into

00:05:10,900 --> 00:05:15,280
detail about that but that's the kind of

00:05:12,340 --> 00:05:16,690
query that you can do this one's a bit

00:05:15,280 --> 00:05:18,340
more complicated I generally advise that

00:05:16,690 --> 00:05:20,169
you keep your queries nice and simple

00:05:18,340 --> 00:05:24,100
this one I think is worthwhile though so

00:05:20,169 --> 00:05:27,430
this one tells us when the when HDFS

00:05:24,100 --> 00:05:29,740
node is unbalanced so it's using more

00:05:27,430 --> 00:05:31,300
than 10% its disk utilization is more

00:05:29,740 --> 00:05:33,940
than temps are different to the the

00:05:31,300 --> 00:05:35,229
cluster average as a whole so you can do

00:05:33,940 --> 00:05:36,970
really powerful query so that to tell

00:05:35,229 --> 00:05:39,669
you when your HDFS cluster is is

00:05:36,970 --> 00:05:42,520
unbalanced for example so a bit about

00:05:39,669 --> 00:05:44,680
how we deploy deploy prometheus

00:05:42,520 --> 00:05:46,360
so as I said it was it's been a year and

00:05:44,680 --> 00:05:50,260
a half of work we started out very small

00:05:46,360 --> 00:05:51,729
and gradually increased the scope of

00:05:50,260 --> 00:05:52,750
what we're doing as we as we got

00:05:51,729 --> 00:05:56,440
agreement and consensus between

00:05:52,750 --> 00:05:58,300
different teams so before using Maggio's

00:05:56,440 --> 00:06:00,340
there's actually a page on the negev

00:05:58,300 --> 00:06:01,990
documentation on how to optimize it and

00:06:00,340 --> 00:06:03,220
I looked through and we'd actually

00:06:01,990 --> 00:06:03,729
applied all of the optimizations

00:06:03,220 --> 00:06:05,380
possible

00:06:03,729 --> 00:06:07,240
we were doing hundreds of thousand check

00:06:05,380 --> 00:06:10,360
hundreds of thousands of checks through

00:06:07,240 --> 00:06:13,540
Nadia's on one machine it was tuned for

00:06:10,360 --> 00:06:15,220
a high volume of checks it was in one on

00:06:13,540 --> 00:06:19,000
one machine in one central location so

00:06:15,220 --> 00:06:21,729
obviously not not great for from a high

00:06:19,000 --> 00:06:23,350
availability point of view and we were

00:06:21,729 --> 00:06:25,090
also using in slightly unusual way and

00:06:23,350 --> 00:06:26,680
that we're actually I'm sending metrics

00:06:25,090 --> 00:06:29,500
data into Nigel's and then using their

00:06:26,680 --> 00:06:33,970
jaws as analyzing back-end from our own

00:06:29,500 --> 00:06:37,030
custom metrics pipeline so we started

00:06:33,970 --> 00:06:38,620
out using Prometheus we evaluated both

00:06:37,030 --> 00:06:39,340
soon as well which uses open testy beers

00:06:38,620 --> 00:06:41,620
is backing store

00:06:39,340 --> 00:06:43,420
which already have in production and

00:06:41,620 --> 00:06:45,880
they say essentially we settled on

00:06:43,420 --> 00:06:48,310
prometheus and we started off by using

00:06:45,880 --> 00:06:50,380
Prometheus also as an alerting back-end

00:06:48,310 --> 00:06:51,210
for our own custom metrics pipeline but

00:06:50,380 --> 00:06:53,890
that didn't really fit with the

00:06:51,210 --> 00:06:55,170
Prometheus semantics as I'm sure you'll

00:06:53,890 --> 00:06:58,540
see over the next couple of days

00:06:55,170 --> 00:06:59,860
Prometheus uses a poor model and so I

00:06:58,540 --> 00:07:01,660
tried to push metrics and using a

00:06:59,860 --> 00:07:03,340
gateway to cache those metrics doesn't

00:07:01,660 --> 00:07:04,720
really fit very well so we looked at how

00:07:03,340 --> 00:07:06,850
we could deploy Prometheus in a more

00:07:04,720 --> 00:07:07,840
idiomatic way we were at specification

00:07:06,850 --> 00:07:09,910
there was a lot of comments a lot

00:07:07,840 --> 00:07:11,920
discussion on it and so it took some

00:07:09,910 --> 00:07:13,570
time to get consensus convince people

00:07:11,920 --> 00:07:14,950
that actually a pull model was going to

00:07:13,570 --> 00:07:16,960
work really well for us and we could

00:07:14,950 --> 00:07:19,030
still keep our existing matrix pipeline

00:07:16,960 --> 00:07:21,340
for long term collection of metrics but

00:07:19,030 --> 00:07:22,780
use the pull model from Prometheus for

00:07:21,340 --> 00:07:23,770
monitoring because what you really care

00:07:22,780 --> 00:07:25,690
about monitoring is knowing when

00:07:23,770 --> 00:07:26,710
something is broken right now you don't

00:07:25,690 --> 00:07:28,030
you don't want to receive data five

00:07:26,710 --> 00:07:29,110
minutes late and then alert on it you

00:07:28,030 --> 00:07:33,160
want to know what's what the state of

00:07:29,110 --> 00:07:36,450
things is right now so inside each each

00:07:33,160 --> 00:07:40,120
point of presence we have one Prometheus

00:07:36,450 --> 00:07:42,370
instance on every management server that

00:07:40,120 --> 00:07:43,780
we have so in the larger points of

00:07:42,370 --> 00:07:45,070
presence we have multiple management

00:07:43,780 --> 00:07:46,630
service they're all configured

00:07:45,070 --> 00:07:49,450
identically and each of them runs

00:07:46,630 --> 00:07:51,340
prometheus so Prometheus on that

00:07:49,450 --> 00:07:54,310
management server is connecting out to

00:07:51,340 --> 00:07:56,530
all of the servers in the datacenter and

00:07:54,310 --> 00:07:58,570
knows which service to collect to using

00:07:56,530 --> 00:08:01,480
the file discovery mechanism which is a

00:07:58,570 --> 00:08:04,030
JSON file which gets generated on the

00:08:01,480 --> 00:08:06,280
same server as Prometheus and we we

00:08:04,030 --> 00:08:08,140
populate that JSON file using an API

00:08:06,280 --> 00:08:09,820
that we have that knows about all of our

00:08:08,140 --> 00:08:11,470
servers and we update it every every

00:08:09,820 --> 00:08:13,270
couple of minutes we add some jitter to

00:08:11,470 --> 00:08:15,280
avoid overwhelming the API and that's

00:08:13,270 --> 00:08:17,350
how Prometheus knows which service

00:08:15,280 --> 00:08:19,180
should exist so if you pull a server a

00:08:17,350 --> 00:08:21,090
production Prometheus will be updated in

00:08:19,180 --> 00:08:23,590
a couple of minutes so it's very dynamic

00:08:21,090 --> 00:08:24,940
so actually rather than looking like

00:08:23,590 --> 00:08:26,860
this Prometheus ad she has multiple

00:08:24,940 --> 00:08:32,650
scrapes per server because we want

00:08:26,860 --> 00:08:35,380
multiple exporters per server and an

00:08:32,650 --> 00:08:39,700
exporter is essentially a daemon that

00:08:35,380 --> 00:08:41,290
exposes metrics so we actually as I

00:08:39,700 --> 00:08:44,680
mentioned because we have multiple

00:08:41,290 --> 00:08:46,600
management nodes in in most of all

00:08:44,680 --> 00:08:48,760
points of presence we actually have

00:08:46,600 --> 00:08:49,690
multiple Prometheus servers running and

00:08:48,760 --> 00:08:51,220
each of those is configured

00:08:49,690 --> 00:08:52,990
independently there's

00:08:51,220 --> 00:08:54,970
no shared stake between them so shared

00:08:52,990 --> 00:08:56,639
nothing architecture and if one of them

00:08:54,970 --> 00:08:58,810
goes down the other one will continue to

00:08:56,639 --> 00:09:00,639
monitor there's no coordination between

00:08:58,810 --> 00:09:03,399
them so from a high availability point

00:09:00,639 --> 00:09:06,129
of view it's very reliable we also

00:09:03,399 --> 00:09:07,629
further eight metrics so the amount of

00:09:06,129 --> 00:09:10,629
date metrics you can store in Prometheus

00:09:07,629 --> 00:09:12,129
in the PT server itself is bound by the

00:09:10,629 --> 00:09:15,250
size of your machine the amount of

00:09:12,129 --> 00:09:16,720
memory the amount of disk so what we do

00:09:15,250 --> 00:09:18,970
is we federates a small number of

00:09:16,720 --> 00:09:22,089
aggregate metrics so service level

00:09:18,970 --> 00:09:24,459
metrics up to a top level Prometheus or

00:09:22,089 --> 00:09:26,439
multiple top level practices in our core

00:09:24,459 --> 00:09:28,329
datacenter so these are three example

00:09:26,439 --> 00:09:30,310
points to presents as I said we have one

00:09:28,329 --> 00:09:31,389
hundred and sixteen but these are three

00:09:30,310 --> 00:09:34,329
examples so we're actually pulling

00:09:31,389 --> 00:09:35,889
metrics select metrics into our core

00:09:34,329 --> 00:09:38,889
date center so we can then do service

00:09:35,889 --> 00:09:40,269
level monitoring this is what a

00:09:38,889 --> 00:09:42,310
Federation configuration looks like if

00:09:40,269 --> 00:09:45,420
you're familiar with Federation we're

00:09:42,310 --> 00:09:47,230
putting the up metric from all all

00:09:45,420 --> 00:09:49,240
points to presence which is really

00:09:47,230 --> 00:09:51,670
useful for seeing how a particular

00:09:49,240 --> 00:09:53,649
export is doing we also pull Prometheus

00:09:51,670 --> 00:09:56,079
metrics about committees itself from

00:09:53,649 --> 00:10:01,149
every points of presence and then any

00:09:56,079 --> 00:10:04,240
any metric that has d prefix Colo : wild

00:10:01,149 --> 00:10:07,300
card or Colo underscore wild card : wild

00:10:04,240 --> 00:10:08,860
card we will federate up to up to the

00:10:07,300 --> 00:10:12,819
the global level the top level

00:10:08,860 --> 00:10:14,649
Prometheus so as I mentioned we have

00:10:12,819 --> 00:10:18,309
actually multiple service running in

00:10:14,649 --> 00:10:19,899
each pointer presence so the top level

00:10:18,309 --> 00:10:23,709
Prometheus server is federating from

00:10:19,899 --> 00:10:25,149
multiples Prometheus servers in each in

00:10:23,709 --> 00:10:26,949
each location that's actually a bit of a

00:10:25,149 --> 00:10:27,910
problem for us which my colleague Lorenz

00:10:26,949 --> 00:10:33,579
is going to explain in more detail

00:10:27,910 --> 00:10:35,620
tomorrow so we also run the top level

00:10:33,579 --> 00:10:36,610
Prometheus in high availability mode so

00:10:35,620 --> 00:10:38,680
are we

00:10:36,610 --> 00:10:40,870
which is to say that we actually run a

00:10:38,680 --> 00:10:44,230
pairs of Prometheus servers at the top

00:10:40,870 --> 00:10:45,370
level and again this is a shared offing

00:10:44,230 --> 00:10:47,079
architectures there's no coordination

00:10:45,370 --> 00:10:49,300
between those two servers if one of them

00:10:47,079 --> 00:10:53,079
goes down the other server continues to

00:10:49,300 --> 00:10:55,000
to send us alerts so at the moment we

00:10:53,079 --> 00:10:56,589
have those top level Prometheus servers

00:10:55,000 --> 00:10:59,439
in the US we're also going to add them

00:10:56,589 --> 00:11:00,279
in au which gives us redundancy across

00:10:59,439 --> 00:11:03,879
to differ

00:11:00,279 --> 00:11:05,589
countenance so I mentioned before we are

00:11:03,879 --> 00:11:06,970
use case for using Prometheus is

00:11:05,589 --> 00:11:10,389
monitoring which is what it's best

00:11:06,970 --> 00:11:11,740
designed for we store metrics for 15

00:11:10,389 --> 00:11:13,420
days we originally started off at two

00:11:11,740 --> 00:11:15,339
days just keep expectations really low

00:11:13,420 --> 00:11:17,470
we didn't have one I have to worry about

00:11:15,339 --> 00:11:18,970
wiping all our data and starting again

00:11:17,470 --> 00:11:21,459
especially when we have multiple servers

00:11:18,970 --> 00:11:23,050
running so so we set things low and

00:11:21,459 --> 00:11:24,639
we've increased it to fifteen days which

00:11:23,050 --> 00:11:26,740
has been really good for us for incident

00:11:24,639 --> 00:11:28,809
analysis post-mortems and that kind of

00:11:26,740 --> 00:11:30,999
thing we scraped metrics every 60

00:11:28,809 --> 00:11:32,410
seconds which sounds quite infrequent

00:11:30,999 --> 00:11:35,439
bad you given the number of machines we

00:11:32,410 --> 00:11:37,720
have we tend to spot any issues that we

00:11:35,439 --> 00:11:39,819
have due to the the wide sample of data

00:11:37,720 --> 00:11:42,370
that we have on Federation we scrape

00:11:39,819 --> 00:11:44,170
every 30 seconds just because otherwise

00:11:42,370 --> 00:11:46,360
with the the way timing works when your

00:11:44,170 --> 00:11:47,620
federating if we scraped every 60

00:11:46,360 --> 00:11:48,970
seconds we could have wait up to two

00:11:47,620 --> 00:11:52,449
minutes to actually collect those

00:11:48,970 --> 00:11:53,800
metrics and if you want we're thinking

00:11:52,449 --> 00:11:56,319
about increasing the this sampling

00:11:53,800 --> 00:11:58,300
frequency for some certain services to

00:11:56,319 --> 00:12:00,129
catch more brief spikes but for the

00:11:58,300 --> 00:12:01,600
moment it's been fine for us and

00:12:00,129 --> 00:12:03,249
Prometheus doesn't support down sampling

00:12:01,600 --> 00:12:04,870
so there's no Dan sampling here every

00:12:03,249 --> 00:12:06,970
data point that we collect over the 15

00:12:04,870 --> 00:12:09,040
days is is retained and stored for those

00:12:06,970 --> 00:12:10,569
15 days

00:12:09,040 --> 00:12:12,699
so something exports us we use some

00:12:10,569 --> 00:12:14,620
mention exporter is a demon that exposed

00:12:12,699 --> 00:12:16,569
metrics there are three main ones that

00:12:14,620 --> 00:12:18,430
we install across our entire fleet so

00:12:16,569 --> 00:12:20,829
every single cloud fly survivor has

00:12:18,430 --> 00:12:22,420
three these three exporters so we use

00:12:20,829 --> 00:12:24,100
the system level exporter which is

00:12:22,420 --> 00:12:26,110
called the node exporter the black box

00:12:24,100 --> 00:12:28,480
exporter which allows you to connect to

00:12:26,110 --> 00:12:29,860
network endpoints and it can also do

00:12:28,480 --> 00:12:31,959
things like tell you if your SSL

00:12:29,860 --> 00:12:34,360
certificates are going to expire and we

00:12:31,959 --> 00:12:38,129
also use em tail which is a project

00:12:34,360 --> 00:12:41,199
under the Google organization and github

00:12:38,129 --> 00:12:42,370
which allows you to match patterns and

00:12:41,199 --> 00:12:44,230
increment counters based on those

00:12:42,370 --> 00:12:46,540
patterns and logs it's really useful for

00:12:44,230 --> 00:12:47,679
monitoring software that you can't

00:12:46,540 --> 00:12:50,139
necessarily instrument yourself either

00:12:47,679 --> 00:12:52,870
because it's too complicated or because

00:12:50,139 --> 00:12:54,699
you just can't change the source code so

00:12:52,870 --> 00:12:56,620
when you're deploying exporters try and

00:12:54,699 --> 00:12:58,059
stick with one exporter per service

00:12:56,620 --> 00:13:00,429
instance so once more mappings between

00:12:58,059 --> 00:13:03,129
each service instance in each exporter

00:13:00,429 --> 00:13:04,480
and keep this the the concern separate

00:13:03,129 --> 00:13:06,249
so you don't want to try and follow all

00:13:04,480 --> 00:13:09,759
your metrics through one exporter

00:13:06,249 --> 00:13:11,230
because you end up it it kind of breaks

00:13:09,759 --> 00:13:13,310
the this Mantic model of using the OP

00:13:11,230 --> 00:13:15,079
metric but also means that is

00:13:13,310 --> 00:13:16,879
one exporter can become a single point

00:13:15,079 --> 00:13:19,730
of failure for all those metrics that

00:13:16,879 --> 00:13:21,050
they is collecting and also deploy your

00:13:19,730 --> 00:13:22,399
exporters as close to the thing they're

00:13:21,050 --> 00:13:24,050
monitoring as possible so deploy them

00:13:22,399 --> 00:13:25,730
within the same failure domain which

00:13:24,050 --> 00:13:27,579
avoid any complications arising from the

00:13:25,730 --> 00:13:31,639
network from connecting across a one

00:13:27,579 --> 00:13:33,980
that kind of thing and also if you want

00:13:31,639 --> 00:13:36,740
to Postgres for example and your running

00:13:33,980 --> 00:13:38,180
application level queries for against

00:13:36,740 --> 00:13:40,009
your post quest so make sure that you

00:13:38,180 --> 00:13:41,899
run that in a separate exporter then the

00:13:40,009 --> 00:13:43,639
exporter this actually monitoring

00:13:41,899 --> 00:13:46,459
Postgres itself you don't want slow

00:13:43,639 --> 00:13:49,759
application level queries to affect your

00:13:46,459 --> 00:13:51,559
monitoring of their database itself so

00:13:49,759 --> 00:13:53,600
in terms of how our loading works so as

00:13:51,559 --> 00:13:55,399
I mentioned we have multiple Prometheus

00:13:53,600 --> 00:13:57,319
servers I'm sending the same alerts

00:13:55,399 --> 00:13:58,850
essentially so how do we not end up with

00:13:57,319 --> 00:14:01,430
typical notes while the alert manager

00:13:58,850 --> 00:14:02,990
deduplication for us we run one instance

00:14:01,430 --> 00:14:04,939
of the other manage currently we're

00:14:02,990 --> 00:14:07,550
gonna run two very soon for high

00:14:04,939 --> 00:14:09,259
availability I'm at the moment the is

00:14:07,550 --> 00:14:10,670
our single point of failure but it does

00:14:09,259 --> 00:14:13,879
support high availability mode so

00:14:10,670 --> 00:14:15,439
although the points of presence all of

00:14:13,879 --> 00:14:18,589
the data centers around the world they

00:14:15,439 --> 00:14:20,180
send the the alerts into one single

00:14:18,589 --> 00:14:23,050
instance of our manager which duplicates

00:14:20,180 --> 00:14:25,939
them and then sends them on to a human

00:14:23,050 --> 00:14:27,949
so as I mentioned at the moment we have

00:14:25,939 --> 00:14:30,009
our manager in the US but we're also

00:14:27,949 --> 00:14:32,660
going to add it in the in Europe as well

00:14:30,009 --> 00:14:34,970
so when you're writing alert rules test

00:14:32,660 --> 00:14:37,490
your query on past data so see how many

00:14:34,970 --> 00:14:39,740
times your metric Widow fight you're

00:14:37,490 --> 00:14:41,300
like what if I'd in the in the past make

00:14:39,740 --> 00:14:44,209
sure you have an alert reference before

00:14:41,300 --> 00:14:46,279
going into production so that's somebody

00:14:44,209 --> 00:14:49,250
responding to the like an action it and

00:14:46,279 --> 00:14:51,319
they know how to action it make your

00:14:49,250 --> 00:14:52,790
alert name descriptive so don't just

00:14:51,319 --> 00:14:54,860
call it Raider raid that doesn't tell me

00:14:52,790 --> 00:14:56,600
anything about what's actually broken so

00:14:54,860 --> 00:14:58,790
I use an adjective or an adverb

00:14:56,600 --> 00:15:00,319
so maybe call it rate health degraded

00:14:58,790 --> 00:15:01,730
that tells me exactly what what the

00:15:00,319 --> 00:15:05,149
problem is and I can stand start using

00:15:01,730 --> 00:15:06,769
dashboards to investigate make sure you

00:15:05,149 --> 00:15:08,209
have no reference and make sure that the

00:15:06,769 --> 00:15:10,399
alert is actionable if somebody's going

00:15:08,209 --> 00:15:12,230
to get notified by buying alerts they

00:15:10,399 --> 00:15:13,399
need to know that they can actually do

00:15:12,230 --> 00:15:14,569
something about it and they're not it's

00:15:13,399 --> 00:15:16,639
not just gonna sit there and somebody

00:15:14,569 --> 00:15:18,170
starts ignoring it and keep it simple as

00:15:16,639 --> 00:15:19,879
well so showed you really complicated

00:15:18,170 --> 00:15:22,519
query before try and avoid those keep

00:15:19,879 --> 00:15:23,689
them as simple as possible because once

00:15:22,519 --> 00:15:25,610
you've written a query people have to

00:15:23,689 --> 00:15:26,880
read it and understand it especially if

00:15:25,610 --> 00:15:28,470
they're responding to a loan

00:15:26,880 --> 00:15:30,180
understand why something is firing it

00:15:28,470 --> 00:15:32,850
really helps if you keep the the metric

00:15:30,180 --> 00:15:35,100
query simple so here's an example

00:15:32,850 --> 00:15:37,320
alerting rule that we have again for the

00:15:35,100 --> 00:15:40,350
r8 health being degraded so we use a

00:15:37,320 --> 00:15:42,660
notify keyword to say which channel to

00:15:40,350 --> 00:15:46,020
notify us in this case I notify the SRE

00:15:42,660 --> 00:15:47,430
team using Giro and we use the space

00:15:46,020 --> 00:15:49,560
separate list of different channels

00:15:47,430 --> 00:15:51,360
which is working well for us so far it's

00:15:49,560 --> 00:15:55,020
quite flexible but we're thinking of

00:15:51,360 --> 00:15:57,090
maybe splitting that into so different

00:15:55,020 --> 00:15:59,040
teams as one label and using another

00:15:57,090 --> 00:16:01,380
label for the actual communication

00:15:59,040 --> 00:16:04,710
channel whether it be HipChat JIRA or or

00:16:01,380 --> 00:16:06,270
I page UT we have a summary which has a

00:16:04,710 --> 00:16:07,410
value so the value of the metrics that

00:16:06,270 --> 00:16:08,730
you include in the summary so it tells

00:16:07,410 --> 00:16:10,410
us how many discs are faulty on that

00:16:08,730 --> 00:16:11,670
machine and we have a template to

00:16:10,410 --> 00:16:13,140
dashboard link as well so you can take

00:16:11,670 --> 00:16:16,740
that dashboard link and see your

00:16:13,140 --> 00:16:17,700
dashboard for that exact server and the

00:16:16,740 --> 00:16:19,380
other reference as well which is really

00:16:17,700 --> 00:16:21,420
important before parting the alert into

00:16:19,380 --> 00:16:22,650
production so what about monitoring your

00:16:21,420 --> 00:16:25,050
monitoring how do you know when

00:16:22,650 --> 00:16:26,250
Prometheus is broken so one of the

00:16:25,050 --> 00:16:29,670
things we do is you run an escalation

00:16:26,250 --> 00:16:31,320
drill every eight hours so our on-call

00:16:29,670 --> 00:16:33,840
shifts are eight hours long we have

00:16:31,320 --> 00:16:37,050
round-the-clock support so we have

00:16:33,840 --> 00:16:38,490
offices in London Singapore and San

00:16:37,050 --> 00:16:40,410
Francisco and they're exactly 8 hours

00:16:38,490 --> 00:16:45,240
apart so at the start of the shift we

00:16:40,410 --> 00:16:46,760
fire a page duty alert which serves

00:16:45,240 --> 00:16:48,630
multiple purposes it tests our

00:16:46,760 --> 00:16:50,160
integration with page duty makes sure

00:16:48,630 --> 00:16:52,200
nobody's reset the API key and not told

00:16:50,160 --> 00:16:53,340
anyone make sure that the personal

00:16:52,200 --> 00:16:54,480
caller said she's still using the same

00:16:53,340 --> 00:16:56,970
phone number that their phone is

00:16:54,480 --> 00:16:58,260
switched on and this what that's going

00:16:56,970 --> 00:17:00,420
through so if the person doesn't

00:16:58,260 --> 00:17:02,670
acknowledge it it will it will escalate

00:17:00,420 --> 00:17:04,920
through to somebody higher up the chain

00:17:02,670 --> 00:17:08,880
and keep going until some until somebody

00:17:04,920 --> 00:17:12,060
somebody until the manager calls you so

00:17:08,880 --> 00:17:14,370
what what we also use Prometheus to

00:17:12,060 --> 00:17:15,810
monitor itself so every every Prometheus

00:17:14,370 --> 00:17:18,390
over in every Colo monitors all the

00:17:15,810 --> 00:17:20,010
other parameter servers in the same in

00:17:18,390 --> 00:17:21,990
the point in the same pot sorry when I

00:17:20,010 --> 00:17:23,520
say oh no I mean point of presence so

00:17:21,990 --> 00:17:25,560
every Prometheus service monitors all

00:17:23,520 --> 00:17:27,720
the others same at the top level and

00:17:25,560 --> 00:17:30,360
then then the top level service in our

00:17:27,720 --> 00:17:32,220
core date centers also monitor the the

00:17:30,360 --> 00:17:35,460
Prometheus service in each point of

00:17:32,220 --> 00:17:36,420
presence in terms of monitoring a lot

00:17:35,460 --> 00:17:37,950
manager how do you know when a lot

00:17:36,420 --> 00:17:39,470
manager is down well we use Griffin as a

00:17:37,950 --> 00:17:41,360
lighting mechanism to page

00:17:39,470 --> 00:17:42,799
and this is the only case in which we

00:17:41,360 --> 00:17:44,030
use co-founder subletting mechanism

00:17:42,799 --> 00:17:45,169
because most the time you want to use

00:17:44,030 --> 00:17:46,970
the loop manager and consolidate

00:17:45,169 --> 00:17:48,260
consolidate around that but we've found

00:17:46,970 --> 00:17:51,230
that this is really useful so we can get

00:17:48,260 --> 00:17:53,510
refiner to send us a page to page duty

00:17:51,230 --> 00:17:56,659
if our manager stops stopped sending us

00:17:53,510 --> 00:18:01,039
alerts but when it's still receiving

00:17:56,659 --> 00:18:03,770
alerts so this is the query we used to

00:18:01,039 --> 00:18:05,960
see when managers receiving less but now

00:18:03,770 --> 00:18:07,580
he's sending any outs and this is an

00:18:05,960 --> 00:18:08,809
example when it actually happened we hit

00:18:07,580 --> 00:18:10,280
a deadlock in there and all the version

00:18:08,809 --> 00:18:12,950
of a local manager they stopped sending

00:18:10,280 --> 00:18:14,570
his alerts and you can see there the red

00:18:12,950 --> 00:18:16,850
spike is when you stop sending us all

00:18:14,570 --> 00:18:18,140
right so we got page by go fauna to a

00:18:16,850 --> 00:18:20,000
license left so we left on that and we

00:18:18,140 --> 00:18:25,190
also learnt on whenever our manager is

00:18:20,000 --> 00:18:27,799
down using the up metric so in terms of

00:18:25,190 --> 00:18:31,510
a lot routing so I mentioned we use the

00:18:27,799 --> 00:18:34,429
notify keyword we use space separated

00:18:31,510 --> 00:18:36,860
labels so this one would go to HipChat

00:18:34,429 --> 00:18:38,270
to the SRE team and also to pay duty to

00:18:36,860 --> 00:18:39,740
the air sorry team we think about

00:18:38,270 --> 00:18:41,480
splitting those up so we have one

00:18:39,740 --> 00:18:44,539
labeled team and one label for the

00:18:41,480 --> 00:18:46,370
communication channel this is what our

00:18:44,539 --> 00:18:48,500
routing configuration looks like alert

00:18:46,370 --> 00:18:50,570
manager yes it's a horrible regex

00:18:48,500 --> 00:18:52,580
but at now she works pretty well for us

00:18:50,570 --> 00:18:55,480
and this is what our routing tree looks

00:18:52,580 --> 00:18:59,059
like so lots of our teams have added

00:18:55,480 --> 00:19:00,320
HipChat tunnels and they've elected to

00:18:59,059 --> 00:19:01,610
start receiving alerts which be

00:19:00,320 --> 00:19:03,620
fantastic for visibility

00:19:01,610 --> 00:19:05,870
I will reaching tree is probably more

00:19:03,620 --> 00:19:08,150
complicated it needs to be so actually

00:19:05,870 --> 00:19:09,530
work spans are and some of those are due

00:19:08,150 --> 00:19:11,350
to options such as group different

00:19:09,530 --> 00:19:13,580
grouping options for alerts to join

00:19:11,350 --> 00:19:15,740
group similar alerts together

00:19:13,580 --> 00:19:17,960
but yeah the multi visibility we've had

00:19:15,740 --> 00:19:18,710
from that has been fantastic we're using

00:19:17,960 --> 00:19:21,409
general Ertz

00:19:18,710 --> 00:19:24,409
which we adapted from the photo fabian

00:19:21,409 --> 00:19:26,929
x' repository here which allows us to

00:19:24,409 --> 00:19:28,220
send less imminent alerts into JIRA so

00:19:26,929 --> 00:19:30,620
that we can then triage them as part of

00:19:28,220 --> 00:19:34,220
a backlog was part of a sprint and

00:19:30,620 --> 00:19:35,600
address lower priority issues this is

00:19:34,220 --> 00:19:37,580
the kind of thing that it this is a kind

00:19:35,600 --> 00:19:39,890
of juror that it creates it updates the

00:19:37,580 --> 00:19:41,419
description as the alerts get updated it

00:19:39,890 --> 00:19:45,350
will reopen the issue if it fires again

00:19:41,419 --> 00:19:47,030
and it uses the label it uses a hash in

00:19:45,350 --> 00:19:48,740
one of the JIRA labels so we'll reopen

00:19:47,030 --> 00:19:51,080
the same ticket so if you have a flappy

00:19:48,740 --> 00:19:52,220
alert it'll just create week it will

00:19:51,080 --> 00:19:53,659
just keep reopen

00:19:52,220 --> 00:19:56,000
the same ticket rather than creating a

00:19:53,659 --> 00:19:57,740
new ticket every time the alert fires

00:19:56,000 --> 00:20:00,140
and we're going to look at improving

00:19:57,740 --> 00:20:01,490
this so that it reopens a new ticket if

00:20:00,140 --> 00:20:03,380
the amount hasn't fired in the last day

00:20:01,490 --> 00:20:05,179
or so I'm also going to look at adding

00:20:03,380 --> 00:20:06,530
comments so that every time it fires

00:20:05,179 --> 00:20:07,760
actually adds a comment rather than just

00:20:06,530 --> 00:20:09,200
updating the description which can be

00:20:07,760 --> 00:20:10,909
kind of confusing but the nice thing

00:20:09,200 --> 00:20:15,289
about this is you can see the history of

00:20:10,909 --> 00:20:17,090
how the alert was fixed last time we

00:20:15,289 --> 00:20:19,100
also know monitor to es which sends

00:20:17,090 --> 00:20:21,409
alerts into elasticsearch which means we

00:20:19,100 --> 00:20:24,260
can use then use Gravano to analyze the

00:20:21,409 --> 00:20:26,330
data so we can you see which team

00:20:24,260 --> 00:20:28,100
received how many alerts over the last

00:20:26,330 --> 00:20:30,409
seven days for example we can monitor

00:20:28,100 --> 00:20:33,230
trends over time as well that's open

00:20:30,409 --> 00:20:34,610
source it's on github and we also wrote

00:20:33,230 --> 00:20:36,400
a tool called unto you which allows you

00:20:34,610 --> 00:20:39,799
to visualize your alerts as a dashboard

00:20:36,400 --> 00:20:41,990
it looks like this it's had quite a few

00:20:39,799 --> 00:20:43,850
contributors now again it's open source

00:20:41,990 --> 00:20:45,260
so please please feel free to use it and

00:20:43,850 --> 00:20:48,710
it will support multiple about managers

00:20:45,260 --> 00:20:51,140
as well we also contributed arm tool

00:20:48,710 --> 00:20:53,450
which is a like manner manager CLI tool

00:20:51,140 --> 00:20:55,010
for viewing silences and alerts from the

00:20:53,450 --> 00:20:56,750
command line again that's open source

00:20:55,010 --> 00:21:00,260
it's in the alert manager repo under the

00:20:56,750 --> 00:21:01,700
command directory so some of the pain

00:21:00,260 --> 00:21:03,710
points we had some of these will become

00:21:01,700 --> 00:21:06,860
covered tomorrow by Lorenz who's a

00:21:03,710 --> 00:21:08,179
systems engineer at CloudFlare and but

00:21:06,860 --> 00:21:12,580
some of the issues we had with storage

00:21:08,179 --> 00:21:16,159
pressure so tweaking our storage

00:21:12,580 --> 00:21:17,450
parameters to ensure that we we were

00:21:16,159 --> 00:21:20,480
able to cope with the amount of metrics

00:21:17,450 --> 00:21:21,799
that we were ingesting the options for

00:21:20,480 --> 00:21:24,260
this and much improved in more recent

00:21:21,799 --> 00:21:25,700
versions I'm aware that I'm running out

00:21:24,260 --> 00:21:27,320
of time so I'm gonna skip through these

00:21:25,700 --> 00:21:28,490
very briefly but this is this is

00:21:27,320 --> 00:21:31,220
improved and it's gonna get even better

00:21:28,490 --> 00:21:33,530
in Prometheus 2.0 we had some issues

00:21:31,220 --> 00:21:35,720
with a load manager so we found that the

00:21:33,530 --> 00:21:38,929
API was very slow to deal with our

00:21:35,720 --> 00:21:40,220
volume we were putting into it

00:21:38,929 --> 00:21:42,980
especially when we were testing a lot of

00:21:40,220 --> 00:21:45,020
notes in the in the initial phases of

00:21:42,980 --> 00:21:47,750
our deployment we're currently using

00:21:45,020 --> 00:21:48,890
version zero point six point two which

00:21:47,750 --> 00:21:50,840
is working really well for us

00:21:48,890 --> 00:21:52,549
I want to thank Fabian for all of his

00:21:50,840 --> 00:21:53,990
efforts on a lot manager an awesome

00:21:52,549 --> 00:21:55,610
access to it who's been working on the

00:21:53,990 --> 00:21:57,230
UI to address a lot of the issues that

00:21:55,610 --> 00:21:59,419
we're seeing we haven't tested that you

00:21:57,230 --> 00:22:01,730
are yet but we will do soon so thank you

00:21:59,419 --> 00:22:04,809
very much for that so I'm gonna just be

00:22:01,730 --> 00:22:06,850
working really well for us recently we

00:22:04,809 --> 00:22:09,549
the cardinality explosion so we had an

00:22:06,850 --> 00:22:11,559
issue server that you see has four

00:22:09,549 --> 00:22:14,230
million metrics suddenly had 14 million

00:22:11,559 --> 00:22:16,210
I finally start about midnight it woke

00:22:14,230 --> 00:22:17,980
me up I was very happy about that

00:22:16,210 --> 00:22:19,990
but it was quite difficult to find out

00:22:17,980 --> 00:22:21,940
what the problem was so infamous you can

00:22:19,990 --> 00:22:23,710
try and see how many metrics you have

00:22:21,940 --> 00:22:25,990
using the underscore underscore name

00:22:23,710 --> 00:22:27,759
underscore underscore label but in this

00:22:25,990 --> 00:22:30,789
case it wasn't a G telling me which

00:22:27,759 --> 00:22:32,679
metrics were causing problems and

00:22:30,789 --> 00:22:35,139
causing the server to stop ingesting

00:22:32,679 --> 00:22:36,700
metrics so I found the storage tool than

00:22:35,139 --> 00:22:38,379
the github repo and that allowed me to

00:22:36,700 --> 00:22:41,470
actually introspect the the heads

00:22:38,379 --> 00:22:43,990
database which I believe has all of the

00:22:41,470 --> 00:22:45,759
had chunks in there and it allowed me to

00:22:43,990 --> 00:22:47,919
see actually which metric was causing

00:22:45,759 --> 00:22:49,480
the issue we stopped that service that

00:22:47,919 --> 00:22:53,190
was actually it was creating a lot of

00:22:49,480 --> 00:22:55,450
metrics Journal of high cardinality by

00:22:53,190 --> 00:22:56,799
including IP addresses and labels and he

00:22:55,450 --> 00:22:58,749
was doing a top 100 so the amount of

00:22:56,799 --> 00:23:01,269
churn for that was was huge so we

00:22:58,749 --> 00:23:03,309
stopped that service and we deleted the

00:23:01,269 --> 00:23:04,570
metrics using the HTTP API which worked

00:23:03,309 --> 00:23:06,070
great and then Prometheus recovered

00:23:04,570 --> 00:23:09,100
really gracefully so that was a really

00:23:06,070 --> 00:23:10,570
good experience one thing I would say if

00:23:09,100 --> 00:23:13,029
you're deploying Prometheus like we are

00:23:10,570 --> 00:23:14,619
why even in a small deployment I try

00:23:13,029 --> 00:23:17,080
standardize your metrics like your

00:23:14,619 --> 00:23:19,330
metric labels early we did some of this

00:23:17,080 --> 00:23:21,999
but not we've not enough so one of the

00:23:19,330 --> 00:23:23,230
things when you're monitoring a network

00:23:21,999 --> 00:23:25,259
endpoint for example there's a source

00:23:23,230 --> 00:23:27,940
and also a target so try and figure out

00:23:25,259 --> 00:23:29,919
how are you differentiate between where

00:23:27,940 --> 00:23:31,809
the probe is originated from originated

00:23:29,919 --> 00:23:34,480
from and where it's actually what it's

00:23:31,809 --> 00:23:35,679
actually probing think about identifying

00:23:34,480 --> 00:23:38,440
environments to production versus

00:23:35,679 --> 00:23:40,299
staging versus development identifying

00:23:38,440 --> 00:23:42,279
different clusters so if you have

00:23:40,299 --> 00:23:44,200
different Postgres installations for

00:23:42,279 --> 00:23:46,149
example how to differentiate between

00:23:44,200 --> 00:23:48,039
those or if you just deploy the same app

00:23:46,149 --> 00:23:51,129
but with a different configuration yeah

00:23:48,039 --> 00:23:54,249
could also be a potential a different

00:23:51,129 --> 00:23:56,679
distinct label to use as well so next

00:23:54,249 --> 00:23:58,210
steps we really pleased about Prometheus

00:23:56,679 --> 00:24:01,090
2.0 it's going to help lower this guy oh

00:23:58,210 --> 00:24:03,009
we I think we're turning quite through

00:24:01,090 --> 00:24:07,090
SSDs quite quickly on our top-level

00:24:03,009 --> 00:24:09,129
primitive servers and so 2.0 will will

00:24:07,090 --> 00:24:09,940
help a lot with that and also handles

00:24:09,129 --> 00:24:12,039
metrics churn

00:24:09,940 --> 00:24:14,649
much much better as well so that's going

00:24:12,039 --> 00:24:15,879
to be really useful for us we're also

00:24:14,649 --> 00:24:18,010
looking at integration with long term

00:24:15,879 --> 00:24:21,520
storage so shipping metrics using

00:24:18,010 --> 00:24:23,050
right endpoint from prometheus into our

00:24:21,520 --> 00:24:25,030
longtime storage which are the moment we

00:24:23,050 --> 00:24:26,890
using authenticity be for that I'm also

00:24:25,030 --> 00:24:28,900
looking at using one query language we

00:24:26,890 --> 00:24:30,520
liked on QL a lot so we're thinking

00:24:28,900 --> 00:24:33,010
about using it for querying open testy

00:24:30,520 --> 00:24:35,290
be for example to actually have a proxy

00:24:33,010 --> 00:24:36,520
that I wrote just in my spare time that

00:24:35,290 --> 00:24:39,610
does that so if your interest in that

00:24:36,520 --> 00:24:41,140
speak to me afterwards other

00:24:39,610 --> 00:24:42,760
improvements so I mentioned that we

00:24:41,140 --> 00:24:45,010
federated multiple sets of metrics from

00:24:42,760 --> 00:24:46,180
every point of presence we want to avoid

00:24:45,010 --> 00:24:47,320
doing that because it creates a lot of

00:24:46,180 --> 00:24:49,390
complications that the rents will

00:24:47,320 --> 00:24:50,680
explain tomorrow we want to make a note

00:24:49,390 --> 00:24:51,820
manager highly available that is the

00:24:50,680 --> 00:24:53,650
weakest point in our deployment

00:24:51,820 --> 00:24:54,880
currently things like visual similarity

00:24:53,650 --> 00:24:56,980
search and Oh interesting Colgate did

00:24:54,880 --> 00:24:58,840
some work on that so you can see similar

00:24:56,980 --> 00:25:00,130
metrics so when you have a certain party

00:24:58,840 --> 00:25:02,050
and part in with the new metrics you can

00:25:00,130 --> 00:25:05,020
see all the metrics that did something

00:25:02,050 --> 00:25:06,760
similar and improving make it easier for

00:25:05,020 --> 00:25:09,250
developers to onboard with free tiers

00:25:06,760 --> 00:25:12,190
giving them templates it alerts or menus

00:25:09,250 --> 00:25:13,630
that they can choose from we're going to

00:25:12,190 --> 00:25:16,630
be blogging more about how we use

00:25:13,630 --> 00:25:18,040
Prometheus on the cloud for a blog I've

00:25:16,630 --> 00:25:19,570
gone through things very quickly here

00:25:18,040 --> 00:25:21,540
and we're going to go into more detail

00:25:19,570 --> 00:25:23,680
on the blog please check out our github

00:25:21,540 --> 00:25:26,350
repos because we have a lot of open

00:25:23,680 --> 00:25:28,360
source tooling relating to Prometheus

00:25:26,350 --> 00:25:30,580
including exporters as well and please

00:25:28,360 --> 00:25:32,020
do try out for easy 2.0 we haven't yet

00:25:30,580 --> 00:25:33,850
but we're really excited about it

00:25:32,020 --> 00:25:35,470
and we plan to start testing it soon and

00:25:33,850 --> 00:25:37,810
if you have any questions I'm at Bostock

00:25:35,470 --> 00:25:39,580
on twitter please feel free to ask me

00:25:37,810 --> 00:25:41,190
any questions about our deployment thank

00:25:39,580 --> 00:25:41,390
you very much

00:25:41,190 --> 00:25:47,930
[Applause]

00:25:41,390 --> 00:25:50,880
[Laughter]

00:25:47,930 --> 00:25:54,300
thank you yes we've got elements for

00:25:50,880 --> 00:25:58,370
questions so just raise your hand we've

00:25:54,300 --> 00:25:58,370
got two people of mix in the middle and

00:25:59,600 --> 00:26:03,750
Halleck are at the site so as in your

00:26:02,220 --> 00:26:05,310
deployment you are monitoring and number

00:26:03,750 --> 00:26:07,350
of notes is quite big number

00:26:05,310 --> 00:26:09,270
you know how you how you're automating

00:26:07,350 --> 00:26:11,430
all these configurations like any new

00:26:09,270 --> 00:26:14,160
server will come up or anything will be

00:26:11,430 --> 00:26:16,890
down then how how the configuration will

00:26:14,160 --> 00:26:18,360
be updated across the environment so so

00:26:16,890 --> 00:26:22,230
really tool you are using or any

00:26:18,360 --> 00:26:23,760
strategy you are using to update the

00:26:22,230 --> 00:26:26,550
complete configurations of all the

00:26:23,760 --> 00:26:29,160
Prometheus and everything so in terms of

00:26:26,550 --> 00:26:31,320
which machines we have in each date

00:26:29,160 --> 00:26:33,000
center we use we're going to go daemon

00:26:31,320 --> 00:26:35,730
the queries and api that we have and

00:26:33,000 --> 00:26:36,990
updates a JSON file so every time the

00:26:35,730 --> 00:26:38,730
server is removed or added it will

00:26:36,990 --> 00:26:40,920
update the JSON file so that's using the

00:26:38,730 --> 00:26:41,790
file service discovery mechanism in

00:26:40,920 --> 00:26:43,260
terms of how we deploy that

00:26:41,790 --> 00:26:45,180
configuration we use our configuration

00:26:43,260 --> 00:26:47,520
management which is salts and we're

00:26:45,180 --> 00:26:50,040
currently we have all of the alerting

00:26:47,520 --> 00:26:51,870
rules all the recording rules structured

00:26:50,040 --> 00:26:59,240
by directory and we deploy those to each

00:26:51,870 --> 00:26:59,240
to each server yeah

00:27:02,700 --> 00:27:07,890
yep so I have a question we are going

00:27:18,720 --> 00:27:22,919
like an repeat the question afterwards

00:27:26,309 --> 00:27:31,809
right so we are undergoing similar

00:27:29,530 --> 00:27:34,120
journey from Magus for two committees

00:27:31,809 --> 00:27:37,570
and now my question is we have a couple

00:27:34,120 --> 00:27:39,820
of really custom even handlers in in

00:27:37,570 --> 00:27:42,700
Nagas which are triggered when when

00:27:39,820 --> 00:27:44,650
specific allies are are fired so that

00:27:42,700 --> 00:27:45,880
they are fully automated and I don't

00:27:44,650 --> 00:27:48,490
have to wake up in the middle of the

00:27:45,880 --> 00:27:49,960
night to do types manually the question

00:27:48,490 --> 00:27:52,360
is can I achieve something like that

00:27:49,960 --> 00:27:54,340
without that manager yeah so for that

00:27:52,360 --> 00:27:55,600
you could use the web poke formal oh man

00:27:54,340 --> 00:27:58,059
I just saw something we're looking at we

00:27:55,600 --> 00:27:59,620
haven't done yet but you just tell the

00:27:58,059 --> 00:28:02,590
manager to send notifications to that

00:27:59,620 --> 00:28:05,460
HTM point and then you can alright don't

00:28:02,590 --> 00:28:08,230
do what you want with that okay thanks

00:28:05,460 --> 00:28:10,150
can you go into a little bit more detail

00:28:08,230 --> 00:28:12,190
into that scenario with graph Anna

00:28:10,150 --> 00:28:13,780
firing dollars when our manager is dead

00:28:12,190 --> 00:28:16,330
because that's pretty interesting I

00:28:13,780 --> 00:28:18,940
think for me yeah sure so I'm good final

00:28:16,330 --> 00:28:22,780
allows you to define an alert based on a

00:28:18,940 --> 00:28:25,210
certain threshold it's specific to each

00:28:22,780 --> 00:28:26,320
dashboard I believe so if you delete

00:28:25,210 --> 00:28:27,159
that dashboard I think you lose the

00:28:26,320 --> 00:28:30,429
alert so you have to be really careful

00:28:27,159 --> 00:28:32,679
with that but you basically you create a

00:28:30,429 --> 00:28:34,299
graph panel and then there's a tab

00:28:32,679 --> 00:28:36,130
within that and you can define your look

00:28:34,299 --> 00:28:37,330
within there and then you can send it to

00:28:36,130 --> 00:28:40,059
different integrations so pages you see

00:28:37,330 --> 00:28:41,890
is one of them okay and so how it works

00:28:40,059 --> 00:28:43,630
is like what is the interaction between

00:28:41,890 --> 00:28:47,880
Prometheus and graph and I in this case

00:28:43,630 --> 00:28:51,159
so Gravano is querying Prometheus yeah

00:28:47,880 --> 00:28:54,130
Prometheus is ingesting the and it's

00:28:51,159 --> 00:28:56,559
going to or so nutcase Gravano is

00:28:54,130 --> 00:28:59,049
monitoring a lot manager so just a load

00:28:56,559 --> 00:29:00,610
manager if Prometheus is down then we

00:28:59,049 --> 00:29:06,179
rely on that using one of the other

00:29:00,610 --> 00:29:06,179
Prometheus servers of course okay thanks

00:29:06,220 --> 00:29:11,860
so in terms of your per pop Prometheus

00:29:09,789 --> 00:29:13,419
instances do you have

00:29:11,860 --> 00:29:14,890
any grifone or instances pointing at

00:29:13,419 --> 00:29:16,720
them like if you want to look at per pop

00:29:14,890 --> 00:29:17,830
metric so you like what does that look

00:29:16,720 --> 00:29:19,450
like or do you just go and look at

00:29:17,830 --> 00:29:21,669
Prometheus itself interested like

00:29:19,450 --> 00:29:23,289
dynamic queries in the query browser

00:29:21,669 --> 00:29:25,840
that's a really good question so our

00:29:23,289 --> 00:29:27,490
Suri's have access to the Prometheus

00:29:25,840 --> 00:29:31,090
service directly so they can use the

00:29:27,490 --> 00:29:32,890
HTTP API in every pop but we actually

00:29:31,090 --> 00:29:34,330
have 208 sources in gravano

00:29:32,890 --> 00:29:35,919
we actually hit the limit of 100 and

00:29:34,330 --> 00:29:38,289
they've increased it to a thousand and

00:29:35,919 --> 00:29:41,529
we're looking at ways of reducing that

00:29:38,289 --> 00:29:43,090
either by using some sort of proxy but

00:29:41,529 --> 00:29:44,830
every every single Prometheus over you

00:29:43,090 --> 00:29:46,600
can access directly from graph owner so

00:29:44,830 --> 00:29:48,700
you only have one graph on or instance

00:29:46,600 --> 00:29:50,139
you haven't shot a graph owner and had

00:29:48,700 --> 00:29:51,639
to put a redirect you in front of it or

00:29:50,139 --> 00:29:53,710
whatever to say oh I'm going to this pop

00:29:51,639 --> 00:29:54,010
so I should use profile of five or

00:29:53,710 --> 00:29:59,610
whatever

00:29:54,010 --> 00:30:04,389
no we just use one instance yeah okay

00:29:59,610 --> 00:30:06,760
high over here so first of all I can

00:30:04,389 --> 00:30:09,730
tell you that when you're you have a

00:30:06,760 --> 00:30:11,710
high availability high available alert

00:30:09,730 --> 00:30:14,500
manager you will not have to use graph

00:30:11,710 --> 00:30:16,630
honor for alerting because you will

00:30:14,500 --> 00:30:18,730
always have one alert manager that will

00:30:16,630 --> 00:30:22,419
alert that other one is down we should

00:30:18,730 --> 00:30:24,190
we might yeah I haven't thought the

00:30:22,419 --> 00:30:25,659
graph owner I'm checking there anyway

00:30:24,190 --> 00:30:27,519
another thing we looked at doing is

00:30:25,659 --> 00:30:29,740
using a soft dead man's handle so having

00:30:27,519 --> 00:30:32,470
a service that exposes metrics that then

00:30:29,740 --> 00:30:34,029
get ingested to Prometheus and check it

00:30:32,470 --> 00:30:35,019
alone and then have that same service

00:30:34,029 --> 00:30:39,700
check if the alert comes all the way

00:30:35,019 --> 00:30:40,899
through so I full loop but yeah for now

00:30:39,700 --> 00:30:43,029
the good fauna check is working really

00:30:40,899 --> 00:30:46,720
well yeah I wanted to ask you if you can

00:30:43,029 --> 00:30:50,110
explain more why do you think it's

00:30:46,720 --> 00:30:52,809
enough to have 15 days retention policy

00:30:50,110 --> 00:30:55,090
because my colleagues are convicts

00:30:52,809 --> 00:30:55,370
convincing me that we need at least one

00:30:55,090 --> 00:30:58,780
year

00:30:55,370 --> 00:31:02,300
[Laughter]

00:30:58,780 --> 00:31:05,240
so I'm Prometheus using its internal

00:31:02,300 --> 00:31:08,060
storage I release stole is bounded by

00:31:05,240 --> 00:31:11,150
the size of one machine so that if that

00:31:08,060 --> 00:31:12,410
machine loses is data then that can be

00:31:11,150 --> 00:31:13,640
problematic and if you're storing a

00:31:12,410 --> 00:31:15,740
year's worth of data then there's

00:31:13,640 --> 00:31:18,080
obviously a concern at the moment we're

00:31:15,740 --> 00:31:19,490
just using parameters for monitoring we

00:31:18,080 --> 00:31:21,500
do want to ship the metrics directly

00:31:19,490 --> 00:31:23,810
into open CTB and we are doing that

00:31:21,500 --> 00:31:27,350
currently but using an assumed that we

00:31:23,810 --> 00:31:29,960
would like to improve so my answer to

00:31:27,350 --> 00:31:31,610
that is we're using really Prometheus

00:31:29,960 --> 00:31:33,890
for monitoring and not long-term storage

00:31:31,610 --> 00:31:36,440
and I with class 1 year is or more as as

00:31:33,890 --> 00:31:38,660
long-term storage but we've definitely

00:31:36,440 --> 00:31:41,000
had people request more than 15 days for

00:31:38,660 --> 00:31:42,620
30 days 3 months that kind of thing I

00:31:41,000 --> 00:31:44,600
think we will increase that probably to

00:31:42,620 --> 00:31:46,640
a month but at the moment we're still

00:31:44,600 --> 00:31:48,830
expanding we're still adding new

00:31:46,640 --> 00:31:51,290
services to previous we just recently

00:31:48,830 --> 00:31:53,690
added nginx metrics we just recently

00:31:51,290 --> 00:31:57,110
added metrics form our DNS server so as

00:31:53,690 --> 00:31:58,250
teams implement Prometheus endpoints our

00:31:57,110 --> 00:31:59,960
number of metrics is going to increase

00:31:58,250 --> 00:32:02,420
and we're still in the early days of our

00:31:59,960 --> 00:32:03,650
deployment really so so we want to kind

00:32:02,420 --> 00:32:05,360
of keep that under control keep

00:32:03,650 --> 00:32:07,520
expectations low at the moment so people

00:32:05,360 --> 00:32:08,720
don't expect that they can have months

00:32:07,520 --> 00:32:10,840
worth of data and then we find out we

00:32:08,720 --> 00:32:13,850
run out of space or memory for example

00:32:10,840 --> 00:32:16,790
so you in the data point we have about

00:32:13,850 --> 00:32:19,310
40 km plus per second and we have them

00:32:16,790 --> 00:32:21,380
for like nine months now and it works

00:32:19,310 --> 00:32:22,760
but you have all those considerations

00:32:21,380 --> 00:32:27,050
and we are waiting for a long-term

00:32:22,760 --> 00:32:28,790
storage next questions over here let's

00:32:27,050 --> 00:32:31,220
have a question then an answer to the

00:32:28,790 --> 00:32:34,040
dead man switch so there is actually an

00:32:31,220 --> 00:32:36,530
integration and for pager duty for

00:32:34,040 --> 00:32:39,530
exactly this so then you can make sure

00:32:36,530 --> 00:32:41,330
that all your metrics pipeline or your

00:32:39,530 --> 00:32:43,010
alerting pipeline from Prometheus to

00:32:41,330 --> 00:32:44,660
alert manager to pay pager Duty is

00:32:43,010 --> 00:32:46,460
working correctly because you can

00:32:44,660 --> 00:32:51,890
basically have one alert that always

00:32:46,460 --> 00:32:56,030
fires and if that alert doesn't arrive

00:32:51,890 --> 00:32:57,740
at pager duty then pager Duty can hate

00:32:56,030 --> 00:33:00,230
you so there's like a third party

00:32:57,740 --> 00:33:06,530
integration for that so testing Josh

00:33:00,230 --> 00:33:08,360
ingestion as well so well you can what

00:33:06,530 --> 00:33:09,560
what do you mean by that so I'm the

00:33:08,360 --> 00:33:10,820
thing I was thinking of is a demon as

00:33:09,560 --> 00:33:13,070
she exposes metrics

00:33:10,820 --> 00:33:15,559
and then maybe changes the metric

00:33:13,070 --> 00:33:17,630
periodically and then you could actually

00:33:15,559 --> 00:33:18,679
test the ingestion as well I think I

00:33:17,630 --> 00:33:20,389
think the iteration you're talking about

00:33:18,679 --> 00:33:22,370
I think you'd use the synthetic alert

00:33:20,389 --> 00:33:23,809
for that or I guess I guess you could do

00:33:22,370 --> 00:33:26,840
something similar yeah so it's like it's

00:33:23,809 --> 00:33:29,539
one alert that always fires and that way

00:33:26,840 --> 00:33:31,820
you know that prometheus can reach alert

00:33:29,539 --> 00:33:34,580
manager and alert manager successfully

00:33:31,820 --> 00:33:40,210
reaches pager duty yeah okay cool

00:33:34,580 --> 00:33:43,429
I'll have a look at that thank you hello

00:33:40,210 --> 00:33:48,200
how do you calculate aggregation metrics

00:33:43,429 --> 00:33:50,000
in federated environment so at the

00:33:48,200 --> 00:33:52,159
moment it's quite complicated because we

00:33:50,000 --> 00:33:54,559
federic multiple sets of metrics from

00:33:52,159 --> 00:33:56,480
every data center I definitely recommend

00:33:54,559 --> 00:33:57,710
that you don't do that my colleague

00:33:56,480 --> 00:34:00,230
winds will talk more about that tomorrow

00:33:57,710 --> 00:34:01,730
and with some example queries as well so

00:34:00,230 --> 00:34:04,059
that's probably best answered them okay

00:34:01,730 --> 00:34:04,059
thank you

00:34:07,080 --> 00:34:19,920
[Music]

00:34:19,750 --> 00:34:23,059
you

00:34:19,920 --> 00:34:23,059

YouTube URL: https://www.youtube.com/watch?v=ypWwvz5t_LE


