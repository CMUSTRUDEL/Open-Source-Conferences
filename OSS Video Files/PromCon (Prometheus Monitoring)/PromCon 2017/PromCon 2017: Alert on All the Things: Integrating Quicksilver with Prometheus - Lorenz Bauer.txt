Title: PromCon 2017: Alert on All the Things: Integrating Quicksilver with Prometheus - Lorenz Bauer
Publication date: 2017-09-01
Playlist: PromCon 2017
Description: 
	* Abstract:

Cloudflare provides its services from 115 data centres in 57 countries. One of the most critical systems is a key-value store that replicates configuration data to every single machine, which we recently rewrote from scratch. As developers we were early adopters of Prometheus at Cloudflare, and this talk will explain how we set up Grafana dashboards for monitoring and Alertmanager for alerting, giving us unprecedented insight. Itâ€™ll also cover the gotchas we encountered. Like that one time when we triggered 7000 alerts at once.

* Speaker biography:

Lorenz Bauer works at Cloudflare, which serves a lot of today's HTTP and DNS requests. There, he's been replacing a legacy replicated database system for the last two years. His favourite number is 42.

* Slides:

https://promcon.io/2017-munich/slides/alert-on-all-the-things-integrating-quicksilver-with-prometheus.pdf

* PromCon website:

https://promcon.io/
Captions: 
	00:00:00,380 --> 00:00:17,490
[Music]

00:00:14,089 --> 00:00:20,039
so as was foreshadowed yesterday several

00:00:17,490 --> 00:00:22,050
times by mash we have Lorenz talking

00:00:20,039 --> 00:00:32,489
more about cloud fare and how they're

00:00:22,050 --> 00:00:35,010
doing your tanks exactly hello great ok

00:00:32,489 --> 00:00:37,170
so the pressures on I'm Lawrence

00:00:35,010 --> 00:00:39,870
I work as a systems engineer for

00:00:37,170 --> 00:00:42,059
CloudFlare I'm a colleague of Matt's I'm

00:00:39,870 --> 00:00:44,250
going to be talking about Quicksilver

00:00:42,059 --> 00:00:47,700
which is an internal project we have and

00:00:44,250 --> 00:00:51,480
how we integrated that with the premium

00:00:47,700 --> 00:00:54,300
Prometheus set up that Matt talked about

00:00:51,480 --> 00:00:55,710
yesterday I'm going to briefly talk

00:00:54,300 --> 00:00:58,620
about what Quicksilver isn't how it

00:00:55,710 --> 00:01:00,300
works and talk about the two most

00:00:58,620 --> 00:01:03,090
interesting metrics I think and how they

00:01:00,300 --> 00:01:04,680
use Prometheus histograms to give us a

00:01:03,090 --> 00:01:07,950
lot of insight into how the system is

00:01:04,680 --> 00:01:10,350
working and then I'm going to talk about

00:01:07,950 --> 00:01:15,590
all of the gotchas that we encountered

00:01:10,350 --> 00:01:23,640
while doing this work without further

00:01:15,590 --> 00:01:24,450
that worked one second sorry without

00:01:23,640 --> 00:01:27,600
further ado

00:01:24,450 --> 00:01:29,579
you've seen this I'll just point you to

00:01:27,600 --> 00:01:31,500
the blog which usually has interesting

00:01:29,579 --> 00:01:35,549
technical stuff so I advise you to check

00:01:31,500 --> 00:01:38,130
it out now what is Quicksilver it's an

00:01:35,549 --> 00:01:39,990
internal system that we use to provision

00:01:38,130 --> 00:01:43,439
configuration information from our that

00:01:39,990 --> 00:01:45,840
our customers create to our edge so this

00:01:43,439 --> 00:01:50,340
is the 110a data centers that you've

00:01:45,840 --> 00:01:53,250
seen yesterday it is what I would call a

00:01:50,340 --> 00:01:56,100
replicator database and I'll go into a

00:01:53,250 --> 00:01:58,439
bit of detail why I call replicated

00:01:56,100 --> 00:02:01,590
versus something else

00:01:58,439 --> 00:02:04,619
it runs on a few thousand machines it's

00:02:01,590 --> 00:02:06,030
written and go I was yet saying that and

00:02:04,619 --> 00:02:07,890
it's a team of three working on it my

00:02:06,030 --> 00:02:11,910
colleagues Jeffrey and Sami also known

00:02:07,890 --> 00:02:13,380
on an office so what is quickserver

00:02:11,910 --> 00:02:16,020
obviously I need a walk

00:02:13,380 --> 00:02:18,630
because it's such a great system or

00:02:16,020 --> 00:02:20,460
large I should say at any point in time

00:02:18,630 --> 00:02:24,620
there's a single what we call root node

00:02:20,460 --> 00:02:27,060
in Quicksilver and this root node

00:02:24,620 --> 00:02:29,910
serializes and kind of orchestrates all

00:02:27,060 --> 00:02:31,620
rights to the system and it takes the

00:02:29,910 --> 00:02:34,470
data that is written to the system puts

00:02:31,620 --> 00:02:37,470
it in a in a log entry put the timestamp

00:02:34,470 --> 00:02:39,330
on it and then there forwards that data

00:02:37,470 --> 00:02:41,460
on to other nodes which we call the

00:02:39,330 --> 00:02:45,000
followers which are spread all over the

00:02:41,460 --> 00:02:46,980
globe and how that is configured isn't

00:02:45,000 --> 00:02:49,290
always under our control and most of the

00:02:46,980 --> 00:02:52,230
time mister you will kind of work on

00:02:49,290 --> 00:02:54,600
that and we can also have a second tier

00:02:52,230 --> 00:02:56,520
of nodes which do not get that data from

00:02:54,600 --> 00:02:57,600
the root node but they get data from a

00:02:56,520 --> 00:03:00,330
different node so there's an

00:02:57,600 --> 00:03:03,690
intermediate step the whole thing is is

00:03:00,330 --> 00:03:05,910
basically a tree or a directed acyclic

00:03:03,690 --> 00:03:07,830
graph if you into that means we don't

00:03:05,910 --> 00:03:10,890
have any loops and there's a single

00:03:07,830 --> 00:03:17,520
source of truth and then there's loads

00:03:10,890 --> 00:03:20,280
more of these now why is this replicated

00:03:17,520 --> 00:03:21,690
and not distributed I call a replicator

00:03:20,280 --> 00:03:25,610
because there's actually no guarantee

00:03:21,690 --> 00:03:29,880
about when data becomes visible at any

00:03:25,610 --> 00:03:32,010
data center that we have this is because

00:03:29,880 --> 00:03:34,440
we don't we have a lot of data centers

00:03:32,010 --> 00:03:36,870
some of them are in parts of the world

00:03:34,440 --> 00:03:39,450
that have at times very bad internet

00:03:36,870 --> 00:03:42,030
connectivity we do not want a single

00:03:39,450 --> 00:03:45,480
data center to impact the service to

00:03:42,030 --> 00:03:48,510
other data centers obviously so this in

00:03:45,480 --> 00:03:51,840
effect means that any conflict change

00:03:48,510 --> 00:03:53,370
that a customer makes has an unbounded

00:03:51,840 --> 00:03:56,880
time that it can take to get to any

00:03:53,370 --> 00:03:59,040
particular color what we have to do is

00:03:56,880 --> 00:04:00,930
because we don't want our customers to

00:03:59,040 --> 00:04:02,850
basically tweet at us and say oh

00:04:00,930 --> 00:04:03,270
actually two hours ago I made that

00:04:02,850 --> 00:04:05,280
change

00:04:03,270 --> 00:04:07,530
nothing's happened what's going on you

00:04:05,280 --> 00:04:10,440
need to be able to monitor this and the

00:04:07,530 --> 00:04:12,330
way we're doing it is by using a metric

00:04:10,440 --> 00:04:14,910
that we call the replication lag so

00:04:12,330 --> 00:04:19,140
let's say we have our root node in the

00:04:14,910 --> 00:04:21,359
US we have some another node in southern

00:04:19,140 --> 00:04:23,790
Europe we have one in South Africa so

00:04:21,359 --> 00:04:26,460
what we do is that we measure the time

00:04:23,790 --> 00:04:27,030
it takes for a what we call the log

00:04:26,460 --> 00:04:28,710
entry

00:04:27,030 --> 00:04:30,870
which is the this packet of data that

00:04:28,710 --> 00:04:33,960
the root node creates for log entry to

00:04:30,870 --> 00:04:37,040
go from the root node to the kind of the

00:04:33,960 --> 00:04:40,710
end no the the data center so to speak

00:04:37,040 --> 00:04:42,090
we do this by comparing local time so

00:04:40,710 --> 00:04:45,389
there's issues with clocks view but

00:04:42,090 --> 00:04:48,030
usually it works out fine so let's say

00:04:45,389 --> 00:04:51,480
from the US to southern Europe it takes

00:04:48,030 --> 00:04:54,330
150 milliseconds to Southern Africa it

00:04:51,480 --> 00:04:56,850
takes 600 milliseconds and this gives a

00:04:54,330 --> 00:04:58,889
very good idea of what the current

00:04:56,850 --> 00:05:01,830
quality of services we provide to the

00:04:58,889 --> 00:05:03,570
users of our system because we take

00:05:01,830 --> 00:05:07,830
these measurements and we put them into

00:05:03,570 --> 00:05:10,230
a Prometheus histogram every single of

00:05:07,830 --> 00:05:12,900
every single machine that runs this

00:05:10,230 --> 00:05:16,470
service exports the Prometheus histogram

00:05:12,900 --> 00:05:18,720
and using the Prometheus setup that my

00:05:16,470 --> 00:05:20,729
colleague Matt described yesterday we

00:05:18,720 --> 00:05:22,590
aggregate and Fed rate them at the Colo

00:05:20,729 --> 00:05:25,260
level and then put them into a global

00:05:22,590 --> 00:05:31,100
Prometheus that we used to query and

00:05:25,260 --> 00:05:34,380
visualize global with this data globally

00:05:31,100 --> 00:05:35,640
this is what it looks like I'm very much

00:05:34,380 --> 00:05:38,250
looking forward to Griffin are

00:05:35,640 --> 00:05:41,310
supporting histograms in its dashboard

00:05:38,250 --> 00:05:43,500
that's gonna be super cool to see this

00:05:41,310 --> 00:05:44,570
is the histogram quantile function with

00:05:43,500 --> 00:05:47,400
a p99

00:05:44,570 --> 00:05:50,580
applied to the raw data that I just

00:05:47,400 --> 00:05:51,960
talked about so you'll see we can kind

00:05:50,580 --> 00:05:54,539
of visually see that most of the time

00:05:51,960 --> 00:05:57,140
you know we're actually below second at

00:05:54,539 --> 00:05:59,880
10 seconds of global propagation

00:05:57,140 --> 00:06:01,860
something weird happened seems like the

00:05:59,880 --> 00:06:03,240
global propagation right up so maybe

00:06:01,860 --> 00:06:05,220
that's something we need to look into

00:06:03,240 --> 00:06:12,240
and that weird red spike on the left is

00:06:05,220 --> 00:06:14,760
something I'll get to later now in

00:06:12,240 --> 00:06:16,680
reality we like information in this

00:06:14,760 --> 00:06:19,530
system because the only thing I can tell

00:06:16,680 --> 00:06:21,810
you by looking at my metrics is actually

00:06:19,530 --> 00:06:23,940
South Africa is is too slow 640

00:06:21,810 --> 00:06:28,460
milliseconds you know arbitrarily we

00:06:23,940 --> 00:06:31,260
just decided that is not fast enough I

00:06:28,460 --> 00:06:34,200
don't actually know what the route is

00:06:31,260 --> 00:06:35,130
that the data takes I don't have any

00:06:34,200 --> 00:06:36,840
control over it

00:06:35,130 --> 00:06:38,700
I can't attach that metadata to

00:06:36,840 --> 00:06:41,200
Prometheus because we get a cardinality

00:06:38,700 --> 00:06:42,730
explosion integrating that into

00:06:41,200 --> 00:06:44,170
protocol will probably be too difficult

00:06:42,730 --> 00:06:47,650
because we kind of have to shuttle

00:06:44,170 --> 00:06:51,100
metadata around but it's a real problem

00:06:47,650 --> 00:06:53,230
because I can see some data center of

00:06:51,100 --> 00:06:56,830
Oz's is lagging behind but I don't

00:06:53,230 --> 00:06:59,590
actually know where in that chain in the

00:06:56,830 --> 00:07:01,630
tree I kind of need to tweak the

00:06:59,590 --> 00:07:04,960
settings to make sure that speed

00:07:01,630 --> 00:07:06,550
increases so the second thing or the

00:07:04,960 --> 00:07:09,100
second metric that we came up with is

00:07:06,550 --> 00:07:11,680
again a histogram and it works very

00:07:09,100 --> 00:07:14,740
similar to the replication like that

00:07:11,680 --> 00:07:16,660
I've been talking about but instead of

00:07:14,740 --> 00:07:18,760
measuring the time it takes between the

00:07:16,660 --> 00:07:24,750
root node and kind of the individual

00:07:18,760 --> 00:07:27,400
nodes we just time what it takes for our

00:07:24,750 --> 00:07:29,590
we call them log entries to traverse a

00:07:27,400 --> 00:07:32,500
single network hop we put that in a

00:07:29,590 --> 00:07:35,430
histogram so for the first case the the

00:07:32,500 --> 00:07:37,660
timing stays the same from the US to

00:07:35,430 --> 00:07:39,790
southern Europe it's still 450

00:07:37,660 --> 00:07:42,670
milliseconds because it is just a single

00:07:39,790 --> 00:07:45,720
hop and so replication lag and network

00:07:42,670 --> 00:07:50,740
lag is the same but if we then look at

00:07:45,720 --> 00:07:52,690
going from Europe to Africa we can see

00:07:50,740 --> 00:07:56,230
actually this is 150 milliseconds so

00:07:52,690 --> 00:07:57,820
that's not that bad this link is

00:07:56,230 --> 00:07:59,740
probably acceptable for the distance it

00:07:57,820 --> 00:08:02,200
travels but really what we want to be

00:07:59,740 --> 00:08:05,140
looking at is the 450 milliseconds that

00:08:02,200 --> 00:08:07,600
we're spending and getting to Europe and

00:08:05,140 --> 00:08:10,000
when you put that on a world map it

00:08:07,600 --> 00:08:12,670
looks like this so these are all of our

00:08:10,000 --> 00:08:16,030
pops that we have or data centers all

00:08:12,670 --> 00:08:18,250
over the world using the histogram

00:08:16,030 --> 00:08:20,470
quantile aggregated by the Colo or the

00:08:18,250 --> 00:08:22,210
data center then you can see that you

00:08:20,470 --> 00:08:25,380
know the big red circles right in the

00:08:22,210 --> 00:08:27,490
screenshot there in Moscow and mobile

00:08:25,380 --> 00:08:28,750
these experience what we call the

00:08:27,490 --> 00:08:30,430
network lag so we could go in and say

00:08:28,750 --> 00:08:32,320
okay what's going on with the network do

00:08:30,430 --> 00:08:36,099
you need to change something is there

00:08:32,320 --> 00:08:39,880
congestion etc etc and this gives us as

00:08:36,099 --> 00:08:42,360
developers a lot of evidence that we can

00:08:39,880 --> 00:08:46,060
give to our ops people and say actually

00:08:42,360 --> 00:08:48,930
look at this you can there's something

00:08:46,060 --> 00:08:48,930
wrong can you have a look

00:08:49,390 --> 00:08:56,260
so combine these two our I think what

00:08:54,010 --> 00:09:00,100
really made as embrace Prometheus I

00:08:56,260 --> 00:09:01,480
don't think I don't know if any other

00:09:00,100 --> 00:09:03,820
solution I would have allowed us to kind

00:09:01,480 --> 00:09:07,899
of get this amount of insight into the

00:09:03,820 --> 00:09:11,740
system that we're running so this is

00:09:07,899 --> 00:09:13,600
very very useful to us now

00:09:11,740 --> 00:09:16,209
having spoken about that I'd like to go

00:09:13,600 --> 00:09:18,089
into all of the issues that we encounter

00:09:16,209 --> 00:09:22,680
when when we are setting the system up

00:09:18,089 --> 00:09:25,089
so number one is the alert cascade and

00:09:22,680 --> 00:09:27,910
just from the title you can tell it this

00:09:25,089 --> 00:09:29,880
is not good news it turns out when you

00:09:27,910 --> 00:09:32,740
set up a system that looks like a tree

00:09:29,880 --> 00:09:35,260
if you have and you set up alerts on

00:09:32,740 --> 00:09:37,870
there it's gonna mean that if there's a

00:09:35,260 --> 00:09:40,209
problem very high up in the tree let's

00:09:37,870 --> 00:09:43,120
say hypothetically the root node goes

00:09:40,209 --> 00:09:45,730
down or something like that you end up

00:09:43,120 --> 00:09:49,630
generating a lot of alerts everywhere in

00:09:45,730 --> 00:09:52,029
that tree and this is the one time when

00:09:49,630 --> 00:09:58,029
we think we're generating 7,000 alerts

00:09:52,029 --> 00:09:59,500
at a particular time the Annoying bit is

00:09:58,029 --> 00:10:04,000
that these alerts were tied to pager

00:09:59,500 --> 00:10:06,399
duty Matt informed me yesterday that

00:10:04,000 --> 00:10:07,990
luckily they were deduplicated by a lot

00:10:06,399 --> 00:10:09,880
of managers that thank you very much for

00:10:07,990 --> 00:10:15,279
that but they still broke the iPhones of

00:10:09,880 --> 00:10:16,660
our s eries which crashed so what do we

00:10:15,279 --> 00:10:19,269
do about this

00:10:16,660 --> 00:10:20,560
the solution for this is inhibition so

00:10:19,269 --> 00:10:23,740
we set up rules are kind of say if

00:10:20,560 --> 00:10:26,019
something is going on very close to the

00:10:23,740 --> 00:10:27,550
root of the tree don't bother alerting

00:10:26,019 --> 00:10:30,279
don't send me the other loads I just

00:10:27,550 --> 00:10:32,260
want to know about the single thing ops

00:10:30,279 --> 00:10:38,529
personnel we can concentrate on fixing

00:10:32,260 --> 00:10:41,910
them and it works it works with caveats

00:10:38,529 --> 00:10:47,170
so the way to get there was a bit rough

00:10:41,910 --> 00:10:50,110
so inhibition oh my this alert that went

00:10:47,170 --> 00:10:52,870
off looks something like this and this

00:10:50,110 --> 00:10:55,329
isn't a lot manager inhibition at all

00:10:52,870 --> 00:10:58,209
I'm skipping some details but it's not

00:10:55,329 --> 00:11:02,529
relevant so let's just say the alert is

00:10:58,209 --> 00:11:03,279
called no heartbeat received and I point

00:11:02,529 --> 00:11:05,800
out that this is

00:11:03,279 --> 00:11:08,829
according to Matt's standards the naming

00:11:05,800 --> 00:11:11,050
of the alert very descriptive so know

00:11:08,829 --> 00:11:13,449
how it be received means that it's kind

00:11:11,050 --> 00:11:15,579
of an end-to-end test that we have every

00:11:13,449 --> 00:11:17,290
few seconds we write a specific value to

00:11:15,579 --> 00:11:19,389
a database and then on the edge we check

00:11:17,290 --> 00:11:22,089
what's the value what's the timestamp

00:11:19,389 --> 00:11:24,819
how much out of date is it and this is a

00:11:22,089 --> 00:11:27,430
useful alert because it catches a wide

00:11:24,819 --> 00:11:30,480
range of issues from miss configuration

00:11:27,430 --> 00:11:34,509
to you know some servers going down etc

00:11:30,480 --> 00:11:36,279
but it's also very unhelpful because if

00:11:34,509 --> 00:11:38,740
it fires you just know something is

00:11:36,279 --> 00:11:40,899
wrong and we don't know what is wrong we

00:11:38,740 --> 00:11:43,720
have a lot of other lights are actually

00:11:40,899 --> 00:11:47,110
much more suited to tell our ops people

00:11:43,720 --> 00:11:49,209
or tell ourselves what to do right so we

00:11:47,110 --> 00:11:50,920
have plenty of alerts which are quite

00:11:49,209 --> 00:11:52,809
specific and tell us what the problem

00:11:50,920 --> 00:11:55,029
isn't how to remedy it we have this one

00:11:52,809 --> 00:11:57,160
alert which is not very specific but it

00:11:55,029 --> 00:11:58,870
still keeps popping up so what we want

00:11:57,160 --> 00:12:01,269
to do in this an inhibition rule is we

00:11:58,870 --> 00:12:07,540
say suppress this heartbeat received a

00:12:01,269 --> 00:12:09,819
lot if any other alert is running now

00:12:07,540 --> 00:12:11,230
what happens when you run this through a

00:12:09,819 --> 00:12:17,920
lot manager and I found this out the

00:12:11,230 --> 00:12:19,990
hard way is this inhibits itself I'd

00:12:17,920 --> 00:12:21,910
like to point out that this is is the

00:12:19,990 --> 00:12:24,899
issue of the beast on the alert manager

00:12:21,910 --> 00:12:24,899
repository

00:12:26,589 --> 00:12:32,930
so my kind of corollary from this is

00:12:30,320 --> 00:12:34,610
that inhibitions definitely comes with

00:12:32,930 --> 00:12:38,990
sharp edges included I think it's hard

00:12:34,610 --> 00:12:40,760
to get right it feels like the way

00:12:38,990 --> 00:12:42,380
inhibition works as a consequence of the

00:12:40,760 --> 00:12:44,839
implementation so when you understand

00:12:42,380 --> 00:12:47,089
how inhibition is implemented in a lot

00:12:44,839 --> 00:12:49,389
of manager it makes perfect sense but

00:12:47,089 --> 00:12:51,470
kind of when you approach it from the

00:12:49,389 --> 00:12:55,339
and a view of somebody who reads the

00:12:51,470 --> 00:12:57,050
documentation is not clear I've put in a

00:12:55,339 --> 00:13:03,529
pull request which now says don't do

00:12:57,050 --> 00:13:07,430
this but we'll see now the next one is a

00:13:03,529 --> 00:13:09,850
pet peeve of mine and I've bothered my

00:13:07,430 --> 00:13:12,860
Matt about this a lot which is that

00:13:09,850 --> 00:13:16,510
aggregated histograms are buggy so I

00:13:12,860 --> 00:13:19,360
want to show this visually so we take

00:13:16,510 --> 00:13:21,560
histograms on this is one data center

00:13:19,360 --> 00:13:24,800
there's a lot of machines which expose a

00:13:21,560 --> 00:13:27,139
histogram and the the query isn't very

00:13:24,800 --> 00:13:29,360
useful usually but what it's supposed to

00:13:27,139 --> 00:13:31,070
show you is that visually you can tell

00:13:29,360 --> 00:13:33,470
there's no sample that is higher than

00:13:31,070 --> 00:13:38,540
five seconds and this colon for this

00:13:33,470 --> 00:13:42,350
particular time span now when I a

00:13:38,540 --> 00:13:43,820
granade this it looks like that so all

00:13:42,350 --> 00:13:45,740
of a sudden I get these works weird

00:13:43,820 --> 00:13:47,839
spikes and they go to 5 minutes and 20

00:13:45,740 --> 00:13:50,630
seconds which is the highest bucket that

00:13:47,839 --> 00:13:54,230
we've configured and apparently this is

00:13:50,630 --> 00:13:56,959
as far as I understand this this is due

00:13:54,230 --> 00:13:59,360
to the atomicity problems that the

00:13:56,959 --> 00:14:03,860
current Prometheus version has so we're

00:13:59,360 --> 00:14:06,649
still on 1.6.2 so even that's been fixed

00:14:03,860 --> 00:14:08,839
I don't know so apparently this is due

00:14:06,649 --> 00:14:14,269
to the atomicity issues for me this

00:14:08,839 --> 00:14:16,459
currently has but it has been much

00:14:14,269 --> 00:14:18,170
improved by APR recent one which kind of

00:14:16,459 --> 00:14:21,920
fudges the histograms for eyes under

00:14:18,170 --> 00:14:27,440
standard and I think it's fixed in 2.0

00:14:21,920 --> 00:14:31,399
nope okay it's good to know maybe we can

00:14:27,440 --> 00:14:32,600
put in a PR and I think there might be

00:14:31,399 --> 00:14:37,220
more problems to come with histograms

00:14:32,600 --> 00:14:38,030
because they're a incredibly useful but

00:14:37,220 --> 00:14:40,730
there be the

00:14:38,030 --> 00:14:42,820
most complicated metric that permeate it

00:14:40,730 --> 00:14:45,140
has to offer so it's the kind of this

00:14:42,820 --> 00:14:48,800
this rude unicorn and how to implement

00:14:45,140 --> 00:14:50,180
at a Prometheus level and also getting

00:14:48,800 --> 00:14:53,290
the implementation in the client

00:14:50,180 --> 00:14:57,160
libraries with right is actually hard

00:14:53,290 --> 00:15:02,540
now Federation and high availability

00:14:57,160 --> 00:15:04,790
this is not a dream team what happens is

00:15:02,540 --> 00:15:06,860
an our set up that the kind of this

00:15:04,790 --> 00:15:09,020
global Prometheus that we have which we

00:15:06,860 --> 00:15:11,900
used to query the status of all of our

00:15:09,020 --> 00:15:14,000
colors which has federated and

00:15:11,900 --> 00:15:16,970
aggregated data in it will end up with

00:15:14,000 --> 00:15:19,100
multiple scrapes per individual machine

00:15:16,970 --> 00:15:21,920
so if I just run in the eve query

00:15:19,100 --> 00:15:25,070
against that I'll kind of have double

00:15:21,920 --> 00:15:29,600
results in it okay

00:15:25,070 --> 00:15:33,020
just continue so what I have to do is I

00:15:29,600 --> 00:15:35,450
have to duplicate these queries or I

00:15:33,020 --> 00:15:37,820
have to duplicate these scrapes in my

00:15:35,450 --> 00:15:42,790
crater somehow need to figure out how to

00:15:37,820 --> 00:15:46,160
take only a single scrape in my query

00:15:42,790 --> 00:15:49,400
and how to do that I think depends on

00:15:46,160 --> 00:15:53,450
your metric I've tried to figure it out

00:15:49,400 --> 00:15:55,370
for histograms it took me like three

00:15:53,450 --> 00:15:57,520
tries and probably it's still wrong but

00:15:55,370 --> 00:15:59,810
I still show you what I've come up with

00:15:57,520 --> 00:16:03,230
which looks roughly like this so you

00:15:59,810 --> 00:16:04,850
have the regular histogram can tell you

00:16:03,230 --> 00:16:07,310
some by whatever you're interested in so

00:16:04,850 --> 00:16:11,420
this is your aggregation function but

00:16:07,310 --> 00:16:14,840
then you take the Max and this could

00:16:11,420 --> 00:16:16,340
also be the min I think and you strip

00:16:14,840 --> 00:16:19,040
out the common labels and what the max

00:16:16,340 --> 00:16:23,930
does is basically chooses one consistent

00:16:19,040 --> 00:16:26,930
scrape and why this works is why this

00:16:23,930 --> 00:16:31,400
works depends on how histograms are

00:16:26,930 --> 00:16:33,710
implemented in Prometheus I would be

00:16:31,400 --> 00:16:35,870
really interested in if this is correct

00:16:33,710 --> 00:16:39,920
or not and maybe we can talk about this

00:16:35,870 --> 00:16:42,170
in the questions now neha mentioned this

00:16:39,920 --> 00:16:46,700
array so I'll skip it probably but very

00:16:42,170 --> 00:16:48,710
briefly service discovery and the up

00:16:46,700 --> 00:16:50,360
metric is not a friend because the sum

00:16:48,710 --> 00:16:51,350
of something that's absent is also

00:16:50,360 --> 00:16:53,510
absent

00:16:51,350 --> 00:16:55,700
so any time you do this you also need to

00:16:53,510 --> 00:17:01,520
alert on the fact that something is

00:16:55,700 --> 00:17:04,580
absent in summary I think prometheus is

00:17:01,520 --> 00:17:07,790
the best choice for us it's amazing and

00:17:04,580 --> 00:17:12,170
the visibility is given us into the

00:17:07,790 --> 00:17:13,790
system that we've built prom QL I think

00:17:12,170 --> 00:17:16,460
deserves higher praise I think it's

00:17:13,790 --> 00:17:18,290
probably the single most the single

00:17:16,460 --> 00:17:21,470
biggest feature for me because it's such

00:17:18,290 --> 00:17:23,120
an such a nice way to kind of access

00:17:21,470 --> 00:17:25,070
your data and it's such a nice way to

00:17:23,120 --> 00:17:27,170
get other people to draw them in and say

00:17:25,070 --> 00:17:29,570
look you can you can actually write a

00:17:27,170 --> 00:17:30,380
query against your metrics data which is

00:17:29,570 --> 00:17:34,130
amazing

00:17:30,380 --> 00:17:36,170
I think inhibition needs love if not

00:17:34,130 --> 00:17:38,390
from documentation need to figure out

00:17:36,170 --> 00:17:42,020
whether this is a good mental model of

00:17:38,390 --> 00:17:45,100
how it's supposed to work and finally

00:17:42,020 --> 00:17:47,660
querying federated high availability

00:17:45,100 --> 00:17:52,970
Prometheus requires extra care this is

00:17:47,660 --> 00:17:55,340
worded very delicately I think this kind

00:17:52,970 --> 00:17:57,170
of day I know that the use cases are not

00:17:55,340 --> 00:18:00,890
very common it's probably not that many

00:17:57,170 --> 00:18:02,300
people running this specific set up but

00:18:00,890 --> 00:18:04,370
it feels like there could be a much

00:18:02,300 --> 00:18:08,660
better story here and how Prometheus

00:18:04,370 --> 00:18:10,280
handles this or how how the work I have

00:18:08,660 --> 00:18:12,920
to do is somebody who writes alerts and

00:18:10,280 --> 00:18:16,460
looks at metrics rather than what is

00:18:12,920 --> 00:18:19,060
today and that's it for me and I'd love

00:18:16,460 --> 00:18:19,060
any questions

00:18:22,160 --> 00:18:26,170
[Applause]

00:18:23,460 --> 00:18:28,720
so answer your question move the max to

00:18:26,170 --> 00:18:30,370
the app to be last because I haven't

00:18:28,720 --> 00:18:33,010
launched the math you probably want that

00:18:30,370 --> 00:18:35,110
on the outside inside otherwise you have

00:18:33,010 --> 00:18:37,780
the same problems again put his film on

00:18:35,110 --> 00:18:43,000
top so max after the Sun so now some

00:18:37,780 --> 00:18:45,550
water okay I have to write another blog

00:18:43,000 --> 00:18:53,560
post to convince myself it works so

00:18:45,550 --> 00:19:06,370
questions you really won't marry break

00:18:53,560 --> 00:19:09,070
LG it's exciting so instead of scraping

00:19:06,370 --> 00:19:11,050
both AJ servers have you thought about

00:19:09,070 --> 00:19:15,310
and like I've been thinking about it I

00:19:11,050 --> 00:19:17,020
haven't done it yet just putting some

00:19:15,310 --> 00:19:20,040
proxy in front that just makes one

00:19:17,020 --> 00:19:23,500
Prometheus or the other to scrape from

00:19:20,040 --> 00:19:26,110
yes I think so Matt is working on that

00:19:23,500 --> 00:19:27,370
stuff that probably will suffer from a

00:19:26,110 --> 00:19:29,410
similar problem that you want your

00:19:27,370 --> 00:19:31,750
scraper to be highly available so all of

00:19:29,410 --> 00:19:34,200
a sudden you then need to figure out how

00:19:31,750 --> 00:19:36,640
do you make that thing highly available

00:19:34,200 --> 00:19:45,160
without support having that support in

00:19:36,640 --> 00:19:47,410
prometheus yes I having a couple of

00:19:45,160 --> 00:19:51,070
hallway conversations today or yesterday

00:19:47,410 --> 00:19:53,290
about building a mixing proxy that will

00:19:51,070 --> 00:19:56,200
sit in front of a main a set to do all

00:19:53,290 --> 00:19:58,510
the the gap filling comes in an H a set

00:19:56,200 --> 00:20:00,820
up that people have complained about for

00:19:58,510 --> 00:20:03,670
a long time so if anybody is interested

00:20:00,820 --> 00:20:05,470
in working on that as a feature we

00:20:03,670 --> 00:20:07,840
should start a thread on the previous

00:20:05,470 --> 00:20:11,110
developers list about creating a mixing

00:20:07,840 --> 00:20:14,850
proxy for H a setups would be amazing

00:20:11,110 --> 00:20:17,110
yeah yeah unfortunately it also tricky

00:20:14,850 --> 00:20:19,450
so as you said you have to do it

00:20:17,110 --> 00:20:21,250
separately for every single metric no

00:20:19,450 --> 00:20:22,570
like for instance for a quantile it's

00:20:21,250 --> 00:20:24,580
like a good percentile your Pokemon -

00:20:22,570 --> 00:20:27,400
max it's the 5th percentile you probably

00:20:24,580 --> 00:20:28,860
want to mean if it's the median what are

00:20:27,400 --> 00:20:31,380
you trying to do it's probably a max

00:20:28,860 --> 00:20:43,879
yeah

00:20:31,380 --> 00:20:44,080
[Music]

00:20:43,879 --> 00:20:47,220
you

00:20:44,080 --> 00:20:47,220

YouTube URL: https://www.youtube.com/watch?v=TRi822rw5b8


