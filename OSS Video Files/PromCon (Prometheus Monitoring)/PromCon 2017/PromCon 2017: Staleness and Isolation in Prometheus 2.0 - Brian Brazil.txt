Title: PromCon 2017: Staleness and Isolation in Prometheus 2.0 - Brian Brazil
Publication date: 2017-09-01
Playlist: PromCon 2017
Description: 
	* Abstract:

The biggest semantic change in Prometheus 2.0 is the new staleness handling. This long awaited feature means there's no longer a fixed 5 minute staleness. Now time series go stale when they're no longer exposed, and targets that no longer exist don't hang around for a full 5 minutes. Learn about how it works and how to take advantage of it.

* Speaker biography:

Brian Brazil is a core developer of Prometheus and the founder of Robust Perception. He works across the ecosystem, and is involved in areas such as best practices, exporters, PromQL semantics and client libraries. He is the main writer of the Reliable Insights blog, which regularly covers Prometheus topics.

* Slides:

* PromCon website:

https://promcon.io/
Captions: 
	00:00:00,380 --> 00:00:16,710
[Music]

00:00:13,070 --> 00:00:19,170
now we will have brine Brazil he's the

00:00:16,710 --> 00:00:21,090
founder of robbers perception a promises

00:00:19,170 --> 00:00:22,590
core developer and most well-known for

00:00:21,090 --> 00:00:25,019
proving that prawn koalas

00:00:22,590 --> 00:00:33,180
turing-complete and he's going to tell

00:00:25,019 --> 00:00:35,579
you about staleness in prometheus 2.0 so

00:00:33,180 --> 00:00:39,540
I presume you will kind of know who I am

00:00:35,579 --> 00:00:40,920
by now and you know I may have done some

00:00:39,540 --> 00:00:44,340
inadvisable things that prompt you out

00:00:40,920 --> 00:00:46,800
of my time but what I want to talk about

00:00:44,340 --> 00:00:48,239
here is that you already had some fabian

00:00:46,800 --> 00:00:49,920
the awesome changes were happening in

00:00:48,239 --> 00:00:51,629
from idiots to zero with a storage

00:00:49,920 --> 00:00:53,190
engine and which is the biggest change

00:00:51,629 --> 00:00:55,590
so i'm gonna talk about the second

00:00:53,190 --> 00:00:58,949
biggest change which is a semantic

00:00:55,590 --> 00:01:00,870
change and which also conveniently is

00:00:58,949 --> 00:01:04,049
one of the things for pull wins over

00:01:00,870 --> 00:01:08,280
push so sorry butum your startup just

00:01:04,049 --> 00:01:09,450
can't handle this so this is a bug this

00:01:08,280 --> 00:01:11,250
is a feature we've talked about for a

00:01:09,450 --> 00:01:13,860
long time like I filed a bug in July

00:01:11,250 --> 00:01:15,450
2014 just basically three years ago soon

00:01:13,860 --> 00:01:17,850
after I got involved in form 8 he smokes

00:01:15,450 --> 00:01:19,560
like me and so we're gonna look at

00:01:17,850 --> 00:01:22,229
basically what permit freebies previous

00:01:19,560 --> 00:01:24,210
versions did with stainless and the

00:01:22,229 --> 00:01:29,159
problems with that and basically how I

00:01:24,210 --> 00:01:32,280
fixed it or improved it so the thing is

00:01:29,159 --> 00:01:34,409
before 2.0 ik anywhere up to well

00:01:32,280 --> 00:01:36,840
Prometheus won 8-1 and probably the last

00:01:34,409 --> 00:01:38,850
one and if you value a range vector like

00:01:36,840 --> 00:01:40,020
looking at a counter for 10 minutes it's

00:01:38,850 --> 00:01:43,500
always going to return all data points

00:01:40,020 --> 00:01:47,430
in that range inclusive nice simple not

00:01:43,500 --> 00:01:49,560
gonna change great if you evaluate an

00:01:47,430 --> 00:01:52,500
inspector like a gauge right it'll

00:01:49,560 --> 00:01:55,710
return the latest value before the

00:01:52,500 --> 00:01:59,219
evaluation time looking back to five

00:01:55,710 --> 00:02:01,200
minutes which is to say that if a time

00:01:59,219 --> 00:02:03,270
series gets no updated data points for

00:02:01,200 --> 00:02:06,119
five minutes it goes stale and is no

00:02:03,270 --> 00:02:08,340
longer returned for instant vectors now

00:02:06,119 --> 00:02:10,259
there is a flag you can use to control

00:02:08,340 --> 00:02:11,940
this setting you shouldn't change it

00:02:10,259 --> 00:02:13,520
because most people have asked about

00:02:11,940 --> 00:02:15,590
this flag we're trying to do event log

00:02:13,520 --> 00:02:17,990
and if you're going to do that well you

00:02:15,590 --> 00:02:19,100
can use influx DB or DL expect something

00:02:17,990 --> 00:02:23,150
else which is better designed for that

00:02:19,100 --> 00:02:26,270
use case so there are several problems

00:02:23,150 --> 00:02:29,180
that come from the old stainless

00:02:26,270 --> 00:02:31,160
semantics the first one people come

00:02:29,180 --> 00:02:34,580
across most common is the few alerts on

00:02:31,160 --> 00:02:37,160
up equals zero and you can imagine you

00:02:34,580 --> 00:02:39,860
have something running on kubernetes it

00:02:37,160 --> 00:02:42,920
starts failing it has optical zero at

00:02:39,860 --> 00:02:44,960
least once and then it's rescheduled but

00:02:42,920 --> 00:02:46,490
up equals zero is still there and will

00:02:44,960 --> 00:02:48,230
be there for five minutes even though

00:02:46,490 --> 00:02:51,770
it's been rescheduled somewhere else and

00:02:48,230 --> 00:02:53,990
is now happy so if you have created a

00:02:51,770 --> 00:02:56,330
hair-trigger alert that's put a fire

00:02:53,990 --> 00:02:58,100
before like five minutes against

00:02:56,330 --> 00:03:00,200
recommendations as Julius earlier then

00:02:58,100 --> 00:03:01,640
yeah that's going to just spam you wish

00:03:00,200 --> 00:03:04,100
an alert which is uninteresting because

00:03:01,640 --> 00:03:07,340
kubernetes is already rescheduled which

00:03:04,100 --> 00:03:08,660
is not particularly useful and you know

00:03:07,340 --> 00:03:10,790
you should increase your foreclose

00:03:08,660 --> 00:03:12,590
because that's a good idea anyway but it

00:03:10,790 --> 00:03:16,850
is something that users do runs into

00:03:12,590 --> 00:03:20,060
reasonably often another issue you run

00:03:16,850 --> 00:03:22,490
into is some time series and this is a

00:03:20,060 --> 00:03:24,170
series which is sometimes exposed on a

00:03:22,490 --> 00:03:26,180
page sometimes not sometimes has about

00:03:24,170 --> 00:03:29,090
labels may have happened in a certain

00:03:26,180 --> 00:03:30,980
lightning talk yesterday but the thing

00:03:29,090 --> 00:03:33,350
is if you can imagine you have a metric

00:03:30,980 --> 00:03:36,080
and some scrapes it has a label foo so

00:03:33,350 --> 00:03:37,640
it has bar if I evaluate that metric I'm

00:03:36,080 --> 00:03:40,190
going to see both because they're Bolton

00:03:37,640 --> 00:03:42,560
says the last five minutes even though

00:03:40,190 --> 00:03:44,450
only one of those came in each spring so

00:03:42,560 --> 00:03:46,250
you know this is not something you

00:03:44,450 --> 00:03:48,290
should be doing but there are some

00:03:46,250 --> 00:03:50,030
advanced to use cases a site for pretty

00:03:48,290 --> 00:03:54,020
pictures where this is actually

00:03:50,030 --> 00:03:55,940
something you want to have another one

00:03:54,020 --> 00:03:57,830
is related to these is double counting

00:03:55,940 --> 00:03:59,990
so you can imagine if you go back to the

00:03:57,830 --> 00:04:02,270
example that you have your process it

00:03:59,990 --> 00:04:03,740
dies is rescheduled and we're still

00:04:02,270 --> 00:04:05,570
seeing boat because the rescheduling

00:04:03,740 --> 00:04:07,010
happened in the last five minutes so

00:04:05,570 --> 00:04:09,590
we're going to double count data stew or

00:04:07,010 --> 00:04:10,730
tree or whatever so the total is it's

00:04:09,590 --> 00:04:12,709
going to be basically how many targets

00:04:10,730 --> 00:04:14,450
have existed over the last five minutes

00:04:12,709 --> 00:04:17,660
rather than how many targets exist now

00:04:14,450 --> 00:04:19,609
which is what you actually want and okay

00:04:17,660 --> 00:04:21,410
up is generally okay because things

00:04:19,609 --> 00:04:23,180
don't turn that fast but some like

00:04:21,410 --> 00:04:24,650
memory usage you know you're going to

00:04:23,180 --> 00:04:27,360
get all sorts of weird artifacts there

00:04:24,650 --> 00:04:30,280
which is not what you want

00:04:27,360 --> 00:04:32,050
another thing that is common problem is

00:04:30,280 --> 00:04:33,250
that there is a practical limit on how

00:04:32,050 --> 00:04:35,860
long your scrape interval and your

00:04:33,250 --> 00:04:37,930
violence there can be if we're only ever

00:04:35,860 --> 00:04:40,120
looking back five minutes if you were

00:04:37,930 --> 00:04:42,669
scraping a little ten minutes half the

00:04:40,120 --> 00:04:44,950
time it's going to show nothing which is

00:04:42,669 --> 00:04:46,360
not exactly useful and a practice the

00:04:44,950 --> 00:04:48,460
limits even lower because you always

00:04:46,360 --> 00:04:50,500
must be tolerant to a failed scrape or a

00:04:48,460 --> 00:04:53,350
failed evaluation because it just takes

00:04:50,500 --> 00:04:54,580
too long or something and so that's kind

00:04:53,350 --> 00:04:56,530
of bad because it means the scrapings

00:04:54,580 --> 00:04:59,050
will come the most be two minutes like

00:04:56,530 --> 00:05:00,160
Prometheus and overload but you know the

00:04:59,050 --> 00:05:02,290
thing on the other end might be able to

00:05:00,160 --> 00:05:04,810
such as certain slower network devices

00:05:02,290 --> 00:05:06,760
and you could bump you know stainless

00:05:04,810 --> 00:05:08,230
Delta but you know that those are

00:05:06,760 --> 00:05:09,490
performance applications and as I said

00:05:08,230 --> 00:05:10,240
people trying to do this or normally try

00:05:09,490 --> 00:05:13,120
to do event logging

00:05:10,240 --> 00:05:15,990
don't touch that setting another issue

00:05:13,120 --> 00:05:18,280
we have is push Gately in time steps

00:05:15,990 --> 00:05:20,320
because the push gateway the data is

00:05:18,280 --> 00:05:22,180
exposed always is now whatever now is

00:05:20,320 --> 00:05:23,950
when the scrape happens rotted in the

00:05:22,180 --> 00:05:25,419
time the dalish occurred it would be

00:05:23,950 --> 00:05:27,430
pretty handy if we could actually have

00:05:25,419 --> 00:05:30,400
that fashion appear with the timestamp

00:05:27,430 --> 00:05:32,229
of when they push occurred and because

00:05:30,400 --> 00:05:33,729
then we could see how recent it was you

00:05:32,229 --> 00:05:35,680
do analysis on and still have shown

00:05:33,729 --> 00:05:37,780
potential queries because some across is

00:05:35,680 --> 00:05:40,090
like hey this is destiny records this

00:05:37,780 --> 00:05:43,720
edition records so over time is great

00:05:40,090 --> 00:05:45,039
that's the total and so that's kind of

00:05:43,720 --> 00:05:46,630
handy but that's not what happens

00:05:45,039 --> 00:05:49,150
instead we get the same data multiple

00:05:46,630 --> 00:05:54,039
times and you have to somehow tell that

00:05:49,150 --> 00:05:57,039
these are from different pushes so this

00:05:54,039 --> 00:05:59,560
isn't great this causes problems let's

00:05:57,039 --> 00:06:01,479
do better so what are the things you

00:05:59,560 --> 00:06:03,880
want to have the first is from the

00:06:01,479 --> 00:06:05,650
target goes away like services covering

00:06:03,880 --> 00:06:08,200
oil returns this or enabling you know

00:06:05,650 --> 00:06:08,919
just decides enough is enough and it

00:06:08,200 --> 00:06:10,150
goes stale

00:06:08,919 --> 00:06:13,240
we stopped seeing as time series

00:06:10,150 --> 00:06:15,940
innocent queries similarly if a target

00:06:13,240 --> 00:06:18,760
just starts stops in a scrape showing a

00:06:15,940 --> 00:06:20,650
time series and wanted to go stale we

00:06:18,760 --> 00:06:22,510
would be nice to be able to expose time

00:06:20,650 --> 00:06:24,970
series back in time free to push gateway

00:06:22,510 --> 00:06:27,610
and we also would like to support longer

00:06:24,970 --> 00:06:30,070
scraping the balances because they do

00:06:27,610 --> 00:06:31,669
come up even if motorcy are using 10

00:06:30,070 --> 00:06:34,759
seconds

00:06:31,669 --> 00:06:37,220
so the implementation it kind of seems

00:06:34,759 --> 00:06:39,439
simple in principle and when the TARDIS

00:06:37,220 --> 00:06:41,629
or a time series goes away we'll ingest

00:06:39,439 --> 00:06:46,039
a special marker value saying hey I'm

00:06:41,629 --> 00:06:47,780
stale then when we are evaluating an

00:06:46,039 --> 00:06:49,219
instant vector if we see that's the most

00:06:47,780 --> 00:06:52,370
recent value we'll discard our time

00:06:49,219 --> 00:06:54,169
series because it's stale and for range

00:06:52,370 --> 00:06:55,759
vectors we'll just ignore those values

00:06:54,169 --> 00:07:00,759
good from those pass them on to rate as

00:06:55,759 --> 00:07:05,169
usual nice simple elegant right yeah

00:07:00,759 --> 00:07:05,169
this reason this is inherent all talk so

00:07:06,250 --> 00:07:14,330
so we need to have a special value that

00:07:10,039 --> 00:07:16,580
we can inject and and you know what

00:07:14,330 --> 00:07:18,440
people say use Nan's problem is Peruggia

00:07:16,580 --> 00:07:19,099
supports Nan's one of the few systems

00:07:18,440 --> 00:07:22,509
that does

00:07:19,099 --> 00:07:25,190
we still full 64 but it turns out -

00:07:22,509 --> 00:07:27,710
there's more than one way you can

00:07:25,190 --> 00:07:30,439
actually write nan in floating-point

00:07:27,710 --> 00:07:32,539
numbers and and that is defined as the

00:07:30,439 --> 00:07:34,969
exponent is all one the fraction is

00:07:32,539 --> 00:07:37,729
nonzero if it's zero it's where the

00:07:34,969 --> 00:07:39,590
infinities so in fact there's 250 to

00:07:37,729 --> 00:07:41,479
minus 1 or maybe 2 to 53 minus 1 with

00:07:39,590 --> 00:07:44,990
the sign bit ways you can represent man

00:07:41,479 --> 00:07:46,610
i considering 3ds only supports wooden

00:07:44,990 --> 00:07:49,370
and we just choose one of those

00:07:46,610 --> 00:07:51,589
arbitrarily as a real man and we can use

00:07:49,370 --> 00:07:56,509
the other two 250 two values for fun

00:07:51,589 --> 00:07:59,479
stuff so NFC as the real man we happen

00:07:56,509 --> 00:08:02,029
to choose to value go users and then we

00:07:59,479 --> 00:08:04,550
just choose a special marker the

00:08:02,029 --> 00:08:05,719
everything knows nine is special signing

00:08:04,550 --> 00:08:07,819
for not a number it's what you normally

00:08:05,719 --> 00:08:09,289
get if you divide one by zero at least

00:08:07,819 --> 00:08:10,490
with floating point if you try it out

00:08:09,289 --> 00:08:13,120
with main feature you're going to get

00:08:10,490 --> 00:08:15,740
your processor exception drops are fun

00:08:13,120 --> 00:08:17,479
so the other thing is that man is

00:08:15,740 --> 00:08:19,580
basically an airspace game floating

00:08:17,479 --> 00:08:21,770
point and it has a special property that

00:08:19,580 --> 00:08:23,509
nan is not equal to nan which is how you

00:08:21,770 --> 00:08:24,710
filter them but it also means that if

00:08:23,509 --> 00:08:27,440
you try to do a floating point math with

00:08:24,710 --> 00:08:29,569
these it will not work so any

00:08:27,440 --> 00:08:31,669
comparisons to see if this is a real man

00:08:29,569 --> 00:08:33,380
or one of these stale markers we need to

00:08:31,669 --> 00:08:36,339
do it at a bit level at the floating

00:08:33,380 --> 00:08:40,610
point level so the good news is that

00:08:36,339 --> 00:08:43,099
golang has functions called fill 64 bits

00:08:40,610 --> 00:08:44,649
which converts a 64 which flows into a

00:08:43,099 --> 00:08:46,660
64-bit integer and

00:08:44,649 --> 00:08:48,970
just compare those directly and his

00:08:46,660 --> 00:08:49,779
utility function I added fur - when I

00:08:48,970 --> 00:08:51,819
started out

00:08:49,779 --> 00:08:53,559
DTS DB is like the previous talks

00:08:51,819 --> 00:08:54,910
covered and it was actually doing some

00:08:53,559 --> 00:08:56,619
floating-point comparisons rather than

00:08:54,910 --> 00:08:59,410
big price comparison so I need to change

00:08:56,619 --> 00:09:01,209
that store actually all bitwise as some

00:08:59,410 --> 00:09:02,709
of the logic about being append only in

00:09:01,209 --> 00:09:04,569
terms of time was doing those sort of

00:09:02,709 --> 00:09:06,220
checks so I had to fix that that was the

00:09:04,569 --> 00:09:09,009
only change I need used to making the TS

00:09:06,220 --> 00:09:12,610
TB aside from that no cleanliness change

00:09:09,009 --> 00:09:14,980
this feature lives entirely infirmities

00:09:12,610 --> 00:09:18,490
itself so it's completely doesn't touch

00:09:14,980 --> 00:09:19,449
the TS dB at all it's kind of neat she

00:09:18,490 --> 00:09:21,699
also pointed out there's actually two

00:09:19,449 --> 00:09:24,009
types of nouns just quiet nouns and

00:09:21,699 --> 00:09:25,509
signaling nouns and depending with our

00:09:24,009 --> 00:09:27,670
signal error states are walking off I

00:09:25,509 --> 00:09:29,019
think I chose a signaling man for the

00:09:27,670 --> 00:09:31,089
stale marker then I move it around so

00:09:29,019 --> 00:09:33,850
good future expansion it doesn't matter

00:09:31,089 --> 00:09:36,639
there's a special bit representation and

00:09:33,850 --> 00:09:38,769
so the simple case the simplest case it

00:09:36,639 --> 00:09:42,339
turns out is the case that also happens

00:09:38,769 --> 00:09:45,029
very rarely and that is the case that I

00:09:42,339 --> 00:09:47,139
scrape of tarnish it has a time series I

00:09:45,029 --> 00:09:49,209
scrape it again it doesn't cut that time

00:09:47,139 --> 00:09:51,970
series so what we do is we just remember

00:09:49,209 --> 00:09:53,499
what we scraped in the previous rate we

00:09:51,970 --> 00:09:55,839
see if anything isn't there anymore and

00:09:53,499 --> 00:09:58,600
then just add in a stale marker as we're

00:09:55,839 --> 00:09:59,559
ingesting it nice and simple means we

00:09:58,600 --> 00:10:01,779
have to remember everything you scraped

00:09:59,559 --> 00:10:03,459
last time that's not it's too difficult

00:10:01,779 --> 00:10:05,800
to do again the optimizations we have

00:10:03,459 --> 00:10:07,449
around add fast so like the tree caches

00:10:05,800 --> 00:10:10,839
we have inside the new scrape loop four

00:10:07,449 --> 00:10:13,439
to zero so this is actually an easy for

00:10:10,839 --> 00:10:15,850
the least common case yay

00:10:13,439 --> 00:10:17,529
and what's more complicated it's like

00:10:15,850 --> 00:10:19,749
scaling is actually pretty much the same

00:10:17,529 --> 00:10:21,550
because we need to mark everything that

00:10:19,749 --> 00:10:24,610
failed and everything that was in the

00:10:21,550 --> 00:10:26,079
previous scrape has stale and otherwise

00:10:24,610 --> 00:10:27,819
that fire could go down and then

00:10:26,079 --> 00:10:29,679
disappear and we just keep my persistent

00:10:27,819 --> 00:10:32,290
gash which is not what we want to do we

00:10:29,679 --> 00:10:34,120
want things to go stale if up is zero so

00:10:32,290 --> 00:10:36,100
we just use the exact same logic as for

00:10:34,120 --> 00:10:37,689
a single time series and say okay take

00:10:36,100 --> 00:10:40,869
everything in the previous scrape market

00:10:37,689 --> 00:10:42,939
stale so it'd be like a gap there and if

00:10:40,869 --> 00:10:44,439
there's more failures after - well the

00:10:42,939 --> 00:10:46,540
previous scrape was empty because it

00:10:44,439 --> 00:10:49,749
failed so we don't do anything so that's

00:10:46,540 --> 00:10:52,310
nice and cheap everyone good so far

00:10:49,749 --> 00:10:57,050
these are the easy ones

00:10:52,310 --> 00:10:58,550
sorry it goes away part 1 yes this is

00:10:57,050 --> 00:11:02,840
not the easy one think it goes up to 4

00:10:58,550 --> 00:11:05,870
or 5 and so service discovery targeting

00:11:02,840 --> 00:11:07,340
the longer exists go stale we just

00:11:05,870 --> 00:11:09,410
ingest stale markers for everything you

00:11:07,340 --> 00:11:11,720
scraped previously and as well as for

00:11:09,410 --> 00:11:13,520
often friends because there's up scrape

00:11:11,720 --> 00:11:16,420
iteration second scrape South escapes

00:11:13,520 --> 00:11:18,560
and save sample scraped post relabeling

00:11:16,420 --> 00:11:21,680
yeah sorry I couldn't give that name

00:11:18,560 --> 00:11:23,330
shorter and so but its course not that

00:11:21,680 --> 00:11:25,010
simple the first question to ask is what

00:11:23,330 --> 00:11:29,570
time stamp should we have on these stale

00:11:25,010 --> 00:11:32,120
markers ok so thing is that scrapes the

00:11:29,570 --> 00:11:34,880
time stamp is used for scrapes is always

00:11:32,120 --> 00:11:36,350
when the scrape starts and because of

00:11:34,880 --> 00:11:37,850
how coal works internally that normally

00:11:36,350 --> 00:11:41,470
ends up being exactly the straight

00:11:37,850 --> 00:11:41,470
interval which is great for compression

00:11:41,740 --> 00:11:45,650
so really what we want to use pretty

00:11:43,940 --> 00:11:46,940
stale markers is when the next scrape

00:11:45,650 --> 00:11:50,480
was going to happen and use that time

00:11:46,940 --> 00:11:54,170
stamp and that's fine as long as the

00:11:50,480 --> 00:11:56,540
next scrape doesn't happen because the

00:11:54,170 --> 00:11:58,160
problem is the service discovery could

00:11:56,540 --> 00:12:00,460
decide hey this target doesn't exist

00:11:58,160 --> 00:12:03,050
oh wait wait a second it's there now and

00:12:00,460 --> 00:12:04,700
we've ingested these stale markers in

00:12:03,050 --> 00:12:06,860
the future when the next scrape is going

00:12:04,700 --> 00:12:08,000
to happen well we can change our minds

00:12:06,860 --> 00:12:09,970
because the time series databases

00:12:08,000 --> 00:12:11,900
append-only

00:12:09,970 --> 00:12:13,880
so that's kind of annoying because we'd

00:12:11,900 --> 00:12:17,300
break the next scrape just because you

00:12:13,880 --> 00:12:18,740
know it's flat now we could try the tech

00:12:17,300 --> 00:12:20,870
this and keep costing you - you would in

00:12:18,740 --> 00:12:23,480
a scrape job and to make that working

00:12:20,870 --> 00:12:25,850
but in actuality that's kind of tricky

00:12:23,480 --> 00:12:27,530
like pretty complicated and also

00:12:25,850 --> 00:12:29,000
impressed well the target could move

00:12:27,530 --> 00:12:31,400
from one scrape config to another speck

00:12:29,000 --> 00:12:33,650
config if someone was doing a

00:12:31,400 --> 00:12:36,500
refactoring or something and that would

00:12:33,650 --> 00:12:40,040
also kind of suck so that doesn't work

00:12:36,500 --> 00:12:42,320
either so take a standard approach what

00:12:40,040 --> 00:12:45,020
is this stupidest thing that would work

00:12:42,320 --> 00:12:48,410
the dumbest solution ever that has the

00:12:45,020 --> 00:12:51,500
right semantics so what we do now is a

00:12:48,410 --> 00:12:53,150
target is stopped so previously service

00:12:51,500 --> 00:12:54,310
discovery makes target stop and that's

00:12:53,150 --> 00:12:57,350
the same as being shut down

00:12:54,310 --> 00:12:59,420
disentangle - and so if it stopped okay

00:12:57,350 --> 00:13:02,240
we stop scraping but we don't completely

00:12:59,420 --> 00:13:02,839
stop a tarnish you just kind of sleep

00:13:02,240 --> 00:13:04,459
furnish

00:13:02,839 --> 00:13:06,140
we sleep until the next scrape would

00:13:04,459 --> 00:13:08,240
have happened and its data would

00:13:06,140 --> 00:13:11,000
being ingested how to safety of her and

00:13:08,240 --> 00:13:13,130
then we ingest stale markers so

00:13:11,000 --> 00:13:15,740
basically after target has stopped and

00:13:13,130 --> 00:13:19,190
we basically sleep for a while and try

00:13:15,740 --> 00:13:20,930
to ingest stale burgers and we delayed

00:13:19,190 --> 00:13:22,730
this enough such as the scrape actually

00:13:20,930 --> 00:13:25,220
happens well when we try to ingest the

00:13:22,730 --> 00:13:27,080
skill markers it'll fail because well

00:13:25,220 --> 00:13:29,780
the database is append Omiya we'll catch

00:13:27,080 --> 00:13:31,070
that and if it has gone away

00:13:29,780 --> 00:13:34,520
then we'll we've ingested the stale

00:13:31,070 --> 00:13:36,890
markers wonderful so the general idea is

00:13:34,520 --> 00:13:38,540
we stop the target we sleep for

00:13:36,890 --> 00:13:40,040
basically two straight printables and

00:13:38,540 --> 00:13:41,500
then try to ingest the stale markers

00:13:40,040 --> 00:13:45,830
with the time something would have had

00:13:41,500 --> 00:13:47,930
its dumb but it works so let's take a

00:13:45,830 --> 00:13:50,630
worked example here the try and see how

00:13:47,930 --> 00:13:52,700
this goes so we have a target we're

00:13:50,630 --> 00:13:56,000
scraping every ten seconds you know on

00:13:52,700 --> 00:13:56,990
the ten seconds and we're on a say at T

00:13:56,000 --> 00:13:59,930
equals 25

00:13:56,990 --> 00:14:01,690
the target is removed so the next scrape

00:13:59,930 --> 00:14:03,800
would have been a T equals turkey and

00:14:01,690 --> 00:14:05,780
conceivably like the scrape time at was

00:14:03,800 --> 00:14:07,700
10 seconds or it's slow or a gesture so

00:14:05,780 --> 00:14:08,690
it could have taken up the T equals 40

00:14:07,700 --> 00:14:12,010
forgot that with the getting to the

00:14:08,690 --> 00:14:15,080
database okay if we're very at the edge

00:14:12,010 --> 00:14:17,180
so we add some slack on the dash so the

00:14:15,080 --> 00:14:18,830
rate function uses 10% slack so I said

00:14:17,180 --> 00:14:19,280
sure let's just follow that because why

00:14:18,830 --> 00:14:23,510
not

00:14:19,280 --> 00:14:25,910
so at T equals 41 we want to ingest

00:14:23,510 --> 00:14:27,650
stale markers what a time step of T

00:14:25,910 --> 00:14:30,710
equals turkey and then we actually make

00:14:27,650 --> 00:14:32,150
the target go away so the answer is that

00:14:30,710 --> 00:14:33,830
basically won't scrape later and would

00:14:32,150 --> 00:14:36,620
have happened we get these fail markers

00:14:33,830 --> 00:14:39,440
in which is far better than five minutes

00:14:36,620 --> 00:14:40,940
like in this case we're basically 10 20

00:14:39,440 --> 00:14:46,910
seconds later which is much better than

00:14:40,940 --> 00:14:48,890
300 and then a couple another case which

00:14:46,910 --> 00:14:51,110
is time stamps back in time this is

00:14:48,890 --> 00:14:53,330
largely to push gateway and some other

00:14:51,110 --> 00:14:55,580
cases as well like graphite already of

00:14:53,330 --> 00:14:58,280
books to do exporter collecti has it as

00:14:55,580 --> 00:14:59,690
well and so for range vector is how to

00:14:58,280 --> 00:15:02,210
deal with time size back in time well we

00:14:59,690 --> 00:15:04,970
just ingest them as normal grande no

00:15:02,210 --> 00:15:06,560
problem and for the instant factors well

00:15:04,970 --> 00:15:08,960
we'd like to keep on returning this old

00:15:06,560 --> 00:15:10,730
value even though it might be old old as

00:15:08,960 --> 00:15:15,140
long as the push gateway is still

00:15:10,730 --> 00:15:18,650
exposing that in every scrape great but

00:15:15,140 --> 00:15:19,970
let's consider a worst case here at T

00:15:18,650 --> 00:15:22,819
equals 5 push

00:15:19,970 --> 00:15:25,790
yep I have a scrape for t plus one like

00:15:22,819 --> 00:15:27,889
four seconds of my gosh so we're you

00:15:25,790 --> 00:15:30,050
know we need high-class infant vectors

00:15:27,889 --> 00:15:31,550
so you saw the interface go to him of

00:15:30,050 --> 00:15:33,379
showing the courier interface will get a

00:15:31,550 --> 00:15:35,360
min T in a max State so you kind of have

00:15:33,379 --> 00:15:36,829
to choose how far you're looking over so

00:15:35,360 --> 00:15:38,329
we have to choose so amount of time to

00:15:36,829 --> 00:15:39,529
look back because we can't look up over

00:15:38,329 --> 00:15:41,629
forever because that could get very

00:15:39,529 --> 00:15:43,279
expensive I'm able to search back in

00:15:41,629 --> 00:15:46,040
time either is that's an inefficient way

00:15:43,279 --> 00:15:48,139
to use that adamance so what we could do

00:15:46,040 --> 00:15:51,439
is have a special sort of like live

00:15:48,139 --> 00:15:53,959
marker like the stale marker and use all

00:15:51,439 --> 00:15:55,910
those other bits you know at 252 values

00:15:53,959 --> 00:15:57,560
we could like put a timestamp in there

00:15:55,910 --> 00:15:59,269
telling us where to look we'd actually

00:15:57,560 --> 00:16:01,430
get like 16 seconds resolution which is

00:15:59,269 --> 00:16:03,470
plenty and that's what we can do with

00:16:01,430 --> 00:16:06,319
that and point that's the actual sample

00:16:03,470 --> 00:16:07,550
and then we just ingest those on a

00:16:06,319 --> 00:16:09,769
regular basis like at every spray

00:16:07,550 --> 00:16:13,910
telling us to go back and look over at

00:16:09,769 --> 00:16:17,689
Eid was one great and and at T equals

00:16:13,910 --> 00:16:19,790
six it exposes timestamp equals 2 which

00:16:17,689 --> 00:16:21,920
is a problem because we already

00:16:19,790 --> 00:16:26,420
interested a point a live marker with a

00:16:21,920 --> 00:16:28,730
timestamp set to 5 and we can't write

00:16:26,420 --> 00:16:34,269
data back in time because databases

00:16:28,730 --> 00:16:37,339
depend only so that's not gonna work and

00:16:34,269 --> 00:16:38,779
so it turns out that with the push

00:16:37,339 --> 00:16:40,430
gateway getting the timestamps to work

00:16:38,779 --> 00:16:43,069
with our semantics whereas someone can

00:16:40,430 --> 00:16:46,040
run a query at any instant it just

00:16:43,069 --> 00:16:48,620
doesn't work out so all we do then if

00:16:46,040 --> 00:16:50,509
I'm ingesting something like stuck like

00:16:48,620 --> 00:16:52,579
in Federation where there is a time

00:16:50,509 --> 00:16:54,949
style how should we integrate that into

00:16:52,579 --> 00:16:57,649
the still mishandling and the answer is

00:16:54,949 --> 00:16:59,300
we ignore it we just basically say okay

00:16:57,649 --> 00:17:01,430
this sample is opting out of all stale

00:16:59,300 --> 00:17:03,829
is handling and no that new staleness

00:17:01,430 --> 00:17:05,890
marker applies cytokine use stale Star

00:17:03,829 --> 00:17:08,120
Trek no still markers to be written and

00:17:05,890 --> 00:17:09,559
yeah the short version is don't try to

00:17:08,120 --> 00:17:10,579
do pusher Prometheus don't use

00:17:09,559 --> 00:17:14,569
timestamps unless you know what you're

00:17:10,579 --> 00:17:17,720
doing and the other thing is if there's

00:17:14,569 --> 00:17:20,240
also another issue that for many years

00:17:17,720 --> 00:17:22,250
now I have been blocking the addition of

00:17:20,240 --> 00:17:24,380
timestamps to client libraries on the

00:17:22,250 --> 00:17:26,000
basis that still this will fix it and

00:17:24,380 --> 00:17:29,510
then we'll be able to support a property

00:17:26,000 --> 00:17:32,929
in an orbit grant however we now know

00:17:29,510 --> 00:17:33,800
that the stainless would fix it and I am

00:17:32,929 --> 00:17:35,840
still blocked

00:17:33,800 --> 00:17:41,030
this feature which you know a very small

00:17:35,840 --> 00:17:42,470
number of users need like for um but the

00:17:41,030 --> 00:17:44,540
other thing is though one the main

00:17:42,470 --> 00:17:46,460
abuses we saw is that even though no

00:17:44,540 --> 00:17:49,340
client library supports this because I

00:17:46,460 --> 00:17:52,280
was blocking it and users were doing it

00:17:49,340 --> 00:17:54,110
by hand anyway and pushing timestamps to

00:17:52,280 --> 00:17:58,160
the push gateway which is the bad bad

00:17:54,110 --> 00:18:00,680
idea and then users learned why it was a

00:17:58,160 --> 00:18:02,350
bad bad idea I wanted to set like query

00:18:00,680 --> 00:18:05,450
Stan was Delta not sort of thing

00:18:02,350 --> 00:18:07,220
so considering a trace and I didn't want

00:18:05,450 --> 00:18:08,870
to add to the client library so people

00:18:07,220 --> 00:18:11,600
would wouldn't do this it wasn't even

00:18:08,870 --> 00:18:13,580
supported or documented anywhere I said

00:18:11,600 --> 00:18:15,160
right what we're going to do is the push

00:18:13,580 --> 00:18:18,590
gateway currently accepts timestamps as

00:18:15,160 --> 00:18:20,300
of 0 for 0 it doesn't it will reject any

00:18:18,590 --> 00:18:23,870
push you make to it with timestamps so

00:18:20,300 --> 00:18:25,820
that abuse stops and because the recheck

00:18:23,870 --> 00:18:27,380
there's no bad use cases for it was all

00:18:25,820 --> 00:18:28,960
people trying to do push with Prometheus

00:18:27,380 --> 00:18:31,970
it for non service that were batch jobs

00:18:28,960 --> 00:18:34,280
but also deal with the use case for you

00:18:31,970 --> 00:18:36,110
I want to aggregate over time and you

00:18:34,280 --> 00:18:38,240
know do that so over time to you know

00:18:36,110 --> 00:18:40,070
see hey all these batch jobs how many

00:18:38,240 --> 00:18:41,840
records that a process in total there is

00:18:40,070 --> 00:18:43,460
a new metric automatically now added by

00:18:41,840 --> 00:18:45,770
the push gateway called push time

00:18:43,460 --> 00:18:47,630
seconds so if you do some crazy stuff

00:18:45,770 --> 00:18:50,540
we're recording rules you can use that

00:18:47,630 --> 00:18:52,640
to generate the right data yeah exactly

00:18:50,540 --> 00:18:54,260
how you do that is less an exercise to

00:18:52,640 --> 00:18:56,990
the reader but at least there is a

00:18:54,260 --> 00:18:58,640
solution there now is possible so I

00:18:56,990 --> 00:19:01,370
haven't solved the actual problem as I

00:18:58,640 --> 00:19:02,630
hoped but it is resolved now and a and

00:19:01,370 --> 00:19:08,300
add time stuff supported client

00:19:02,630 --> 00:19:11,540
libraries now so everything so far has

00:19:08,300 --> 00:19:13,370
been on a largely happy path because we

00:19:11,540 --> 00:19:15,200
all know that you know servers never

00:19:13,370 --> 00:19:17,390
ever crash especially not Prometheus

00:19:15,200 --> 00:19:19,250
especially not you know to zero code

00:19:17,390 --> 00:19:19,850
that's enough and then beta never

00:19:19,250 --> 00:19:22,610
happens

00:19:19,850 --> 00:19:24,380
um so the question of what happens when

00:19:22,610 --> 00:19:27,350
for me to use crashes before this fail

00:19:24,380 --> 00:19:28,940
markers are written you know we could

00:19:27,350 --> 00:19:30,920
just use the old 5-minute logic and fall

00:19:28,940 --> 00:19:33,500
back to - well I think we can do a

00:19:30,920 --> 00:19:35,420
little bit better and because what we

00:19:33,500 --> 00:19:37,130
could do is look at the last two samples

00:19:35,420 --> 00:19:40,880
inside that 5 minutes and presume that's

00:19:37,130 --> 00:19:44,060
the interval and then basically we can

00:19:40,880 --> 00:19:46,100
say that's the interval and if if that's

00:19:44,060 --> 00:19:47,780
more than like four intervals away plus

00:19:46,100 --> 00:19:50,720
some safety net buffer

00:19:47,780 --> 00:19:52,730
it is stale so basic kind of try to

00:19:50,720 --> 00:19:54,020
guess what the interval is because like

00:19:52,730 --> 00:19:55,540
the last example should be a tighter

00:19:54,020 --> 00:19:57,110
scale markers and all steel markers and

00:19:55,540 --> 00:20:01,430
work from there

00:19:57,110 --> 00:20:03,650
and you're asking me brine why for

00:20:01,430 --> 00:20:05,300
intervals that seems like a lot of

00:20:03,650 --> 00:20:07,160
intervals to have to wait to mark

00:20:05,300 --> 00:20:10,400
something stale shouldn't it be like 1

00:20:07,160 --> 00:20:11,600
or 2 that we have 4 targets normally the

00:20:10,400 --> 00:20:14,150
reason why we have to do this is

00:20:11,600 --> 00:20:16,280
Federation because we have to consider a

00:20:14,150 --> 00:20:17,480
worst case let's say we're working a

00:20:16,280 --> 00:20:20,150
system that has 10 second intervals

00:20:17,480 --> 00:20:22,970
right so you're scraping something every

00:20:20,150 --> 00:20:25,340
10 seconds and then federating it every

00:20:22,970 --> 00:20:25,880
10 seconds the absolute worst case is as

00:20:25,340 --> 00:20:28,100
follows

00:20:25,880 --> 00:20:30,860
I could start a scrape at T equals 10

00:20:28,100 --> 00:20:33,020
and data only gets into previous at t

00:20:30,860 --> 00:20:35,300
equals 20 which is only pulled by

00:20:33,020 --> 00:20:39,200
Federation at equals turkey and ingested

00:20:35,300 --> 00:20:40,820
by Federation at T equals 40 and then

00:20:39,200 --> 00:20:43,880
while the next scrape from Federation

00:20:40,820 --> 00:20:46,370
will Kapil get into tables 50 so it's

00:20:43,880 --> 00:20:49,220
actually plausible the T equals 50 that

00:20:46,370 --> 00:20:51,110
a value of T equals 10 in the worst case

00:20:49,220 --> 00:20:53,780
where everything is exactly wrongly

00:20:51,110 --> 00:20:56,920
timed is in fact the newest value and

00:20:53,780 --> 00:21:01,460
fresh so that's why it's for intervals

00:20:56,920 --> 00:21:02,750
plus 0.1 for slack yeah it took me a

00:21:01,460 --> 00:21:04,610
while to realize this is actually a case

00:21:02,750 --> 00:21:09,680
and why I have to expand it to allow for

00:21:04,610 --> 00:21:10,940
all these hops and so you won't ask

00:21:09,680 --> 00:21:14,000
about Federation which I briefly

00:21:10,940 --> 00:21:15,950
mentioned and Federation there's no

00:21:14,000 --> 00:21:18,110
special logic here added at all because

00:21:15,950 --> 00:21:19,790
Federation is fundamentally an instant

00:21:18,110 --> 00:21:23,840
vector query which means stay on the

00:21:19,790 --> 00:21:27,110
supplies and Federation also exposes

00:21:23,840 --> 00:21:29,360
time stamps so the new stainless logic

00:21:27,110 --> 00:21:33,110
won't apply either per like six slides

00:21:29,360 --> 00:21:34,940
ago the good news is Federation is for a

00:21:33,110 --> 00:21:36,800
job aggregator metrics that will remain

00:21:34,940 --> 00:21:39,380
present they don't appear and disappear

00:21:36,800 --> 00:21:40,550
there's no churn and so it's basically

00:21:39,380 --> 00:21:42,710
immune the targets appearing and

00:21:40,550 --> 00:21:46,100
disappearing and everyone uses it that

00:21:42,710 --> 00:21:47,630
way so that's no problem whatsoever has

00:21:46,100 --> 00:21:49,730
any per instance alerts will be on that

00:21:47,630 --> 00:21:52,420
lower level premedia so it doesn't apply

00:21:49,730 --> 00:21:55,370
so as long as everyone is using

00:21:52,420 --> 00:22:00,260
Federation correctly you won't notice at

00:21:55,370 --> 00:22:00,890
all yeah so if you are using Federation

00:22:00,260 --> 00:22:01,430
incorrectly

00:22:00,890 --> 00:22:03,320
like

00:22:01,430 --> 00:22:05,960
by say pulling in anything instance

00:22:03,320 --> 00:22:07,940
level or putting entire from media

00:22:05,960 --> 00:22:09,710
service to Federation you all not to get

00:22:07,940 --> 00:22:11,930
a positive news stylist and you might

00:22:09,710 --> 00:22:14,240
see some new odd behaviors they stood

00:22:11,930 --> 00:22:17,330
out so please use Federation as it's

00:22:14,240 --> 00:22:23,150
designed for aggregated metrics that are

00:22:17,330 --> 00:22:25,010
missing an instance table so longer

00:22:23,150 --> 00:22:27,260
intervals what we do at those so as

00:22:25,010 --> 00:22:28,790
we've already covered for Metis to zero

00:22:27,260 --> 00:22:32,330
you have to basically choose the

00:22:28,790 --> 00:22:33,860
interpreter looking at so for longer

00:22:32,330 --> 00:22:35,960
intervals we'd have to ask for more than

00:22:33,860 --> 00:22:37,310
five minutes of data like maybe some

00:22:35,960 --> 00:22:39,140
also straight points in our stuff that's

00:22:37,310 --> 00:22:40,520
for an hour of data that's 12 times more

00:22:39,140 --> 00:22:41,810
than that we could conceivably have to

00:22:40,520 --> 00:22:44,480
request for everyone is doing like the

00:22:41,810 --> 00:22:47,120
10 second poll and that will be very

00:22:44,480 --> 00:22:48,890
inefficient so a we could reuse that

00:22:47,120 --> 00:22:51,380
live marker idea we had for time steps

00:22:48,890 --> 00:22:52,940
and ingest those every two is finished

00:22:51,380 --> 00:22:54,860
and point back to the actual things so

00:22:52,940 --> 00:22:57,110
just a little slower doesn't work out

00:22:54,860 --> 00:22:59,360
but the whole goal of a longer interval

00:22:57,110 --> 00:23:01,520
normally is to reduce data volume for

00:22:59,360 --> 00:23:04,340
from radius if we're ingesting these

00:23:01,520 --> 00:23:06,890
live markers every two minutes that's

00:23:04,340 --> 00:23:08,750
not helping because well that's the same

00:23:06,890 --> 00:23:10,910
as a two-minute interval in terms of TST

00:23:08,750 --> 00:23:12,080
B load it's probably best not to think

00:23:10,910 --> 00:23:14,150
what that does the compression either

00:23:12,080 --> 00:23:16,250
because like that's live nan

00:23:14,150 --> 00:23:18,590
looks quite different to the actual data

00:23:16,250 --> 00:23:20,900
and maybe something we looked at in the

00:23:18,590 --> 00:23:23,720
future but you know increase your

00:23:20,900 --> 00:23:27,830
interval we also need to talk about

00:23:23,720 --> 00:23:29,540
remote freedom right so as I said the TS

00:23:27,830 --> 00:23:30,680
DB has basically no logic whatsoever

00:23:29,540 --> 00:23:33,160
relation to staleness

00:23:30,680 --> 00:23:35,600
which means also remote read and write

00:23:33,160 --> 00:23:37,400
also needs to have no logic or status

00:23:35,600 --> 00:23:40,160
which is handy because this is just an

00:23:37,400 --> 00:23:42,110
opaque value but we are depending on a

00:23:40,160 --> 00:23:44,240
special nanny value that is all in

00:23:42,110 --> 00:23:45,650
protobufs and so that should be

00:23:44,240 --> 00:23:47,240
preserved correctly to the stack if

00:23:45,650 --> 00:23:49,790
everyone's doing correctly but it does

00:23:47,240 --> 00:23:51,520
mean that if anyone is doing math they

00:23:49,790 --> 00:23:53,810
need to watch out for these nets or

00:23:51,520 --> 00:23:55,490
maybe they'll just filter out go through

00:23:53,810 --> 00:23:56,780
the boughs and just rely on the back of

00:23:55,490 --> 00:23:58,730
logic because the are secure and

00:23:56,780 --> 00:24:00,650
long-term storage demands and still is

00:23:58,730 --> 00:24:02,780
handling probably not quite so important

00:24:00,650 --> 00:24:03,770
as the active stuff but it's just

00:24:02,780 --> 00:24:05,570
something to keep in mind if you're

00:24:03,770 --> 00:24:08,120
writing a real read or write adapter you

00:24:05,570 --> 00:24:13,220
need to pass this stuff these things on

00:24:08,120 --> 00:24:14,820
bit permission so let's review how this

00:24:13,220 --> 00:24:17,309
little project works

00:24:14,820 --> 00:24:19,350
a type-c target goes away it's time

00:24:17,309 --> 00:24:21,799
Sears accuser stale yeah it was a little

00:24:19,350 --> 00:24:23,700
tricky it's a little hacky but it works

00:24:21,799 --> 00:24:26,190
okay it takes the infomercial

00:24:23,700 --> 00:24:28,289
quite a time series my target no longer

00:24:26,190 --> 00:24:30,299
returns the time series it's stale yep

00:24:28,289 --> 00:24:33,659
happens on the next scrape absolutely

00:24:30,299 --> 00:24:34,769
perfect if I want expose time stands

00:24:33,659 --> 00:24:36,590
back in time for the push can't wait

00:24:34,769 --> 00:24:39,029
yeah sorry it's never gonna happen

00:24:36,590 --> 00:24:40,320
but there's other solutions put in place

00:24:39,029 --> 00:24:42,059
you know we've unblock times that

00:24:40,320 --> 00:24:44,279
sometimes librarians push time seconds

00:24:42,059 --> 00:24:45,779
people can now solve the actual problems

00:24:44,279 --> 00:24:48,509
they were trying to solve so that's

00:24:45,779 --> 00:24:50,370
still a wing and longer about scrape

00:24:48,509 --> 00:24:53,190
intervals well nothing's improved there

00:24:50,370 --> 00:24:59,100
unfortunately but you know some ideas

00:24:53,190 --> 00:25:02,879
maybe two might help in future so you've

00:24:59,100 --> 00:25:08,399
enjoyed this talk so far no one seems to

00:25:02,879 --> 00:25:11,519
fall asleep just actually maybe um he's

00:25:08,399 --> 00:25:12,750
moving out spun so what do you need to

00:25:11,519 --> 00:25:13,980
do to care about stealing was four to

00:25:12,750 --> 00:25:16,440
zero because as I said that is the

00:25:13,980 --> 00:25:18,809
second biggest change in communities to

00:25:16,440 --> 00:25:21,090
zero so the first thing is if you have

00:25:18,809 --> 00:25:22,980
hair trigger alerts on absent to need to

00:25:21,090 --> 00:25:25,649
add or clause extended by at least five

00:25:22,980 --> 00:25:27,960
minutes if you are federal aging

00:25:25,649 --> 00:25:31,110
instance level metrics switch to only

00:25:27,960 --> 00:25:33,330
aggregator metrics and otherwise hey

00:25:31,110 --> 00:25:37,379
enjoy the improved semantics and a

00:25:33,330 --> 00:25:39,690
pretty picture so in summary we have a

00:25:37,379 --> 00:25:41,460
special Nance Dale markers they indicate

00:25:39,690 --> 00:25:43,049
in Scottsdale we insert them when

00:25:41,460 --> 00:25:44,970
targets and time series go away it

00:25:43,049 --> 00:25:47,850
doesn't apply to everyone explicit time

00:25:44,970 --> 00:25:49,259
stamps and the fallback of corporal

00:25:47,850 --> 00:25:50,610
intervals is there and we still were

00:25:49,259 --> 00:25:53,809
five-minute look back it's not a

00:25:50,610 --> 00:25:55,769
different lame flag it's like query dots

00:25:53,809 --> 00:25:57,720
sub-query look back I think rather than

00:25:55,769 --> 00:26:02,789
and still Delta but that's what our

00:25:57,720 --> 00:26:04,470
works but a you're thinking you haven't

00:26:02,789 --> 00:26:08,759
used up your hour yet and it's like no

00:26:04,470 --> 00:26:10,679
no I haven't I have more for you if

00:26:08,759 --> 00:26:13,740
you're still awake if you still have a

00:26:10,679 --> 00:26:15,559
will to live I can do something about

00:26:13,740 --> 00:26:18,149
that

00:26:15,559 --> 00:26:20,309
so there's another big change and this

00:26:18,149 --> 00:26:21,750
is actually something we've fair that's

00:26:20,309 --> 00:26:23,519
already mentioned at the conference and

00:26:21,750 --> 00:26:25,350
that's the isolation feature not

00:26:23,519 --> 00:26:27,179
atomicity which is my fault for using

00:26:25,350 --> 00:26:28,280
the wrong name loading feature and

00:26:27,179 --> 00:26:30,350
research this so

00:26:28,280 --> 00:26:32,120
is a problem that we believe tree users

00:26:30,350 --> 00:26:34,550
have noticed so far we already had cloud

00:26:32,120 --> 00:26:35,990
for dimensioning yeah but it's actually

00:26:34,550 --> 00:26:37,970
quite a big problem it affects everyone

00:26:35,990 --> 00:26:39,440
most people don't realize it and it's a

00:26:37,970 --> 00:26:42,440
semantical issue that we need to kind of

00:26:39,440 --> 00:26:45,200
fix so what is isolation

00:26:42,440 --> 00:26:46,360
the answer is isolation is the I in acid

00:26:45,200 --> 00:26:50,150
[Music]

00:26:46,360 --> 00:26:52,490
so three T's to zero has the rest the

00:26:50,150 --> 00:26:54,290
atomicity is actually slightly debatable

00:26:52,490 --> 00:26:55,670
if you're using it incorrectly because

00:26:54,290 --> 00:26:58,880
of how we deal with things out of order

00:26:55,670 --> 00:27:01,070
and isolation means is if I'm halfway

00:26:58,880 --> 00:27:03,080
through a scrape and I run a query I

00:27:01,070 --> 00:27:05,120
don't see half of that scrape I just

00:27:03,080 --> 00:27:06,350
seen a previous script so you don't

00:27:05,120 --> 00:27:09,050
that's basically is your time in

00:27:06,350 --> 00:27:11,690
transactions so where this is normally

00:27:09,050 --> 00:27:13,130
causes problems is histograms because

00:27:11,690 --> 00:27:16,730
it's really important we see those the

00:27:13,130 --> 00:27:18,080
right way and because we have to see the

00:27:16,730 --> 00:27:19,670
whole thing at once but the problem

00:27:18,080 --> 00:27:20,810
isn't specific histograms it's just for

00:27:19,670 --> 00:27:22,670
users notice it first

00:27:20,810 --> 00:27:24,500
you can also happen it between any two

00:27:22,670 --> 00:27:26,830
different time series that you don't

00:27:24,500 --> 00:27:28,730
want to divide like a total and a failed

00:27:26,830 --> 00:27:30,970
so you can imagine one of them gets to

00:27:28,730 --> 00:27:34,480
that it's likely before the other or

00:27:30,970 --> 00:27:37,160
some weird stuff you can do what rules

00:27:34,480 --> 00:27:38,750
so this is an issue that's been around

00:27:37,160 --> 00:27:41,650
for a while I think we discussed you for

00:27:38,750 --> 00:27:44,000
a year and a half two years maybe and

00:27:41,650 --> 00:27:46,580
gets faced with other issues like

00:27:44,000 --> 00:27:48,800
partial scrapes that's where we're

00:27:46,580 --> 00:27:50,780
scraping something and it just stops

00:27:48,800 --> 00:27:52,340
sending stuff and we don't know that

00:27:50,780 --> 00:27:53,660
actually it was an interrupted

00:27:52,340 --> 00:27:55,460
connection rather than something else

00:27:53,660 --> 00:27:58,130
which is a different problem we'll solve

00:27:55,460 --> 00:27:59,810
in a different way so how do we solve

00:27:58,130 --> 00:28:01,880
this isolation issue well what we could

00:27:59,810 --> 00:28:04,370
do is track the oldest strip that's

00:28:01,880 --> 00:28:06,200
still happening and ignore all that

00:28:04,370 --> 00:28:09,680
after - we're just note that timestamp

00:28:06,200 --> 00:28:11,420
availabe good so I'm sure everyone here

00:28:09,680 --> 00:28:13,550
will be perfectly happy with minute old

00:28:11,420 --> 00:28:15,770
a fat tomb initial data juice or however

00:28:13,550 --> 00:28:17,390
long the longest grape was and that

00:28:15,770 --> 00:28:18,950
wouldn't support explicit timestamps

00:28:17,390 --> 00:28:20,510
which we kind of need the Federation

00:28:18,950 --> 00:28:23,060
which kind of comes up with the

00:28:20,510 --> 00:28:25,970
histograms near aggregating them up or

00:28:23,060 --> 00:28:27,980
we could add a watermark for targets and

00:28:25,970 --> 00:28:29,870
real and then link each to the time

00:28:27,980 --> 00:28:31,820
series back to that and just clip a

00:28:29,870 --> 00:28:33,890
 and this will be the way to solve

00:28:31,820 --> 00:28:35,870
this and three T's 1 0 because it has

00:28:33,890 --> 00:28:37,970
those internal data structures but with

00:28:35,870 --> 00:28:40,670
reduce to 0 the TS DB has no knowledge

00:28:37,970 --> 00:28:42,029
of targets or rules or anything it just

00:28:40,670 --> 00:28:44,729
has the interface to go to show

00:28:42,029 --> 00:28:46,590
just well we commit our batches with the

00:28:44,729 --> 00:28:48,690
appenders and that's about it

00:28:46,590 --> 00:28:49,889
there's no targets and this also

00:28:48,690 --> 00:28:53,999
wouldn't help but it's just the time

00:28:49,889 --> 00:28:55,440
stamps so how could you so much well the

00:28:53,999 --> 00:28:57,419
good news with previous to zero

00:28:55,440 --> 00:28:58,830
everything is committed as batches you

00:28:57,419 --> 00:29:01,139
know since the Commission roll back on

00:28:58,830 --> 00:29:02,639
the offenders and that's just the

00:29:01,139 --> 00:29:05,190
primitive we need to build the top of

00:29:02,639 --> 00:29:07,769
this so each appender which is a batch

00:29:05,190 --> 00:29:09,599
we do is we have a monophyletic 64-bit

00:29:07,769 --> 00:29:13,229
counter that is basically the

00:29:09,599 --> 00:29:15,119
transaction ID and we basically when

00:29:13,229 --> 00:29:16,529
we're writing a sample we have the time

00:29:15,119 --> 00:29:19,529
stamp we have the value and we have this

00:29:16,529 --> 00:29:21,779
ready for reading then we basically

00:29:19,529 --> 00:29:22,979
tract which appenders which transactions

00:29:21,779 --> 00:29:26,879
write transactions are currently in

00:29:22,979 --> 00:29:29,369
progress and we know basically already

00:29:26,879 --> 00:29:31,440
open depends what's the highest right ID

00:29:29,369 --> 00:29:33,299
would give an out and we just snapshot

00:29:31,440 --> 00:29:35,190
that when doing a read and ignore all

00:29:33,299 --> 00:29:39,450
the data which is related to an open

00:29:35,190 --> 00:29:42,059
read or after we start the query nice

00:29:39,450 --> 00:29:44,969
and simple right this is one of the ways

00:29:42,059 --> 00:29:46,440
to do isolation and so like this is a

00:29:44,969 --> 00:29:50,969
classic database problem this is the

00:29:46,440 --> 00:29:52,619
approach I took for Prometheus yes of

00:29:50,969 --> 00:29:54,029
course this is a classic database

00:29:52,619 --> 00:29:55,289
problem and it's not scripting about it

00:29:54,029 --> 00:29:56,509
and there's lots of written about it for

00:29:55,289 --> 00:29:58,739
a reason

00:29:56,509 --> 00:30:01,200
because you want to do this and you want

00:29:58,739 --> 00:30:02,940
to do it efficiently and the thing is

00:30:01,200 --> 00:30:04,739
that none of the storage engines of

00:30:02,940 --> 00:30:05,999
Prometheus whether B 1 B 2 or B tree

00:30:04,739 --> 00:30:07,529
which is 0-1

00:30:05,999 --> 00:30:10,139
none of these were designed with tree

00:30:07,529 --> 00:30:11,369
values all they have is well time stuff

00:30:10,139 --> 00:30:14,369
and a value they don't have a time sense

00:30:11,369 --> 00:30:16,679
of value in a writer ID and I'm sure you

00:30:14,369 --> 00:30:19,889
know Fabian we overjoyed if I suggested

00:30:16,679 --> 00:30:22,919
them the tearing apart just add this so

00:30:19,889 --> 00:30:24,239
I tore it apart a different way and so

00:30:22,919 --> 00:30:26,399
what we're gonna do well the thing is we

00:30:24,239 --> 00:30:28,019
could keep it entirely in memory because

00:30:26,399 --> 00:30:29,159
there's a write head log we don't

00:30:28,019 --> 00:30:31,200
actually need to persist this stuff

00:30:29,159 --> 00:30:34,229
after a crashed or whatnot because

00:30:31,200 --> 00:30:36,419
that's already atomic so that's fine and

00:30:34,229 --> 00:30:37,589
and we could also only keep the right of

00:30:36,419 --> 00:30:39,029
these we need we don't need to keep all

00:30:37,589 --> 00:30:40,229
of them from the entire life of the

00:30:39,029 --> 00:30:42,330
Prometheus because that would be

00:30:40,229 --> 00:30:43,019
infinite memory we just need enough to

00:30:42,330 --> 00:30:45,719
cover

00:30:43,019 --> 00:30:48,989
well what reads are in progress and what

00:30:45,719 --> 00:30:50,369
rights were in progress so when I

00:30:48,989 --> 00:30:53,609
started off doing this I kept it very

00:30:50,369 --> 00:30:54,960
simple and I did something very

00:30:53,609 --> 00:30:57,299
inefficient

00:30:54,960 --> 00:31:00,929
the right IDs were just a list which was

00:30:57,299 --> 00:31:02,399
appended to and kept forever which is

00:31:00,929 --> 00:31:05,639
very inefficient for a number of reasons

00:31:02,399 --> 00:31:07,529
and I also got it all working which

00:31:05,639 --> 00:31:08,879
involved lots of API changes apparently

00:31:07,529 --> 00:31:11,249
because it's all kind of hard code and

00:31:08,879 --> 00:31:13,109
what not and there was no separation

00:31:11,249 --> 00:31:14,429
between the internal the external API so

00:31:13,109 --> 00:31:17,070
that's kind of break those apart of it

00:31:14,429 --> 00:31:19,529
and I wrote unit tests and stress tests

00:31:17,070 --> 00:31:22,950
so I can tell is this code working as I

00:31:19,529 --> 00:31:24,869
kept after few days or a week what not I

00:31:22,950 --> 00:31:26,460
had this code all working and working

00:31:24,869 --> 00:31:27,929
with the stress test not spoken to

00:31:26,460 --> 00:31:30,570
previous at all just working with the

00:31:27,929 --> 00:31:32,580
stress test and okay now it's working

00:31:30,570 --> 00:31:34,589
now I need to make it you know not

00:31:32,580 --> 00:31:37,200
soccer performance wise because you saw

00:31:34,589 --> 00:31:38,570
those pretty graphs of all the work that

00:31:37,200 --> 00:31:40,499
Fabian is done improving performance

00:31:38,570 --> 00:31:43,589
he's not gonna let me get away with

00:31:40,499 --> 00:31:44,879
eating Lord resources back again so we

00:31:43,589 --> 00:31:46,889
need some better data structures than

00:31:44,879 --> 00:31:48,119
like a list which is appended to you

00:31:46,889 --> 00:31:51,960
which would cause amazing amounts of

00:31:48,119 --> 00:31:53,279
garbage so the track the open appenders

00:31:51,960 --> 00:31:55,349
like these instruments actually the

00:31:53,279 --> 00:31:56,039
right transactions in progress and we

00:31:55,349 --> 00:31:58,349
just use a map

00:31:56,039 --> 00:32:00,179
so when appender is created we add the

00:31:58,349 --> 00:32:02,519
ID to the map when it's committed roll

00:32:00,179 --> 00:32:04,830
back its real map maybe not the most

00:32:02,519 --> 00:32:06,239
efficient but you know it's what go

00:32:04,830 --> 00:32:09,989
provides it's going to be pretty cheap

00:32:06,239 --> 00:32:12,419
and there's a mutex on and the right ids

00:32:09,989 --> 00:32:14,070
well we need to store the right ids an

00:32:12,419 --> 00:32:16,139
association winter time series there is

00:32:14,070 --> 00:32:17,999
a structure for each series already in

00:32:16,139 --> 00:32:19,649
the database it already has like 20 30

00:32:17,999 --> 00:32:21,389
bytes worth of stuff in there

00:32:19,649 --> 00:32:23,969
and didn't like the last four samples

00:32:21,389 --> 00:32:26,549
for efficiency and so what we do is use

00:32:23,969 --> 00:32:28,679
a ring buffer and ring buffers or a

00:32:26,549 --> 00:32:30,539
circular buffers basically just track

00:32:28,679 --> 00:32:32,789
the start and end and you just move them

00:32:30,539 --> 00:32:34,559
along as you go around and the thing

00:32:32,789 --> 00:32:37,649
it's good news is it's constant time to

00:32:34,559 --> 00:32:38,789
add in Reuven elements and if you do

00:32:37,649 --> 00:32:40,049
need a bigger buffer because like

00:32:38,789 --> 00:32:41,279
there's queries in progress holding

00:32:40,049 --> 00:32:43,019
brighter IDs in place because you might

00:32:41,279 --> 00:32:45,419
eat them we just double size the buffer

00:32:43,019 --> 00:32:46,649
and copy two values over fortunately you

00:32:45,419 --> 00:32:48,539
have to hand with them at this because

00:32:46,649 --> 00:32:49,799
go doesn't have generics or any

00:32:48,539 --> 00:32:52,109
generation or whatnot and they didn't

00:32:49,799 --> 00:32:54,179
want to use the interface stuff and so

00:32:52,109 --> 00:32:56,519
have to hand code this but basically

00:32:54,179 --> 00:32:58,859
this ring buffer represents the right

00:32:56,519 --> 00:33:01,049
IDs these transaction IDs of the most

00:32:58,859 --> 00:33:02,519
researched and written samples we also

00:33:01,049 --> 00:33:04,799
kind of need to clean these up at some

00:33:02,519 --> 00:33:06,809
point so you could have a background

00:33:04,799 --> 00:33:07,950
thread going through all the time series

00:33:06,809 --> 00:33:08,510
and just leaking the stuff you don't

00:33:07,950 --> 00:33:13,309
need anymore

00:33:08,510 --> 00:33:15,320
and but that could take basically 20-30

00:33:13,309 --> 00:33:16,610
minutes maybe an hour to get true and

00:33:15,320 --> 00:33:20,000
considering the block is two hours

00:33:16,610 --> 00:33:22,250
that's not particularly useful so

00:33:20,000 --> 00:33:25,309
instead what I do is like that when we

00:33:22,250 --> 00:33:27,110
are doing the pinch the next event we

00:33:25,309 --> 00:33:29,179
pass by the way this is the right hand

00:33:27,110 --> 00:33:31,490
even clean up until so we basically are

00:33:29,179 --> 00:33:32,929
already looking at that a time series

00:33:31,490 --> 00:33:36,320
then we clean it up then so it's all

00:33:32,929 --> 00:33:37,970
done in line it does me know that if a

00:33:36,320 --> 00:33:39,500
time series you if it has right IDs and

00:33:37,970 --> 00:33:42,230
it's never written to again those lying

00:33:39,500 --> 00:33:43,400
around maybe a future optimization we

00:33:42,230 --> 00:33:45,710
maybe will have that background shred

00:33:43,400 --> 00:33:48,250
but no one's noticed yet probably

00:33:45,710 --> 00:33:51,080
because they only just learned about it

00:33:48,250 --> 00:33:52,549
so then there's the queriers so this is

00:33:51,080 --> 00:33:54,110
the object which performs queries

00:33:52,549 --> 00:33:57,049
because reads and writes are independent

00:33:54,110 --> 00:33:58,280
with this yes to me so what we do is we

00:33:57,049 --> 00:33:59,960
use a doubly linked list for the

00:33:58,280 --> 00:34:02,809
isolation station so the isolation

00:33:59,960 --> 00:34:05,410
stayed is the snapshot we take of the

00:34:02,809 --> 00:34:07,730
open appenders and the current readily

00:34:05,410 --> 00:34:10,280
so when running a query we snapchat

00:34:07,730 --> 00:34:11,869
these international stage and the thing

00:34:10,280 --> 00:34:13,850
is we keep a doubly linked list of all

00:34:11,869 --> 00:34:16,520
these isolation States and the reason we

00:34:13,850 --> 00:34:19,669
do this is so nice if it's causes doubly

00:34:16,520 --> 00:34:21,109
linked list the oldest one has you know

00:34:19,669 --> 00:34:24,200
the oldest right ID we need to keep

00:34:21,109 --> 00:34:25,879
around and then the other ones well they

00:34:24,200 --> 00:34:28,580
can just remove themselves from this

00:34:25,879 --> 00:34:30,109
doubly linked list pretty cheaply it's

00:34:28,580 --> 00:34:32,450
just updating to pointers so that's

00:34:30,109 --> 00:34:33,919
handy and yeah we want to do clean up

00:34:32,450 --> 00:34:35,990
and find out this right ID we can remove

00:34:33,919 --> 00:34:39,560
it's just a constant time lookup which

00:34:35,990 --> 00:34:41,270
is good it's a nice and efficient so

00:34:39,560 --> 00:34:43,669
after choosing those data structures

00:34:41,270 --> 00:34:45,350
what did prom bench say so like a prom

00:34:43,669 --> 00:34:47,929
bench working you know remove the hard

00:34:45,350 --> 00:34:50,090
code into core OS is DNS name and other

00:34:47,929 --> 00:34:53,419
things like that and the answer was it

00:34:50,090 --> 00:34:57,050
uses about five percent more CPU and 11

00:34:53,419 --> 00:34:58,670
percent more RAM and this is basically

00:34:57,050 --> 00:35:00,350
the numbers I was expecting just doing

00:34:58,670 --> 00:35:03,470
the math I expected about 10% round

00:35:00,350 --> 00:35:06,350
increase so I was right and my instinct

00:35:03,470 --> 00:35:11,630
told me it cost five ish percent CPU and

00:35:06,350 --> 00:35:13,430
it did so good good and and good news is

00:35:11,630 --> 00:35:14,750
though okay that's a bit of a hitch but

00:35:13,430 --> 00:35:16,340
this is still with all the other

00:35:14,750 --> 00:35:18,830
improvements far better in a previous

00:35:16,340 --> 00:35:21,170
one zero and I haven't done many micro

00:35:18,830 --> 00:35:22,339
optimizations yet but you know this is

00:35:21,170 --> 00:35:24,950
probably pretty close to what

00:35:22,339 --> 00:35:27,019
to optimal already so we might get that

00:35:24,950 --> 00:35:30,410
CPU down to 4% or something or maybe 10%

00:35:27,019 --> 00:35:31,940
RAM but I wouldn't expect more - but

00:35:30,410 --> 00:35:33,890
it's does show is pretty highly - you

00:35:31,940 --> 00:35:35,180
choose to write data structures because

00:35:33,890 --> 00:35:38,150
all of that came from just choosing to

00:35:35,180 --> 00:35:40,999
write down structures so this is not

00:35:38,150 --> 00:35:41,989
committed yet you can see the pull

00:35:40,999 --> 00:35:42,440
requests there it's been sitting there

00:35:41,989 --> 00:35:43,910
for a while

00:35:42,440 --> 00:35:45,049
I believe is currently conflicts because

00:35:43,910 --> 00:35:47,749
I haven't touched it for a month I

00:35:45,049 --> 00:35:50,329
believe that that is to incorporate this

00:35:47,749 --> 00:35:52,729
intermediate stew once rather than

00:35:50,329 --> 00:35:53,420
holding back to zero and some of the

00:35:52,729 --> 00:35:54,920
things right now

00:35:53,420 --> 00:35:57,890
isolation is only done for a single

00:35:54,920 --> 00:36:00,349
query which in prom ql terms is a single

00:35:57,890 --> 00:36:01,519
selector so that means we do have a

00:36:00,349 --> 00:36:03,880
prompt to our expression

00:36:01,519 --> 00:36:06,619
different selectors will have different

00:36:03,880 --> 00:36:08,900
snapshots of the data that's not great

00:36:06,619 --> 00:36:10,489
so this whole isolation thing needs to

00:36:08,900 --> 00:36:13,009
be exposed all the way up to the ets

00:36:10,489 --> 00:36:14,690
db-api just so we can have the same

00:36:13,009 --> 00:36:18,229
steps but for the entirety of a single

00:36:14,690 --> 00:36:19,849
prompt throughout expressions and also

00:36:18,229 --> 00:36:22,640
on the right side currently an

00:36:19,849 --> 00:36:25,609
implementation is that the scrape data

00:36:22,640 --> 00:36:28,039
and then up and the other scrape stats

00:36:25,609 --> 00:36:29,930
are done via separate appenders so when

00:36:28,039 --> 00:36:31,519
we want to unify those so basically that

00:36:29,930 --> 00:36:33,859
you'll see up at the same snapshot at

00:36:31,519 --> 00:36:36,469
the ingestion data that will be a little

00:36:33,859 --> 00:36:40,039
trickier to do the other one is fairly

00:36:36,469 --> 00:36:41,930
roshan and so to summarize this we just

00:36:40,039 --> 00:36:44,329
really don't want to see partial scrapes

00:36:41,930 --> 00:36:47,029
partial evaluating and repairing them we

00:36:44,329 --> 00:36:49,849
makes me want to tract in which commish

00:36:47,029 --> 00:36:51,559
all the writes came from and/or a query

00:36:49,849 --> 00:36:54,009
we ignore samples rights which are still

00:36:51,559 --> 00:36:55,999
in progress or after whom we start query

00:36:54,009 --> 00:36:58,190
using the right data structures is

00:36:55,999 --> 00:37:00,349
really important but even if they do is

00:36:58,190 --> 00:37:02,150
implemented by hand and this is coming

00:37:00,349 --> 00:37:05,799
soon to a free case near you which will

00:37:02,150 --> 00:37:08,089
make at least half they're happy and so

00:37:05,799 --> 00:37:09,650
so to say though there's the blog we've

00:37:08,089 --> 00:37:11,719
launched rain there last week you can

00:37:09,650 --> 00:37:13,849
use the code munich for 10% off it's

00:37:11,719 --> 00:37:17,680
like 20% off and any questions in

00:37:13,849 --> 00:37:17,680
general so questions

00:37:21,700 --> 00:37:25,510
[Applause]

00:37:25,609 --> 00:37:36,690
so regarding time stamps I'm who

00:37:29,700 --> 00:37:40,440
confused than ever but as far as I

00:37:36,690 --> 00:37:44,789
remember them time stamp underscore me a

00:37:40,440 --> 00:37:46,770
so it's still in the brothel and what

00:37:44,789 --> 00:37:50,069
the part is case when people want to

00:37:46,770 --> 00:37:53,730
import the historical data using push

00:37:50,069 --> 00:37:56,339
gateway you know they they you always

00:37:53,730 --> 00:37:58,410
want to have an time stamp here yeah and

00:37:56,339 --> 00:37:59,579
they could and it's now very very clear

00:37:58,410 --> 00:38:01,260
from the error mastered the push game

00:37:59,579 --> 00:38:04,109
either you can't do - because that was

00:38:01,260 --> 00:38:05,640
never going to work it's not okay and

00:38:04,109 --> 00:38:08,520
it's not even theoretically possible

00:38:05,640 --> 00:38:12,720
anymore because I stopped that working

00:38:08,520 --> 00:38:17,309
explicitly thank you the good news is

00:38:12,720 --> 00:38:18,690
though that with the TS TV 2.0 at least

00:38:17,309 --> 00:38:20,670
the priests to zero there's going to be

00:38:18,690 --> 00:38:22,109
an API for actual batch inserts it

00:38:20,670 --> 00:38:25,049
hasn't been implemented yet but we all

00:38:22,109 --> 00:38:26,609
agreed would happen and and then that's

00:38:25,049 --> 00:38:28,020
a proper way to actually ingest these

00:38:26,609 --> 00:38:28,500
things rather than doing hacks with

00:38:28,020 --> 00:38:31,950
timestamps

00:38:28,500 --> 00:38:34,650
I mean it's legitimate use case right

00:38:31,950 --> 00:38:38,760
when people want to import the old data

00:38:34,650 --> 00:38:40,470
and preserve timestamps yeah so it's

00:38:38,760 --> 00:38:42,089
something that will happen but you know

00:38:40,470 --> 00:38:44,160
whenever someone gets around too much

00:38:42,089 --> 00:38:45,270
but it's not going to happen by a push

00:38:44,160 --> 00:38:52,160
gateway because that's not what the push

00:38:45,270 --> 00:38:58,049
gateway is for haha okay great thank you

00:38:52,160 --> 00:39:01,829
it's interesting why do you use this

00:38:58,049 --> 00:39:05,279
very quite quick hacky approach with all

00:39:01,829 --> 00:39:09,930
these markers and mantissa beats and not

00:39:05,279 --> 00:39:13,529
just as explicitly attributes or Flags

00:39:09,930 --> 00:39:15,599
maybe because you know there are so many

00:39:13,529 --> 00:39:18,210
libraries that doesn't care about these

00:39:15,599 --> 00:39:21,839
beats and for example protobufs will not

00:39:18,210 --> 00:39:23,819
restore them if you just I would hope

00:39:21,839 --> 00:39:26,970
that they restored them because it's

00:39:23,819 --> 00:39:28,020
required by I Triple E 754 I think

00:39:26,970 --> 00:39:30,299
that's what the answer I thought they

00:39:28,020 --> 00:39:32,430
asked and go like nuts like hey can I

00:39:30,299 --> 00:39:33,690
like abuse man's like this and so

00:39:32,430 --> 00:39:38,160
said oh yeah it's totally designed for

00:39:33,690 --> 00:39:39,150
that it's like awesome and so yeah so we

00:39:38,160 --> 00:39:41,430
didn't for me to use this all works

00:39:39,150 --> 00:39:42,480
about unit tests and so on and the one

00:39:41,430 --> 00:39:43,380
place to be careful with is remote

00:39:42,480 --> 00:39:44,460
reading right to make sure you're

00:39:43,380 --> 00:39:46,650
preserving it in the rest of your system

00:39:44,460 --> 00:39:48,480
this should all work perfectly if it

00:39:46,650 --> 00:39:51,930
doesn't we'll just switch already

00:39:48,480 --> 00:39:54,420
extremely api's to be you in 64 and make

00:39:51,930 --> 00:39:55,980
it the problem of the other engine which

00:39:54,420 --> 00:39:58,050
we hopefully will not have to do but

00:39:55,980 --> 00:40:00,000
that would be an option in the worst

00:39:58,050 --> 00:40:06,330
case if it turns out protobuf isn't you

00:40:00,000 --> 00:40:12,000
know float64 clean so go back to time

00:40:06,330 --> 00:40:14,300
stands so for the non push gateway stuff

00:40:12,000 --> 00:40:16,980
should they now work so for things like

00:40:14,300 --> 00:40:18,960
the cowboy exporter which is passing

00:40:16,980 --> 00:40:21,150
through data that happened a little

00:40:18,960 --> 00:40:23,640
while ago yeah so i haven't implemented

00:40:21,150 --> 00:40:26,400
it yet but the plan is so if you don't

00:40:23,640 --> 00:40:27,630
know cloud watch it can take up to about

00:40:26,400 --> 00:40:29,610
10 minutes for the data close to

00:40:27,630 --> 00:40:30,930
converge because that how it works i

00:40:29,610 --> 00:40:31,650
considering per medias has always been

00:40:30,930 --> 00:40:33,750
append-only

00:40:31,650 --> 00:40:35,550
we can't take the unconverted values

00:40:33,750 --> 00:40:37,020
because we can't fix them so the

00:40:35,550 --> 00:40:39,540
approach is we request that at 10

00:40:37,020 --> 00:40:41,010
minutes ago and yes that is now because

00:40:39,540 --> 00:40:42,120
the time sells i supportive comfort by

00:40:41,010 --> 00:40:44,610
any client library because i was

00:40:42,120 --> 00:40:46,650
blocking it so now that that's unlocked

00:40:44,610 --> 00:40:48,570
when I get around to adding timestamp

00:40:46,650 --> 00:40:50,190
support to the Java client it will be

00:40:48,570 --> 00:40:51,810
for the purposes of doing that for a

00:40:50,190 --> 00:40:53,880
time watching sport and then it will

00:40:51,810 --> 00:40:56,040
show you data from 10 minutes ago and

00:40:53,880 --> 00:40:57,870
the news tailless panics won't apply

00:40:56,040 --> 00:40:59,550
because there's no timestamps Amish I

00:40:57,870 --> 00:41:00,840
didn't ask to care for the grass and so

00:40:59,550 --> 00:41:02,490
on but hey it would be the actual

00:41:00,840 --> 00:41:04,860
timestamp riving you'll remember that

00:41:02,490 --> 00:41:07,290
it's 10 minutes offset and there's other

00:41:04,860 --> 00:41:09,420
places as well so we've got the in folks

00:41:07,290 --> 00:41:11,340
to be exporter select the exporter and

00:41:09,420 --> 00:41:13,260
graphite exporter which also have time

00:41:11,340 --> 00:41:15,120
stamps and there's one or two other

00:41:13,260 --> 00:41:17,100
cases like that where there is another

00:41:15,120 --> 00:41:18,690
monitoring system which is usually

00:41:17,100 --> 00:41:20,220
pushed based or just has timestamps in

00:41:18,690 --> 00:41:22,200
there that even Federation is kind of

00:41:20,220 --> 00:41:23,820
this case and that's the case where you

00:41:22,200 --> 00:41:25,770
want to using timestamps just because

00:41:23,820 --> 00:41:27,720
it's another monitoring system that has

00:41:25,770 --> 00:41:33,540
timestamps but that's about all it's

00:41:27,720 --> 00:41:35,630
really for can you say more about the

00:41:33,540 --> 00:41:38,120
appropriate use cases for Federation

00:41:35,630 --> 00:41:40,230
specifically in the cross service

00:41:38,120 --> 00:41:42,060
Federation case where you have one

00:41:40,230 --> 00:41:44,250
Prometheus server that's evaluating

00:41:42,060 --> 00:41:46,100
alerts that depend maybe on instance

00:41:44,250 --> 00:41:50,250
level metrics collected by a sec

00:41:46,100 --> 00:41:52,350
and so that shouldn't happen so if you

00:41:50,250 --> 00:41:53,940
have instance deserts like my target is

00:41:52,350 --> 00:41:55,710
down you always want to push down your

00:41:53,940 --> 00:41:59,400
alerts as far down the hierarchy as you

00:41:55,710 --> 00:42:01,500
can because that's if you were points of

00:41:59,400 --> 00:42:02,310
failure and it's gonna happen faster

00:42:01,500 --> 00:42:04,890
because you'll have to wait for next

00:42:02,310 --> 00:42:06,030
extra Federation scraper so if you want

00:42:04,890 --> 00:42:07,020
to alert on things are down you're going

00:42:06,030 --> 00:42:09,480
to do without the instance devil

00:42:07,020 --> 00:42:11,010
normally when you have cross service

00:42:09,480 --> 00:42:12,420
alerts like you're put you like you of a

00:42:11,010 --> 00:42:13,920
database team and you want to just pull

00:42:12,420 --> 00:42:15,570
their metrics in for graphs or something

00:42:13,920 --> 00:42:17,240
or to notice that they're already paged

00:42:15,570 --> 00:42:18,840
for something or other things like that

00:42:17,240 --> 00:42:20,730
normally you're not going to be putting

00:42:18,840 --> 00:42:22,620
instance level data it's going to be a

00:42:20,730 --> 00:42:23,790
job level attribute or a cluster level

00:42:22,620 --> 00:42:25,980
attribute or something like that

00:42:23,790 --> 00:42:28,050
so this doesn't come up like this will

00:42:25,980 --> 00:42:30,510
still work it just might not have the

00:42:28,050 --> 00:42:32,760
improves daily semantics is it fair to

00:42:30,510 --> 00:42:34,380
say that instance level metrics in all

00:42:32,760 --> 00:42:37,020
cases should never be federated that's

00:42:34,380 --> 00:42:39,720
not completely true and sometimes you

00:42:37,020 --> 00:42:41,910
want to ingest like one or two time

00:42:39,720 --> 00:42:44,190
series just to know basically where the

00:42:41,910 --> 00:42:45,540
instances are so you can imagine if you

00:42:44,190 --> 00:42:47,940
were to shard it set up and you had four

00:42:45,540 --> 00:42:49,770
shards and they're basically randomly

00:42:47,940 --> 00:42:52,110
distributed no it's not actually random

00:42:49,770 --> 00:42:53,760
and if you want to from your top level

00:42:52,110 --> 00:42:57,500
master tell which slave has the

00:42:53,760 --> 00:43:00,480
information well if you make it if you

00:42:57,500 --> 00:43:01,830
Feder a tour or use from a read or

00:43:00,480 --> 00:43:03,660
something to tell you hey it's this one

00:43:01,830 --> 00:43:05,640
here has it because you've just ingested

00:43:03,660 --> 00:43:07,170
like the optometric then you can tie

00:43:05,640 --> 00:43:09,390
that back and use that and consoles or

00:43:07,170 --> 00:43:11,610
so on so it's very use cases where you

00:43:09,390 --> 00:43:13,410
do want instance level to come up but

00:43:11,610 --> 00:43:14,910
you don't want all the instance just

00:43:13,410 --> 00:43:16,800
like one or two metrics so I think that

00:43:14,910 --> 00:43:18,690
was the use case now for has they were

00:43:16,800 --> 00:43:19,860
just putting up up for everything which

00:43:18,690 --> 00:43:22,890
I presume they were using for something

00:43:19,860 --> 00:43:24,990
like that thank you

00:43:22,890 --> 00:43:29,010
and sometimes it's handy for graphing as

00:43:24,990 --> 00:43:31,170
well but you know so hi yeah I'm quite

00:43:29,010 --> 00:43:33,210
surprised at 11% increase because you're

00:43:31,170 --> 00:43:34,560
just storing transaction IDs and there's

00:43:33,210 --> 00:43:36,930
like a whole bunch of like you're

00:43:34,560 --> 00:43:39,300
storing actual values and actual samples

00:43:36,930 --> 00:43:41,370
and why do you think that 11 person and

00:43:39,300 --> 00:43:43,680
quite II think that's optimal it doesn't

00:43:41,370 --> 00:43:45,870
look optimal whenever I did the math of

00:43:43,680 --> 00:43:48,360
just write odds cost of time-series

00:43:45,870 --> 00:43:50,220
positive cost this buffer and thing is

00:43:48,360 --> 00:43:51,620
that it's going to be because the

00:43:50,220 --> 00:43:54,300
minimum size is 4 bytes

00:43:51,620 --> 00:43:56,490
sorry for entries each of which is 8

00:43:54,300 --> 00:43:59,220
bytes because it's 64 bits and then you

00:43:56,490 --> 00:43:59,829
have your counters as well 2 or 3 in on

00:43:59,220 --> 00:44:03,579
top

00:43:59,829 --> 00:44:05,259
so that comes up to what's at 64 you do

00:44:03,579 --> 00:44:07,719
the math anyway and look at the size as

00:44:05,259 --> 00:44:09,430
well of the amount of utter - it has to

00:44:07,719 --> 00:44:10,839
be stored and it worked out as about 10

00:44:09,430 --> 00:44:12,880
percentage was what I was predicting

00:44:10,839 --> 00:44:15,160
especially when queries are holding

00:44:12,880 --> 00:44:16,779
right headies present because a query is

00:44:15,160 --> 00:44:18,430
referencing you know if gree takes 30

00:44:16,779 --> 00:44:20,499
seconds it means you need to buffer up

00:44:18,430 --> 00:44:21,759
30 seconds with the right IDs now

00:44:20,499 --> 00:44:23,799
there's other approaches we could take

00:44:21,759 --> 00:44:25,839
but when I did the math the number came

00:44:23,799 --> 00:44:27,519
out just look really this is going to

00:44:25,839 --> 00:44:29,670
take ten percent of RAM and that's what

00:44:27,519 --> 00:44:31,989
the number came out as a patronage

00:44:29,670 --> 00:44:33,069
so I'm not surprised with the number if

00:44:31,989 --> 00:44:36,189
we're gonna prove that'll be great

00:44:33,069 --> 00:44:40,180
so one of the things as was in your talk

00:44:36,189 --> 00:44:41,709
is that we iterate over the series set

00:44:40,180 --> 00:44:44,739
like the list of series returned from

00:44:41,709 --> 00:44:46,689
the posting indexes and then we get the

00:44:44,739 --> 00:44:48,789
data from each of them if we could

00:44:46,689 --> 00:44:50,559
instead get all those indexes and figure

00:44:48,789 --> 00:44:54,189
out all the staleness marker stuff so

00:44:50,559 --> 00:44:55,569
the isolation stuff and then we wouldn't

00:44:54,189 --> 00:44:57,459
have to keep all these write IDs around

00:44:55,569 --> 00:44:58,719
we don't need to keep enough for actual

00:44:57,459 --> 00:45:00,880
rights which take like a few

00:44:58,719 --> 00:45:03,910
milliseconds rather than potentially the

00:45:00,880 --> 00:45:06,160
whole query but it now means that we've

00:45:03,910 --> 00:45:09,039
saved all this RAM on rice but we now

00:45:06,160 --> 00:45:10,599
cost more RAM on Reed so maybe that's a

00:45:09,039 --> 00:45:14,920
better approach maybe it's not and half

00:45:10,599 --> 00:45:16,869
the investigation in the valuation hi-c

00:45:14,920 --> 00:45:19,089
consider that we have a requirement so

00:45:16,869 --> 00:45:21,459
that a time series is very random in

00:45:19,089 --> 00:45:23,109
nature maybe today we inserted a time

00:45:21,459 --> 00:45:27,509
series 10 tomorrow we need to insert the

00:45:23,109 --> 00:45:30,789
same time sorry as an example so in 2.0

00:45:27,509 --> 00:45:32,979
we need to insert as in a client or

00:45:30,789 --> 00:45:35,469
which producing the matrix to insert

00:45:32,979 --> 00:45:37,449
this stainless Marcos or it will work

00:45:35,469 --> 00:45:41,199
automatically this is all internal super

00:45:37,449 --> 00:45:43,689
Mateus ok so it means if if I'm just

00:45:41,199 --> 00:45:44,289
giving examples it today I inserted a

00:45:43,689 --> 00:45:46,989
metric

00:45:44,289 --> 00:45:48,849
sorry a time series and tomorrow I will

00:45:46,989 --> 00:45:50,920
insert one more time series the same

00:45:48,849 --> 00:45:52,869
time series better to say if I will draw

00:45:50,920 --> 00:45:56,109
the graph then I will only get two data

00:45:52,869 --> 00:45:58,239
points yeah if you if you are using

00:45:56,109 --> 00:46:00,640
query to caches you will see those two

00:45:58,239 --> 00:46:03,219
query points very range you will

00:46:00,640 --> 00:46:05,709
probably see two as well yeah that my my

00:46:03,219 --> 00:46:07,779
question is more on range definitely

00:46:05,709 --> 00:46:09,759
because if I will query for say two days

00:46:07,779 --> 00:46:12,740
data it will only return two data points

00:46:09,759 --> 00:46:14,540
yeah if you're using the query and doing

00:46:12,740 --> 00:46:16,250
director look a perfect perfect so it

00:46:14,540 --> 00:46:18,380
only returns to data point because we're

00:46:16,250 --> 00:46:20,750
actually it while returning correct it

00:46:18,380 --> 00:46:23,240
it will return maybe ten data points or

00:46:20,750 --> 00:46:24,800
some more data points but well that's if

00:46:23,240 --> 00:46:27,650
you're using query and asked for a range

00:46:24,800 --> 00:46:29,840
vector if you're using query range and

00:46:27,650 --> 00:46:31,610
asking for an instant vector and there's

00:46:29,840 --> 00:46:33,320
only one data point because you're

00:46:31,610 --> 00:46:35,270
somehow pushing down into Prometheus it

00:46:33,320 --> 00:46:37,520
will still show for five minutes because

00:46:35,270 --> 00:46:40,280
we don't one data point we can't infer

00:46:37,520 --> 00:46:41,600
the interval so the existing logic of

00:46:40,280 --> 00:46:44,570
hanging around for five minutes will

00:46:41,600 --> 00:46:47,210
remain even in case of a state vector of

00:46:44,570 --> 00:46:48,110
sorry stain marker oh you're saying you

00:46:47,210 --> 00:46:49,910
inserted the data

00:46:48,110 --> 00:46:51,830
tenzin exactly like if it was a scrape

00:46:49,910 --> 00:46:53,840
and for me just ruining the target went

00:46:51,830 --> 00:46:55,820
away the time series go away that the

00:46:53,840 --> 00:46:57,560
stale marker will be ingested and it

00:46:55,820 --> 00:46:59,990
only that one point will be used right

00:46:57,560 --> 00:47:01,460
but potentially if you've rearranged

00:46:59,990 --> 00:47:02,740
with a steps one or done interval you

00:47:01,460 --> 00:47:05,960
are still going to get multiple results

00:47:02,740 --> 00:47:07,730
okay because that's how it works because

00:47:05,960 --> 00:47:09,830
it's still stale until that stale marker

00:47:07,730 --> 00:47:11,090
happens so if you have a step of one

00:47:09,830 --> 00:47:13,730
second you're going to get that ten

00:47:11,090 --> 00:47:16,820
times because it takes ten seconds for

00:47:13,730 --> 00:47:19,369
that stale marker to appear can you go

00:47:16,820 --> 00:47:26,420
back to the to do slide for the

00:47:19,369 --> 00:47:28,280
isolation stuff so like this cross Korea

00:47:26,420 --> 00:47:31,580
isolation and you said that we just have

00:47:28,280 --> 00:47:36,380
one query upper labor selector we just

00:47:31,580 --> 00:47:39,109
have one Korea per query right is that

00:47:36,380 --> 00:47:41,990
is that work with offsets yeah we just

00:47:39,109 --> 00:47:43,670
like select the entire range that case

00:47:41,990 --> 00:47:45,619
we can skip - and if we ever optimize

00:47:43,670 --> 00:47:47,540
offsets later we can change it because

00:47:45,619 --> 00:47:49,700
otherwise like the results will be quite

00:47:47,540 --> 00:47:52,100
funky because at least have like

00:47:49,700 --> 00:47:53,900
per Korea series creation isolation

00:47:52,100 --> 00:47:54,830
already and otherwise it would get like

00:47:53,900 --> 00:47:56,030
different serious

00:47:54,830 --> 00:47:58,190
depending on the neck when the car is

00:47:56,030 --> 00:48:00,680
actually executed okay yeah I didn't

00:47:58,190 --> 00:48:02,210
actually close but yeah so that means we

00:48:00,680 --> 00:48:03,020
can skip that but maybe optimization

00:48:02,210 --> 00:48:05,390
will have in the future

00:48:03,020 --> 00:48:08,570
cuz I know and remote redoes not have

00:48:05,390 --> 00:48:09,950
that optimization it does have that

00:48:08,570 --> 00:48:12,410
optimization where it'll explicitly ask

00:48:09,950 --> 00:48:14,119
us so the offset feature means that sure

00:48:12,410 --> 00:48:15,800
I'm asking for an hour to go to two

00:48:14,119 --> 00:48:18,619
hours ago but some of that could be

00:48:15,800 --> 00:48:20,180
offset by a day so what Bobby is saying

00:48:18,619 --> 00:48:23,060
is implemented will always ask for 25

00:48:20,180 --> 00:48:24,950
hours of data I presumed it worked the

00:48:23,060 --> 00:48:26,580
other way but that means we don't have

00:48:24,950 --> 00:48:31,020
to worry about this right now

00:48:26,580 --> 00:48:32,250
until we optimize that later on the

00:48:31,020 --> 00:48:35,430
second point we should also do at some

00:48:32,250 --> 00:48:38,100
point but it's not as urgent hey

00:48:35,430 --> 00:48:42,000
so there are a couple of times in the

00:48:38,100 --> 00:48:42,990
stillness times in sternness bit where I

00:48:42,000 --> 00:48:45,240
would have been useful for you to be

00:48:42,990 --> 00:48:48,170
able to write in the past would it not

00:48:45,240 --> 00:48:50,460
be possible for the STB to support

00:48:48,170 --> 00:48:52,500
bounded writes into the past if it would

00:48:50,460 --> 00:48:55,200
say keep two chunks in memory before

00:48:52,500 --> 00:48:56,880
persisting them to disk it already keeps

00:48:55,200 --> 00:48:59,280
two blocks in memory for persisting the

00:48:56,880 --> 00:49:00,870
disk so King is the compression is part

00:48:59,280 --> 00:49:03,060
of it because we'd have to go back and

00:49:00,870 --> 00:49:05,130
redo all the progression for - okay cool

00:49:03,060 --> 00:49:07,260
yeah it's just generally you can make

00:49:05,130 --> 00:49:10,110
that work we've basically just decided

00:49:07,260 --> 00:49:12,840
not to and it's usually not a problem

00:49:10,110 --> 00:49:14,580
guys already comes over places like real

00:49:12,840 --> 00:49:16,740
rice on the other end for a site coming

00:49:14,580 --> 00:49:18,120
up that an order piece because then

00:49:16,740 --> 00:49:21,330
you'd normally have to start a new chunk

00:49:18,120 --> 00:49:22,650
merged those two chunks together yeah so

00:49:21,330 --> 00:49:32,400
it's not a problem for me as it comes up

00:49:22,650 --> 00:49:34,620
more elsewhere any other questions well

00:49:32,400 --> 00:49:38,070
thanks for taking us on that wild ride I

00:49:34,620 --> 00:49:40,680
have to admit I only understand 50% of

00:49:38,070 --> 00:49:43,400
all that but it's pretty amazing yeah

00:49:40,680 --> 00:49:43,400
thank you

00:49:45,140 --> 00:49:57,840
[Music]

00:49:57,650 --> 00:50:00,979
you

00:49:57,840 --> 00:50:00,979

YouTube URL: https://www.youtube.com/watch?v=GcTzd2CLH7I


