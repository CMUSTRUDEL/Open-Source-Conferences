Title: PromCon 2017: Start Your Engines: White Box Monitoring for Your Load Tests - Alexander Schwartz
Publication date: 2017-09-01
Playlist: PromCon 2017
Description: 
	* Abstract:

You think monitoring is only for production? Wrong: Add a metrics endpoint to your application to get insights during your load tests - and use them for free to monitor production!

This talk shows how to setup up the load testing tools JMeter and Gatling to push their metrics to Prometheus. It also makes the case to expose metrics as part of core application development instead of treating them as a small add-on before go-live.

* Speaker biography:

Alexander Schwartz is Principal IT Consultant at msg systems. Heâ€™s been in Web development for more than 15 years and enjoys productive working environments, agile projects and automated tests. At conferences and user group meetings he talks about the things he is passionate about.

* Slides:

https://promcon.io/2017-munich/slides/start-your-engines-white-box-monitoring-for-load-tests.pdf

* PromCon website:

https://promcon.io/
Captions: 
	00:00:00,380 --> 00:00:15,929
[Music]

00:00:12,920 --> 00:00:18,090
right so welcome to the talk start your

00:00:15,929 --> 00:00:21,869
engines white box monitoring for your

00:00:18,090 --> 00:00:24,859
love tests so the talk yet before was

00:00:21,869 --> 00:00:27,330
about how to monitor protection and

00:00:24,859 --> 00:00:29,730
maybe your tools already have metrics

00:00:27,330 --> 00:00:31,890
that you can expose but now we want to

00:00:29,730 --> 00:00:33,930
look at how you actually get to your

00:00:31,890 --> 00:00:36,719
metrics in your product maybe to get

00:00:33,930 --> 00:00:39,149
your developers to instrument your the

00:00:36,719 --> 00:00:42,270
application so this is the agenda for

00:00:39,149 --> 00:00:44,399
the next like 20 minutes why to use load

00:00:42,270 --> 00:00:46,260
testing in the first place how to set up

00:00:44,399 --> 00:00:49,770
an environment with Prometheus for your

00:00:46,260 --> 00:00:51,600
load tests a short demo that will might

00:00:49,770 --> 00:00:54,870
probably fall so to to just to be sort

00:00:51,600 --> 00:00:58,199
of the machine and then what to expect

00:00:54,870 --> 00:01:00,510
what worked good for us what could have

00:00:58,199 --> 00:01:03,149
been improved and things to watch out

00:01:00,510 --> 00:01:06,600
for you could try something like this at

00:01:03,149 --> 00:01:08,790
work so I'm going to use low test well

00:01:06,600 --> 00:01:10,710
naive world we have a developer that

00:01:08,790 --> 00:01:14,070
develops an application on his or her

00:01:10,710 --> 00:01:16,049
machine and what is then being tested by

00:01:14,070 --> 00:01:17,970
another human on his or her machine and

00:01:16,049 --> 00:01:20,540
once you're in production you have lots

00:01:17,970 --> 00:01:23,340
of users and what Daniel II happens

00:01:20,540 --> 00:01:25,409
something that you said the catches fire

00:01:23,340 --> 00:01:27,509
because all the loads of the users and

00:01:25,409 --> 00:01:29,970
you find out well what could have we

00:01:27,509 --> 00:01:32,369
done different maybe we could have used

00:01:29,970 --> 00:01:34,530
some metrics in here but obviously in

00:01:32,369 --> 00:01:36,299
the first place you didn't put in any

00:01:34,530 --> 00:01:38,880
metrics and you're lost in production

00:01:36,299 --> 00:01:43,290
right so let's do it better

00:01:38,880 --> 00:01:45,060
so let's again have a developer at his

00:01:43,290 --> 00:01:47,399
or her machine developer software they

00:01:45,060 --> 00:01:50,189
want to deploy do the testing with some

00:01:47,399 --> 00:01:52,439
no testing tools with scaling in jmeter

00:01:50,189 --> 00:01:57,630
and then make your server burn up in

00:01:52,439 --> 00:01:59,280
flames and well at that point make the

00:01:57,630 --> 00:02:01,979
developers add some metrics to your

00:01:59,280 --> 00:02:05,159
software just the metrics they need to

00:02:01,979 --> 00:02:07,229
find out why it's burning down and as if

00:02:05,159 --> 00:02:09,750
they have added enough metrics to get

00:02:07,229 --> 00:02:12,540
the insights they need to pass the low

00:02:09,750 --> 00:02:15,930
tests only then go into production

00:02:12,540 --> 00:02:17,640
and well as you have already put in all

00:02:15,930 --> 00:02:19,770
the white box metrics into the

00:02:17,640 --> 00:02:23,670
application you can use the same metrics

00:02:19,770 --> 00:02:26,220
in production that's the basic idea

00:02:23,670 --> 00:02:27,960
in test environments you might have a

00:02:26,220 --> 00:02:30,630
short retention time and might also

00:02:27,960 --> 00:02:32,600
scrape every second in production you

00:02:30,630 --> 00:02:35,040
might have like 15 days we heard and

00:02:32,600 --> 00:02:38,160
scrape for every 30 seconds every 60

00:02:35,040 --> 00:02:39,570
seconds what fits you best and we tried

00:02:38,160 --> 00:02:41,210
that approach and let's see what comes

00:02:39,570 --> 00:02:46,890
out of them

00:02:41,210 --> 00:02:50,640
so then how to set up an environment for

00:02:46,890 --> 00:02:53,130
this the technical building blocks that

00:02:50,640 --> 00:02:55,290
we've seen here well there's the

00:02:53,130 --> 00:02:57,660
application under test well we are

00:02:55,290 --> 00:02:59,600
testing Java applications so we put in

00:02:57,660 --> 00:03:02,520
the driver simple client in our

00:02:59,600 --> 00:03:05,040
applications and the no testing tool

00:03:02,520 --> 00:03:06,660
either getting or jmeter both of these

00:03:05,040 --> 00:03:08,250
building blocks they are delivered by

00:03:06,660 --> 00:03:11,370
the product team consisting of

00:03:08,250 --> 00:03:14,550
developers consisting of testers and

00:03:11,370 --> 00:03:15,570
business members and they make up their

00:03:14,550 --> 00:03:18,420
mind how they want to test this

00:03:15,570 --> 00:03:20,100
application but mother needs to have

00:03:18,420 --> 00:03:22,500
some bits and pieces of infrastructure

00:03:20,100 --> 00:03:24,510
in the test environment so what we set

00:03:22,500 --> 00:03:27,000
up for them is a premises server

00:03:24,510 --> 00:03:29,130
you see advisor so we get some metrics

00:03:27,000 --> 00:03:32,100
about the containers they are running at

00:03:29,130 --> 00:03:35,520
least RAM usage CPU usage the things

00:03:32,100 --> 00:03:39,560
that were most interested in and well we

00:03:35,520 --> 00:03:42,540
also spun up a graphite exporter so

00:03:39,560 --> 00:03:44,220
jisub and then it's up to them to link

00:03:42,540 --> 00:03:47,100
their load testing to the graphic

00:03:44,220 --> 00:03:49,980
exporter and also up to them to make

00:03:47,100 --> 00:03:51,510
their app discoverable you might have in

00:03:49,980 --> 00:03:53,880
our test environment we had a simple

00:03:51,510 --> 00:03:57,510
firebase discovery mechanism so the

00:03:53,880 --> 00:04:00,030
developers enter their applications they

00:03:57,510 --> 00:04:02,030
want to have more authority in the

00:04:00,030 --> 00:04:05,550
testing environment into this file

00:04:02,030 --> 00:04:07,550
that's then pushed to the service are

00:04:05,550 --> 00:04:12,150
using automatic configuration management

00:04:07,550 --> 00:04:13,920
so and yeah they're done and also we

00:04:12,150 --> 00:04:16,080
provided a central group Hana instance

00:04:13,920 --> 00:04:18,090
and each team could then create

00:04:16,080 --> 00:04:20,669
dashboards they needed basically

00:04:18,090 --> 00:04:23,880
matching if every low test running was

00:04:20,669 --> 00:04:27,930
won at least one dashboard showing

00:04:23,880 --> 00:04:30,300
how how this actually works and well

00:04:27,930 --> 00:04:32,040
looking at the colors again the

00:04:30,300 --> 00:04:36,630
infrastructure blocks are then here in

00:04:32,040 --> 00:04:41,100
the light like purple reddish thing they

00:04:36,630 --> 00:04:43,320
set up in the very beginning and after

00:04:41,100 --> 00:04:45,330
after a while more more services were

00:04:43,320 --> 00:04:47,490
installed on this normal low tests were

00:04:45,330 --> 00:04:49,980
running and we had lots of more blue

00:04:47,490 --> 00:04:51,810
boxes it was like a monitoring

00:04:49,980 --> 00:04:54,000
self-service for all our people in

00:04:51,810 --> 00:04:57,750
testing environment so this was no need

00:04:54,000 --> 00:04:59,550
for an admin or to actually change the

00:04:57,750 --> 00:05:01,470
configuration of Prometheus because

00:04:59,550 --> 00:05:04,500
people were just adding things to the

00:05:01,470 --> 00:05:06,360
commissars for themselves and they're

00:05:04,500 --> 00:05:09,270
pushing metrics are themselves to the

00:05:06,360 --> 00:05:12,600
graphite exporter and that worked quite

00:05:09,270 --> 00:05:17,910
well for us any scale was the teams that

00:05:12,600 --> 00:05:19,770
were using it so short thing about

00:05:17,910 --> 00:05:22,889
getting versus Jamie - they're both low

00:05:19,770 --> 00:05:26,639
testing tools jmeter is more like

00:05:22,889 --> 00:05:28,530
graphical user interface thing you can

00:05:26,639 --> 00:05:32,400
point and click your configurations for

00:05:28,530 --> 00:05:34,020
a load test but at some point you

00:05:32,400 --> 00:05:37,050
usually come to a point where you need

00:05:34,020 --> 00:05:39,840
some scripting gatling on the other hand

00:05:37,050 --> 00:05:42,030
is a load testing tool based on ask

00:05:39,840 --> 00:05:44,880
allah to yourself so you do scripting

00:05:42,030 --> 00:05:48,770
all the time but some at least when we

00:05:44,880 --> 00:05:51,510
scarless a bit mind twisting now onthe

00:05:48,770 --> 00:05:55,080
Dream Eater stores the configuration has

00:05:51,510 --> 00:05:56,520
an XML block I usually don't want to put

00:05:55,080 --> 00:05:59,490
that in the closet area but still

00:05:56,520 --> 00:06:01,710
sometimes - but getting stalls and

00:05:59,490 --> 00:06:06,240
external codes like normal source code

00:06:01,710 --> 00:06:09,330
into your repository and that's fine

00:06:06,240 --> 00:06:11,130
yeah probably getting is a bit more but

00:06:09,330 --> 00:06:14,280
what is cooler if it's because it's

00:06:11,130 --> 00:06:16,229
money been a non-blocking eventually

00:06:14,280 --> 00:06:18,479
based on occur but that might not make a

00:06:16,229 --> 00:06:20,550
difference for your load test and yet we

00:06:18,479 --> 00:06:24,599
and we settle for Gatling

00:06:20,550 --> 00:06:27,690
and we yeah get some good results to

00:06:24,599 --> 00:06:29,460
know a little test here so then you

00:06:27,690 --> 00:06:31,530
might ask well why actually why are we

00:06:29,460 --> 00:06:33,479
using the graphite and graphite exporter

00:06:31,530 --> 00:06:36,449
well we're not using graphite as a

00:06:33,479 --> 00:06:39,930
graphite installation but only the

00:06:36,449 --> 00:06:43,379
mechanism of jmeter and getting to

00:06:39,930 --> 00:06:46,349
export graphite metrics stuff so we sent

00:06:43,379 --> 00:06:49,470
both Jamie ten getting support this

00:06:46,349 --> 00:06:51,000
export of graphite metrics and then we

00:06:49,470 --> 00:06:54,680
send it to the graphite exporter that is

00:06:51,000 --> 00:06:54,680
just another exporter like any others

00:06:54,740 --> 00:07:01,349
the good thing is well I know this pull

00:06:58,319 --> 00:07:03,870
and push this cousin here it's but it's

00:07:01,349 --> 00:07:05,819
it's quite easy for if you run the

00:07:03,870 --> 00:07:07,919
scheduling test on a local developers

00:07:05,819 --> 00:07:09,539
machine or test this machine and this

00:07:07,919 --> 00:07:12,330
then just sends two metrics to the

00:07:09,539 --> 00:07:15,180
graphite exporter and the graphic

00:07:12,330 --> 00:07:19,969
exporters yeah it doesn't care about it

00:07:15,180 --> 00:07:23,819
when information comes from it's also

00:07:19,969 --> 00:07:25,380
well it's a simple very simple exporter

00:07:23,819 --> 00:07:28,710
all the metrics that we push to the

00:07:25,380 --> 00:07:33,270
graphite exporter there are they spy off

00:07:28,710 --> 00:07:35,219
after five minutes so there's nothing to

00:07:33,270 --> 00:07:37,259
clean up afterwards after load test just

00:07:35,219 --> 00:07:39,180
wait five minutes and all the metrics

00:07:37,259 --> 00:07:43,440
will be gone you don't need to restart

00:07:39,180 --> 00:07:45,180
it not nothing to clean just fine and

00:07:43,440 --> 00:07:47,909
the metrics that we get for example from

00:07:45,180 --> 00:07:51,509
Gatling other than the number of users

00:07:47,909 --> 00:07:53,610
in the load test how many are active we

00:07:51,509 --> 00:07:56,250
get some aggregated timings and the

00:07:53,610 --> 00:08:00,240
percentiles and and we all get that into

00:07:56,250 --> 00:08:02,069
from from from getting to the graphite

00:08:00,240 --> 00:08:04,729
exporter to Prometheus and then enough

00:08:02,069 --> 00:08:04,729
into our test scores

00:08:04,949 --> 00:08:08,610
something that you might watch want to

00:08:07,169 --> 00:08:11,190
watch out if you use to graph at

00:08:08,610 --> 00:08:13,199
exporter as it is it will give you

00:08:11,190 --> 00:08:15,569
metric names that are looking like like

00:08:13,199 --> 00:08:19,190
lots of underscores in there things that

00:08:15,569 --> 00:08:22,319
should probably be labels label values

00:08:19,190 --> 00:08:24,120
but well you can do without but maybe if

00:08:22,319 --> 00:08:26,400
you want to be a good leading example to

00:08:24,120 --> 00:08:27,410
with good naming metrics you want to put

00:08:26,400 --> 00:08:28,970
in a conversion there

00:08:27,410 --> 00:08:31,850
otherwise you have lots of underscores

00:08:28,970 --> 00:08:34,340
and people will be asking you well why

00:08:31,850 --> 00:08:38,440
should I use neighbors or at all if the

00:08:34,340 --> 00:08:41,000
can same thing using it's not using them

00:08:38,440 --> 00:08:43,250
so let's see if the demo works

00:08:41,000 --> 00:08:46,160
oh well probably put it that again I

00:08:43,250 --> 00:08:55,900
don't hope it I just rebooted the

00:08:46,160 --> 00:09:03,200
Machine a bit of a pity it came up again

00:08:55,900 --> 00:09:08,630
good so did the dashboards that we came

00:09:03,200 --> 00:09:11,600
up with right thanks for feeding so the

00:09:08,630 --> 00:09:13,430
dashboards they the testers and

00:09:11,600 --> 00:09:15,920
developers they don't they didn't do any

00:09:13,430 --> 00:09:17,450
really fancy - boss it's usually we have

00:09:15,920 --> 00:09:19,940
the number of users that are in the in

00:09:17,450 --> 00:09:22,420
the low test probably should start a

00:09:19,940 --> 00:09:24,920
load test to make it a bit more exciting

00:09:22,420 --> 00:09:27,140
we have to number of users you have some

00:09:24,920 --> 00:09:30,320
drop without metrics that we exported

00:09:27,140 --> 00:09:33,230
there we add it all so while we are a

00:09:30,320 --> 00:09:35,240
Java shop in the end we added hystrix

00:09:33,230 --> 00:09:39,140
that's a circuit breaker library for

00:09:35,240 --> 00:09:44,240
Netflix in here this is ten capturing

00:09:39,140 --> 00:09:46,190
lots of metrics out of lots of metrics

00:09:44,240 --> 00:09:49,490
of the communication between our

00:09:46,190 --> 00:09:51,320
services so we have the timings of the

00:09:49,490 --> 00:09:54,200
one service talking to another service

00:09:51,320 --> 00:09:59,150
in our desk force as well and then again

00:09:54,200 --> 00:10:01,640
like CPU metrics from our from C advisor

00:09:59,150 --> 00:10:04,310
plus some JVM metrics about memory users

00:10:01,640 --> 00:10:06,980
and then we can follow up on our load

00:10:04,310 --> 00:10:08,840
tests how that progresses how many users

00:10:06,980 --> 00:10:12,380
are currently active that comes from

00:10:08,840 --> 00:10:15,170
getting how much CPU and RAM we are

00:10:12,380 --> 00:10:17,930
using that comes from C advisor and how

00:10:15,170 --> 00:10:20,030
our case is doing using some metrics

00:10:17,930 --> 00:10:21,350
from either drop wizard and how the

00:10:20,030 --> 00:10:22,610
communication is working between the

00:10:21,350 --> 00:10:25,430
applications coming from histories

00:10:22,610 --> 00:10:33,280
auto-mode s port networks quite nasty

00:10:25,430 --> 00:10:33,280
for us questions before the end

00:10:34,100 --> 00:10:39,839
so yeah these are the dashboards we've

00:10:37,949 --> 00:10:41,660
seen them actually live after restart

00:10:39,839 --> 00:10:44,579
thank god it worked

00:10:41,660 --> 00:10:47,699
but well what to expect how did it

00:10:44,579 --> 00:10:50,819
actually work out looking at its at the

00:10:47,699 --> 00:10:51,899
end of the project so we're pulling all

00:10:50,819 --> 00:10:53,930
these information from the

00:10:51,899 --> 00:10:56,819
infrastructure from the application and

00:10:53,930 --> 00:11:00,899
people who are using counters quite a

00:10:56,819 --> 00:11:03,720
bit or also sorry gauges but that's a

00:11:00,899 --> 00:11:05,130
difficult word for a Janice or for for

00:11:03,720 --> 00:11:07,500
queues and pools we'll be looking at

00:11:05,130 --> 00:11:10,019
database connection pools if they're

00:11:07,500 --> 00:11:13,199
filling up or not the tomcat connector

00:11:10,019 --> 00:11:14,970
pools all the things that you find in

00:11:13,199 --> 00:11:18,329
the low test that might just fail or

00:11:14,970 --> 00:11:22,110
actually fail until up until the point

00:11:18,329 --> 00:11:23,850
where it's human then we could hit some

00:11:22,110 --> 00:11:27,120
drop without metrics that are then

00:11:23,850 --> 00:11:28,560
forwarded to previous metrics part of

00:11:27,120 --> 00:11:31,500
the driver collector and at least the

00:11:28,560 --> 00:11:33,449
timings for mystics and is well

00:11:31,500 --> 00:11:34,920
hysteresis I think a pretty cool thing

00:11:33,449 --> 00:11:36,660
because it gives me the statistics how

00:11:34,920 --> 00:11:38,250
many calls between services are

00:11:36,660 --> 00:11:43,500
successful how many failed and also the

00:11:38,250 --> 00:11:46,560
timings looking at lessons learned here

00:11:43,500 --> 00:11:49,980
the approach worked well for us one

00:11:46,560 --> 00:11:52,139
enough to pass the long tests because we

00:11:49,980 --> 00:11:55,730
were able to put everything into one

00:11:52,139 --> 00:11:58,319
dashboard and find out what was going on

00:11:55,730 --> 00:12:01,800
the interpretation communication was

00:11:58,319 --> 00:12:04,649
pretty good covered by districts and

00:12:01,800 --> 00:12:06,449
because we did this self-service

00:12:04,649 --> 00:12:09,329
functionality for monitoring in our test

00:12:06,449 --> 00:12:14,160
environments it was also breeze for the

00:12:09,329 --> 00:12:15,600
different teams to work independently on

00:12:14,160 --> 00:12:17,180
the other hand looking at all the

00:12:15,600 --> 00:12:18,569
metrics that we now put into our

00:12:17,180 --> 00:12:20,430
application

00:12:18,569 --> 00:12:22,439
looking good more like into production

00:12:20,430 --> 00:12:23,910
how that works out

00:12:22,439 --> 00:12:26,189
the exported metrics they don't

00:12:23,910 --> 00:12:28,860
necessarily follow the Prometheus naming

00:12:26,189 --> 00:12:33,569
conventions people just add the metrics

00:12:28,860 --> 00:12:34,889
they need to their low tests if you want

00:12:33,569 --> 00:12:37,139
to use them in production you might do

00:12:34,889 --> 00:12:40,030
some training and some advanced training

00:12:37,139 --> 00:12:43,450
uh our good prometheus naming metric

00:12:40,030 --> 00:12:45,490
shut up like also if you're using the

00:12:43,450 --> 00:12:48,580
droplet of metrics and they are then

00:12:45,490 --> 00:12:50,410
like collected for Prometheus metrics

00:12:48,580 --> 00:12:53,770
they don't fill out all the health

00:12:50,410 --> 00:12:57,100
labels usually with proper meaningful

00:12:53,770 --> 00:12:59,650
texts so I think next time we will to

00:12:57,100 --> 00:13:03,700
use Prometheus metrics directly instead

00:12:59,650 --> 00:13:05,620
of droplet metrics also the thing like

00:13:03,700 --> 00:13:07,510
in when people start with low testing

00:13:05,620 --> 00:13:11,650
they usually use like counters and

00:13:07,510 --> 00:13:13,840
Everett is a necessarily look too much

00:13:11,650 --> 00:13:17,560
into histograms in summary and

00:13:13,840 --> 00:13:19,110
percentiles also probably you want to

00:13:17,560 --> 00:13:21,660
get them going in the first place

00:13:19,110 --> 00:13:26,230
some good experience because Prometheus

00:13:21,660 --> 00:13:27,970
and after than a few days or weeks tell

00:13:26,230 --> 00:13:30,820
them about history words and some reason

00:13:27,970 --> 00:13:32,560
how they actually work in the end you

00:13:30,820 --> 00:13:34,120
might up ending with summaries but you

00:13:32,560 --> 00:13:36,640
want to aggregate these metrics in

00:13:34,120 --> 00:13:37,690
Augusta but you can't so yeah you should

00:13:36,640 --> 00:13:41,010
have done history ones in the first

00:13:37,690 --> 00:13:45,190
place but that depends on your usage

00:13:41,010 --> 00:13:46,840
also talking to developers and the ones

00:13:45,190 --> 00:13:51,880
to in low test they probably never heard

00:13:46,840 --> 00:13:54,340
about this use method or rap method that

00:13:51,880 --> 00:13:57,970
you have like a consistent metrics about

00:13:54,340 --> 00:14:01,210
utilization saturation errors also or if

00:13:57,970 --> 00:14:03,940
you looking at race yet we have an in a

00:14:01,210 --> 00:14:06,960
rate of requests rate of requests and

00:14:03,940 --> 00:14:10,360
the duration of the requests also that

00:14:06,960 --> 00:14:12,520
yeah I was then giving some developers

00:14:10,360 --> 00:14:16,180
some awareness that there's a little bit

00:14:12,520 --> 00:14:21,310
more than just counters and averages all

00:14:16,180 --> 00:14:23,280
right well these are some links at some

00:14:21,310 --> 00:14:25,940
point we contributed back to the

00:14:23,280 --> 00:14:27,990
Prometheus hystrix metrics publisher

00:14:25,940 --> 00:14:30,250
[Music]

00:14:27,990 --> 00:14:33,220
some work started at SoundCloud

00:14:30,250 --> 00:14:33,990
originally and we know contributing to

00:14:33,220 --> 00:14:38,800
that

00:14:33,990 --> 00:14:42,760
that's tools that we used yeah 20

00:14:38,800 --> 00:14:45,760
minutes time okay

00:14:42,760 --> 00:14:45,760
questions

00:14:54,410 --> 00:15:03,829
ah yes you talked about the best

00:15:01,730 --> 00:15:06,800
practices for naming conventions so

00:15:03,829 --> 00:15:08,629
other product teams configuring the

00:15:06,800 --> 00:15:11,029
promises instance themselves or a say a

00:15:08,629 --> 00:15:13,009
team who's taking the input and they are

00:15:11,029 --> 00:15:15,439
doing a lot configuration for them it's

00:15:13,009 --> 00:15:17,600
well for the load tests they just didn't

00:15:15,439 --> 00:15:19,579
the team the product teams only care

00:15:17,600 --> 00:15:21,410
that their metrics that are adding to

00:15:19,579 --> 00:15:22,550
the application end up in Prometheus so

00:15:21,410 --> 00:15:26,350
they can show them in a graph on a

00:15:22,550 --> 00:15:26,350
dashboard they don't care about alerts

00:15:28,329 --> 00:15:34,250
hi I have a question about the service

00:15:32,180 --> 00:15:36,500
level objectives that you might have do

00:15:34,250 --> 00:15:38,750
you have any like formal definition of

00:15:36,500 --> 00:15:41,029
what objectives you want to meet and if

00:15:38,750 --> 00:15:43,189
so is it a manual process to check if

00:15:41,029 --> 00:15:47,990
this is done or is it some automatic

00:15:43,189 --> 00:15:50,000
step during the build so there are

00:15:47,990 --> 00:15:52,910
formal requirements in terms of what

00:15:50,000 --> 00:15:55,509
kind of load we need to tackle like how

00:15:52,910 --> 00:15:57,980
many requests per minute or per second

00:15:55,509 --> 00:16:01,100
and how that distributes or different

00:15:57,980 --> 00:16:03,319
use cases so that's quite laid down in

00:16:01,100 --> 00:16:05,959
the requirements and we automated that

00:16:03,319 --> 00:16:08,630
it's it's not running well we can push a

00:16:05,959 --> 00:16:12,949
button on our Jenkins so they run but we

00:16:08,630 --> 00:16:17,630
don't run it every night also to the

00:16:12,949 --> 00:16:19,610
question yes hello

00:16:17,630 --> 00:16:22,509
how do you investigate the problem in

00:16:19,610 --> 00:16:29,839
application with thousands of metrics

00:16:22,509 --> 00:16:32,899
some correlations the metrics that there

00:16:29,839 --> 00:16:34,490
are they then probably best understood

00:16:32,899 --> 00:16:37,490
by the product team because they put

00:16:34,490 --> 00:16:40,250
them in there in the first place

00:16:37,490 --> 00:16:42,730
the investigation from an operational

00:16:40,250 --> 00:16:47,709
point of view is then usually done

00:16:42,730 --> 00:16:50,980
memory usage CPU usage but the metrics

00:16:47,709 --> 00:16:54,639
well we have to see how that works out I

00:16:50,980 --> 00:16:54,639
don't have answers yet sir

00:16:59,770 --> 00:17:07,470
who is getting the alerts is it's a

00:17:02,320 --> 00:17:07,470
product team um well not that far yet

00:17:09,280 --> 00:17:13,120
just to clarify so the Prometheus

00:17:11,410 --> 00:17:14,410
instance which you guys are consuming

00:17:13,120 --> 00:17:16,240
it's totally up with our application

00:17:14,410 --> 00:17:18,880
team to maintain right it's not

00:17:16,240 --> 00:17:21,610
something which are really monitoring

00:17:18,880 --> 00:17:23,890
the infrastructure or doing alerting and

00:17:21,610 --> 00:17:26,350
the complete thing it's just a developer

00:17:23,890 --> 00:17:29,200
environment where application team owns

00:17:26,350 --> 00:17:31,180
and use accordingly right like Ravana

00:17:29,200 --> 00:17:34,120
they can directly integrate or something

00:17:31,180 --> 00:17:35,440
like that right well that would also be

00:17:34,120 --> 00:17:37,510
my recommendation if you have a test

00:17:35,440 --> 00:17:38,860
environment or development of different

00:17:37,510 --> 00:17:40,570
test environments you should have a

00:17:38,860 --> 00:17:42,220
Prometheus instance just for that test

00:17:40,570 --> 00:17:43,840
environment that is independent of your

00:17:42,220 --> 00:17:46,000
production environment right you have no

00:17:43,840 --> 00:17:47,830
problem with that yeah because in that

00:17:46,000 --> 00:17:49,900
case alerting is not that meaningful

00:17:47,830 --> 00:17:52,480
because in that case people more

00:17:49,900 --> 00:17:54,310
interested on dashboard like grow fauna

00:17:52,480 --> 00:17:57,430
so they can monitor how the system is

00:17:54,310 --> 00:17:59,020
performing yeah yes that's true then

00:17:57,430 --> 00:18:00,580
mostly interested in graph Anna to see

00:17:59,020 --> 00:18:03,930
what their load tests are doing right

00:18:00,580 --> 00:18:05,890
but on the other hand it gives you some

00:18:03,930 --> 00:18:07,740
instrumentation at least a standard

00:18:05,890 --> 00:18:11,170
metrics about the java virtual machine

00:18:07,740 --> 00:18:13,810
so you don't get any longer black box

00:18:11,170 --> 00:18:15,940
out of test but at least some basic gray

00:18:13,810 --> 00:18:18,220
vm metrics out of test that you can then

00:18:15,940 --> 00:18:21,400
use no more from if you were than in a

00:18:18,220 --> 00:18:23,380
third system reliability engineer point

00:18:21,400 --> 00:18:26,130
of view right some sentiment o'clock

00:18:23,380 --> 00:18:26,130
yeah yeah

00:18:27,780 --> 00:18:31,410
any more questions

00:18:33,580 --> 00:18:37,029
Roland thank you

00:18:40,030 --> 00:18:52,720
[Music]

00:18:52,519 --> 00:18:55,859
you

00:18:52,720 --> 00:18:55,859

YouTube URL: https://www.youtube.com/watch?v=NQ3DYMs6KWc


