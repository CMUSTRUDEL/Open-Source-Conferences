Title: PromCon 2017: Play with Prometheus - Journey to Make “testing in Production” More Reliable
Publication date: 2017-09-04
Playlist: PromCon 2017
Description: 
	* Abstract:

Gilt is a high end fashion e-commerce that in the recent years has moved from a monolithic architecture to 150+ distributed microservices running on AWS.

In Gilt we have to make sure our website runs smoothly and our customers are getting the best experience we can deliver. For this reason we have to keep an eye on our microservices and make sure they behave as expected.

We’ve been looking for a long time to way to keep long lasting time series, we could aggregate, query, visualize and use for alerting. After a lot of trial and error, we ended up finding the right pieces of the puzzle: Prometheus, Alertmanager, Push Gateway, and Grafana.

Our success story went viral in Gilt, and very recently Prometheus and Grafana have been added to the Gilt (and HBC!) techradar. A few teams have already lined up to adopt our Prometheus Stack in production and eventually we will implement a hierarchical Prometheus federation alongside meta-monitoring.

I would like to share with you our journey, what worked well, the problems we faced, and how we fixed them.

* Speaker biography:

I'm a Senior Software Engineer at HBC Digital and I've been working for the past two years in the Gilt Personalization team.

With twelve years of commercial experience architecting, building and delivering software, I’ve focused most of my career developing in Java. Since joining GILT two years ago, I’ve pleasantly switched to Scala and functional programming. I’m a DevOps advocate and passionate about quality, automation and attention to detail.

* Slides:

https://promcon.io/2017-munich/slides/play-with-prometheus.pdf

* PromCon website:

https://promcon.io/
Captions: 
	00:00:00,380 --> 00:00:08,350
[Music]

00:00:12,469 --> 00:00:19,380
and now welcome Giovanni I got judo - oh

00:00:18,170 --> 00:00:24,390
good one

00:00:19,380 --> 00:00:28,500
yes who is going to talk about play with

00:00:24,390 --> 00:00:31,260
for me theus and yeah yeah

00:00:28,500 --> 00:00:34,170
all right cool hi everyone I'm happy to

00:00:31,260 --> 00:00:36,600
be here very excited and my name is

00:00:34,170 --> 00:00:38,790
Johanna guru and before starting I'm

00:00:36,600 --> 00:00:40,890
software engineer and before starting

00:00:38,790 --> 00:00:42,989
I'd like to I'm sure that in the crowd

00:00:40,890 --> 00:00:44,670
there are lots of products guys as well

00:00:42,989 --> 00:00:46,289
a system engineer is there any

00:00:44,670 --> 00:00:49,850
developers that I shall use Prometheus

00:00:46,289 --> 00:00:52,320
app to monitor service in production

00:00:49,850 --> 00:00:54,750
okay so there are a few of them yeah any

00:00:52,320 --> 00:00:57,480
one of you running in AWS Amazon Web

00:00:54,750 --> 00:00:59,070
Services alright cool so they're gonna

00:00:57,480 --> 00:01:01,969
be like a couple of takeaways for you

00:00:59,070 --> 00:01:05,189
and so today I'm gonna speak about the

00:01:01,969 --> 00:01:11,070
little journey that we started last year

00:01:05,189 --> 00:01:13,619
in in guilt into adopting from me to

00:01:11,070 --> 00:01:15,390
Sangha vana as a monitoring system so

00:01:13,619 --> 00:01:17,939
very very brief introduction about

00:01:15,390 --> 00:01:19,890
myself I'm a software engineer I've been

00:01:17,939 --> 00:01:24,090
working on jvm languages for the past 12

00:01:19,890 --> 00:01:26,909
years the last two on Scala I am part

00:01:24,090 --> 00:01:30,090
since 2015 of the gift personalization

00:01:26,909 --> 00:01:34,079
and Tim and here there are a couple of

00:01:30,090 --> 00:01:37,500
details for not reach me on github and

00:01:34,079 --> 00:01:39,840
Twitter and very quick story about gift

00:01:37,500 --> 00:01:43,380
and gif design fashion online retailer

00:01:39,840 --> 00:01:45,750
so we sell very expensive clothes on on

00:01:43,380 --> 00:01:47,640
internet at the discounted price the the

00:01:45,750 --> 00:01:50,280
business model is called flash sales and

00:01:47,640 --> 00:01:52,200
every day afternoon and there would be

00:01:50,280 --> 00:01:54,000
like a very high spike in usage of the

00:01:52,200 --> 00:01:58,009
website because we would release the new

00:01:54,000 --> 00:02:00,930
products at very low cost high discount

00:01:58,009 --> 00:02:03,509
it was initially launched in November of

00:02:00,930 --> 00:02:06,840
2007 was a monolithic Ruby on Rails up

00:02:03,509 --> 00:02:09,319
in 2010 when the website started to get

00:02:06,840 --> 00:02:12,290
very popular we very quickly hit

00:02:09,319 --> 00:02:13,609
and a problem in scalability and this is

00:02:12,290 --> 00:02:15,859
the reason why we started to break the

00:02:13,609 --> 00:02:17,060
the monolith down and it took quite a

00:02:15,859 --> 00:02:19,609
few years to get there

00:02:17,060 --> 00:02:22,609
we eventually managed to split the Ruby

00:02:19,609 --> 00:02:25,909
on Rails up into pretty much the 350

00:02:22,609 --> 00:02:29,540
mostly Scala micro-services and last

00:02:25,909 --> 00:02:33,200
year in 2016 the give to join the HPC

00:02:29,540 --> 00:02:37,069
family that is awesome a company to

00:02:33,200 --> 00:02:39,200
which a more online retailer shops are

00:02:37,069 --> 00:02:41,659
part of there are a few names here

00:02:39,200 --> 00:02:44,810
Lauren Taylor it itself sucks of 5th and

00:02:41,659 --> 00:02:46,489
6th 5th Avenue so the development

00:02:44,810 --> 00:02:48,139
processing guilt is very very simple

00:02:46,489 --> 00:02:50,060
this is that this has evolved over the

00:02:48,139 --> 00:02:51,949
years but we eventually managed to get

00:02:50,060 --> 00:02:53,930
the point whereby we do shortly

00:02:51,949 --> 00:02:56,120
directions so we deliver to production

00:02:53,930 --> 00:02:57,469
very quickly we make use of continuous

00:02:56,120 --> 00:02:59,930
deployment and continual continuous

00:02:57,469 --> 00:03:02,719
integration Jenkins is your friend and

00:02:59,930 --> 00:03:04,280
in our team we do not have QA people we

00:03:02,719 --> 00:03:05,599
do not have test results oft where

00:03:04,280 --> 00:03:07,969
engineers they follow the whole

00:03:05,599 --> 00:03:10,669
lifecycle of the app from development to

00:03:07,969 --> 00:03:12,230
testing we do that in production and we

00:03:10,669 --> 00:03:14,209
also look after the operational side of

00:03:12,230 --> 00:03:17,299
the things and this is what Prometheus

00:03:14,209 --> 00:03:19,250
with cinnamon it is like fundamental we

00:03:17,299 --> 00:03:21,439
do test an in production using the

00:03:19,250 --> 00:03:23,780
methodology of Canadian production

00:03:21,439 --> 00:03:26,000
deployment Martine Fuller has a bleakie

00:03:23,780 --> 00:03:28,340
and a blog about this thing is very

00:03:26,000 --> 00:03:29,810
clear how it is implemented but for

00:03:28,340 --> 00:03:31,519
simplicity if you're familiar with the

00:03:29,810 --> 00:03:33,949
Condor the concept of auto scaling group

00:03:31,519 --> 00:03:35,060
we have two auto scaling groups in AWS

00:03:33,949 --> 00:03:37,129
that are basically two flicked off

00:03:35,060 --> 00:03:38,989
servers the auto scaling group is only

00:03:37,129 --> 00:03:41,239
sorry the cannery auto scaling group is

00:03:38,989 --> 00:03:42,799
only composed by one instance that is

00:03:41,239 --> 00:03:45,109
what we ship with the code initially so

00:03:42,799 --> 00:03:46,579
version of the code X plus one and then

00:03:45,109 --> 00:03:49,060
we start to monitor out that things are

00:03:46,579 --> 00:03:51,409
going and if everything is fine

00:03:49,060 --> 00:03:53,269
depending on how critic the change of

00:03:51,409 --> 00:03:54,979
the fixes we wait a couple of minutes or

00:03:53,269 --> 00:03:57,229
even a couple of days if it's a nicer

00:03:54,979 --> 00:04:00,620
change usually not on a Friday as we

00:03:57,229 --> 00:04:01,849
want quite weekends and once that we are

00:04:00,620 --> 00:04:03,590
happy with the change we push one button

00:04:01,849 --> 00:04:05,569
and the new version goes to the

00:04:03,590 --> 00:04:07,039
production our production out of scaling

00:04:05,569 --> 00:04:11,000
group that usually is like three four or

00:04:07,039 --> 00:04:12,109
more servers on top of it they on top of

00:04:11,000 --> 00:04:13,819
the two auto scaling group there is

00:04:12,109 --> 00:04:16,099
always an LP and the cannery of the

00:04:13,819 --> 00:04:18,650
scaling group only receives a portion of

00:04:16,099 --> 00:04:20,269
the production traffic so that we have

00:04:18,650 --> 00:04:22,090
really good understanding of what's

00:04:20,269 --> 00:04:24,250
going on and to this node

00:04:22,090 --> 00:04:26,320
the release checklist I think is not any

00:04:24,250 --> 00:04:28,270
different from the one that you use as

00:04:26,320 --> 00:04:30,220
well we smoke test the app and we check

00:04:28,270 --> 00:04:33,430
the usual parameters and shocked yet

00:04:30,220 --> 00:04:35,320
health of the service response time

00:04:33,430 --> 00:04:39,010
requests per minute errors either on the

00:04:35,320 --> 00:04:42,610
API endpoint or in the logs so let's go

00:04:39,010 --> 00:04:47,370
back a little bit in 2016 in engaged we

00:04:42,610 --> 00:04:50,380
were migrating from node hosted version

00:04:47,370 --> 00:04:52,840
hosting system called the carpet into

00:04:50,380 --> 00:04:54,610
AWS and back in in those days only one

00:04:52,840 --> 00:04:57,970
year ago we didn't have like hard rules

00:04:54,610 --> 00:05:00,790
when regards of monitoring and alerting

00:04:57,970 --> 00:05:03,100
and we were mostly making use of a relic

00:05:00,790 --> 00:05:04,780
we started also taught up cloud watch

00:05:03,100 --> 00:05:07,060
that turned out to be like very powerful

00:05:04,780 --> 00:05:09,490
at the beginning but then we ended up

00:05:07,060 --> 00:05:11,440
like copy and paste seminar the same

00:05:09,490 --> 00:05:13,210
alerts here and there when we want to

00:05:11,440 --> 00:05:14,680
top that one it was a messed up data and

00:05:13,210 --> 00:05:16,930
everywhere so there is a lot of

00:05:14,680 --> 00:05:19,389
duplication there and with regards to a

00:05:16,930 --> 00:05:20,889
lefting and also there the situation was

00:05:19,389 --> 00:05:24,940
a little bit blurred we didn't have got

00:05:20,889 --> 00:05:26,620
here organ system of course we make use

00:05:24,940 --> 00:05:28,479
of pager duty that is one of the most

00:05:26,620 --> 00:05:30,789
popular on the market but again

00:05:28,479 --> 00:05:32,380
repetition in having new relic pumping a

00:05:30,789 --> 00:05:35,440
lesson to pager duty as well as cloud

00:05:32,380 --> 00:05:38,110
watch with those tools we had a 10 back

00:05:35,440 --> 00:05:40,720
then and we really did not have a

00:05:38,110 --> 00:05:42,550
user-friendly way of creating custom

00:05:40,720 --> 00:05:44,830
dashboards as well as custom matrices

00:05:42,550 --> 00:05:47,169
you could still do that but it would be

00:05:44,830 --> 00:05:49,210
very cumbersome and a couple of times

00:05:47,169 --> 00:05:51,310
we've been bitten by a reliable a

00:05:49,210 --> 00:05:53,139
lefting while the false positive they

00:05:51,310 --> 00:05:55,120
still be ok because you get paged when

00:05:53,139 --> 00:05:56,650
nothing is really happening other times

00:05:55,120 --> 00:05:58,360
you get paid you don't get pissed when

00:05:56,650 --> 00:05:59,560
all your services are done so I don't

00:05:58,360 --> 00:06:01,660
know if you remember early this year

00:05:59,560 --> 00:06:05,320
there was a problem with the Indian s

00:06:01,660 --> 00:06:07,150
all the hosts West were down sorry the

00:06:05,320 --> 00:06:09,460
DNS resolution was working all our oaths

00:06:07,150 --> 00:06:11,680
went down for specific service New Relic

00:06:09,460 --> 00:06:12,970
didn't see any error so did on page but

00:06:11,680 --> 00:06:15,940
thank God we had the parameters that

00:06:12,970 --> 00:06:18,789
promptly started to I left because no no

00:06:15,940 --> 00:06:20,530
instances were up and with cloud washer

00:06:18,789 --> 00:06:22,150
ready said that there are there is no

00:06:20,530 --> 00:06:24,910
single place for or galettes

00:06:22,150 --> 00:06:26,650
and the fail the failure of the ry

00:06:24,910 --> 00:06:28,539
principle do not repeat yourself is you

00:06:26,650 --> 00:06:31,539
want but the straw that broke the

00:06:28,539 --> 00:06:33,729
camel's back up and when you relic start

00:06:31,539 --> 00:06:35,260
to fail tracing as color future a

00:06:33,729 --> 00:06:37,150
synchronous computation

00:06:35,260 --> 00:06:39,040
so you can see here on the on the left

00:06:37,150 --> 00:06:41,530
that is a craft of New Relic saying that

00:06:39,040 --> 00:06:43,810
my API is has an average response time

00:06:41,530 --> 00:06:45,070
of 1.5 milliseconds that's that was

00:06:43,810 --> 00:06:47,470
definitely too quick for what I think

00:06:45,070 --> 00:06:49,300
the API was doing so when we are not

00:06:47,470 --> 00:06:51,550
initially adopted Prometheus and graph

00:06:49,300 --> 00:06:53,140
Anna you can see that the response time

00:06:51,550 --> 00:06:54,580
was actually a thousand times lower and

00:06:53,140 --> 00:06:55,900
this was actually matching what we had

00:06:54,580 --> 00:06:58,780
in the log so I didn't invent anything

00:06:55,900 --> 00:07:00,640
here so we've definitely needed

00:06:58,780 --> 00:07:02,380
something new and the key things that

00:07:00,640 --> 00:07:04,030
we're driving our decision were based on

00:07:02,380 --> 00:07:06,370
the fact that the new solution should

00:07:04,030 --> 00:07:08,110
have adopted a time series and should be

00:07:06,370 --> 00:07:11,260
designed for time series should be

00:07:08,110 --> 00:07:12,820
scalable in production we have 350 micro

00:07:11,260 --> 00:07:14,980
services if you consider that the

00:07:12,820 --> 00:07:16,510
average is three instances per service

00:07:14,980 --> 00:07:20,170
this very quickly adds up to more than

00:07:16,510 --> 00:07:22,810
1,000 instances in l WS and percentiles

00:07:20,170 --> 00:07:24,670
and derived metrics when you write a lot

00:07:22,810 --> 00:07:27,310
of when you instrumental code yourself

00:07:24,670 --> 00:07:29,260
you wanna be when you wanna be very

00:07:27,310 --> 00:07:30,790
simple you don't wanna build very

00:07:29,260 --> 00:07:33,970
complicated medics in the pod

00:07:30,790 --> 00:07:35,740
you wanna delay custom metrics and more

00:07:33,970 --> 00:07:39,130
complicated metrics at a later stage in

00:07:35,740 --> 00:07:42,550
this thing permittees is great and and

00:07:39,130 --> 00:07:46,840
we also wanted that the tool itself

00:07:42,550 --> 00:07:48,580
would actually give you the possibility

00:07:46,840 --> 00:07:50,830
of create the custom and the very

00:07:48,580 --> 00:07:53,650
beautiful dashboard or at least would be

00:07:50,830 --> 00:07:55,480
kind of partnered up or working

00:07:53,650 --> 00:07:57,760
alongside with something else the

00:07:55,480 --> 00:08:00,520
decision to take was very simple we

00:07:57,760 --> 00:08:02,950
adopted parameters angriff an and we now

00:08:00,520 --> 00:08:06,210
know very well what was two things are

00:08:02,950 --> 00:08:08,380
so what was the plan to adopt those two

00:08:06,210 --> 00:08:10,000
technologies so in the personalization

00:08:08,380 --> 00:08:11,710
thing that is only one of the five or

00:08:10,000 --> 00:08:13,900
six things that we have in the gift and

00:08:11,710 --> 00:08:16,960
office we wanted to start to evaluate

00:08:13,900 --> 00:08:18,880
the product so and get our hands dirty

00:08:16,960 --> 00:08:21,250
with the parameter suite and graph Anna

00:08:18,880 --> 00:08:24,010
and see how it worked for us try to

00:08:21,250 --> 00:08:26,020
create reusable templates so that so

00:08:24,010 --> 00:08:28,980
that other teams could actually adopt

00:08:26,020 --> 00:08:32,470
the same solution as well and if

00:08:28,980 --> 00:08:34,180
excuse me m and if enough teams would

00:08:32,470 --> 00:08:37,240
have adopted the same thing maybe end up

00:08:34,180 --> 00:08:38,620
with the parameters Federation and a

00:08:37,240 --> 00:08:41,110
centralized caravana this is gonna be

00:08:38,620 --> 00:08:43,120
clear in one second why so we started

00:08:41,110 --> 00:08:45,430
from code instrumentation and as you

00:08:43,120 --> 00:08:47,510
know there is a Java client for for

00:08:45,430 --> 00:08:49,490
committees but there is an Oscar kind

00:08:47,510 --> 00:08:51,320
skaar developers a little bit PD we

00:08:49,490 --> 00:08:52,790
don't like to mix up the Java stuff with

00:08:51,320 --> 00:08:54,530
the Scala stuff so we decided to write

00:08:52,790 --> 00:08:56,480
our own code one of the reason is that

00:08:54,530 --> 00:08:58,370
we could pimp my lab we use the pin from

00:08:56,480 --> 00:09:01,010
a library pattern but if we wanted to if

00:08:58,370 --> 00:09:02,240
we had to spend time and in writing code

00:09:01,010 --> 00:09:04,730
we wanted to write something from

00:09:02,240 --> 00:09:07,460
scratch so we implemented kgs instagrams

00:09:04,730 --> 00:09:10,640
encounters and with and we release the

00:09:07,460 --> 00:09:13,910
software as open source so the details

00:09:10,640 --> 00:09:15,830
of the project are here and you can go

00:09:13,910 --> 00:09:18,650
take a look there is the github repo and

00:09:15,830 --> 00:09:20,630
also a very extended guide and so

00:09:18,650 --> 00:09:22,460
takeaway number one we started to get

00:09:20,630 --> 00:09:25,190
our hands dirty with the new newly

00:09:22,460 --> 00:09:28,700
created Scala client but very quickly we

00:09:25,190 --> 00:09:31,190
ended up writing lots of boilerplate

00:09:28,700 --> 00:09:33,590
code and repeated code because we must

00:09:31,190 --> 00:09:36,410
redesign ABI is REST API s and we always

00:09:33,590 --> 00:09:39,830
have to copy and paste the same way of

00:09:36,410 --> 00:09:41,360
reporting upon response time fragrance

00:09:39,830 --> 00:09:43,160
and all scam things it's very

00:09:41,360 --> 00:09:45,710
frustrating a nice error prone and this

00:09:43,160 --> 00:09:47,840
could actually slow down the the process

00:09:45,710 --> 00:09:50,030
of adopting those technologies in your

00:09:47,840 --> 00:09:53,150
daily life so solution solution is to

00:09:50,030 --> 00:09:55,370
extend the basic primitive Scala client

00:09:53,150 --> 00:09:57,170
that will be developed and provide

00:09:55,370 --> 00:09:59,810
support for the other frameworks like a

00:09:57,170 --> 00:10:01,340
framework that is a framework a Scala

00:09:59,810 --> 00:10:05,500
framework for creating for quickly

00:10:01,340 --> 00:10:08,330
creating rest api's our HTTP and HTTPS

00:10:05,500 --> 00:10:11,270
so let's see how the instrumentation

00:10:08,330 --> 00:10:14,540
works and there are a couple of lines of

00:10:11,270 --> 00:10:17,840
code here importing libraries etc but is

00:10:14,540 --> 00:10:20,060
one-liner - instrument JM x and b be

00:10:17,840 --> 00:10:23,390
reported opponent is just literally an X

00:10:20,060 --> 00:10:24,500
dot register and it is also as simple to

00:10:23,390 --> 00:10:27,440
report

00:10:24,500 --> 00:10:28,880
response time and requests per minute

00:10:27,440 --> 00:10:30,290
the number of errors on all the

00:10:28,880 --> 00:10:32,510
endpoints of a relay framework

00:10:30,290 --> 00:10:34,490
application play

00:10:32,510 --> 00:10:35,930
uses this concept of filters that are

00:10:34,490 --> 00:10:38,660
basically snippet of code executed

00:10:35,930 --> 00:10:41,540
around your at rest API calls so we

00:10:38,660 --> 00:10:43,910
brought those method calls with the

00:10:41,540 --> 00:10:47,090
parameters filter and we introduced a

00:10:43,910 --> 00:10:49,580
histogram a parameter cystogram - bucket

00:10:47,090 --> 00:10:51,680
the response time and then based on

00:10:49,580 --> 00:10:53,710
those metrics we also use the graph Anna

00:10:51,680 --> 00:10:56,080
we leverage to graph Anya

00:10:53,710 --> 00:10:58,360
templating engine and you can see like

00:10:56,080 --> 00:11:01,779
there are resources as a title of every

00:10:58,360 --> 00:11:03,970
row this is basically an Reston point of

00:11:01,779 --> 00:11:05,649
our API and for every single end point

00:11:03,970 --> 00:11:07,570
we automatically generated the three

00:11:05,649 --> 00:11:10,570
graphs it's funny because this morning I

00:11:07,570 --> 00:11:11,890
didn't know about the read rules but we

00:11:10,570 --> 00:11:14,440
naturally came up with the same solution

00:11:11,890 --> 00:11:16,209
so hey we're doing it right and we have

00:11:14,440 --> 00:11:18,370
press requests per second the response

00:11:16,209 --> 00:11:27,970
time and the number of errors and we

00:11:18,370 --> 00:11:33,130
generally don't have errors we have meta

00:11:27,970 --> 00:11:36,550
monitoring so the second part again here

00:11:33,130 --> 00:11:39,130
we also adopted the primitives

00:11:36,550 --> 00:11:41,200
age and all exporter for our for our ec2

00:11:39,130 --> 00:11:43,230
instances so on the top you can see host

00:11:41,200 --> 00:11:47,020
the disk usage and host memory usage and

00:11:43,230 --> 00:11:48,640
and then of course the gem JVM

00:11:47,020 --> 00:11:50,230
instrumentation three graphs these are

00:11:48,640 --> 00:11:52,330
repeated for every single host so there

00:11:50,230 --> 00:11:54,070
is the host ID there and so how much

00:11:52,330 --> 00:11:55,660
memory were using what is the garbage

00:11:54,070 --> 00:11:58,930
collector activity and the dread thread

00:11:55,660 --> 00:12:01,270
count but instrumenting our code wasn't

00:11:58,930 --> 00:12:03,250
the only one area where we were focusing

00:12:01,270 --> 00:12:05,110
and really wanted to make a difference

00:12:03,250 --> 00:12:08,410
and it's also to permit a stock

00:12:05,110 --> 00:12:09,790
management and if you use AWS you know

00:12:08,410 --> 00:12:13,180
that you don't have to mean intercessor

00:12:09,790 --> 00:12:16,200
service so use initially you stuff to to

00:12:13,180 --> 00:12:18,580
manage your stuff yourself and and

00:12:16,200 --> 00:12:20,230
opinion is fine but if you want another

00:12:18,580 --> 00:12:21,730
if another team wants product your same

00:12:20,230 --> 00:12:23,380
solution start to be cumbersome you

00:12:21,730 --> 00:12:25,630
can't just copy and paste stuff and then

00:12:23,380 --> 00:12:27,190
it works and the funny part was that

00:12:25,630 --> 00:12:28,660
after couple of months that this thing

00:12:27,190 --> 00:12:30,010
was running I was showing off with all

00:12:28,660 --> 00:12:32,260
my colleagues then the instance craft

00:12:30,010 --> 00:12:34,089
and a loss configuration in that I was

00:12:32,260 --> 00:12:36,880
like this this is not the right approach

00:12:34,089 --> 00:12:39,550
so take away number two if you work in a

00:12:36,880 --> 00:12:42,400
DevOps team and there are no ops guys

00:12:39,550 --> 00:12:44,529
then you have to do ops yourself and you

00:12:42,400 --> 00:12:45,850
all really want that the operation side

00:12:44,529 --> 00:12:47,500
of the things since we are ready to this

00:12:45,850 --> 00:12:49,600
with all our service needs to be simple

00:12:47,500 --> 00:12:51,220
and efficient and of course we don't

00:12:49,600 --> 00:12:52,779
want teams supporting from Integra phone

00:12:51,220 --> 00:12:55,300
in production they have to develop new

00:12:52,779 --> 00:12:57,790
features so what we really wanted was to

00:12:55,300 --> 00:12:59,770
create a template that could have been a

00:12:57,790 --> 00:13:02,110
reusable customizable easy to maintain

00:12:59,770 --> 00:13:04,720
and upgrade so lots of other objectives

00:13:02,110 --> 00:13:06,670
but we actually managed to do that so

00:13:04,720 --> 00:13:07,810
you need a base there is a feature

00:13:06,670 --> 00:13:09,790
called the a double

00:13:07,810 --> 00:13:12,180
confirmation template where you can

00:13:09,790 --> 00:13:14,950
describe the resources that the service

00:13:12,180 --> 00:13:18,600
requires create the template and you can

00:13:14,950 --> 00:13:20,470
buy a command line create and destroy

00:13:18,600 --> 00:13:23,620
templates very quickly they're called

00:13:20,470 --> 00:13:25,990
stocks and again we come up with with a

00:13:23,620 --> 00:13:28,960
CloudFormation template and we open

00:13:25,990 --> 00:13:31,900
sourced it is available and the github

00:13:28,960 --> 00:13:33,630
project there and how does it work so

00:13:31,900 --> 00:13:36,160
the parameters confirmation templates

00:13:33,630 --> 00:13:37,600
spawn starts one easy to instance in

00:13:36,160 --> 00:13:40,060
which there is a very fat to talk a

00:13:37,600 --> 00:13:42,730
distance-learning and inside the pie

00:13:40,060 --> 00:13:44,710
attacker compose we have the Orem it is

00:13:42,730 --> 00:13:47,950
sweet so from it's a server a left

00:13:44,710 --> 00:13:51,100
manager and push gateway as well as an

00:13:47,950 --> 00:13:52,570
instance of graph annum we separate

00:13:51,100 --> 00:13:54,160
learning from the past we learned that

00:13:52,570 --> 00:13:56,680
the volume needs to be the past so we

00:13:54,160 --> 00:13:58,330
also provide an the touch DBS volume

00:13:56,680 --> 00:14:00,520
where your data and configuration are

00:13:58,330 --> 00:14:02,890
safely saved if something was wrong and

00:14:00,520 --> 00:14:06,070
the ec2 instance Falls you just gonna

00:14:02,890 --> 00:14:09,460
lose like a few seconds of metrics and a

00:14:06,070 --> 00:14:10,870
SS will restart anyone additionally what

00:14:09,460 --> 00:14:14,230
you can do is you can provide the key

00:14:10,870 --> 00:14:16,900
table repo and github token and within

00:14:14,230 --> 00:14:19,030
the same CloudFormation template and the

00:14:16,900 --> 00:14:20,589
template will take care of pooling the

00:14:19,030 --> 00:14:25,300
latest parameters configuration every

00:14:20,589 --> 00:14:27,280
time it's changed it uses AWS sqs simple

00:14:25,300 --> 00:14:30,070
queue service simple queue service

00:14:27,280 --> 00:14:33,400
system from AWS and the fact that the

00:14:30,070 --> 00:14:35,740
github can be configured to in a web web

00:14:33,400 --> 00:14:37,450
way to send notification every time you

00:14:35,740 --> 00:14:39,220
merge a poor request this worked pretty

00:14:37,450 --> 00:14:40,660
well for us because if you want to have

00:14:39,220 --> 00:14:42,550
some control your configuration of

00:14:40,660 --> 00:14:45,790
another pair by looking after your

00:14:42,550 --> 00:14:47,230
changes and it's a stay there and the

00:14:45,790 --> 00:14:48,760
second thing is also share knowledge if

00:14:47,230 --> 00:14:50,440
you create a new fancy alert or whatever

00:14:48,760 --> 00:14:52,870
other people are gonna see the pull

00:14:50,440 --> 00:14:54,400
request and learn from that how to

00:14:52,870 --> 00:14:56,530
manage the cluster is very simple it

00:14:54,400 --> 00:15:00,310
takes actually longer to get all the AWS

00:14:56,530 --> 00:15:02,170
data like VPC ID subnet security groups

00:15:00,310 --> 00:15:03,730
etc find them in your console and

00:15:02,170 --> 00:15:06,690
putting into struct rather than creating

00:15:03,730 --> 00:15:09,880
the stock it uses mimic use of

00:15:06,690 --> 00:15:13,270
conformation in it and hope to spawn to

00:15:09,880 --> 00:15:14,560
start and all good all make file to

00:15:13,270 --> 00:15:17,020
create the stack and update the stack

00:15:14,560 --> 00:15:18,190
and we also use docker composer so if

00:15:17,020 --> 00:15:19,970
you want to change the version of

00:15:18,190 --> 00:15:22,879
commedia so to graph an ax

00:15:19,970 --> 00:15:24,560
just edit the file and you go like make

00:15:22,879 --> 00:15:26,360
updates talk and after two minutes is

00:15:24,560 --> 00:15:29,300
all upper running and working and

00:15:26,360 --> 00:15:32,990
already briefly explained how to hook up

00:15:29,300 --> 00:15:35,839
github and SQS another thing that we

00:15:32,990 --> 00:15:37,579
wanted to do since it is a bunch of

00:15:35,839 --> 00:15:40,129
developer starting this thing is we

00:15:37,579 --> 00:15:41,839
wanted to share all the coaches and the

00:15:40,129 --> 00:15:44,540
parameters is very powerful but when it

00:15:41,839 --> 00:15:45,500
comes to integration in into a SS there

00:15:44,540 --> 00:15:50,920
are a couple of things you have to be

00:15:45,500 --> 00:15:50,920
aware of so I provided the number of get

00:15:51,279 --> 00:15:59,300
example so now to configure and commit

00:15:54,680 --> 00:16:00,800
just within a a SS what's next what did

00:15:59,300 --> 00:16:03,439
we achieve so the first term owns the

00:16:00,800 --> 00:16:06,889
word the most the most important thing

00:16:03,439 --> 00:16:08,930
because we immediately have lots of

00:16:06,889 --> 00:16:10,189
traction working on those project two

00:16:08,930 --> 00:16:10,790
teams immediately adopt that parameter

00:16:10,189 --> 00:16:13,850
sangrahani

00:16:10,790 --> 00:16:16,699
in the first two months and we had the

00:16:13,850 --> 00:16:18,079
ready beautiful friendly dashboards and

00:16:16,699 --> 00:16:20,959
the number of services link was between

00:16:18,079 --> 00:16:22,639
four and five and level of a lefting was

00:16:20,959 --> 00:16:24,829
really greatly improved for the first

00:16:22,639 --> 00:16:26,870
time we had warnings and critical so if

00:16:24,829 --> 00:16:28,730
something was going bad but not too bad

00:16:26,870 --> 00:16:31,850
we wouldn't wake up developers would we

00:16:28,730 --> 00:16:33,309
would just frightened slack if it was

00:16:31,850 --> 00:16:35,930
really bad we would wake up the people

00:16:33,309 --> 00:16:37,490
the client library which I was talking

00:16:35,930 --> 00:16:39,620
earlier on really supported a couple of

00:16:37,490 --> 00:16:41,620
version of play frameworks and it was

00:16:39,620 --> 00:16:44,809
the first release of the AWS

00:16:41,620 --> 00:16:46,550
CloudFormation template it I went to

00:16:44,809 --> 00:16:47,870
truly two different iterations because

00:16:46,550 --> 00:16:49,189
the first release it wasn't really

00:16:47,870 --> 00:16:51,620
friendly while the second one are just

00:16:49,189 --> 00:16:53,629
released few weeks ago in occasion for

00:16:51,620 --> 00:16:55,189
this prom gone is actually much more

00:16:53,629 --> 00:16:57,259
friendly user friendly should be much

00:16:55,189 --> 00:16:59,389
easier one funny thing is that we found

00:16:57,259 --> 00:17:01,639
out very quickly that we were over

00:16:59,389 --> 00:17:03,259
provisioning so once you start to apply

00:17:01,639 --> 00:17:05,209
Catriona understanding of what your

00:17:03,259 --> 00:17:07,669
services are actually using in terms of

00:17:05,209 --> 00:17:09,140
resources CPU memory etc etc you may

00:17:07,669 --> 00:17:10,730
even find out that maybe you over

00:17:09,140 --> 00:17:12,980
provision and so if you're familiar with

00:17:10,730 --> 00:17:14,569
AWS and you're using I don't know an m4

00:17:12,980 --> 00:17:16,339
extra large that costs hundreds of

00:17:14,569 --> 00:17:17,689
dollars per month and then all of a

00:17:16,339 --> 00:17:19,159
sudden you're only using half of you to

00:17:17,689 --> 00:17:22,220
say why am I wasting those money maybe

00:17:19,159 --> 00:17:23,820
I'll get a pay rise and now you can

00:17:22,220 --> 00:17:28,380
scale down our you won't never dies

00:17:23,820 --> 00:17:30,960
and socks aside one power very powerful

00:17:28,380 --> 00:17:34,350
thing happened in early December one of

00:17:30,960 --> 00:17:36,990
another team got a big problem with the

00:17:34,350 --> 00:17:38,490
disk space usage in in AWS they didn't

00:17:36,990 --> 00:17:40,380
have prometheus didn't get paged I

00:17:38,490 --> 00:17:41,370
didn't have those alerts that you can

00:17:40,380 --> 00:17:43,770
see on the right hand side of the slide

00:17:41,370 --> 00:17:46,920
and I was like I want to implement disk

00:17:43,770 --> 00:17:48,750
space warning and critical so that if it

00:17:46,920 --> 00:17:50,700
happens to us we will not have a

00:17:48,750 --> 00:17:52,260
downtime how can I do that it took

00:17:50,700 --> 00:17:54,120
literally 30 minutes to install the

00:17:52,260 --> 00:17:55,980
knowledge and the agent exporter on all

00:17:54,120 --> 00:17:58,980
the ec2 instances and to write these

00:17:55,980 --> 00:18:01,080
beautiful eight lines of code and those

00:17:58,980 --> 00:18:02,880
eight lines of code they work for all

00:18:01,080 --> 00:18:04,710
the ec2 instance we had at the time it

00:18:02,880 --> 00:18:06,840
was like hundred and twenty so I didn't

00:18:04,710 --> 00:18:08,880
have to copy and paste anything anywhere

00:18:06,840 --> 00:18:11,340
and that there is the five minutes thing

00:18:08,880 --> 00:18:12,510
there so whoever was presenting and

00:18:11,340 --> 00:18:16,080
earlier on you see I put the five

00:18:12,510 --> 00:18:17,760
minutes and as of today so now we're

00:18:16,080 --> 00:18:20,100
getting towards the end of the of the

00:18:17,760 --> 00:18:21,920
presentation what did you achieve four

00:18:20,100 --> 00:18:24,090
teams emigrated to premier sankirtana

00:18:21,920 --> 00:18:26,250
twenty most services have been fully

00:18:24,090 --> 00:18:28,350
migrated to it and we have 60 - ports

00:18:26,250 --> 00:18:30,690
thanks to the phenomenal community as

00:18:28,350 --> 00:18:32,190
well is an amazing project at the Scala

00:18:30,690 --> 00:18:36,480
client lab right now supports the most

00:18:32,190 --> 00:18:39,090
common color frameworks play fame work

00:18:36,480 --> 00:18:40,860
acacia TP I mentioned them earlier on we

00:18:39,090 --> 00:18:42,990
released the new parameters template as

00:18:40,860 --> 00:18:45,510
well as the Federation and this is the

00:18:42,990 --> 00:18:47,610
third take away may may not apply to

00:18:45,510 --> 00:18:50,850
everyone but for the way be working is

00:18:47,610 --> 00:18:53,490
we love to give teams extreme autonomy

00:18:50,850 --> 00:18:55,620
so we wouldn't force anyone to use a

00:18:53,490 --> 00:18:58,020
specific communities for a specific way

00:18:55,620 --> 00:19:00,960
of building dashboards so what we do is

00:18:58,020 --> 00:19:02,520
we have the two teams we have two

00:19:00,960 --> 00:19:03,600
example teams on the bottom of the slide

00:19:02,520 --> 00:19:05,610
is the search team and the

00:19:03,600 --> 00:19:08,130
personalization team that they only

00:19:05,610 --> 00:19:09,720
manage the same instance they their own

00:19:08,130 --> 00:19:11,790
instance sorry of committee sangra fauna

00:19:09,720 --> 00:19:13,980
and then what we do with the Federation

00:19:11,790 --> 00:19:15,870
we cover we provide a user experience

00:19:13,980 --> 00:19:18,000
like New Relic where you have a one-stop

00:19:15,870 --> 00:19:20,040
shop where you go there and you can see

00:19:18,000 --> 00:19:22,380
the health status or generic matrices of

00:19:20,040 --> 00:19:24,180
all the system that were are that are

00:19:22,380 --> 00:19:27,360
underlying in the underlying level

00:19:24,180 --> 00:19:31,200
hosted by sorry monitored by and from E

00:19:27,360 --> 00:19:33,090
to Sanger farm we release this thing a

00:19:31,200 --> 00:19:35,460
couple of months ago has been a great

00:19:33,090 --> 00:19:37,620
success as well so what did we really

00:19:35,460 --> 00:19:39,270
achieve so we now have

00:19:37,620 --> 00:19:41,460
for custom dashboards that really give

00:19:39,270 --> 00:19:44,130
us the tail picture what's going on on

00:19:41,460 --> 00:19:45,780
our services and we are very confident

00:19:44,130 --> 00:19:48,300
when we do production releases we really

00:19:45,780 --> 00:19:49,830
know if the new service the cannery the

00:19:48,300 --> 00:19:51,480
health the health status of the pork

00:19:49,830 --> 00:19:54,720
cannery if it is doing well if it is

00:19:51,480 --> 00:19:58,680
seeking spying and of course production

00:19:54,720 --> 00:20:00,590
release overall reliability as well and

00:19:58,680 --> 00:20:03,990
saving some money when you found out

00:20:00,590 --> 00:20:06,660
that your other provisioning so what is

00:20:03,990 --> 00:20:08,820
next what's next is failover in the

00:20:06,660 --> 00:20:12,150
confirmation template is there but is

00:20:08,820 --> 00:20:14,220
not fully it's not fully there yet we

00:20:12,150 --> 00:20:16,200
only have one instance so if we want an

00:20:14,220 --> 00:20:17,580
active active or active passive

00:20:16,200 --> 00:20:18,960
configuration this still needs to be

00:20:17,580 --> 00:20:20,100
implemented we really love

00:20:18,960 --> 00:20:22,620
I would really would love to add this

00:20:20,100 --> 00:20:24,510
soon meta monitoring we have a very

00:20:22,620 --> 00:20:26,790
simplistic simplistic way of doing this

00:20:24,510 --> 00:20:29,100
the federated distance is controlling

00:20:26,790 --> 00:20:30,929
the child and the children the children

00:20:29,100 --> 00:20:32,880
the children are checking their peers as

00:20:30,929 --> 00:20:35,550
well as the federated distance last but

00:20:32,880 --> 00:20:37,050
not least many times we we thought we

00:20:35,550 --> 00:20:38,760
had delivered a new version of the

00:20:37,050 --> 00:20:41,490
configuration in production but because

00:20:38,760 --> 00:20:42,750
the Yama was incorrect just nothing

00:20:41,490 --> 00:20:45,570
really changed we would love to

00:20:42,750 --> 00:20:49,170
integrate from tool as well into the

00:20:45,570 --> 00:20:50,790
github pull request mechanism and maybe

00:20:49,170 --> 00:20:55,910
this is gonna be done as well in next

00:20:50,790 --> 00:20:55,910
few days this is it questions

00:20:59,330 --> 00:21:10,159
I hope you're still awake first of all

00:21:08,570 --> 00:21:13,249
thanks for option sourcing so much of

00:21:10,159 --> 00:21:15,860
your work and before now have you

00:21:13,249 --> 00:21:17,619
considered common for monitoring Scala

00:21:15,860 --> 00:21:23,779
occur and so on before writing your own

00:21:17,619 --> 00:21:26,090
and yes we did look very quick look but

00:21:23,779 --> 00:21:27,889
the parameters use case was fitting as

00:21:26,090 --> 00:21:29,480
so well our requirements and the fact

00:21:27,889 --> 00:21:31,009
that we were running into the cloud that

00:21:29,480 --> 00:21:33,859
we decided to give it a try and give a

00:21:31,009 --> 00:21:35,450
shot and this actually also fits kind of

00:21:33,859 --> 00:21:39,460
nicely what what we have in the roadmap

00:21:35,450 --> 00:21:42,080
we are thinking the next few months to

00:21:39,460 --> 00:21:44,869
adopt kubernetes as well and Grenadiers

00:21:42,080 --> 00:21:48,019
has support for communities like out of

00:21:44,869 --> 00:21:51,590
the box so this is one of the reason why

00:21:48,019 --> 00:21:53,809
we decided to go for committees but for

00:21:51,590 --> 00:21:57,049
writing your own framework that exposed

00:21:53,809 --> 00:21:58,789
Skylar metrics I know we we were happy

00:21:57,049 --> 00:22:02,809
just to write our own yeah no we didn't

00:21:58,789 --> 00:22:05,539
read like a deep look into that the site

00:22:02,809 --> 00:22:08,659
so as you told like you we put lot of

00:22:05,539 --> 00:22:10,999
metrics in your code which was provided

00:22:08,659 --> 00:22:14,330
by New Relic right like Layton sees or

00:22:10,999 --> 00:22:16,279
RPS something like that so what all the

00:22:14,330 --> 00:22:19,789
stuff you turn it's now completely

00:22:16,279 --> 00:22:22,100
removed New Relic or still like New

00:22:19,789 --> 00:22:23,840
Relic or AppDynamics like these are the

00:22:22,100 --> 00:22:26,269
tools which also give lot of other

00:22:23,840 --> 00:22:28,369
insights right so this is a very good

00:22:26,269 --> 00:22:29,899
question so at the beginning we weren't

00:22:28,369 --> 00:22:31,820
sure how well we were playing with

00:22:29,899 --> 00:22:33,499
parameters so there was a there was a

00:22:31,820 --> 00:22:35,419
period in which we had the Botany relic

00:22:33,499 --> 00:22:38,419
and Prometheus client running on the

00:22:35,419 --> 00:22:40,519
same house and because of course we

00:22:38,419 --> 00:22:42,109
didn't just want to switch to the new

00:22:40,519 --> 00:22:43,399
solution may be the solution wasn't

00:22:42,109 --> 00:22:45,200
working and all of a sudden we didn't

00:22:43,399 --> 00:22:46,549
have a lab thing so it was a gradual at

00:22:45,200 --> 00:22:47,749
the beginning we had a new relic and

00:22:46,549 --> 00:22:50,779
Prometheus and then as soon as the

00:22:47,749 --> 00:22:52,909
confidence and abusing product

00:22:50,779 --> 00:22:55,159
Prometheus was increasing we started to

00:22:52,909 --> 00:22:57,559
make less and less use of euralia cantle

00:22:55,159 --> 00:22:59,480
will completely drop the new relic and

00:22:57,559 --> 00:23:02,210
we're only using committees with with

00:22:59,480 --> 00:23:04,609
the automatic reporting that is doing

00:23:02,210 --> 00:23:05,919
out the rest api calls plus all the

00:23:04,609 --> 00:23:08,820
instrumentation that we will

00:23:05,919 --> 00:23:11,340
programmatically doing inside the code

00:23:08,820 --> 00:23:13,500
in that case you have to change your

00:23:11,340 --> 00:23:15,210
code a lot right because you have to

00:23:13,500 --> 00:23:17,280
instrument everything but these tools

00:23:15,210 --> 00:23:20,070
like provide lot of things out of box

00:23:17,280 --> 00:23:21,990
like click how how the call flow will go

00:23:20,070 --> 00:23:23,820
through you know complete applications

00:23:21,990 --> 00:23:27,840
yes where is the exceptions and lot of

00:23:23,820 --> 00:23:29,760
other stuff right but the two like

00:23:27,840 --> 00:23:31,740
Prometheus you just gather the metrics

00:23:29,760 --> 00:23:34,740
may be latency or some numbers basic

00:23:31,740 --> 00:23:37,020
latest but it will not catch whoa what's

00:23:34,740 --> 00:23:38,970
the call stack what all the components

00:23:37,020 --> 00:23:42,390
have it's what all the latency is right

00:23:38,970 --> 00:23:44,940
so what's your suggestion on the same

00:23:42,390 --> 00:23:46,680
and it may sound weird to you but we had

00:23:44,940 --> 00:23:48,090
exactly the same fear at the beginning

00:23:46,680 --> 00:23:49,380
was like how we're gonna lose the stack

00:23:48,090 --> 00:23:51,420
trace if there is an error or the

00:23:49,380 --> 00:23:53,760
exception or all these kind of things

00:23:51,420 --> 00:23:56,220
and but whenever we migrated then we

00:23:53,760 --> 00:23:58,230
were able to detect the problems at the

00:23:56,220 --> 00:24:01,380
root we really didn't need access to the

00:23:58,230 --> 00:24:02,790
stack trace anymore so ideally would be

00:24:01,380 --> 00:24:05,190
nice just to have the stack trace be

00:24:02,790 --> 00:24:06,840
reported as well but in practice when we

00:24:05,190 --> 00:24:08,850
stopped having it we really didn't feel

00:24:06,840 --> 00:24:10,530
like we were missing it so if you're

00:24:08,850 --> 00:24:13,350
thinking to do the same I may suggest

00:24:10,530 --> 00:24:15,210
you to be brave and try to one stage

00:24:13,350 --> 00:24:17,250
well your confidence is good enough with

00:24:15,210 --> 00:24:18,990
parameters just don't you stone you

00:24:17,250 --> 00:24:21,270
relic anymore and see and see where you

00:24:18,990 --> 00:24:23,360
lands if you still feel you need it you

00:24:21,270 --> 00:24:26,220
come back and you use what I'm saying is

00:24:23,360 --> 00:24:27,930
that maybe makes sense for you to have

00:24:26,220 --> 00:24:29,160
both of them if you really need a good

00:24:27,930 --> 00:24:42,780
insight as well as the out-of-the-box

00:24:29,160 --> 00:24:56,040
solution but yeah we thank you I just

00:24:42,780 --> 00:24:58,800
had the same question as before so what

00:24:56,040 --> 00:25:02,130
kind of instance is a Tobias instance

00:24:58,800 --> 00:25:04,020
types do you use for Prometheus so and

00:25:02,130 --> 00:25:06,810
initially I was running everything

00:25:04,020 --> 00:25:10,080
online for large and that is a

00:25:06,810 --> 00:25:12,630
relatively small instance and that is

00:25:10,080 --> 00:25:14,550
not bustable instance and we found out

00:25:12,630 --> 00:25:15,860
it was still loaded provisioning because

00:25:14,550 --> 00:25:18,390
we were starting to use also the

00:25:15,860 --> 00:25:20,190
federation and whenever you install the

00:25:18,390 --> 00:25:21,059
federation parameter seems to be working

00:25:20,190 --> 00:25:23,129
a little bit

00:25:21,059 --> 00:25:25,919
but we eventually tuned that the

00:25:23,129 --> 00:25:28,499
parameters of them and the frequency of

00:25:25,919 --> 00:25:30,869
the poll and then we noticed that the

00:25:28,499 --> 00:25:33,509
tweaking those values accordingly would

00:25:30,869 --> 00:25:34,889
actually make much less CPU pressure on

00:25:33,509 --> 00:25:37,559
the instances so we've managed to go

00:25:34,889 --> 00:25:41,340
from an m4 instance to t2 instance that

00:25:37,559 --> 00:25:44,879
gives you possible and solution so if in

00:25:41,340 --> 00:25:45,929
case for whichever amount of time one of

00:25:44,879 --> 00:25:47,909
these since he's working a little bit

00:25:45,929 --> 00:25:49,559
harder there is no problem because it's

00:25:47,909 --> 00:25:52,139
gonna burst automatically and provide

00:25:49,559 --> 00:25:53,489
more computational power so for what

00:25:52,139 --> 00:25:54,989
does it for me to stock so we're talking

00:25:53,489 --> 00:25:57,210
about parameters manager left

00:25:54,989 --> 00:25:59,279
sorry producer left manager and the push

00:25:57,210 --> 00:26:01,169
gateway plus Agrafena instance it was

00:25:59,279 --> 00:26:04,739
all in the same VM sorry on same

00:26:01,169 --> 00:26:06,629
assistance and it was a TT to lodge and

00:26:04,739 --> 00:26:12,809
did you have like any additional

00:26:06,629 --> 00:26:13,759
latencies because you're using EFS no

00:26:12,809 --> 00:26:17,549
not really

00:26:13,759 --> 00:26:19,710
those instances are EBS optimized so the

00:26:17,549 --> 00:26:20,850
the the transfer rate on the volume is

00:26:19,710 --> 00:26:25,669
actually really high and we are using

00:26:20,850 --> 00:26:28,049
SSD instances yes SSE sorry SSD volume

00:26:25,669 --> 00:26:29,789
types that are really fast that one used

00:26:28,049 --> 00:26:32,429
for the database they're unbelievably

00:26:29,789 --> 00:26:34,559
performant and we wrongly configured

00:26:32,429 --> 00:26:38,009
them the spinning disk the country

00:26:34,559 --> 00:26:40,049
member was The Economist in in in AWS

00:26:38,009 --> 00:26:42,149
but we had the major problems because

00:26:40,049 --> 00:26:43,980
times if there was some software

00:26:42,149 --> 00:26:45,419
indexing or something like he promised

00:26:43,980 --> 00:26:47,429
would become completely responsible that

00:26:45,419 --> 00:26:51,299
was an initial our path because to

00:26:47,429 --> 00:26:52,859
configure the stock funding thank you in

00:26:51,299 --> 00:26:56,220
relation to the earlier question about

00:26:52,859 --> 00:26:57,690
camel and akka if you're looking to

00:26:56,220 --> 00:26:59,340
monitor those and work things

00:26:57,690 --> 00:27:00,779
open-source something called Prometheus

00:26:59,340 --> 00:27:02,879
akka which will basically provide

00:27:00,779 --> 00:27:06,059
similar instrumentation as calendars

00:27:02,879 --> 00:27:08,369
Bush for Prometheus this camera doesn't

00:27:06,059 --> 00:27:09,899
quite sink up and have radius works but

00:27:08,369 --> 00:27:16,499
also want to use come on stuff and

00:27:09,899 --> 00:27:19,710
Monsanto have a library for - what's

00:27:16,499 --> 00:27:21,899
your retention period potato and so we

00:27:19,710 --> 00:27:24,419
started with 15 days and we had the

00:27:21,899 --> 00:27:27,869
moment pushed to 30 days but we will

00:27:24,419 --> 00:27:30,599
soon try to take a look at how to

00:27:27,869 --> 00:27:32,190
persist the data after 30 days somewhere

00:27:30,599 --> 00:27:33,900
else and the boxes later

00:27:32,190 --> 00:27:35,940
and at the moment at the beginning it

00:27:33,900 --> 00:27:37,830
was really one of our problems because

00:27:35,940 --> 00:27:39,510
we had the bigger fish to catch to the

00:27:37,830 --> 00:27:41,130
stage but as soon as like heyou stuff

00:27:39,510 --> 00:27:44,280
product and say i would be nice to see

00:27:41,130 --> 00:27:46,350
how the system has been done after the

00:27:44,280 --> 00:27:48,480
15 times span 50 days time span or 30

00:27:46,350 --> 00:27:52,770
days and we start to talking about that

00:27:48,480 --> 00:27:56,180
as well and definitely gonna save the

00:27:52,770 --> 00:27:56,180
data somewhere else for a long term

00:27:56,870 --> 00:28:00,140
alright thank you

00:28:05,390 --> 00:28:13,280
ah one more one more question

00:28:09,920 --> 00:28:15,340
one more question yeah it's not a

00:28:13,280 --> 00:28:19,250
technical question but it seems

00:28:15,340 --> 00:28:22,820
interesting to ask how much time do you

00:28:19,250 --> 00:28:24,890
spend to convert your formal system to

00:28:22,820 --> 00:28:29,809
the new one like how do we would

00:28:24,890 --> 00:28:31,610
quantify the effort so and this was one

00:28:29,809 --> 00:28:33,470
of the point of the representation so at

00:28:31,610 --> 00:28:35,690
the beginning that the plan was only one

00:28:33,470 --> 00:28:37,790
team starts to work on this thing and I

00:28:35,690 --> 00:28:40,669
was working on these on the parameter

00:28:37,790 --> 00:28:42,770
sanger of my integration for 25% 50% of

00:28:40,669 --> 00:28:44,720
my time between November and December

00:28:42,770 --> 00:28:46,400
but once that we created the Scala

00:28:44,720 --> 00:28:49,040
library as well as the CloudFormation

00:28:46,400 --> 00:28:51,530
template everything and started to spin

00:28:49,040 --> 00:28:53,240
off very quickly because all the

00:28:51,530 --> 00:28:55,429
repetition all the boilerplate was

00:28:53,240 --> 00:28:57,380
already nicely wrapped up somewhere so

00:28:55,429 --> 00:28:59,150
that if you wanted to spin yet another

00:28:57,380 --> 00:29:01,070
service and start parameters from

00:28:59,150 --> 00:29:02,600
scratch it would and instrumented from

00:29:01,070 --> 00:29:04,340
scratch with parameters it would come

00:29:02,600 --> 00:29:05,870
basically for free but if you were

00:29:04,340 --> 00:29:09,020
migrating a service I would have

00:29:05,870 --> 00:29:10,460
recommend my colleagues to have as I was

00:29:09,020 --> 00:29:11,750
mentioning earlier on bottom you rarely

00:29:10,460 --> 00:29:14,809
came from you just ran him on same

00:29:11,750 --> 00:29:16,100
service and then with the time trying to

00:29:14,809 --> 00:29:17,419
replace all the new relic

00:29:16,100 --> 00:29:18,620
instrumentation or college

00:29:17,419 --> 00:29:21,620
instrumentation with the parameters

00:29:18,620 --> 00:29:24,650
instrumentation so if one day you ask me

00:29:21,620 --> 00:29:26,750
like I have 6,000 I own line lock code

00:29:24,650 --> 00:29:28,130
up and I want to cycle on you really

00:29:26,750 --> 00:29:29,419
cannot start with Prometheus well that

00:29:28,130 --> 00:29:31,490
is going to take probably a couple of

00:29:29,419 --> 00:29:34,280
weeks time if you do that in one go but

00:29:31,490 --> 00:29:37,100
if you gradually go from one to the

00:29:34,280 --> 00:29:38,710
other it should be much much faster yeah

00:29:37,100 --> 00:29:51,490
all right

00:29:38,710 --> 00:29:51,680
[Music]

00:29:51,490 --> 00:29:54,819
you

00:29:51,680 --> 00:29:54,819

YouTube URL: https://www.youtube.com/watch?v=btnXuFLcpS8


