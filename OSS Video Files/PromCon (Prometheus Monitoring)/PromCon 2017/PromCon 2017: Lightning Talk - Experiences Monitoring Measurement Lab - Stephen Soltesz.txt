Title: PromCon 2017: Lightning Talk - Experiences Monitoring Measurement Lab - Stephen Soltesz
Publication date: 2017-09-01
Playlist: PromCon 2017
Description: 
	* Abstract:

Lightning Talk

* Speaker:

Stephen Soltesz

* Slides:

https://goo.gl/UY7uXT

* PromCon website:

https://promcon.io/
Captions: 
	00:00:00,380 --> 00:00:17,970
[Music]

00:00:13,820 --> 00:00:19,050
I'm Steven I'm an SRE and today I'm

00:00:17,970 --> 00:00:22,609
gonna tell you about experiences

00:00:19,050 --> 00:00:24,210
monitoring measurement lab since 2009

00:00:22,609 --> 00:00:25,410
with your help

00:00:24,210 --> 00:00:27,570
measurement lab has been measuring the

00:00:25,410 --> 00:00:29,580
public Internet and making that data

00:00:27,570 --> 00:00:32,040
publicly available and lab is a

00:00:29,580 --> 00:00:34,739
partnership between Public Interest

00:00:32,040 --> 00:00:37,890
nonprofit groups network researchers and

00:00:34,739 --> 00:00:39,660
companies like Google who are interested

00:00:37,890 --> 00:00:42,120
in the internet being faster for

00:00:39,660 --> 00:00:44,969
absolutely everyone over the last year

00:00:42,120 --> 00:00:46,800
we've upgraded almost 450 machines

00:00:44,969 --> 00:00:49,280
around the world and we're continuing to

00:00:46,800 --> 00:00:53,850
deploy to new sites all the time

00:00:49,280 --> 00:00:55,079
all emmab tests are user initiated so we

00:00:53,850 --> 00:00:57,180
are very dependent on client

00:00:55,079 --> 00:01:00,210
integrations like the internet speed

00:00:57,180 --> 00:01:02,430
test one box in some countries when you

00:01:00,210 --> 00:01:04,110
enter how fast is my internet you'll be

00:01:02,430 --> 00:01:07,760
presented with the option of running a

00:01:04,110 --> 00:01:10,490
speed test that connect mmm lab servers

00:01:07,760 --> 00:01:12,810
we get about a million tests a day and

00:01:10,490 --> 00:01:15,150
we're on track by the end of this year

00:01:12,810 --> 00:01:18,270
to get about a billion tests over the

00:01:15,150 --> 00:01:19,920
lifetime of the project and once we have

00:01:18,270 --> 00:01:21,479
enough tests from enough people in a

00:01:19,920 --> 00:01:23,009
given region we can perform performance

00:01:21,479 --> 00:01:27,270
analysis for user-visible

00:01:23,009 --> 00:01:30,170
Internet performance for example in 2014

00:01:27,270 --> 00:01:33,420
the isp interconnection report

00:01:30,170 --> 00:01:34,790
discovered severe congestion in network

00:01:33,420 --> 00:01:37,500
interconnects in the United States

00:01:34,790 --> 00:01:40,770
ultimately that report informed the

00:01:37,500 --> 00:01:43,350
FCC's open Internet order in support of

00:01:40,770 --> 00:01:44,640
network neutrality and more recently our

00:01:43,350 --> 00:01:48,780
team has been working with regulators in

00:01:44,640 --> 00:01:51,869
the EU to support similar efforts so

00:01:48,780 --> 00:01:53,759
since 2009 the software stack we've

00:01:51,869 --> 00:01:55,049
depended on has been as largely say the

00:01:53,759 --> 00:01:57,840
same and it's starting to show its age

00:01:55,049 --> 00:02:00,149
so this year we're literally rewriting

00:01:57,840 --> 00:02:02,130
or updating everything to use modern

00:02:00,149 --> 00:02:04,680
open-source tools like kubernetes

00:02:02,130 --> 00:02:06,360
griffin ax and prometheus and putting

00:02:04,680 --> 00:02:09,750
running everything we can in Google

00:02:06,360 --> 00:02:13,340
cloud platform so far four new tools

00:02:09,750 --> 00:02:13,340
have helped us adopt prometheus

00:02:14,050 --> 00:02:18,260
conventions for unit tests for alerts we

00:02:17,120 --> 00:02:20,030
write unit tests for code we run in

00:02:18,260 --> 00:02:21,379
production infrastructure and we write

00:02:20,030 --> 00:02:23,930
alerts for the infrastructure we care

00:02:21,379 --> 00:02:26,780
about we really want to write unit tests

00:02:23,930 --> 00:02:28,340
for alerts before we depend on them that

00:02:26,780 --> 00:02:29,810
feature isn't supported directly yet but

00:02:28,340 --> 00:02:32,709
there is an open issue tracking the

00:02:29,810 --> 00:02:35,450
feature and but and until that feature

00:02:32,709 --> 00:02:36,530
is available we've adopted a convention

00:02:35,450 --> 00:02:39,049
around a very simple utility we've

00:02:36,530 --> 00:02:44,060
created called query tester query tester

00:02:39,049 --> 00:02:48,440
uses prom QL test interface and the prom

00:02:44,060 --> 00:02:50,150
QL test language basically for every

00:02:48,440 --> 00:02:52,790
alert that we create we create a

00:02:50,150 --> 00:02:54,650
corresponding prom ql test file we

00:02:52,790 --> 00:02:56,599
verify that the query tester passes and

00:02:54,650 --> 00:02:57,829
then during code review we do a manual

00:02:56,599 --> 00:02:59,269
comparison between the alert condition

00:02:57,829 --> 00:03:05,660
and the prom QL test

00:02:59,269 --> 00:03:07,760
oh no yeah I didn't want to include this

00:03:05,660 --> 00:03:08,870
slide because I didn't want to admit to

00:03:07,760 --> 00:03:12,799
everyone that we have production

00:03:08,870 --> 00:03:15,709
dependencies on Nadya's because we do

00:03:12,799 --> 00:03:16,910
for eight years so some of those

00:03:15,709 --> 00:03:19,579
dependencies are going to take time to

00:03:16,910 --> 00:03:21,739
untangle but as we were learning more

00:03:19,579 --> 00:03:22,940
about from Prometheus and Griffon a-- we

00:03:21,739 --> 00:03:24,560
wanted to take advantage of some of the

00:03:22,940 --> 00:03:26,230
visualization capabilities of the

00:03:24,560 --> 00:03:28,730
performance data that

00:03:26,230 --> 00:03:29,720
Nadya's is collecting but otherwise

00:03:28,730 --> 00:03:32,510
useless to us

00:03:29,720 --> 00:03:34,730
and while Prometheus has tons of

00:03:32,510 --> 00:03:36,680
third-party exporters at the time there

00:03:34,730 --> 00:03:39,799
was no Prometheus exporter so we created

00:03:36,680 --> 00:03:41,540
one using the Nagios Live status plugin

00:03:39,799 --> 00:03:43,819
the Nagios exporter queries the service

00:03:41,540 --> 00:03:47,000
status of all services and then does a

00:03:43,819 --> 00:03:49,430
very manual augment mech mechanic

00:03:47,000 --> 00:03:53,980
translation into metrics that Prometheus

00:03:49,430 --> 00:03:56,919
can scrape new service discovery for GCP

00:03:53,980 --> 00:03:59,959
one of our new services is an app engine

00:03:56,919 --> 00:04:01,480
flexible environment service and by

00:03:59,959 --> 00:04:03,980
design app engine does not expect

00:04:01,480 --> 00:04:07,120
clients to need to address individual

00:04:03,980 --> 00:04:10,010
instances so we can't use the front door

00:04:07,120 --> 00:04:14,030
and even though Flex VMs our GCE

00:04:10,010 --> 00:04:16,549
instances under the surface the GCE API

00:04:14,030 --> 00:04:19,310
does not rip does not list or return

00:04:16,549 --> 00:04:21,669
Flex VMs so the GCE service discovery

00:04:19,310 --> 00:04:23,659
and Prometheus doesn't help us here

00:04:21,669 --> 00:04:25,730
fortunately the App Engine management

00:04:23,659 --> 00:04:29,060
API does make it possible

00:04:25,730 --> 00:04:30,140
to list all of the nodes and instances

00:04:29,060 --> 00:04:32,090
associated which we give an app engine

00:04:30,140 --> 00:04:34,850
service inversion so if you set up the

00:04:32,090 --> 00:04:37,070
GCP service discovery as a companion to

00:04:34,850 --> 00:04:40,430
Prometheus every instance can be

00:04:37,070 --> 00:04:42,140
discovered and scraped we also run

00:04:40,430 --> 00:04:44,300
multiple kubernetes clusters in GTA ii

00:04:42,140 --> 00:04:49,400
each with a local Prometheus instance to

00:04:44,300 --> 00:04:52,370
automatically okay to automatically

00:04:49,400 --> 00:04:53,900
discover new clusters and monitor the

00:04:52,370 --> 00:04:57,020
availability in federated deployments

00:04:53,900 --> 00:04:59,840
for that we use the Google container

00:04:57,020 --> 00:05:01,520
engine API and the kerbin API and we

00:04:59,840 --> 00:05:03,530
think we can use the same techniques to

00:05:01,520 --> 00:05:08,840
automatically add data sources to graph

00:05:03,530 --> 00:05:11,300
on a new alert manager web book receiver

00:05:08,840 --> 00:05:13,100
for github yeah and not only do we have

00:05:11,300 --> 00:05:17,360
operational dependencies on adios but

00:05:13,100 --> 00:05:18,740
Nadia sends notifications by email we do

00:05:17,360 --> 00:05:21,350
not want to repeat those mistakes from

00:05:18,740 --> 00:05:25,700
the past and at this point our team has

00:05:21,350 --> 00:05:28,700
mostly been working from github in slack

00:05:25,700 --> 00:05:31,310
so we use the alert managers web book

00:05:28,700 --> 00:05:31,910
API github API and created the github

00:05:31,310 --> 00:05:34,130
receiver

00:05:31,910 --> 00:05:37,810
thank you very much please let me know

00:05:34,130 --> 00:05:37,810
if you've solved seamless problems

00:05:37,830 --> 00:05:50,520
[Music]

00:05:50,320 --> 00:05:53,660
you

00:05:50,520 --> 00:05:53,660

YouTube URL: https://www.youtube.com/watch?v=ZJCrn5FV-TA


