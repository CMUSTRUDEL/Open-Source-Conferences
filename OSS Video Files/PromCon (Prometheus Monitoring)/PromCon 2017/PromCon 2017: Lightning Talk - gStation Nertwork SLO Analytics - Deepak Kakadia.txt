Title: PromCon 2017: Lightning Talk - gStation Nertwork SLO Analytics - Deepak Kakadia
Publication date: 2017-09-01
Playlist: PromCon 2017
Description: 
	* Abstract:

Lightning Talk

* Speaker:

Deepak Kakadia

* Slides:

* PromCon website:

https://promcon.io/
Captions: 
	00:00:00,380 --> 00:00:16,980
[Music]

00:00:13,820 --> 00:00:18,210
okay thank you for this speaking slot I

00:00:16,980 --> 00:00:19,640
don't really have a presentation ready

00:00:18,210 --> 00:00:22,529
so I'll just give you a quick overview

00:00:19,640 --> 00:00:25,019
of what I'm working on this is actually

00:00:22,529 --> 00:00:27,630
the third data science talks so the

00:00:25,019 --> 00:00:29,070
basic idea is that we have all these

00:00:27,630 --> 00:00:31,230
metrics but what really what really

00:00:29,070 --> 00:00:34,410
matters at the end of the day so what we

00:00:31,230 --> 00:00:36,989
do is we provide a network service

00:00:34,410 --> 00:00:40,079
actually it's a Wi-Fi service we do

00:00:36,989 --> 00:00:41,340
railTel link NYC and we provide a

00:00:40,079 --> 00:00:44,420
certain level of service why not

00:00:41,340 --> 00:00:46,440
availabilities we promise to nines

00:00:44,420 --> 00:00:49,559
whenever it goes below a certain level

00:00:46,440 --> 00:00:52,020
we want to know what what the root cause

00:00:49,559 --> 00:00:53,670
is so the basic idea is we have so many

00:00:52,020 --> 00:00:54,660
metrics a lot of them don't really

00:00:53,670 --> 00:00:56,670
matter because there's so much

00:00:54,660 --> 00:00:58,949
redundancy in a network even if a router

00:00:56,670 --> 00:01:01,140
fails who cares that units will be trout

00:00:58,949 --> 00:01:05,309
transporting customer traffic around the

00:01:01,140 --> 00:01:07,790
surviving nodes so basically this is

00:01:05,309 --> 00:01:11,280
very similar to Georgia's talk and

00:01:07,790 --> 00:01:12,360
Mathias has released talk but it's a

00:01:11,280 --> 00:01:14,040
little bit different instead of having

00:01:12,360 --> 00:01:15,710
CSV files we want to automate the

00:01:14,040 --> 00:01:19,619
processes we're constantly getting

00:01:15,710 --> 00:01:23,130
Prometheus metric data we're pumping it

00:01:19,619 --> 00:01:24,900
into various different intermediate data

00:01:23,130 --> 00:01:26,549
stores that we probably want to improve

00:01:24,900 --> 00:01:28,799
the integration of that as we're talking

00:01:26,549 --> 00:01:30,000
to some folks here to try to make it

00:01:28,799 --> 00:01:32,579
more seamless but basically what we do

00:01:30,000 --> 00:01:37,409
is we pump it from Prometheus to open TS

00:01:32,579 --> 00:01:38,970
DB the bigquery I do my analytics so the

00:01:37,409 --> 00:01:41,189
way we do analytics is actually a

00:01:38,970 --> 00:01:43,740
multiple with phases but the basic idea

00:01:41,189 --> 00:01:45,509
is that we very similar to what George

00:01:43,740 --> 00:01:48,509
did he put into R so we have something

00:01:45,509 --> 00:01:49,920
called collab notebook which I know is

00:01:48,509 --> 00:01:51,780
Georgia did the same thing but I guess

00:01:49,920 --> 00:01:53,850
he's using a Jupiter notebook that's the

00:01:51,780 --> 00:01:56,250
open source version then we come up with

00:01:53,850 --> 00:01:59,399
the analysis so that here's an example

00:01:56,250 --> 00:02:00,899
of this takes too long to run as a demo

00:01:59,399 --> 00:02:03,180
but basically this is an example the

00:02:00,899 --> 00:02:06,030
cold lab notebook and the basic idea is

00:02:03,180 --> 00:02:07,229
that we have so many different analytics

00:02:06,030 --> 00:02:08,640
algorithms how do you know which is the

00:02:07,229 --> 00:02:10,979
best one how do you want to know which

00:02:08,640 --> 00:02:13,530
one actually what is actually accurate

00:02:10,979 --> 00:02:16,500
so what our approach is

00:02:13,530 --> 00:02:18,540
we take about 800 different metrics and

00:02:16,500 --> 00:02:21,060
we have to process it in different ways

00:02:18,540 --> 00:02:22,230
to normalize it but then we run it

00:02:21,060 --> 00:02:23,910
through all the different classifiers

00:02:22,230 --> 00:02:25,980
but we also do actually this is not

00:02:23,910 --> 00:02:27,330
really a public yet so if you don't take

00:02:25,980 --> 00:02:29,700
pictures I appreciate that thank sorry

00:02:27,330 --> 00:02:31,890
about that and I don't know if I don't

00:02:29,700 --> 00:02:33,780
even get approval to speak about this

00:02:31,890 --> 00:02:36,450
but oh please could be okay but

00:02:33,780 --> 00:02:37,980
basically what we do what we do is we

00:02:36,450 --> 00:02:39,960
actually evaluate how accurate is

00:02:37,980 --> 00:02:42,810
because we bust it up into training data

00:02:39,960 --> 00:02:43,860
and test data so the test date is not

00:02:42,810 --> 00:02:47,070
contaminate with any of the training

00:02:43,860 --> 00:02:50,700
data and then we take the best the best

00:02:47,070 --> 00:02:52,770
accurate results that will be there used

00:02:50,700 --> 00:02:54,720
for prediction so in this case here I

00:02:52,770 --> 00:02:57,360
just finished running it right now but

00:02:54,720 --> 00:03:01,709
basically here you can see that instead

00:02:57,360 --> 00:03:03,360
of getting a CSV data the nice thing

00:03:01,709 --> 00:03:04,860
about this is we can actually pop it

00:03:03,360 --> 00:03:07,560
into a date of him directly by doing a

00:03:04,860 --> 00:03:09,270
sequel command the main key thing is

00:03:07,560 --> 00:03:12,570
that we have the right libraries to

00:03:09,270 --> 00:03:17,970
import it so I'll show you real quick

00:03:12,570 --> 00:03:19,790
here that here's all these different

00:03:17,970 --> 00:03:21,660
sequel queries that we're taking the top

00:03:19,790 --> 00:03:24,239
predictors we don't even know which ones

00:03:21,660 --> 00:03:25,260
any wouldn't really matter so we went

00:03:24,239 --> 00:03:27,810
through all the different classifiers

00:03:25,260 --> 00:03:33,870
and then we actually evaluate it so I'll

00:03:27,810 --> 00:03:37,470
just jump real quick to somebody's uh so

00:03:33,870 --> 00:03:40,650
so we tried basically most of the linear

00:03:37,470 --> 00:03:42,060
regression classifiers are completely

00:03:40,650 --> 00:03:44,400
worthless because most of these problems

00:03:42,060 --> 00:03:46,709
are not linear so I think so - I'd also

00:03:44,400 --> 00:03:51,360
talked about that when he asked earlier

00:03:46,709 --> 00:03:53,430
on what we also tried as Georgia did

00:03:51,360 --> 00:03:55,230
lasso regression the nice thing about

00:03:53,430 --> 00:03:57,660
that is that instead of having to do all

00:03:55,230 --> 00:03:59,220
this factor analysis it will regularize

00:03:57,660 --> 00:04:00,269
all the coefficients down to zero that

00:03:59,220 --> 00:04:03,060
ones that are useless

00:04:00,269 --> 00:04:05,250
and it will actually give high weights

00:04:03,060 --> 00:04:07,830
to ones that are not users like for

00:04:05,250 --> 00:04:10,140
example here this is a station the three

00:04:07,830 --> 00:04:14,910
stars means it's actually it has a very

00:04:10,140 --> 00:04:17,459
low p-value and that's a useful metric

00:04:14,910 --> 00:04:19,350
so here you can see here amongst all the

00:04:17,459 --> 00:04:20,700
different metrics these are the ones

00:04:19,350 --> 00:04:22,979
that may actually matter for the dataset

00:04:20,700 --> 00:04:25,530
and for the prom we're trying to solve

00:04:22,979 --> 00:04:29,280
in this case here my response variable

00:04:25,530 --> 00:04:31,350
is the SLO level so basically in a

00:04:29,280 --> 00:04:32,700
nutshell what this thing does is you

00:04:31,350 --> 00:04:35,400
have all this type of data what really

00:04:32,700 --> 00:04:37,230
matters from the for you for your

00:04:35,400 --> 00:04:39,570
particular objective and this will

00:04:37,230 --> 00:04:41,160
reduce it so this was trying to automate

00:04:39,570 --> 00:04:44,000
a lot of these tasks you don't have to

00:04:41,160 --> 00:04:48,230
manually but the best fits all so far

00:04:44,000 --> 00:04:50,730
are using support vector machines and

00:04:48,230 --> 00:04:55,530
did we and that's pretty obvious because

00:04:50,730 --> 00:04:57,840
these are nonlinear solutions and here's

00:04:55,530 --> 00:05:00,540
an example of I think I have a quick

00:04:57,840 --> 00:05:03,060
right up here here's an example of a

00:05:00,540 --> 00:05:05,250
very simple solution that helped a lot

00:05:03,060 --> 00:05:07,620
pinpoint what's really going on so in

00:05:05,250 --> 00:05:09,810
this case here as you can see that a

00:05:07,620 --> 00:05:11,910
decision tree was made and we actually

00:05:09,810 --> 00:05:15,180
get some results making sure it's the

00:05:11,910 --> 00:05:17,550
optimal number of nodes as well as to

00:05:15,180 --> 00:05:19,740
see if any benefit was achieved by

00:05:17,550 --> 00:05:21,030
pruning and it wasn't and we were also

00:05:19,740 --> 00:05:23,490
you know verifying that we're getting

00:05:21,030 --> 00:05:25,020
good mean square error so in this case

00:05:23,490 --> 00:05:27,120
here we were able to pinpoint which

00:05:25,020 --> 00:05:30,900
metric really mattered and what time of

00:05:27,120 --> 00:05:35,030
day the problem was actually impacting

00:05:30,900 --> 00:05:35,030
our SLO level and that's about it

00:05:36,010 --> 00:05:48,880
[Music]

00:05:48,700 --> 00:05:52,019
you

00:05:48,880 --> 00:05:52,019

YouTube URL: https://www.youtube.com/watch?v=03x4Rc4hkXI


