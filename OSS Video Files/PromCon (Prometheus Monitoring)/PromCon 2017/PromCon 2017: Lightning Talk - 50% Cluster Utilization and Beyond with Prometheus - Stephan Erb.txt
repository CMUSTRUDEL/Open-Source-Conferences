Title: PromCon 2017: Lightning Talk - 50% Cluster Utilization and Beyond with Prometheus - Stephan Erb
Publication date: 2017-09-01
Playlist: PromCon 2017
Description: 
	* Abstract:

Lightning Talk

* Speaker:

Stephan Erb

* Slides:

* PromCon website:

https://promcon.io/
Captions: 
	00:00:00,380 --> 00:00:16,170
[Music]

00:00:13,219 --> 00:00:18,060
yes oh hi everybody my name is Jeff and

00:00:16,170 --> 00:00:21,150
I'm from a German company called blue

00:00:18,060 --> 00:00:24,539
yonder and we also joined this container

00:00:21,150 --> 00:00:27,210
bandwagon and we're now running a petit

00:00:24,539 --> 00:00:29,279
Maison FFC Aurora Tehran opened on our

00:00:27,210 --> 00:00:30,990
container work floats so this includes

00:00:29,279 --> 00:00:33,360
services but also machine learning

00:00:30,990 --> 00:00:34,739
application that we run and in this

00:00:33,360 --> 00:00:36,719
short talk I will try to give you an

00:00:34,739 --> 00:00:39,930
impression or an idea what we did to

00:00:36,719 --> 00:00:42,059
kind of increase our utilization so that

00:00:39,930 --> 00:00:44,850
some of you might kind of replicate and

00:00:42,059 --> 00:00:47,579
get similar results unless of course

00:00:44,850 --> 00:00:49,920
your micro service architecture looks

00:00:47,579 --> 00:00:51,570
like the Death Star if this is the case

00:00:49,920 --> 00:00:53,640
I might not be able to help you because

00:00:51,570 --> 00:00:55,680
you will have big problems with stay

00:00:53,640 --> 00:00:57,390
latency and stuff like that but for the

00:00:55,680 --> 00:00:59,430
rest of us that have kind of simpler

00:00:57,390 --> 00:01:01,859
yeah simple architecture simpler

00:00:59,430 --> 00:01:04,799
problems you might actually get

00:01:01,859 --> 00:01:07,350
something out of this talk so the first

00:01:04,799 --> 00:01:09,240
thing we did was that we essentially

00:01:07,350 --> 00:01:10,500
attached two labels to everything that

00:01:09,240 --> 00:01:13,020
we run on shared infrastructure

00:01:10,500 --> 00:01:15,479
the first one is roll which kind of ties

00:01:13,020 --> 00:01:19,799
back into our business domains it's like

00:01:15,479 --> 00:01:21,780
a unique ID four percent unique idea of

00:01:19,799 --> 00:01:23,670
the business service or the project that

00:01:21,780 --> 00:01:25,280
we do and the second one is just a

00:01:23,670 --> 00:01:27,750
simple environment tag that tells us

00:01:25,280 --> 00:01:29,430
when the life cycle of a service are we

00:01:27,750 --> 00:01:31,770
is this development or is this already

00:01:29,430 --> 00:01:33,030
running production and it sounds super

00:01:31,770 --> 00:01:35,369
simple but once you start building

00:01:33,030 --> 00:01:37,170
dashboards and a lot of people do the

00:01:35,369 --> 00:01:40,020
drillin you find pretty interesting

00:01:37,170 --> 00:01:43,530
things and it's also quite useful to

00:01:40,020 --> 00:01:46,409
also educate your users because it might

00:01:43,530 --> 00:01:48,689
not be a good idea to say that you

00:01:46,409 --> 00:01:50,790
launch a big performance test and then

00:01:48,689 --> 00:01:52,890
go on vacation for two weeks and all the

00:01:50,790 --> 00:01:55,020
instances you launched then just sitting

00:01:52,890 --> 00:01:57,810
idle and with definite like these you

00:01:55,020 --> 00:02:00,030
can kind of get rid of those we did lots

00:01:57,810 --> 00:02:01,890
of stuff in that direction and once we

00:02:00,030 --> 00:02:04,320
kind of figure the basic stuff out we

00:02:01,890 --> 00:02:05,640
could also enable maysa oversubscription

00:02:04,320 --> 00:02:07,680
here's kind of the idea if you launch

00:02:05,640 --> 00:02:09,179
your tasks on the cluster and not all

00:02:07,680 --> 00:02:10,979
resources are used you can kind of

00:02:09,179 --> 00:02:13,409
launch additional tasks using

00:02:10,979 --> 00:02:16,260
select resources but if you do that you

00:02:13,409 --> 00:02:18,030
can have to be careful because suddenly

00:02:16,260 --> 00:02:20,190
you might have overloaded a node with

00:02:18,030 --> 00:02:22,890
too much workload and the workload has

00:02:20,190 --> 00:02:24,930
to be preempted and in that case your

00:02:22,890 --> 00:02:27,569
overall service quality might suffer and

00:02:24,930 --> 00:02:29,849
what we did here is like overall cluster

00:02:27,569 --> 00:02:31,500
view of about our all tasks that

00:02:29,849 --> 00:02:33,660
supposed to be running actually running

00:02:31,500 --> 00:02:35,250
and that helped us quite a bit to make

00:02:33,660 --> 00:02:38,099
sure we're not going overboard with this

00:02:35,250 --> 00:02:42,420
hello subscription and our overloading

00:02:38,099 --> 00:02:45,599
our nodes too much and the next thing is

00:02:42,420 --> 00:02:47,700
something you don't want just to cluster

00:02:45,599 --> 00:02:49,859
view but also you won't like to know on

00:02:47,700 --> 00:02:54,930
the service level if you're still in

00:02:49,859 --> 00:02:58,379
acceptable bounds so it's like we kind

00:02:54,930 --> 00:03:00,959
of edit like service

00:02:58,379 --> 00:03:02,280
it's like SLS we have our services we

00:03:00,959 --> 00:03:04,200
can't just make sure we have a lot to

00:03:02,280 --> 00:03:05,280
those but even if you don't do a

00:03:04,200 --> 00:03:07,950
subscription you still want to have

00:03:05,280 --> 00:03:09,660
those and this is basically directly

00:03:07,950 --> 00:03:12,959
from the promises documentation so not

00:03:09,660 --> 00:03:14,940
much magic going on there and the last

00:03:12,959 --> 00:03:15,930
thing we had that helped us quite a bit

00:03:14,940 --> 00:03:19,319
and that's funny

00:03:15,930 --> 00:03:21,389
was Nagios because we ended up

00:03:19,319 --> 00:03:25,079
overheating quite a few of our notes and

00:03:21,389 --> 00:03:27,450
the cpu swaddle down we might not have

00:03:25,079 --> 00:03:29,849
discovered that at least that fast

00:03:27,450 --> 00:03:31,889
without checking pay which is kind of

00:03:29,849 --> 00:03:34,260
funny I believe you could also get D

00:03:31,889 --> 00:03:36,299
data from from meteors but we just had a

00:03:34,260 --> 00:03:39,239
little urge to check for it yeah that's

00:03:36,299 --> 00:03:42,419
kind of funny but in the end what's kind

00:03:39,239 --> 00:03:45,030
of the takeaway is it's not just for

00:03:42,419 --> 00:03:47,250
Google or the other big players to reach

00:03:45,030 --> 00:03:49,739
high utilization in your clusters even

00:03:47,250 --> 00:03:51,630
small players can do it and monitoring

00:03:49,739 --> 00:03:53,340
is the first step to get there because

00:03:51,630 --> 00:03:54,750
that's kind of what you need to get the

00:03:53,340 --> 00:03:57,030
insights to figure out what's going on

00:03:54,750 --> 00:04:01,120
and I hope you got some few ideas to

00:03:57,030 --> 00:04:13,660
actually replicate our results thank you

00:04:01,120 --> 00:04:13,840
[Music]

00:04:13,660 --> 00:04:16,979
you

00:04:13,840 --> 00:04:16,979

YouTube URL: https://www.youtube.com/watch?v=-53todbei6c


