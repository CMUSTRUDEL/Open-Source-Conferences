Title: PromCon Online 2020 - What the fork‽ Reporting metrics in a multi-process environment
Publication date: 2020-07-23
Playlist: PromCon Online 2020
Description: 
	PromCon Online 2020 - What the fork‽ Reporting metrics in a multi-process environment, by Daniel Magliola GoCardless

For many years, most Ruby applications couldn't use Prometheus, because the Ruby client didn't support pre-fork servers.

It turns out this wasn't solved for such a long time because it's a surprisingly hard problem.

Many have tried to solve this with different approaches, but we found the best one to be the simplest.

Let me show you the dark arts of inter-process communication, and how we ended up fully supporting metrics in Ruby.
Captions: 
	00:00:01,770 --> 00:00:04,940
[Music]

00:00:14,920 --> 00:00:18,640
um talk is going to be a bit different

00:00:17,520 --> 00:00:20,560
as we're going to be looking at

00:00:18,640 --> 00:00:21,439
exporting metrics from the application

00:00:20,560 --> 00:00:23,680
side

00:00:21,439 --> 00:00:25,599
last year my company decided to revamp

00:00:23,680 --> 00:00:27,680
the prometheus ruby client to bring it

00:00:25,599 --> 00:00:29,359
up to the standards of the other clients

00:00:27,680 --> 00:00:31,039
it was quite an unusual project and i

00:00:29,359 --> 00:00:33,440
hope you find it interesting

00:00:31,039 --> 00:00:35,200
first of all though hi my name is daniel

00:00:33,440 --> 00:00:36,880
and i work for gocarlas we are a

00:00:35,200 --> 00:00:38,559
payments company based in london

00:00:36,880 --> 00:00:40,399
as you probably noticed now though i'm

00:00:38,559 --> 00:00:42,000
not originally from london i come from

00:00:40,399 --> 00:00:43,440
argentina so in case you're wondering

00:00:42,000 --> 00:00:45,680
that's the accent

00:00:43,440 --> 00:00:46,640
now for us to go call us uptime is very

00:00:45,680 --> 00:00:48,559
critical

00:00:46,640 --> 00:00:50,879
and that means among other things that

00:00:48,559 --> 00:00:51,840
having good metrics for apps is critical

00:00:50,879 --> 00:00:54,079
and as you probably guessed by the

00:00:51,840 --> 00:00:54,640
conference name we mainly use prometheus

00:00:54,079 --> 00:00:56,800
for this

00:00:54,640 --> 00:00:58,239
now we love primitives but we had a bit

00:00:56,800 --> 00:00:59,680
of a hard time getting it to work with

00:00:58,239 --> 00:01:00,480
our main application which is written in

00:00:59,680 --> 00:01:02,160
ruby

00:01:00,480 --> 00:01:04,080
now to see why let's look at how metrics

00:01:02,160 --> 00:01:05,680
are exported as you know

00:01:04,080 --> 00:01:07,520
prometheus periodically scrapes the

00:01:05,680 --> 00:01:07,920
server for their numbers so we have our

00:01:07,520 --> 00:01:09,280
apps

00:01:07,920 --> 00:01:10,720
which in our case are mainly written in

00:01:09,280 --> 00:01:12,720
ruby and they're keeping a bunch of

00:01:10,720 --> 00:01:13,680
counters in there prometheus hits us

00:01:12,720 --> 00:01:15,280
every few seconds

00:01:13,680 --> 00:01:17,360
and you return something that looks a

00:01:15,280 --> 00:01:18,640
bit like this and for most languages you

00:01:17,360 --> 00:01:20,400
just use the appropriate client library

00:01:18,640 --> 00:01:22,479
and the story instead

00:01:20,400 --> 00:01:24,320
the problem we had is that in ruby land

00:01:22,479 --> 00:01:25,840
and by the way also in python land

00:01:24,320 --> 00:01:28,000
this is a bit trickier because ruby and

00:01:25,840 --> 00:01:29,439
python have a global interpreted lock

00:01:28,000 --> 00:01:31,119
which doesn't let you run your code in

00:01:29,439 --> 00:01:33,439
multiple threads at the same time

00:01:31,119 --> 00:01:35,119
because of this our web servers are not

00:01:33,439 --> 00:01:36,560
usually multi-threaded in ruby this

00:01:35,119 --> 00:01:38,079
doesn't parallel as well

00:01:36,560 --> 00:01:39,920
instead we tend to use separate

00:01:38,079 --> 00:01:41,280
processes processes don't share memory

00:01:39,920 --> 00:01:42,079
which solves the same problem the guild

00:01:41,280 --> 00:01:43,840
is trying to solve

00:01:42,079 --> 00:01:45,439
and everybody can run in parallel

00:01:43,840 --> 00:01:46,960
ignoring each other happily

00:01:45,439 --> 00:01:48,159
this is for example how unicorn works

00:01:46,960 --> 00:01:49,439
which is one of the most common web

00:01:48,159 --> 00:01:51,119
servers in ruby

00:01:49,439 --> 00:01:52,799
when unicorn starts it loads your app

00:01:51,119 --> 00:01:54,159
and then it forks itself into a master

00:01:52,799 --> 00:01:54,720
orchestrator process as listening to

00:01:54,159 --> 00:01:56,320
request

00:01:54,720 --> 00:01:57,920
and a bunch of worker processes are

00:01:56,320 --> 00:01:59,759
actually going to be working on them

00:01:57,920 --> 00:02:01,119
when a request comes in the master sends

00:01:59,759 --> 00:02:02,560
a request to a worker that is free and

00:02:01,119 --> 00:02:04,000
then takes a response and sends it back

00:02:02,560 --> 00:02:05,759
to the client

00:02:04,000 --> 00:02:07,280
this system works great it scales really

00:02:05,759 --> 00:02:08,000
well across course and is the way the

00:02:07,280 --> 00:02:10,160
most ruby web

00:02:08,000 --> 00:02:11,599
apps are run in production however for

00:02:10,160 --> 00:02:12,800
the promiscuous use case this is a bit

00:02:11,599 --> 00:02:14,400
of a problem because

00:02:12,800 --> 00:02:16,239
now that we have multiple processes and

00:02:14,400 --> 00:02:18,080
they don't share memory each process

00:02:16,239 --> 00:02:19,680
will have its own internal accounts

00:02:18,080 --> 00:02:21,440
when promising pulse and asks for your

00:02:19,680 --> 00:02:22,879
metrics the master process will send a

00:02:21,440 --> 00:02:24,879
request to some random worker

00:02:22,879 --> 00:02:26,560
and that worker will report its numbers

00:02:24,879 --> 00:02:28,239
but this is never what you want because

00:02:26,560 --> 00:02:29,760
this number is only a partial view

00:02:28,239 --> 00:02:32,000
what you want is the global view of all

00:02:29,760 --> 00:02:33,680
of these processes added together

00:02:32,000 --> 00:02:35,360
but because these are separate processes

00:02:33,680 --> 00:02:36,800
there is no way for any single worker to

00:02:35,360 --> 00:02:37,840
see the counters in the other workers to

00:02:36,800 --> 00:02:39,200
be able to add them

00:02:37,840 --> 00:02:41,120
this is by design and it's the main

00:02:39,200 --> 00:02:43,360
reason we use separate processes but for

00:02:41,120 --> 00:02:45,599
the previous use case this is a problem

00:02:43,360 --> 00:02:46,879
and so since most ruby web applications

00:02:45,599 --> 00:02:48,480
are running in preferred servers

00:02:46,879 --> 00:02:49,920
and since the permissions ruby client

00:02:48,480 --> 00:02:51,360
didn't have a solution for that

00:02:49,920 --> 00:02:53,040
multi-process aggregation

00:02:51,360 --> 00:02:54,720
that meant that lots of ruby projects

00:02:53,040 --> 00:02:55,680
that wanted to use prometheus basically

00:02:54,720 --> 00:02:57,120
couldn't

00:02:55,680 --> 00:02:58,640
and we definitely had this problem we

00:02:57,120 --> 00:02:59,920
were using prometheus we wanted to have

00:02:58,640 --> 00:03:02,400
metrics for our main application

00:02:59,920 --> 00:03:04,000
and we couldn't and it wasn't just us

00:03:02,400 --> 00:03:07,200
this had been the case for quite

00:03:04,000 --> 00:03:08,720
some people for quite a long time also

00:03:07,200 --> 00:03:10,159
the ruby was one of the first libraries

00:03:08,720 --> 00:03:11,760
to be made and it was very early in the

00:03:10,159 --> 00:03:13,120
lifetime of the promises project

00:03:11,760 --> 00:03:14,400
this was before the community had

00:03:13,120 --> 00:03:16,000
figured out what some of the best

00:03:14,400 --> 00:03:17,440
practices were for example so

00:03:16,000 --> 00:03:19,920
it also had a bunch of other issues like

00:03:17,440 --> 00:03:21,519
the way it validates labels

00:03:19,920 --> 00:03:24,000
there were some workarounds out there

00:03:21,519 --> 00:03:25,920
gitlab for example forked the official

00:03:24,000 --> 00:03:27,360
client and had a multi-process solution

00:03:25,920 --> 00:03:28,480
that's based on memory maps

00:03:27,360 --> 00:03:30,159
but it wasn't getting merged into the

00:03:28,480 --> 00:03:31,599
official repo and it also shared all of

00:03:30,159 --> 00:03:33,760
the other issues with best practices so

00:03:31,599 --> 00:03:35,200
we prefer not to use it

00:03:33,760 --> 00:03:36,720
this course has a different solution

00:03:35,200 --> 00:03:37,760
with a separate process running

00:03:36,720 --> 00:03:39,200
alongside your app

00:03:37,760 --> 00:03:40,480
which your worker process would push

00:03:39,200 --> 00:03:42,000
metrics to and then that process would

00:03:40,480 --> 00:03:43,599
report the total super methods

00:03:42,000 --> 00:03:45,120
this one also didn't match very well

00:03:43,599 --> 00:03:46,799
with the way we entered our setup

00:03:45,120 --> 00:03:48,720
but mainly we didn't want to be on a

00:03:46,799 --> 00:03:49,680
fork we much prefer to use the official

00:03:48,720 --> 00:03:51,200
client so

00:03:49,680 --> 00:03:52,799
we got in touch with the maintainers we

00:03:51,200 --> 00:03:54,400
roughly agreed on a plan function and we

00:03:52,799 --> 00:03:56,080
decided we'd do pretty much a rewrite

00:03:54,400 --> 00:03:58,080
fixing all these things

00:03:56,080 --> 00:03:59,599
but there was one more thing to consider

00:03:58,080 --> 00:04:01,040
the permissions maintainers

00:03:59,599 --> 00:04:02,640
really really care about performance for

00:04:01,040 --> 00:04:04,239
these libraries and for good reason

00:04:02,640 --> 00:04:05,760
instrumentation should be free for your

00:04:04,239 --> 00:04:07,200
app if you want to increment the counter

00:04:05,760 --> 00:04:08,480
inside a tight loop for example

00:04:07,200 --> 00:04:10,400
you should be able to do that and not

00:04:08,480 --> 00:04:12,080
notice it and buried in that long

00:04:10,400 --> 00:04:13,599
discussion from the 2015 ticket

00:04:12,080 --> 00:04:15,040
there was a target one of the

00:04:13,599 --> 00:04:16,400
maintainers said basically that

00:04:15,040 --> 00:04:17,680
incrementing a counter should take less

00:04:16,400 --> 00:04:19,440
than a microsecond

00:04:17,680 --> 00:04:20,639
which prompted an interesting discussion

00:04:19,440 --> 00:04:21,280
of whether this could even be done in

00:04:20,639 --> 00:04:23,680
ruby with

00:04:21,280 --> 00:04:24,639
comments including surely ruby can't be

00:04:23,680 --> 00:04:26,560
that slow

00:04:24,639 --> 00:04:27,759
and my favorite ever i guess there's a

00:04:26,560 --> 00:04:30,720
reason people like writing c

00:04:27,759 --> 00:04:32,400
extensions so whatever we did to solve

00:04:30,720 --> 00:04:33,680
this had to be fast and we had an actual

00:04:32,400 --> 00:04:35,280
target

00:04:33,680 --> 00:04:36,320
finally there was one more thing that we

00:04:35,280 --> 00:04:37,360
and the maintainers agreed that we

00:04:36,320 --> 00:04:38,880
wanted to do

00:04:37,360 --> 00:04:40,560
instead of having our metric classes

00:04:38,880 --> 00:04:41,360
with the numbers in an instance variable

00:04:40,560 --> 00:04:42,720
like this

00:04:41,360 --> 00:04:44,560
we want to change this so that these

00:04:42,720 --> 00:04:46,479
numbers will instead be stored somewhere

00:04:44,560 --> 00:04:47,520
instead being stored somewhere separate

00:04:46,479 --> 00:04:48,800
we wanted to have stores that were

00:04:47,520 --> 00:04:49,520
interchangeable and keep all of the

00:04:48,800 --> 00:04:51,120
numbers there

00:04:49,520 --> 00:04:52,560
so we go from something like this to

00:04:51,120 --> 00:04:53,919
something more like that

00:04:52,560 --> 00:04:55,280
the reason for this is that different

00:04:53,919 --> 00:04:56,800
projects will have different needs some

00:04:55,280 --> 00:04:58,000
people would be working on a single

00:04:56,800 --> 00:04:59,919
thread a single process

00:04:58,000 --> 00:05:01,520
and some may really really care about

00:04:59,919 --> 00:05:02,160
performance whereas other people have a

00:05:01,520 --> 00:05:04,000
web app

00:05:02,160 --> 00:05:05,199
in multiple processes hitting a database

00:05:04,000 --> 00:05:06,560
over the network so

00:05:05,199 --> 00:05:08,240
who cares about microseconds at that

00:05:06,560 --> 00:05:09,680
point right

00:05:08,240 --> 00:05:11,520
so this was the plan we had we were

00:05:09,680 --> 00:05:13,360
going to support preferred service

00:05:11,520 --> 00:05:14,639
we're going to abstract away the storage

00:05:13,360 --> 00:05:16,240
of numbers we

00:05:14,639 --> 00:05:18,000
wanted to follow all of the current best

00:05:16,240 --> 00:05:21,440
practices and whatever we did

00:05:18,000 --> 00:05:23,039
had to be fast now the first question is

00:05:21,440 --> 00:05:23,680
can we even do this within that time

00:05:23,039 --> 00:05:25,199
budget

00:05:23,680 --> 00:05:26,479
now according to this exchange this

00:05:25,199 --> 00:05:27,919
either could or couldn't be done in a

00:05:26,479 --> 00:05:30,560
microsecond and the truth is

00:05:27,919 --> 00:05:32,400
at the time i had no idea i am mostly a

00:05:30,560 --> 00:05:34,960
ruby vacant developer which means

00:05:32,400 --> 00:05:36,320
my normal intuition for code timings are

00:05:34,960 --> 00:05:39,039
milliseconds land

00:05:36,320 --> 00:05:40,160
so the answer to this is a bit long but

00:05:39,039 --> 00:05:42,400
the short version is

00:05:40,160 --> 00:05:44,320
we basically can't let's look at this

00:05:42,400 --> 00:05:44,800
like if you just go ahead and do this

00:05:44,320 --> 00:05:47,360
you just

00:05:44,800 --> 00:05:49,360
increment the hash you can do this about

00:05:47,360 --> 00:05:50,880
12 times in a microsecond which is great

00:05:49,360 --> 00:05:52,479
it's well within our budget

00:05:50,880 --> 00:05:54,160
but in our case it's not that simple

00:05:52,479 --> 00:05:54,800
because if you think about it why are we

00:05:54,160 --> 00:05:56,319
even

00:05:54,800 --> 00:05:58,160
using hashes instead of just having an

00:05:56,319 --> 00:05:58,720
integer and the reason for that is

00:05:58,160 --> 00:06:00,720
labels

00:05:58,720 --> 00:06:02,240
in the ruby client labels are specified

00:06:00,720 --> 00:06:03,520
with a hash of keys and values because

00:06:02,240 --> 00:06:04,479
that's the interface that makes the most

00:06:03,520 --> 00:06:06,400
sense in ruby

00:06:04,479 --> 00:06:08,560
and we store numbers in hash where the

00:06:06,400 --> 00:06:09,600
key itself is also a hash with those key

00:06:08,560 --> 00:06:11,280
value pairs

00:06:09,600 --> 00:06:12,880
now the problem is when you're using

00:06:11,280 --> 00:06:14,160
hashes as the key for your hash

00:06:12,880 --> 00:06:15,360
whenever you need to find an entry

00:06:14,160 --> 00:06:16,880
you're going to be comparing hashes to

00:06:15,360 --> 00:06:18,880
hashes instead of comparing constants

00:06:16,880 --> 00:06:20,960
and this is much much slower

00:06:18,880 --> 00:06:22,639
now how slow depends on how big those

00:06:20,960 --> 00:06:23,440
hashes are in our case how many labels

00:06:22,639 --> 00:06:24,800
you have

00:06:23,440 --> 00:06:26,560
so for example if you have three labels

00:06:24,800 --> 00:06:27,840
for your metric that's going to be 60

00:06:26,560 --> 00:06:30,479
times slower which is

00:06:27,840 --> 00:06:31,199
pretty terrible now according to best

00:06:30,479 --> 00:06:33,280
practices

00:06:31,199 --> 00:06:34,960
most metrics should have no labels so if

00:06:33,280 --> 00:06:37,440
we take that as a benchmark

00:06:34,960 --> 00:06:38,319
then it's better but it's still nine

00:06:37,440 --> 00:06:39,919
times

00:06:38,319 --> 00:06:41,840
slower than with a constant and you will

00:06:39,919 --> 00:06:43,759
notice that that's already quite close

00:06:41,840 --> 00:06:45,680
to the budget of one microsecond

00:06:43,759 --> 00:06:47,360
and we're only incrementing hash here in

00:06:45,680 --> 00:06:48,639
real life we need to add some validation

00:06:47,360 --> 00:06:50,160
and processing to these labels and we're

00:06:48,639 --> 00:06:51,759
not even thread safe you put a mutex

00:06:50,160 --> 00:06:53,360
around this and now you're at about a

00:06:51,759 --> 00:06:54,880
microsecond and a half per increase and

00:06:53,360 --> 00:06:56,240
we blew our target

00:06:54,880 --> 00:06:57,919
so it turns out in our particular

00:06:56,240 --> 00:06:59,199
situation we can't do this in the

00:06:57,919 --> 00:07:01,120
microscope

00:06:59,199 --> 00:07:02,479
now we already passed a target and we're

00:07:01,120 --> 00:07:03,680
still in a single process i mean we can

00:07:02,479 --> 00:07:05,440
optimize this a little bit

00:07:03,680 --> 00:07:07,680
but any multi-processor solution will

00:07:05,440 --> 00:07:10,160
definitely not be in budget

00:07:07,680 --> 00:07:11,360
but what's important to me is that we

00:07:10,160 --> 00:07:12,880
should keep to the spirit

00:07:11,360 --> 00:07:16,080
of this target the important thing here

00:07:12,880 --> 00:07:18,319
is we really care about making these

00:07:16,080 --> 00:07:19,840
this instrumentation as fast as possible

00:07:18,319 --> 00:07:21,280
and because this will impact lots of

00:07:19,840 --> 00:07:22,800
projects for lots of people

00:07:21,280 --> 00:07:24,479
it's really worth investing a lot of

00:07:22,800 --> 00:07:26,240
time into that

00:07:24,479 --> 00:07:28,080
so this is where we were at this point

00:07:26,240 --> 00:07:30,720
we have an idea of times

00:07:28,080 --> 00:07:31,759
and we can exchange stores very easily

00:07:30,720 --> 00:07:33,520
now we need to actually share the

00:07:31,759 --> 00:07:35,680
numbers across processes

00:07:33,520 --> 00:07:37,680
and the way we approach that was to

00:07:35,680 --> 00:07:39,759
basically experiment a lot

00:07:37,680 --> 00:07:41,360
at this point we pretty much started

00:07:39,759 --> 00:07:42,800
trying everything that we could think of

00:07:41,360 --> 00:07:44,080
because with these separate stores it

00:07:42,800 --> 00:07:46,000
was very cheap for us to

00:07:44,080 --> 00:07:47,360
try different things and i would like to

00:07:46,000 --> 00:07:49,919
walk you through four of these

00:07:47,360 --> 00:07:51,759
experiments now with it

00:07:49,919 --> 00:07:54,000
first off let's make this really easy

00:07:51,759 --> 00:07:55,440
for ourselves ruby has this unusual data

00:07:54,000 --> 00:07:57,440
structure called the p store

00:07:55,440 --> 00:07:59,120
basically it's a hash backed by a file

00:07:57,440 --> 00:08:00,800
on disk every time you set a value

00:07:59,120 --> 00:08:02,080
the entire hash gets serialized to this

00:08:00,800 --> 00:08:03,199
every time you read a value it's all

00:08:02,080 --> 00:08:04,800
read from disk again

00:08:03,199 --> 00:08:05,919
and the file has a look around it so you

00:08:04,800 --> 00:08:07,280
can actually read and write the same

00:08:05,919 --> 00:08:07,919
file for multiple processes all at the

00:08:07,280 --> 00:08:09,280
same time

00:08:07,919 --> 00:08:11,360
everywhere see the latest information

00:08:09,280 --> 00:08:12,639
and it will be safe and if you think

00:08:11,360 --> 00:08:14,080
about it this is pretty much exactly

00:08:12,639 --> 00:08:15,440
what we need right we have this global

00:08:14,080 --> 00:08:17,280
hash shared by everyone

00:08:15,440 --> 00:08:19,440
all the processes see all the data we're

00:08:17,280 --> 00:08:23,280
done performance wise

00:08:19,440 --> 00:08:24,319
though this is pretty terrible now

00:08:23,280 --> 00:08:27,120
if you think about what we're trying to

00:08:24,319 --> 00:08:28,560
do here we're basically trying to have a

00:08:27,120 --> 00:08:29,919
chunk of memory that is shared between

00:08:28,560 --> 00:08:32,080
different processes

00:08:29,919 --> 00:08:33,279
as a complete aside one of the great

00:08:32,080 --> 00:08:35,360
things about gocales

00:08:33,279 --> 00:08:36,399
is the amazing people i get to work with

00:08:35,360 --> 00:08:38,320
now granted

00:08:36,399 --> 00:08:40,399
some of those people are trolls the

00:08:38,320 --> 00:08:42,640
trolls sometimes have good ideas

00:08:40,399 --> 00:08:44,320
i mean sqlite was probably not going to

00:08:42,640 --> 00:08:45,120
be the right solution but like he got me

00:08:44,320 --> 00:08:46,560
thinking

00:08:45,120 --> 00:08:48,480
you know what's a great way of having a

00:08:46,560 --> 00:08:50,640
chunk of memory shared between processes

00:08:48,480 --> 00:08:52,000
it's my good old friend redis rest is

00:08:50,640 --> 00:08:53,680
basically that right it's a chunk of

00:08:52,000 --> 00:08:54,480
memory running in a process alongside

00:08:53,680 --> 00:08:56,000
your app

00:08:54,480 --> 00:08:57,839
all of your processes can access it and

00:08:56,000 --> 00:08:59,200
we all know it's super fast

00:08:57,839 --> 00:09:01,360
so this sounded like exactly what we

00:08:59,200 --> 00:09:03,360
need and actually the prometheus client

00:09:01,360 --> 00:09:04,720
for php uses rates for this example

00:09:03,360 --> 00:09:06,959
exact reason so that was pretty

00:09:04,720 --> 00:09:08,480
promising so i wrote up a little store

00:09:06,959 --> 00:09:10,160
that would use race as a back-end i

00:09:08,480 --> 00:09:11,680
knew this was going to be the solution

00:09:10,160 --> 00:09:14,080
fired on my benchmarks

00:09:11,680 --> 00:09:16,080
and it's actually much slower than just

00:09:14,080 --> 00:09:19,760
dumping the entire hash to disk

00:09:16,080 --> 00:09:21,279
every time and that makes sense right i

00:09:19,760 --> 00:09:22,800
mean talking to radius involves a lot of

00:09:21,279 --> 00:09:24,480
ceremony and networking even if it's in

00:09:22,800 --> 00:09:26,480
localhost compared to just writing a

00:09:24,480 --> 00:09:29,600
tiny file within your own process

00:09:26,480 --> 00:09:31,760
it really had no chance and to be honest

00:09:29,600 --> 00:09:33,440
this was surprising and disappointing

00:09:31,760 --> 00:09:34,959
but it was also a relief i didn't want

00:09:33,440 --> 00:09:36,399
to force everyone to have a localhost

00:09:34,959 --> 00:09:37,839
race in each server just to have

00:09:36,399 --> 00:09:39,279
promises metrics so

00:09:37,839 --> 00:09:41,360
it was kind of good that this didn't

00:09:39,279 --> 00:09:43,440
work

00:09:41,360 --> 00:09:45,440
at this point i had no choice but to

00:09:43,440 --> 00:09:47,279
face a giant elephant in the room

00:09:45,440 --> 00:09:49,120
the consensus from pretty much everyone

00:09:47,279 --> 00:09:50,560
on how to solve the multi-process issue

00:09:49,120 --> 00:09:52,560
was that we should use memory maps for

00:09:50,560 --> 00:09:55,120
this this is for example what the python

00:09:52,560 --> 00:09:56,800
client does and also the gitlab fork

00:09:55,120 --> 00:09:58,320
but during most of this project i was

00:09:56,800 --> 00:09:59,680
trying to avoid them i really really

00:09:58,320 --> 00:10:01,360
wanted to find another way

00:09:59,680 --> 00:10:02,720
and the reason was mostly that i didn't

00:10:01,360 --> 00:10:04,320
really know how to use them and i had

00:10:02,720 --> 00:10:06,560
heard someone had

00:10:04,320 --> 00:10:07,440
at some point some problems with sex

00:10:06,560 --> 00:10:10,560
faulting which

00:10:07,440 --> 00:10:11,760
is a bit scary now as a super quick

00:10:10,560 --> 00:10:13,440
introduction to what we're doing

00:10:11,760 --> 00:10:15,680
we're going to be using a shared memory

00:10:13,440 --> 00:10:17,279
map file what you do is you map a file

00:10:15,680 --> 00:10:18,880
to memory with the shared flag

00:10:17,279 --> 00:10:20,720
and then if two processes do the same

00:10:18,880 --> 00:10:22,399
thing with the same file they end up

00:10:20,720 --> 00:10:23,680
reading the same page in memory so one

00:10:22,399 --> 00:10:25,440
can write to this memory and the other

00:10:23,680 --> 00:10:27,200
one can immediately see this data

00:10:25,440 --> 00:10:28,480
this is actually famously one of the

00:10:27,200 --> 00:10:29,600
fastest ways to do inter-process

00:10:28,480 --> 00:10:31,200
communication so

00:10:29,600 --> 00:10:32,480
i can see why everyone was suggesting to

00:10:31,200 --> 00:10:34,000
use this thing it does sound like the

00:10:32,480 --> 00:10:36,720
perfect solution

00:10:34,000 --> 00:10:37,519
there's only one tiny problem map is a

00:10:36,720 --> 00:10:38,959
cisco

00:10:37,519 --> 00:10:41,120
do you all know how to call it cisco in

00:10:38,959 --> 00:10:43,839
ruby you don't you do it in c

00:10:41,120 --> 00:10:45,360
or something like c now granted there's

00:10:43,839 --> 00:10:46,399
this thing which has the most hilarious

00:10:45,360 --> 00:10:47,440
line i've ever seen in official

00:10:46,399 --> 00:10:49,200
documentation

00:10:47,440 --> 00:10:51,200
but it's not particularly useful for

00:10:49,200 --> 00:10:53,120
this and even if you actually managed to

00:10:51,200 --> 00:10:54,720
call a map with this

00:10:53,120 --> 00:10:56,800
now i have a chunk of raw memory and a

00:10:54,720 --> 00:10:59,200
pointer to it now what am i going to do

00:10:56,800 --> 00:11:00,640
with that from ruby so we need some sort

00:10:59,200 --> 00:11:02,240
of gem to use this

00:11:00,640 --> 00:11:03,839
preferably something written in c or

00:11:02,240 --> 00:11:06,000
rust or something like that

00:11:03,839 --> 00:11:07,680
unfortunately there's this which is

00:11:06,000 --> 00:11:08,640
maintained by one of the core ruby

00:11:07,680 --> 00:11:10,720
contributors

00:11:08,640 --> 00:11:12,399
this is aaron patterson he goes by

00:11:10,720 --> 00:11:13,920
tenderlove on the internet

00:11:12,399 --> 00:11:15,440
and he's normally hacking on the memory

00:11:13,920 --> 00:11:17,440
management part of ruby so this actually

00:11:15,440 --> 00:11:19,120
filled me with confidence

00:11:17,440 --> 00:11:20,560
now how do you end up with this gem well

00:11:19,120 --> 00:11:22,560
you basically make a file

00:11:20,560 --> 00:11:24,560
you initialize it to a certain size say

00:11:22,560 --> 00:11:25,600
one megabyte and then you in it an end

00:11:24,560 --> 00:11:28,399
map and now you have this

00:11:25,600 --> 00:11:30,399
m variable which behaves sort of kind of

00:11:28,399 --> 00:11:32,560
like a byte array now importantly

00:11:30,399 --> 00:11:34,240
it is not a ruby array the map gem is

00:11:32,560 --> 00:11:35,120
doing a bunch of magic to make it behave

00:11:34,240 --> 00:11:36,320
like one

00:11:35,120 --> 00:11:37,839
but if you have a bunch of bytes to

00:11:36,320 --> 00:11:40,160
write you then basically set a range on

00:11:37,839 --> 00:11:42,240
this thing and that'll do it

00:11:40,160 --> 00:11:44,320
now the tricky bit is turning the stuff

00:11:42,240 --> 00:11:46,880
you want to store into a bunch of bytes

00:11:44,320 --> 00:11:48,399
now i shamelessly store stole this idea

00:11:46,880 --> 00:11:50,480
from a branch that julius volts

00:11:48,399 --> 00:11:52,000
wrote now the short version is you store

00:11:50,480 --> 00:11:53,360
the labels encoded as a string

00:11:52,000 --> 00:11:55,279
and then it float with the number that

00:11:53,360 --> 00:11:57,040
you want but you keep track of where in

00:11:55,279 --> 00:11:58,720
the file you put that's float

00:11:57,040 --> 00:12:01,040
in a hash that you keep in memory this

00:11:58,720 --> 00:12:02,639
add positions here

00:12:01,040 --> 00:12:04,560
now the next time you need to update the

00:12:02,639 --> 00:12:06,000
float you just need to find the position

00:12:04,560 --> 00:12:07,360
and change those eight bytes

00:12:06,000 --> 00:12:09,279
you don't have to change the entire file

00:12:07,360 --> 00:12:10,800
just those little eight bytes which is

00:12:09,279 --> 00:12:12,000
pretty clever right i wouldn't have come

00:12:10,800 --> 00:12:14,959
up with this myself so

00:12:12,000 --> 00:12:15,519
thank you julius now you may be thinking

00:12:14,959 --> 00:12:18,320
rightly

00:12:15,519 --> 00:12:19,040
how fast is this and the truth is it's

00:12:18,320 --> 00:12:20,320
pretty fast

00:12:19,040 --> 00:12:21,920
actually it's it's pretty decent

00:12:20,320 --> 00:12:22,959
compared to all of our other options so

00:12:21,920 --> 00:12:25,920
far

00:12:22,959 --> 00:12:27,440
there's only one problem in addition to

00:12:25,920 --> 00:12:29,040
having a little benchmark script that i

00:12:27,440 --> 00:12:30,720
was using to time these things

00:12:29,040 --> 00:12:32,160
i had another script that would stress

00:12:30,720 --> 00:12:33,760
test these stores basically

00:12:32,160 --> 00:12:34,720
it would do terrible things to them just

00:12:33,760 --> 00:12:36,079
to make sure they were safe you would

00:12:34,720 --> 00:12:37,920
have a ridiculously high number of

00:12:36,079 --> 00:12:39,120
metric with lots of different labels and

00:12:37,920 --> 00:12:40,480
we use it from different threads from

00:12:39,120 --> 00:12:42,079
different many different processes all

00:12:40,480 --> 00:12:44,480
competing for the same file you're just

00:12:42,079 --> 00:12:46,480
hammering it to make sure it's solid and

00:12:44,480 --> 00:12:47,680
as soon as i run the script for the nmap

00:12:46,480 --> 00:12:50,000
store

00:12:47,680 --> 00:12:50,800
i got one of these now i don't know

00:12:50,000 --> 00:12:52,240
about you

00:12:50,800 --> 00:12:54,160
but until this point i've never

00:12:52,240 --> 00:12:57,440
encountered a sec fault in ruby at least

00:12:54,160 --> 00:12:59,680
no one that was my fault so now

00:12:57,440 --> 00:13:00,959
i'm using this gem that does some memory

00:12:59,680 --> 00:13:02,720
voodoo that quite frankly

00:13:00,959 --> 00:13:04,959
i don't fully understand and now i got a

00:13:02,720 --> 00:13:06,639
secret and this was kind of the reason i

00:13:04,959 --> 00:13:08,079
was trying to avoid a maps right like i

00:13:06,639 --> 00:13:09,920
was worried this precise thing might

00:13:08,079 --> 00:13:12,320
happen and now it did

00:13:09,920 --> 00:13:14,639
and given that this gem was written by

00:13:12,320 --> 00:13:16,399
tenderlove and it was being used by me

00:13:14,639 --> 00:13:18,000
it was most likely not a bug in the gym

00:13:16,399 --> 00:13:20,639
i'm almost certainly using it wrong

00:13:18,000 --> 00:13:21,760
but it's a very obscure gem no not many

00:13:20,639 --> 00:13:23,279
people need to do this

00:13:21,760 --> 00:13:25,440
so there's not much documentation i

00:13:23,279 --> 00:13:26,720
haven't found anyone actually using it

00:13:25,440 --> 00:13:29,200
it's kind of hard to figure out what i'm

00:13:26,720 --> 00:13:30,880
doing wrong i'm also not very strong at

00:13:29,200 --> 00:13:32,480
c i still try to look at the source code

00:13:30,880 --> 00:13:34,079
but it's a giant file with two and a

00:13:32,480 --> 00:13:35,440
half thousand lines of code

00:13:34,079 --> 00:13:37,360
i didn't really manage to figure out

00:13:35,440 --> 00:13:39,519
what the problem might be

00:13:37,360 --> 00:13:40,959
through experimenting i did narrow it

00:13:39,519 --> 00:13:42,800
down to the code that resize

00:13:40,959 --> 00:13:44,720
our map if the initial size wasn't

00:13:42,800 --> 00:13:45,839
enough so that one megabyte sometimes it

00:13:44,720 --> 00:13:47,519
needs to grow

00:13:45,839 --> 00:13:49,120
i still had no idea why it happened but

00:13:47,519 --> 00:13:49,839
at least i could reproduce it reliably

00:13:49,120 --> 00:13:51,760
and so

00:13:49,839 --> 00:13:53,199
we did what we all do when this happens

00:13:51,760 --> 00:13:53,680
we try random stuff until something

00:13:53,199 --> 00:13:55,920
sticks

00:13:53,680 --> 00:13:57,760
right it took a while because i had no

00:13:55,920 --> 00:13:59,440
idea what i was trying to do

00:13:57,760 --> 00:14:01,040
but i did end up making the sexful stop

00:13:59,440 --> 00:14:02,320
through an elaborate dance of like

00:14:01,040 --> 00:14:03,519
closing the file reopening the file

00:14:02,320 --> 00:14:04,959
resizing the file across and again

00:14:03,519 --> 00:14:06,320
reopening and remapping and for some

00:14:04,959 --> 00:14:08,240
reason the segfaults didn't happen

00:14:06,320 --> 00:14:12,160
anymore

00:14:08,240 --> 00:14:14,320
now if you have a problem

00:14:12,160 --> 00:14:15,680
you don't understand the problem you try

00:14:14,320 --> 00:14:16,079
random stuff until the bottom stops

00:14:15,680 --> 00:14:17,680
happening

00:14:16,079 --> 00:14:19,519
but you don't know why it was happening

00:14:17,680 --> 00:14:20,959
or why it stopped

00:14:19,519 --> 00:14:22,560
how confident are you that that's not

00:14:20,959 --> 00:14:24,560
gonna happen again in production in some

00:14:22,560 --> 00:14:25,680
random server in some random company's

00:14:24,560 --> 00:14:27,920
mission critical app

00:14:25,680 --> 00:14:28,959
i really really was and i did not like

00:14:27,920 --> 00:14:31,279
this one bit

00:14:28,959 --> 00:14:33,040
and this is precisely why i was avoiding

00:14:31,279 --> 00:14:35,600
the memory maps at the beginning

00:14:33,040 --> 00:14:37,040
so this was fast but i wanted to keep

00:14:35,600 --> 00:14:39,360
looking

00:14:37,040 --> 00:14:40,959
and this takes us to our final solution

00:14:39,360 --> 00:14:42,160
now if we look at this code again

00:14:40,959 --> 00:14:44,079
we're basically working with a thing

00:14:42,160 --> 00:14:45,760
that looks a lot like a file doing the

00:14:44,079 --> 00:14:47,600
kind of things you would do with a file

00:14:45,760 --> 00:14:49,519
we just happen to be doing in memory

00:14:47,600 --> 00:14:51,360
which was faster than normal files

00:14:49,519 --> 00:14:52,959
but what happens if we ditch the maps

00:14:51,360 --> 00:14:54,800
and just do this

00:14:52,959 --> 00:14:56,399
to a file pretty much the only

00:14:54,800 --> 00:14:57,680
difference is instead of setting a range

00:14:56,399 --> 00:14:58,320
in something that's pretending to be an

00:14:57,680 --> 00:15:00,000
array

00:14:58,320 --> 00:15:01,680
we need to call this function to jump

00:15:00,000 --> 00:15:03,760
around on to right so basically file

00:15:01,680 --> 00:15:05,440
seek and file right

00:15:03,760 --> 00:15:07,519
now when changing the value it now looks

00:15:05,440 --> 00:15:08,800
like this and while it's a file

00:15:07,519 --> 00:15:10,399
it's super efficient because you're just

00:15:08,800 --> 00:15:11,839
doing one seek and writing eight dice

00:15:10,399 --> 00:15:13,040
you're not dumping the entire hash like

00:15:11,839 --> 00:15:14,560
with the p store

00:15:13,040 --> 00:15:16,000
and we are also using separate files for

00:15:14,560 --> 00:15:16,880
each method metrics so if you're

00:15:16,000 --> 00:15:18,240
multi-threaded

00:15:16,880 --> 00:15:20,399
you're going to have less contention on

00:15:18,240 --> 00:15:21,279
those files and now you have no memory

00:15:20,399 --> 00:15:23,760
maps no c

00:15:21,279 --> 00:15:24,800
no sequels nothing fancy just plain old

00:15:23,760 --> 00:15:26,959
boring ruby

00:15:24,800 --> 00:15:28,160
writing to follow now obviously the

00:15:26,959 --> 00:15:29,759
problem with fuss and disc is that

00:15:28,160 --> 00:15:31,920
they're kind of slow right and so if

00:15:29,759 --> 00:15:33,360
you're thinking how slow is this

00:15:31,920 --> 00:15:35,199
this is kind of insane this completely

00:15:33,360 --> 00:15:36,800
blew my mind in

00:15:35,199 --> 00:15:38,480
initially like considering how slow i

00:15:36,800 --> 00:15:39,279
expected this to be i couldn't believe

00:15:38,480 --> 00:15:40,320
this

00:15:39,279 --> 00:15:42,240
now it makes sense because we're

00:15:40,320 --> 00:15:42,639
basically not f syncing so we're not

00:15:42,240 --> 00:15:44,079
actually

00:15:42,639 --> 00:15:45,920
touching the disk we're really just

00:15:44,079 --> 00:15:47,040
doing this in memory and it's fine

00:15:45,920 --> 00:15:48,000
because we don't really care about this

00:15:47,040 --> 00:15:49,680
data staying if

00:15:48,000 --> 00:15:51,040
our process dies but it's still

00:15:49,680 --> 00:15:52,000
surprisingly fast and if we compare

00:15:51,040 --> 00:15:53,839
these two memory maps

00:15:52,000 --> 00:15:56,399
we have no external dependencies no

00:15:53,839 --> 00:15:57,839
risky c code definitely no segfaults

00:15:56,399 --> 00:15:59,360
it's compatible with all versions of

00:15:57,839 --> 00:16:01,040
ruby which we care about because we want

00:15:59,360 --> 00:16:03,279
to support jruby

00:16:01,040 --> 00:16:05,360
it is 100 understandable by any ruby

00:16:03,279 --> 00:16:06,399
programmer and very importantly there's

00:16:05,360 --> 00:16:08,800
nothing in that code

00:16:06,399 --> 00:16:10,399
that keeps me up at night all of this

00:16:08,800 --> 00:16:13,199
for just three extra microseconds

00:16:10,399 --> 00:16:13,839
this was a complete no-brainer and we

00:16:13,199 --> 00:16:15,360
were done we

00:16:13,839 --> 00:16:16,959
ended up going with this as our final

00:16:15,360 --> 00:16:18,320
solution because he was fast enough

00:16:16,959 --> 00:16:19,600
and we think that the trade-off is more

00:16:18,320 --> 00:16:21,199
than worth it for the peace of mind that

00:16:19,600 --> 00:16:23,440
the solution brings us

00:16:21,199 --> 00:16:24,399
now we did make a separate gem with the

00:16:23,440 --> 00:16:25,600
memory map store

00:16:24,399 --> 00:16:27,680
and it has a better solution for the

00:16:25,600 --> 00:16:29,440
resize i think that's the correct way

00:16:27,680 --> 00:16:31,759
and i think it's stable

00:16:29,440 --> 00:16:33,600
but we don't trust it it's if you really

00:16:31,759 --> 00:16:35,279
want those three microseconds back

00:16:33,600 --> 00:16:36,480
you can use it at your own risk but we

00:16:35,279 --> 00:16:38,560
don't really recommend it we don't we

00:16:36,480 --> 00:16:40,399
don't trust that thing

00:16:38,560 --> 00:16:42,079
so this is where we are now after the

00:16:40,399 --> 00:16:44,079
rewrite we ended up with

00:16:42,079 --> 00:16:45,839
three built-in stores for different

00:16:44,079 --> 00:16:47,680
scenarios so you can have

00:16:45,839 --> 00:16:49,440
a if you have a single thread in a

00:16:47,680 --> 00:16:50,320
single process or multiple threads in

00:16:49,440 --> 00:16:53,600
the single process

00:16:50,320 --> 00:16:55,199
or multiple processes we now you're not

00:16:53,600 --> 00:16:56,320
going to configure how to aggregate

00:16:55,199 --> 00:16:57,279
values because now that you have

00:16:56,320 --> 00:16:58,639
multiple processes

00:16:57,279 --> 00:17:00,160
you can have multiple values so

00:16:58,639 --> 00:17:01,360
customers and histograms you just want

00:17:00,160 --> 00:17:03,120
to sum them

00:17:01,360 --> 00:17:04,400
but for gauges that doesn't really

00:17:03,120 --> 00:17:05,199
necessarily make sense it depends on

00:17:04,400 --> 00:17:07,120
your use case

00:17:05,199 --> 00:17:08,959
so gauges will default to all of the

00:17:07,120 --> 00:17:10,480
time series where they get tagged with a

00:17:08,959 --> 00:17:12,000
process id label

00:17:10,480 --> 00:17:15,280
but you can for example take the maximum

00:17:12,000 --> 00:17:17,520
or the minimum or the most recent one

00:17:15,280 --> 00:17:19,039
we now require declaring all of your

00:17:17,520 --> 00:17:20,559
labels up front when you register the

00:17:19,039 --> 00:17:21,839
metric which is what the best practices

00:17:20,559 --> 00:17:23,439
indicate

00:17:21,839 --> 00:17:24,959
we are much better aligned with ruby

00:17:23,439 --> 00:17:26,400
conventions for example we use keyword

00:17:24,959 --> 00:17:28,079
arguments which is where the ruby world

00:17:26,400 --> 00:17:30,640
is going towards

00:17:28,079 --> 00:17:32,720
we ended up doing a lot of profiling and

00:17:30,640 --> 00:17:34,160
a lot of performance work as part of

00:17:32,720 --> 00:17:35,600
optimizing the stores and we ended up

00:17:34,160 --> 00:17:36,080
optimizing a lot of little things here

00:17:35,600 --> 00:17:37,760
and there

00:17:36,080 --> 00:17:39,280
especially histograms histograms are a

00:17:37,760 --> 00:17:40,960
lot faster now

00:17:39,280 --> 00:17:42,559
and finally we now have a benchmark

00:17:40,960 --> 00:17:43,840
script so if you want to try different

00:17:42,559 --> 00:17:45,679
scenarios and see

00:17:43,840 --> 00:17:46,799
how this affects your particular

00:17:45,679 --> 00:17:49,039
performance in your app you can now

00:17:46,799 --> 00:17:50,480
measure it fairly easily

00:17:49,039 --> 00:17:52,400
so we have that running for several

00:17:50,480 --> 00:17:53,280
months in production ago carlos and then

00:17:52,400 --> 00:17:56,480
our pr got

00:17:53,280 --> 00:17:57,840
merged and we released 1.0 so since late

00:17:56,480 --> 00:17:59,440
last year everybody can now use

00:17:57,840 --> 00:18:00,960
prometheus with ruby so if you're a ruby

00:17:59,440 --> 00:18:03,039
shop and you had this problem

00:18:00,960 --> 00:18:04,720
now you can have your matrix and as part

00:18:03,039 --> 00:18:06,000
of this we ended up becoming maintainers

00:18:04,720 --> 00:18:06,400
of the gem ourselves which is pretty

00:18:06,000 --> 00:18:09,520
nice

00:18:06,400 --> 00:18:11,039
and it allows us to do more work on it

00:18:09,520 --> 00:18:13,360
which is important because there are

00:18:11,039 --> 00:18:14,720
still some issues and we really

00:18:13,360 --> 00:18:16,720
we still really need to put some work

00:18:14,720 --> 00:18:18,080
into this first of all we have the too

00:18:16,720 --> 00:18:19,120
many files problem now to keep

00:18:18,080 --> 00:18:21,360
contention low

00:18:19,120 --> 00:18:23,039
as i mentioned we we keep separate files

00:18:21,360 --> 00:18:24,400
for each metric in each process

00:18:23,039 --> 00:18:25,679
that means if you have a lot of metrics

00:18:24,400 --> 00:18:26,320
you're going to have a huge number of

00:18:25,679 --> 00:18:28,720
files and

00:18:26,320 --> 00:18:30,080
you end up getting problems from that so

00:18:28,720 --> 00:18:30,640
what we want to do is give you the

00:18:30,080 --> 00:18:32,240
choice

00:18:30,640 --> 00:18:34,160
to trade off having maybe more

00:18:32,240 --> 00:18:37,120
contention but fewer files

00:18:34,160 --> 00:18:37,840
and maybe that'll help most importantly

00:18:37,120 --> 00:18:39,200
we focused

00:18:37,840 --> 00:18:40,880
all of our attention on having the

00:18:39,200 --> 00:18:42,080
flexible stores and

00:18:40,880 --> 00:18:44,000
having good performance when

00:18:42,080 --> 00:18:45,120
incrementing counters but we had a

00:18:44,000 --> 00:18:46,880
pretty large blind spot

00:18:45,120 --> 00:18:48,480
the way we structure things exports can

00:18:46,880 --> 00:18:50,880
get really really slow if you have

00:18:48,480 --> 00:18:51,760
lots of metrics around tens of thousands

00:18:50,880 --> 00:18:53,360
of metrics

00:18:51,760 --> 00:18:54,720
so we're now starting to work more

00:18:53,360 --> 00:18:56,559
seriously on this because more people

00:18:54,720 --> 00:18:57,440
are being affected by it we hope to have

00:18:56,559 --> 00:19:00,720
a

00:18:57,440 --> 00:19:02,080
response reasonably soon finally we want

00:19:00,720 --> 00:19:03,440
to support openmetrics

00:19:02,080 --> 00:19:04,559
as soon as it is ready so we have

00:19:03,440 --> 00:19:05,840
already started working in this

00:19:04,559 --> 00:19:07,440
direction

00:19:05,840 --> 00:19:08,640
and that's all i have for you today i

00:19:07,440 --> 00:19:09,280
hope you found some bits of that

00:19:08,640 --> 00:19:12,320
interesting

00:19:09,280 --> 00:19:12,320
thank you very much for listening

00:19:19,200 --> 00:19:24,080
so first question how do i connect two

00:19:22,840 --> 00:19:26,480
clusters

00:19:24,080 --> 00:19:27,840
uh i don't think that one's for me yeah

00:19:26,480 --> 00:19:31,520
that's that's an old one

00:19:27,840 --> 00:19:31,520
i mean i hope it's not for me because

00:19:31,600 --> 00:19:36,640
oh sorry that's an old one

00:19:34,640 --> 00:19:38,080
and that's the only one in this in the

00:19:36,640 --> 00:19:41,360
soon a

00:19:38,080 --> 00:19:43,280
excellent so it was super clear um

00:19:41,360 --> 00:19:44,960
there is interest in your in your

00:19:43,280 --> 00:19:47,520
streaming setup and in your

00:19:44,960 --> 00:19:48,240
in your stream and your video pipeline i

00:19:47,520 --> 00:19:51,520
assume

00:19:48,240 --> 00:19:52,080
it's obs and the green screen it's not

00:19:51,520 --> 00:19:54,799
obs

00:19:52,080 --> 00:19:56,080
i could have been uh if i were streaming

00:19:54,799 --> 00:19:57,679
i would have used that

00:19:56,080 --> 00:19:59,760
uh the screen the green screen you can

00:19:57,679 --> 00:20:02,480
see right there um

00:19:59,760 --> 00:20:03,679
the setup is insane i'll i'll retweet it

00:20:02,480 --> 00:20:05,280
uh

00:20:03,679 --> 00:20:07,760
hashtag in you guys so you can see it

00:20:05,280 --> 00:20:09,280
it's a bit mad but it kind of works i'm

00:20:07,760 --> 00:20:11,120
using camtasia

00:20:09,280 --> 00:20:13,760
just for reasons but you could have used

00:20:11,120 --> 00:20:17,520
premiere or whatever obs is not as fun

00:20:13,760 --> 00:20:17,520
to edit the video afterwards

00:20:18,840 --> 00:20:26,080
okay i don't see any other questions in

00:20:22,720 --> 00:20:27,679
any of the channels so oh you're gonna

00:20:26,080 --> 00:20:29,679
have to wing it down

00:20:27,679 --> 00:20:30,960
yeah i mean you can i can ask some

00:20:29,679 --> 00:20:36,000
questions

00:20:30,960 --> 00:20:36,000
oh no not again hi then

00:20:36,159 --> 00:20:40,320
so uh i'm ben i am also part of the

00:20:39,600 --> 00:20:42,400
maintaining

00:20:40,320 --> 00:20:43,360
of the ruby uh although i don't do a

00:20:42,400 --> 00:20:45,120
whole lot uh

00:20:43,360 --> 00:20:46,799
i just kind of give prometheus advice to

00:20:45,120 --> 00:20:48,159
the to

00:20:46,799 --> 00:20:50,000
the gold cardless team they've done a

00:20:48,159 --> 00:20:53,120
great job of maintaining this

00:20:50,000 --> 00:20:56,799
um uh

00:20:53,120 --> 00:20:59,679
the the map thing yes is very very

00:20:56,799 --> 00:21:00,480
uh very fragile it took us quite a long

00:20:59,679 --> 00:21:03,679
time to get

00:21:00,480 --> 00:21:06,080
the gitlab fork of the

00:21:03,679 --> 00:21:07,600
so git lab took the same code and we

00:21:06,080 --> 00:21:11,360
forked the same

00:21:07,600 --> 00:21:13,760
julius volts map branch

00:21:11,360 --> 00:21:15,039
and began our development of a file

00:21:13,760 --> 00:21:18,320
based backing store

00:21:15,039 --> 00:21:18,880
using mmap and what we ended up doing

00:21:18,320 --> 00:21:22,799
was

00:21:18,880 --> 00:21:25,280
we dropped the in we dropped the map gem

00:21:22,799 --> 00:21:26,080
and just brought over the c code and

00:21:25,280 --> 00:21:29,520
then

00:21:26,080 --> 00:21:30,559
basically made the c code self-contained

00:21:29,520 --> 00:21:33,280
within

00:21:30,559 --> 00:21:34,480
uh within our gem so we did we we no

00:21:33,280 --> 00:21:37,679
longer use the upstream

00:21:34,480 --> 00:21:39,679
map jam because there were some bugs

00:21:37,679 --> 00:21:40,880
and so we basically started implementing

00:21:39,679 --> 00:21:44,640
more and more

00:21:40,880 --> 00:21:47,280
of the metrics handling in c

00:21:44,640 --> 00:21:48,000
and then just used it more as a real

00:21:47,280 --> 00:21:49,840
library

00:21:48,000 --> 00:21:52,240
and that's that solved a bunch of the

00:21:49,840 --> 00:21:55,120
crashing and seg faulting and other

00:21:52,240 --> 00:21:56,720
race condition bugs in addition to the

00:21:55,120 --> 00:22:00,159
really really hard part

00:21:56,720 --> 00:22:02,720
which uh i believe you ran into with the

00:22:00,159 --> 00:22:04,400
with the map and the font and the file

00:22:02,720 --> 00:22:06,240
based storage which is

00:22:04,400 --> 00:22:09,039
reading back metrics from the file

00:22:06,240 --> 00:22:12,240
storage was super slow

00:22:09,039 --> 00:22:16,080
especially when you get to say 20 30 000

00:22:12,240 --> 00:22:19,520
metrics on a uh on a unicorn

00:22:16,080 --> 00:22:22,159
uh and i don't know if you've uh uh

00:22:19,520 --> 00:22:24,240
had any additional thoughts on on how to

00:22:22,159 --> 00:22:26,480
how to make that part faster

00:22:24,240 --> 00:22:28,159
so i've had thoughts uh i actually think

00:22:26,480 --> 00:22:29,440
think of you very frequently because i

00:22:28,159 --> 00:22:31,520
want to get you guys on the

00:22:29,440 --> 00:22:33,039
on the official client rather on your

00:22:31,520 --> 00:22:34,640
own fork and i know

00:22:33,039 --> 00:22:36,400
you you're the the ones that have the

00:22:34,640 --> 00:22:39,679
biggest problem with this

00:22:36,400 --> 00:22:40,159
so part of the problem i have is we have

00:22:39,679 --> 00:22:42,559
to

00:22:40,159 --> 00:22:44,400
sort of balance code cleanliness and

00:22:42,559 --> 00:22:47,679
just elegance if you will

00:22:44,400 --> 00:22:49,039
with performance because

00:22:47,679 --> 00:22:51,520
the abstractions that we have done

00:22:49,039 --> 00:22:53,440
really help like it's not just reading

00:22:51,520 --> 00:22:56,000
back the files is slow

00:22:53,440 --> 00:22:56,880
you can read the raw files it's not that

00:22:56,000 --> 00:22:59,280
terrible

00:22:56,880 --> 00:23:01,200
but what you end up with is now you have

00:22:59,280 --> 00:23:03,360
to massage all of these hashes

00:23:01,200 --> 00:23:04,480
to turn them into one mega hash and also

00:23:03,360 --> 00:23:05,360
you have things like you don't want the

00:23:04,480 --> 00:23:07,679
exporter

00:23:05,360 --> 00:23:09,280
to be talking to the stores directly

00:23:07,679 --> 00:23:10,559
because that's kind of dirty

00:23:09,280 --> 00:23:12,559
but if you don't do that it's going to

00:23:10,559 --> 00:23:14,559
be slow forever so we're trying to

00:23:12,559 --> 00:23:16,000
sort of strike the right balance there

00:23:14,559 --> 00:23:18,160
to make it not disgusting

00:23:16,000 --> 00:23:19,520
but also fast enough um

00:23:18,160 --> 00:23:22,400
[Music]

00:23:19,520 --> 00:23:23,440
i have a bunch of thoughts but i have i

00:23:22,400 --> 00:23:26,400
don't have any

00:23:23,440 --> 00:23:28,960
exciting results to report yet but i

00:23:26,400 --> 00:23:28,960
would love to

00:23:31,120 --> 00:23:36,240
so stefano torresii is asking what about

00:23:34,640 --> 00:23:37,760
clusters of distributed apps

00:23:36,240 --> 00:23:39,600
a centralized store like radius is the

00:23:37,760 --> 00:23:41,520
only way there so

00:23:39,600 --> 00:23:43,840
what you would do there is you would

00:23:41,520 --> 00:23:45,279
basically report

00:23:43,840 --> 00:23:46,960
so you have a bunch of boxes and each

00:23:45,279 --> 00:23:47,600
box will report its metrics and then you

00:23:46,960 --> 00:23:49,279
aggregate

00:23:47,600 --> 00:23:50,880
when you're querying or at least that's

00:23:49,279 --> 00:23:52,000
how i understand it as a best practice

00:23:50,880 --> 00:23:54,400
you don't want to have

00:23:52,000 --> 00:23:55,760
different machines centralized into one

00:23:54,400 --> 00:23:58,240
ready store and then

00:23:55,760 --> 00:23:58,960
reporting everything from there because

00:23:58,240 --> 00:24:00,640
that would

00:23:58,960 --> 00:24:03,679
basically obscure if you have problems

00:24:00,640 --> 00:24:06,799
in one machine or another

00:24:03,679 --> 00:24:08,960
correct me if i'm wrong ben or richie

00:24:06,799 --> 00:24:10,720
sure yeah no that's absolutely correct

00:24:08,960 --> 00:24:12,640
uh

00:24:10,720 --> 00:24:13,840
you don't want to use a central store

00:24:12,640 --> 00:24:17,360
for an entire

00:24:13,840 --> 00:24:19,279
fleet of applications because

00:24:17,360 --> 00:24:20,480
some of those may be taking dip more or

00:24:19,279 --> 00:24:23,600
less traffic or

00:24:20,480 --> 00:24:26,480
say one one server is having

00:24:23,600 --> 00:24:27,440
uh out of memory problems or out of disk

00:24:26,480 --> 00:24:30,559
problems

00:24:27,440 --> 00:24:32,320
uh you wanna make sure that you

00:24:30,559 --> 00:24:35,760
know that that particular server is

00:24:32,320 --> 00:24:39,039
broken versus all the other healthy ones

00:24:35,760 --> 00:24:40,799
i got a question from the q a uh of the

00:24:39,039 --> 00:24:43,279
list of uh features on the roadmap which

00:24:40,799 --> 00:24:46,320
ones are your prioritizing

00:24:43,279 --> 00:24:47,440
yeah hi josh so the one i would like to

00:24:46,320 --> 00:24:50,960
prioritize the most

00:24:47,440 --> 00:24:53,120
is this export speed we've had other

00:24:50,960 --> 00:24:54,400
issues come up so other github issues

00:24:53,120 --> 00:24:56,240
come up with

00:24:54,400 --> 00:25:01,520
people reporting that so i would really

00:24:56,240 --> 00:25:04,720
like to get that as soon as possible

00:25:01,520 --> 00:25:06,480
another question from the q a uh

00:25:04,720 --> 00:25:08,320
when do we need to read from files when

00:25:06,480 --> 00:25:09,919
instrumenting so i'm not sure i

00:25:08,320 --> 00:25:11,679
understand this one correctly

00:25:09,919 --> 00:25:13,840
if you're instrumenting your own app you

00:25:11,679 --> 00:25:15,120
wouldn't need to do that like the client

00:25:13,840 --> 00:25:16,880
library will do that for you

00:25:15,120 --> 00:25:18,880
it basically transparently handles those

00:25:16,880 --> 00:25:20,880
files

00:25:18,880 --> 00:25:22,559
so you would ideally not even notice

00:25:20,880 --> 00:25:26,400
that those files are there

00:25:22,559 --> 00:25:29,440
at most if you have a persistent

00:25:26,400 --> 00:25:30,080
file system you have to these files are

00:25:29,440 --> 00:25:31,919
stored in the

00:25:30,080 --> 00:25:33,360
slash temp directory and you want to

00:25:31,919 --> 00:25:34,559
empty that so if you're in containers

00:25:33,360 --> 00:25:36,480
that will come for free

00:25:34,559 --> 00:25:38,480
but if you're basically rebooting yours

00:25:36,480 --> 00:25:40,799
your server your app server on the same

00:25:38,480 --> 00:25:42,640
machine and you keep files from the

00:25:40,799 --> 00:25:43,600
previous runs you want to kill those

00:25:42,640 --> 00:25:44,240
files otherwise you're going to be

00:25:43,600 --> 00:25:46,960
reporting

00:25:44,240 --> 00:25:47,679
sort of the metrics of the previous run

00:25:46,960 --> 00:25:56,480
instead of starting

00:25:47,679 --> 00:25:58,159
from zero

00:25:56,480 --> 00:26:00,159
if there's no more questions i just

00:25:58,159 --> 00:26:01,279
thought i had a comment from the python

00:26:00,159 --> 00:26:03,840
side which has

00:26:01,279 --> 00:26:05,919
a similar implementation now we've had a

00:26:03,840 --> 00:26:07,279
few pr's over time and community has

00:26:05,919 --> 00:26:08,799
been kind enough to basically

00:26:07,279 --> 00:26:10,640
tune our buffers and so on so the

00:26:08,799 --> 00:26:13,360
reading itself is not a problem

00:26:10,640 --> 00:26:14,640
the problem is process churn because we

00:26:13,360 --> 00:26:17,120
create four files

00:26:14,640 --> 00:26:18,080
per page and that's fine if you have 16

00:26:17,120 --> 00:26:19,600
workers

00:26:18,080 --> 00:26:21,600
but not when they change every 30

00:26:19,600 --> 00:26:22,320
seconds and that's what the big problem

00:26:21,600 --> 00:26:24,400
is there

00:26:22,320 --> 00:26:26,880
yeah yes we have lots of users doing

00:26:24,400 --> 00:26:28,880
dash and that's where the problem is

00:26:26,880 --> 00:26:30,400
and we hear our keys inside the files

00:26:28,880 --> 00:26:31,760
are just encoded json

00:26:30,400 --> 00:26:34,400
but we don't have to read it too often

00:26:31,760 --> 00:26:35,760
so that's fine and so there's been talks

00:26:34,400 --> 00:26:37,760
about compaction but unfortunately

00:26:35,760 --> 00:26:38,320
everything proposed was not tread safe

00:26:37,760 --> 00:26:40,400
because

00:26:38,320 --> 00:26:42,159
well another process can start with the

00:26:40,400 --> 00:26:44,320
same kid and we need to make sure that's

00:26:42,159 --> 00:26:45,360
correct and we have no control over this

00:26:44,320 --> 00:26:47,840
because we don't know if people are

00:26:45,360 --> 00:26:49,200
using unicorn or multi-process python

00:26:47,840 --> 00:26:50,720
stuff or anything so

00:26:49,200 --> 00:26:52,640
if you ever find a solution for that

00:26:50,720 --> 00:26:54,960
please let us know

00:26:52,640 --> 00:26:56,240
yeah so i'll take your four files per

00:26:54,960 --> 00:26:59,600
process and ratio

00:26:56,240 --> 00:27:00,400
a file per metric per process which i

00:26:59,600 --> 00:27:02,880
now regret

00:27:00,400 --> 00:27:04,400
it did make it faster to write but i

00:27:02,880 --> 00:27:06,960
thoroughly regret that that's the other

00:27:04,400 --> 00:27:08,880
thing i really want to prioritize having

00:27:06,960 --> 00:27:10,000
given implementers the option whether

00:27:08,880 --> 00:27:12,000
they they want that

00:27:10,000 --> 00:27:13,919
if they're seeing a lot of contention

00:27:12,000 --> 00:27:15,039
and slow rights or if they would rather

00:27:13,919 --> 00:27:19,039
have one file

00:27:15,039 --> 00:27:20,480
per process total

00:27:19,039 --> 00:27:24,559
you still have the same problem you have

00:27:20,480 --> 00:27:24,559
but i guess a fourth of it

00:27:25,760 --> 00:27:29,200
we have another one why don't you just

00:27:27,760 --> 00:27:32,000
go with

00:27:29,200 --> 00:27:34,000
stats the exporter so so this is where i

00:27:32,000 --> 00:27:37,279
face up that i'm not a stereotypical

00:27:34,000 --> 00:27:39,200
sre and so to be honest i don't know so

00:27:37,279 --> 00:27:41,600
in my company

00:27:39,200 --> 00:27:43,440
we didn't want to go with that one i

00:27:41,600 --> 00:27:44,960
don't remember the exact

00:27:43,440 --> 00:27:46,960
discussion around i don't remember the

00:27:44,960 --> 00:27:49,360
exact reason but like

00:27:46,960 --> 00:27:50,559
it wasn't ideal for our use case but i

00:27:49,360 --> 00:27:52,880
don't really have a good answer i'm

00:27:50,559 --> 00:27:52,880
sorry

00:27:55,200 --> 00:28:00,320
okay then the other obvious question

00:27:58,240 --> 00:28:04,640
when will you rewrite everything in rust

00:28:00,320 --> 00:28:04,640
because that's uh so

00:28:07,840 --> 00:28:11,039
so here's well there's another trailer

00:28:09,440 --> 00:28:12,480
there right so

00:28:11,039 --> 00:28:14,640
with the system now where you have the

00:28:12,480 --> 00:28:17,840
interchangeable stores you could have

00:28:14,640 --> 00:28:18,480
a store written in rust and then if you

00:28:17,840 --> 00:28:21,200
can

00:28:18,480 --> 00:28:22,640
do the magic memory thing there then you

00:28:21,200 --> 00:28:23,520
don't need to be dealing with the files

00:28:22,640 --> 00:28:25,279
and then

00:28:23,520 --> 00:28:26,640
the exporter is still written in ruby

00:28:25,279 --> 00:28:28,320
which kind of sucks

00:28:26,640 --> 00:28:30,720
but if you put that store in a separate

00:28:28,320 --> 00:28:33,840
gem then it's great the reason

00:28:30,720 --> 00:28:36,080
we don't want rust or c code in the main

00:28:33,840 --> 00:28:38,159
exporter code is that then you exclude

00:28:36,080 --> 00:28:41,039
the j room people

00:28:38,159 --> 00:28:42,240
which we i think is it's not a majority

00:28:41,039 --> 00:28:43,120
of our store i think it's nice to

00:28:42,240 --> 00:28:44,399
support them

00:28:43,120 --> 00:28:45,600
particularly because they don't have

00:28:44,399 --> 00:28:46,880
this problem they can actually just have

00:28:45,600 --> 00:28:48,080
a hashing memory because they're not

00:28:46,880 --> 00:28:51,520
doing multi-process

00:28:48,080 --> 00:28:54,880
in general um so ideally

00:28:51,520 --> 00:28:55,279
no but it i haven't discovered it as an

00:28:54,880 --> 00:28:57,760
option

00:28:55,279 --> 00:28:59,279
it can totally be a separate gem and i'm

00:28:57,760 --> 00:29:01,039
trying to figure out how to have

00:28:59,279 --> 00:29:03,200
we can have like a rust exporter and a

00:29:01,039 --> 00:29:04,559
rust a store and they can talk directly

00:29:03,200 --> 00:29:07,440
to each other

00:29:04,559 --> 00:29:09,440
and then it sort of breaks my beautiful

00:29:07,440 --> 00:29:10,320
abstractions but it could be the way to

00:29:09,440 --> 00:29:15,120
make this fast

00:29:10,320 --> 00:29:17,679
for mri people so see what we will

00:29:15,120 --> 00:29:18,640
for the record i was joking but no i

00:29:17,679 --> 00:29:21,120
know you were

00:29:18,640 --> 00:29:22,960
but we actually we're actually

00:29:21,120 --> 00:29:23,520
considering this because we want to make

00:29:22,960 --> 00:29:25,679
this better

00:29:23,520 --> 00:29:27,200
and also rust is fun and a bunch of

00:29:25,679 --> 00:29:31,039
people want to play with it

00:29:27,200 --> 00:29:34,240
and this is a great excuse so

00:29:31,039 --> 00:29:35,679
so yeah so we're basically out of time

00:29:34,240 --> 00:29:37,279
but we have one good question

00:29:35,679 --> 00:29:39,120
but we need to be quick about that and

00:29:37,279 --> 00:29:40,240
then we will pivot over to lightning

00:29:39,120 --> 00:29:42,240
talks

00:29:40,240 --> 00:29:44,240
did you ever expose all available i know

00:29:42,240 --> 00:29:46,320
it's because of the five parametric

00:29:44,240 --> 00:29:47,520
uh process approach is this a major

00:29:46,320 --> 00:29:50,320
concern

00:29:47,520 --> 00:29:50,880
yes so we haven't other users of the gem

00:29:50,320 --> 00:29:52,880
have

00:29:50,880 --> 00:29:54,880
it is a major consensus one of these the

00:29:52,880 --> 00:29:56,410
too many files problem and is is

00:29:54,880 --> 00:29:59,479
part of what i want to fix

00:29:56,410 --> 00:29:59,479
[Music]

00:30:00,000 --> 00:30:04,960
all right that's it for me thank you

00:30:01,679 --> 00:30:08,080
very much and retweet your your setup

00:30:04,960 --> 00:30:23,200
uh so more people can see yes i will

00:30:08,080 --> 00:30:23,200

YouTube URL: https://www.youtube.com/watch?v=fsambb-nMTk


