Title: PromCon Online 2020 - TSDB: Year In Review, Ganesh Vernekar @ Grafana Labs
Publication date: 2020-07-23
Playlist: PromCon Online 2020
Description: 
	Prometheusâ€™ storage engine has seen a lot of improvements and optimizations in the past 1 year since we had PromCon 2019 where I gave a review on TSDB developments since Prometheus 2.0. Similarly, in this talk, I will speak about all the new enhancements and features that have been added to TSDB since PromCon 2019 and will be added in the future. Some of the notable ones include:

Various memory optimizations for loading blocks and compaction.
* Isolation
* M-map of Head chunks.
* Faster restarts with a snapshot of chunks.
* Lifting the block index size limit.

I will also talk about how each of these developments impacted Prometheus.
Captions: 
	00:00:01,770 --> 00:00:04,940
[Music]

00:00:14,960 --> 00:00:19,760
welcome everyone to my talk tsd

00:00:16,880 --> 00:00:21,920
another year in so in the last prom con

00:00:19,760 --> 00:00:23,920
which happened in november 2019 i gave a

00:00:21,920 --> 00:00:24,720
similar talk called tsd one year in

00:00:23,920 --> 00:00:26,960
where i

00:00:24,720 --> 00:00:29,199
talked about what went into the tsb

00:00:26,960 --> 00:00:31,279
since the prometheus 2.0 release

00:00:29,199 --> 00:00:33,120
and in this talk i am going to talk what

00:00:31,279 --> 00:00:35,760
went into the tsdb

00:00:33,120 --> 00:00:37,760
since we had the last prop con which was

00:00:35,760 --> 00:00:39,680
in number 2019

00:00:37,760 --> 00:00:41,520
so little about myself i am a software

00:00:39,680 --> 00:00:43,360
engineer at grafana labs

00:00:41,520 --> 00:00:45,360
and i am a prometheus member and i

00:00:43,360 --> 00:00:46,879
maintain the time series database part

00:00:45,360 --> 00:00:50,079
of the prometheus

00:00:46,879 --> 00:00:51,360
and i'll be sharing these slides after

00:00:50,079 --> 00:00:53,680
the talk so

00:00:51,360 --> 00:00:56,160
make sure you follow me on twitter i'll

00:00:53,680 --> 00:00:58,800
be sharing it there

00:00:56,160 --> 00:01:00,399
so let's see what happened in tsub since

00:00:58,800 --> 00:01:02,800
from con 2019

00:01:00,399 --> 00:01:03,920
the outland outline of this talk is

00:01:02,800 --> 00:01:06,720
going to be

00:01:03,920 --> 00:01:07,520
first we start with what went into tcp

00:01:06,720 --> 00:01:10,479
already which are

00:01:07,520 --> 00:01:12,080
available in the prometheus releases and

00:01:10,479 --> 00:01:14,400
we will move on to what's coming

00:01:12,080 --> 00:01:16,240
up and we'll end with the features that

00:01:14,400 --> 00:01:20,080
were added for the downstream users

00:01:16,240 --> 00:01:20,080
like cortex thanos etc

00:01:20,159 --> 00:01:25,439
so what's already in isolation

00:01:23,439 --> 00:01:28,720
so isolation is finally in i talked

00:01:25,439 --> 00:01:31,360
about this even in the last prom con

00:01:28,720 --> 00:01:32,880
so the basic idea about this is earlier

00:01:31,360 --> 00:01:34,720
if you had like 1000

00:01:32,880 --> 00:01:36,960
metrics that you are scraping and you

00:01:34,720 --> 00:01:39,119
use that in a histogram query

00:01:36,960 --> 00:01:41,119
and if the scrape is complete only up to

00:01:39,119 --> 00:01:42,159
like 500 metrics and if you query the

00:01:41,119 --> 00:01:44,560
tsdb

00:01:42,159 --> 00:01:46,640
you could see partial results like 500

00:01:44,560 --> 00:01:48,720
metrics of the new script 500

00:01:46,640 --> 00:01:50,159
matrix from the old script so this

00:01:48,720 --> 00:01:53,200
creates problems

00:01:50,159 --> 00:01:54,399
in various scenarios and isolation fixes

00:01:53,200 --> 00:01:57,840
it so

00:01:54,399 --> 00:01:59,680
this work was started by brian long ago

00:01:57,840 --> 00:02:01,280
and in the last prom con i mentioned

00:01:59,680 --> 00:02:04,479
gotham took it forward

00:02:01,280 --> 00:02:06,880
but that went stale again finally beyond

00:02:04,479 --> 00:02:08,879
who who is one of the earliest member of

00:02:06,880 --> 00:02:11,840
prometheus

00:02:08,879 --> 00:02:12,959
took the charge and finally finished the

00:02:11,840 --> 00:02:15,840
work

00:02:12,959 --> 00:02:17,680
which brian and gotham had started and

00:02:15,840 --> 00:02:21,040
isolation is finally in

00:02:17,680 --> 00:02:22,400
so if you are querying you won't see

00:02:21,040 --> 00:02:25,840
partial script

00:02:22,400 --> 00:02:28,239
so how this is implemented is we assign

00:02:25,840 --> 00:02:30,800
something called appender id for every

00:02:28,239 --> 00:02:34,160
append that comes into the tsdb

00:02:30,800 --> 00:02:36,800
and using this append id when we query

00:02:34,160 --> 00:02:38,480
we have a isolation state when we are

00:02:36,800 --> 00:02:39,840
iterating through the samples during the

00:02:38,480 --> 00:02:42,879
query we see if the

00:02:39,840 --> 00:02:43,599
append id that the sample corresponds to

00:02:42,879 --> 00:02:46,400
whether it's

00:02:43,599 --> 00:02:48,319
an active query like the active append

00:02:46,400 --> 00:02:49,040
which is not committed then we discard

00:02:48,319 --> 00:02:52,959
it

00:02:49,040 --> 00:02:56,480
so with some logic around that we

00:02:52,959 --> 00:02:59,280
have achieved the isolation

00:02:56,480 --> 00:03:00,480
now the next thing so i'm going to talk

00:02:59,280 --> 00:03:03,760
about a series of

00:03:00,480 --> 00:03:07,200
optimizations which brian put in in

00:03:03,760 --> 00:03:09,200
the 2.15 release so in the bottom right

00:03:07,200 --> 00:03:12,319
corner for of every slide

00:03:09,200 --> 00:03:13,360
i have the release number mentioned in

00:03:12,319 --> 00:03:15,360
which

00:03:13,360 --> 00:03:18,000
these optimizations are there for

00:03:15,360 --> 00:03:22,000
example isolation had 2.17

00:03:18,000 --> 00:03:24,799
so it is there since the release 2.17.

00:03:22,000 --> 00:03:25,440
now let's talk about these optimizations

00:03:24,799 --> 00:03:28,080
so

00:03:25,440 --> 00:03:29,040
in the couple of uh optimizers that i'm

00:03:28,080 --> 00:03:31,760
going to talk about

00:03:29,040 --> 00:03:33,519
a brand reduce the memory footprint that

00:03:31,760 --> 00:03:36,799
a loaded block takes

00:03:33,519 --> 00:03:38,799
and both have some similar ideas in the

00:03:36,799 --> 00:03:39,920
uh in the slide that you are seeing

00:03:38,799 --> 00:03:43,440
right now

00:03:39,920 --> 00:03:45,120
uh a posting is offset

00:03:43,440 --> 00:03:47,519
in the index where the series

00:03:45,120 --> 00:03:50,080
information is stored so when i

00:03:47,519 --> 00:03:51,280
tell a posting it actually refers to a

00:03:50,080 --> 00:03:54,480
series

00:03:51,280 --> 00:03:55,920
and for every label value pair we have a

00:03:54,480 --> 00:03:57,519
list of postings

00:03:55,920 --> 00:03:59,360
that they appear in like the list of

00:03:57,519 --> 00:04:01,680
series that they appear in

00:03:59,360 --> 00:04:02,640
and that list is stored somewhere again

00:04:01,680 --> 00:04:04,480
in the index

00:04:02,640 --> 00:04:06,560
and the offset where it's stored is

00:04:04,480 --> 00:04:09,920
called posting offset

00:04:06,560 --> 00:04:10,959
so for every label available value pair

00:04:09,920 --> 00:04:13,040
that exists

00:04:10,959 --> 00:04:14,799
we were storing that in memory along

00:04:13,040 --> 00:04:16,639
with its offset

00:04:14,799 --> 00:04:18,959
so in this application what brand did

00:04:16,639 --> 00:04:22,160
was we don't actually need to store

00:04:18,959 --> 00:04:24,720
all those offsets we can store only like

00:04:22,160 --> 00:04:27,040
nth offset in this case we are storing

00:04:24,720 --> 00:04:30,000
every 30 second offset

00:04:27,040 --> 00:04:31,040
so whenever you want to get the posting

00:04:30,000 --> 00:04:33,600
offset

00:04:31,040 --> 00:04:34,880
like the posting list offset you can

00:04:33,600 --> 00:04:38,000
just binary search

00:04:34,880 --> 00:04:39,280
through this uh 10th position and when

00:04:38,000 --> 00:04:40,800
you are satisfied

00:04:39,280 --> 00:04:43,600
it falls in a certain bucket then you

00:04:40,800 --> 00:04:46,880
can iterate through those 32 items

00:04:43,600 --> 00:04:47,759
this reduced the memory footprint taken

00:04:46,880 --> 00:04:50,720
by

00:04:47,759 --> 00:04:51,280
the this offset table in the memory by

00:04:50,720 --> 00:04:54,960
about

00:04:51,280 --> 00:04:58,080
98 which was great and another thing was

00:04:54,960 --> 00:05:00,320
the symbols so symbols take up a lot of

00:04:58,080 --> 00:05:03,360
memory if you have lots of series

00:05:00,320 --> 00:05:05,520
and again symbols

00:05:03,360 --> 00:05:07,840
are just strings which appear in the

00:05:05,520 --> 00:05:10,800
series label and value

00:05:07,840 --> 00:05:12,320
so they are sorted and stored on the

00:05:10,800 --> 00:05:15,919
disk so we need not store

00:05:12,320 --> 00:05:17,600
all the symbols in the memory

00:05:15,919 --> 00:05:19,360
so in this operation what brand it was

00:05:17,600 --> 00:05:21,759
we again store every

00:05:19,360 --> 00:05:22,560
nth symbol i guess it's again every 30

00:05:21,759 --> 00:05:25,280
second symbol

00:05:22,560 --> 00:05:26,160
and the symbols that are used throughout

00:05:25,280 --> 00:05:29,280
the index

00:05:26,160 --> 00:05:31,600
are via offsets for example we store the

00:05:29,280 --> 00:05:33,360
symbols only once in the index

00:05:31,600 --> 00:05:35,199
and we use the offset at where it's

00:05:33,360 --> 00:05:36,320
stored in other places so that we can

00:05:35,199 --> 00:05:37,840
save space

00:05:36,320 --> 00:05:40,400
so whenever we want to look at some

00:05:37,840 --> 00:05:41,039
symbol you can just binary search

00:05:40,400 --> 00:05:44,080
through this

00:05:41,039 --> 00:05:45,840
nth symbol and then iterate inside the

00:05:44,080 --> 00:05:49,600
bucket to find your

00:05:45,840 --> 00:05:50,720
uh symbol so this actually reduce the

00:05:49,600 --> 00:05:53,360
memory footprint

00:05:50,720 --> 00:05:54,960
by a huge amount obviously with

00:05:53,360 --> 00:05:57,120
performance penalty

00:05:54,960 --> 00:05:58,639
on querying side but this optimization

00:05:57,120 --> 00:06:01,840
pays off

00:05:58,639 --> 00:06:03,919
when you have larger installations

00:06:01,840 --> 00:06:05,680
and these are some set of optimizations

00:06:03,919 --> 00:06:08,319
that were performed on

00:06:05,680 --> 00:06:08,880
a compaction i'm not going in detail

00:06:08,319 --> 00:06:11,199
about

00:06:08,880 --> 00:06:12,400
these optimizations else it'll take up

00:06:11,199 --> 00:06:14,639
the entire time

00:06:12,400 --> 00:06:16,639
the main idea behind this was whenever

00:06:14,639 --> 00:06:18,479
you're compacting and writing the data

00:06:16,639 --> 00:06:21,440
into the disk

00:06:18,479 --> 00:06:23,440
mainly the index you need not store all

00:06:21,440 --> 00:06:26,160
the data in the memory

00:06:23,440 --> 00:06:27,759
while you are in the compaction process

00:06:26,160 --> 00:06:30,400
what you could do is when you are

00:06:27,759 --> 00:06:31,840
at somewhere in mid process you could

00:06:30,400 --> 00:06:33,680
read back some data which you have

00:06:31,840 --> 00:06:35,360
already written in the index

00:06:33,680 --> 00:06:38,880
and do some calculations over that so

00:06:35,360 --> 00:06:40,800
that you can write the further data

00:06:38,880 --> 00:06:42,479
basically the whole idea of all these

00:06:40,800 --> 00:06:43,360
optimizations is don't store it in the

00:06:42,479 --> 00:06:45,199
memory

00:06:43,360 --> 00:06:47,280
re-read it from the disk whenever

00:06:45,199 --> 00:06:49,680
possible this

00:06:47,280 --> 00:06:51,759
makes the compaction slower obviously

00:06:49,680 --> 00:06:54,319
but it's a background process

00:06:51,759 --> 00:06:55,840
so it's fine but it actually makes the

00:06:54,319 --> 00:06:59,440
compaction process

00:06:55,840 --> 00:07:00,880
constant memory and all these block of

00:06:59,440 --> 00:07:04,160
block memory optimizations and the

00:07:00,880 --> 00:07:06,960
compaction optimizations came into 2.15

00:07:04,160 --> 00:07:08,800
so if you are on version before that you

00:07:06,960 --> 00:07:10,240
could see some spikes memory happening

00:07:08,800 --> 00:07:11,840
when there is a compaction

00:07:10,240 --> 00:07:13,759
and even when you have more and more

00:07:11,840 --> 00:07:17,039
blocks maybe your

00:07:13,759 --> 00:07:18,160
heap size is gradually increasing but if

00:07:17,039 --> 00:07:21,680
you shift to

00:07:18,160 --> 00:07:23,599
any version about 2.15 then you should

00:07:21,680 --> 00:07:24,960
see there are no longer any spikes

00:07:23,599 --> 00:07:27,520
during compaction

00:07:24,960 --> 00:07:28,639
and the memory footprint stays almost

00:07:27,520 --> 00:07:30,720
constant

00:07:28,639 --> 00:07:33,120
because the head memory grows and comes

00:07:30,720 --> 00:07:36,479
back grows and comes back

00:07:33,120 --> 00:07:37,280
so that's a nice thing upgrading to 2.15

00:07:36,479 --> 00:07:41,919
or

00:07:37,280 --> 00:07:45,280
newer versions now another very simple

00:07:41,919 --> 00:07:47,440
optimization but very powerful this

00:07:45,280 --> 00:07:48,479
uh reduces the right headlock replay

00:07:47,440 --> 00:07:51,360
time

00:07:48,479 --> 00:07:52,160
uh before to version 2.18 we used to

00:07:51,360 --> 00:07:54,479
store like

00:07:52,160 --> 00:07:56,000
six hours of right ahead log which is

00:07:54,479 --> 00:07:58,240
totally unnecessary

00:07:56,000 --> 00:08:00,639
and nobody took a look i guess for a

00:07:58,240 --> 00:08:03,599
very long time that we are doing this

00:08:00,639 --> 00:08:05,599
and one day brian just comes up saying

00:08:03,599 --> 00:08:07,520
hey we are storing a lot of all and we

00:08:05,599 --> 00:08:09,840
don't need to keep run so much wall

00:08:07,520 --> 00:08:10,560
and if you see the simple change here we

00:08:09,840 --> 00:08:12,879
have like

00:08:10,560 --> 00:08:14,479
multiplying the amount of wall to delete

00:08:12,879 --> 00:08:16,479
by one by three

00:08:14,479 --> 00:08:18,240
so just make it two by three like double

00:08:16,479 --> 00:08:21,680
the amount of all you delete

00:08:18,240 --> 00:08:23,919
this doesn't cause any data loss

00:08:21,680 --> 00:08:25,120
because the way the truncation of right

00:08:23,919 --> 00:08:29,759
headlock works

00:08:25,120 --> 00:08:32,640
but if your system is a disk i o bound

00:08:29,759 --> 00:08:34,560
then it can make the replays actually

00:08:32,640 --> 00:08:37,039
like the restarts actually 50

00:08:34,560 --> 00:08:38,240
faster so this was a very nice

00:08:37,039 --> 00:08:42,399
optimization

00:08:38,240 --> 00:08:44,480
which came in version 2.18

00:08:42,399 --> 00:08:46,000
so let's move on to another interesting

00:08:44,480 --> 00:08:48,399
optimization which came in

00:08:46,000 --> 00:08:49,519
version 2.19 which is memory mapping of

00:08:48,399 --> 00:08:52,800
chunks

00:08:49,519 --> 00:08:53,600
so the samples in the head block or even

00:08:52,800 --> 00:08:55,839
the disk

00:08:53,600 --> 00:08:57,680
so head block is the in memory block and

00:08:55,839 --> 00:08:58,959
the samples are stored in something

00:08:57,680 --> 00:09:02,240
called chunks

00:08:58,959 --> 00:09:04,640
120 samples comprises of one

00:09:02,240 --> 00:09:05,760
chunk so whenever a chunk is full like

00:09:04,640 --> 00:09:08,880
120 samples

00:09:05,760 --> 00:09:11,440
we cut another chunk so if you have like

00:09:08,880 --> 00:09:14,560
15 second scrape interval

00:09:11,440 --> 00:09:17,440
it can uh turn out to be like six

00:09:14,560 --> 00:09:18,000
chunks per member per series in memory

00:09:17,440 --> 00:09:20,320
because

00:09:18,000 --> 00:09:22,399
head stores up to three hours of data so

00:09:20,320 --> 00:09:25,600
in this optimization what we do is

00:09:22,399 --> 00:09:26,480
once a chunk is full for example 120

00:09:25,600 --> 00:09:28,959
samples

00:09:26,480 --> 00:09:30,000
we just write it to the disk and store

00:09:28,959 --> 00:09:32,880
the reference

00:09:30,000 --> 00:09:34,720
of the chunk on the disk into the memory

00:09:32,880 --> 00:09:36,399
so the entire chunk is replaced by a

00:09:34,720 --> 00:09:38,800
single reference

00:09:36,399 --> 00:09:39,440
and whenever you want to read this chunk

00:09:38,800 --> 00:09:42,080
you use

00:09:39,440 --> 00:09:43,040
the feature given by the os which is

00:09:42,080 --> 00:09:46,080
memory mapping

00:09:43,040 --> 00:09:48,560
which fetches the chunk from the disk on

00:09:46,080 --> 00:09:51,120
demand whenever you need it

00:09:48,560 --> 00:09:52,320
so if you don't query a lot you're on

00:09:51,120 --> 00:09:55,279
your head block

00:09:52,320 --> 00:09:56,000
this means uh earlier we're storing six

00:09:55,279 --> 00:09:58,560
chunks per

00:09:56,000 --> 00:09:59,920
series in the memory now you will

00:09:58,560 --> 00:10:03,760
effectively store

00:09:59,920 --> 00:10:06,880
up to one chunk per series in the memory

00:10:03,760 --> 00:10:07,279
so in real world this turns out to be

00:10:06,880 --> 00:10:10,560
like

00:10:07,279 --> 00:10:11,279
20 to 40 percent reduction in the heap

00:10:10,560 --> 00:10:13,200
size

00:10:11,279 --> 00:10:15,600
and as a side effect of this because you

00:10:13,200 --> 00:10:16,160
are already have stored the chunk on the

00:10:15,600 --> 00:10:17,360
disc

00:10:16,160 --> 00:10:18,959
when you are replaying the right

00:10:17,360 --> 00:10:20,240
headlock you can discard a lot of

00:10:18,959 --> 00:10:21,920
samples because

00:10:20,240 --> 00:10:23,600
the chunk is already memory mapped on

00:10:21,920 --> 00:10:26,399
the disk and this

00:10:23,600 --> 00:10:26,880
this makes the right headlock replay

00:10:26,399 --> 00:10:30,160
like

00:10:26,880 --> 00:10:31,760
20 to 30 percent faster so for every

00:10:30,160 --> 00:10:33,760
release we run something called prom

00:10:31,760 --> 00:10:34,880
bench to benchmark previous release with

00:10:33,760 --> 00:10:38,160
the current release

00:10:34,880 --> 00:10:41,200
and these are some graphs

00:10:38,160 --> 00:10:43,279
showing the memory benefits of this

00:10:41,200 --> 00:10:45,360
memory mapping work

00:10:43,279 --> 00:10:47,040
so this is for a very high churn when i

00:10:45,360 --> 00:10:47,920
say churn it means the rate at which

00:10:47,040 --> 00:10:50,160
memory series

00:10:47,920 --> 00:10:51,440
new memory series are being created or

00:10:50,160 --> 00:10:54,959
they disappear

00:10:51,440 --> 00:10:56,079
so in uh high churn scenario we have

00:10:54,959 --> 00:10:58,720
like 10 to 20

00:10:56,079 --> 00:10:59,440
reduction in memory but if you look at

00:10:58,720 --> 00:11:01,040
low churn

00:10:59,440 --> 00:11:03,040
scenarios which might be most of the

00:11:01,040 --> 00:11:05,519
cases i guess

00:11:03,040 --> 00:11:07,519
there is 30 to a 40 percent reduction in

00:11:05,519 --> 00:11:10,880
heap and if you look closely this

00:11:07,519 --> 00:11:12,880
green line is actually the heap size of

00:11:10,880 --> 00:11:14,480
the memory mapping and not the lines

00:11:12,880 --> 00:11:18,000
above it

00:11:14,480 --> 00:11:18,560
so that's pretty good so this was all

00:11:18,000 --> 00:11:20,399
about

00:11:18,560 --> 00:11:22,160
these were the main highlights of the

00:11:20,399 --> 00:11:25,120
optimizations and the features that

00:11:22,160 --> 00:11:26,480
went into tstb since last prom con it's

00:11:25,120 --> 00:11:28,000
like less than a year but

00:11:26,480 --> 00:11:29,519
there's still lots of optimization that

00:11:28,000 --> 00:11:32,320
went in now

00:11:29,519 --> 00:11:35,360
let's talk about the future what you can

00:11:32,320 --> 00:11:37,680
expect in the coming releases

00:11:35,360 --> 00:11:39,600
so this is another optimization

00:11:37,680 --> 00:11:42,000
following the memory mapping work

00:11:39,600 --> 00:11:44,079
which is snapshotting of the chunks that

00:11:42,000 --> 00:11:47,120
in that are there in the memory

00:11:44,079 --> 00:11:48,959
to speed up the replay process

00:11:47,120 --> 00:11:50,639
so this idea came from the right

00:11:48,959 --> 00:11:52,480
headlock work which was being done in

00:11:50,639 --> 00:11:55,600
the cortex project

00:11:52,480 --> 00:11:58,720
and i did a experiment

00:11:55,600 --> 00:12:02,160
on this like last year in the november

00:11:58,720 --> 00:12:02,800
sometime but because there was no memory

00:12:02,160 --> 00:12:06,079
mapping

00:12:02,800 --> 00:12:08,079
work in existence that would cause

00:12:06,079 --> 00:12:09,600
a lot of right amplification because we

00:12:08,079 --> 00:12:11,680
had to snapshot

00:12:09,600 --> 00:12:14,720
all the data that's in the memory to the

00:12:11,680 --> 00:12:18,000
disk now that we have memory mapping

00:12:14,720 --> 00:12:18,959
uh what we could do is the la i said we

00:12:18,000 --> 00:12:21,200
will we store

00:12:18,959 --> 00:12:23,519
one chunk per memory series in the

00:12:21,200 --> 00:12:25,040
memory so whenever you are shutting down

00:12:23,519 --> 00:12:27,200
the prometheus

00:12:25,040 --> 00:12:29,120
you can take the snapshot of that last

00:12:27,200 --> 00:12:29,920
chunk for every series along with the

00:12:29,120 --> 00:12:33,040
series

00:12:29,920 --> 00:12:35,279
label values and the tombstones and

00:12:33,040 --> 00:12:37,360
flush it to the disk so whenever you are

00:12:35,279 --> 00:12:40,399
restarting prometheus

00:12:37,360 --> 00:12:41,920
it can read the series and the in-memory

00:12:40,399 --> 00:12:45,040
data from

00:12:41,920 --> 00:12:46,399
the snapshot and it can iterate through

00:12:45,040 --> 00:12:48,880
the memory map chunk

00:12:46,399 --> 00:12:50,240
to restore the references and that's all

00:12:48,880 --> 00:12:52,480
you need to do you don't need to go

00:12:50,240 --> 00:12:55,519
through the right head log again

00:12:52,480 --> 00:12:57,519
this makes the restart time up to 80

00:12:55,519 --> 00:13:00,240
percent faster or even more

00:12:57,519 --> 00:13:00,959
from the current test that we have done

00:13:00,240 --> 00:13:03,440
so

00:13:00,959 --> 00:13:04,959
the the pr is already up with the proof

00:13:03,440 --> 00:13:07,600
of concept

00:13:04,959 --> 00:13:08,480
implementation that vr just needs a

00:13:07,600 --> 00:13:10,560
little more work

00:13:08,480 --> 00:13:13,279
and a lot of testing before it can be

00:13:10,560 --> 00:13:16,959
merged and i actually expect it to go

00:13:13,279 --> 00:13:19,760
out in the next release which is 2.20

00:13:16,959 --> 00:13:21,120
as a best case but in the worst case i

00:13:19,760 --> 00:13:25,279
hope it will be in

00:13:21,120 --> 00:13:27,760
by the release 2.21

00:13:25,279 --> 00:13:29,040
so yep you can expect faster restarts in

00:13:27,760 --> 00:13:32,639
the upcoming releases

00:13:29,040 --> 00:13:36,320
that's great and this is

00:13:32,639 --> 00:13:39,440
a very long-standing feature request and

00:13:36,320 --> 00:13:42,399
a long-standing pr

00:13:39,440 --> 00:13:44,560
so there were lots of uh reviews on this

00:13:42,399 --> 00:13:47,600
pr lots of iterations have been done

00:13:44,560 --> 00:13:48,000
to solve some of the critical issues

00:13:47,600 --> 00:13:50,639
that

00:13:48,000 --> 00:13:52,399
can happen with data input like a large

00:13:50,639 --> 00:13:54,959
block size memory explosion

00:13:52,399 --> 00:13:57,440
other problems but the only thing that

00:13:54,959 --> 00:14:00,079
it's blocked on is what format

00:13:57,440 --> 00:14:00,880
to support in this tool like what should

00:14:00,079 --> 00:14:03,839
be the format

00:14:00,880 --> 00:14:04,480
to input data into the prometheus so

00:14:03,839 --> 00:14:06,720
bartek

00:14:04,480 --> 00:14:08,399
ran bartek who is another maintenance

00:14:06,720 --> 00:14:10,800
ran a twitter poll

00:14:08,399 --> 00:14:13,120
where the remote write format actually

00:14:10,800 --> 00:14:16,079
won in terms of full but this pr

00:14:13,120 --> 00:14:16,639
implements open matrix format so this

00:14:16,079 --> 00:14:18,639
there is

00:14:16,639 --> 00:14:20,480
there is still requirement for some

00:14:18,639 --> 00:14:24,079
discussions upon

00:14:20,480 --> 00:14:26,639
which format to use and maybe we'll also

00:14:24,079 --> 00:14:30,079
support multiple formats who knows

00:14:26,639 --> 00:14:31,920
and after that is fixed it the pr just

00:14:30,079 --> 00:14:32,560
needs a little more review and the tool

00:14:31,920 --> 00:14:34,000
should be in

00:14:32,560 --> 00:14:35,839
like the first version of the tool

00:14:34,000 --> 00:14:36,880
obviously some optimizations will follow

00:14:35,839 --> 00:14:39,680
this

00:14:36,880 --> 00:14:40,639
but a tool for bulk import of data into

00:14:39,680 --> 00:14:43,760
prometheus

00:14:40,639 --> 00:14:47,680
is somewhere around the corner

00:14:43,760 --> 00:14:50,560
and about lifting the index size limit

00:14:47,680 --> 00:14:51,199
so this work has been standing for over

00:14:50,560 --> 00:14:53,279
uh

00:14:51,199 --> 00:14:54,639
an year i guess this was done in gsoc

00:14:53,279 --> 00:14:58,079
00:14:54,639 --> 00:15:01,279
so uh post as i said before postings

00:14:58,079 --> 00:15:03,760
are the offset in the index where the

00:15:01,279 --> 00:15:06,480
data of the series is stored

00:15:03,760 --> 00:15:08,959
currently the postings are 32 bits which

00:15:06,480 --> 00:15:11,600
are 16 byte aligned with that i mean

00:15:08,959 --> 00:15:12,079
if you take a posting of 32 bit the

00:15:11,600 --> 00:15:14,480
actual

00:15:12,079 --> 00:15:17,199
offset in the index is that 32-bit

00:15:14,480 --> 00:15:19,279
number multiplied by 16

00:15:17,199 --> 00:15:21,279
and series are always 16 byte aligned on

00:15:19,279 --> 00:15:24,320
the disk 2.

00:15:21,279 --> 00:15:27,680
so with this with 32 bits and 16 band

00:15:24,320 --> 00:15:30,000
alignment the index size limit is capped

00:15:27,680 --> 00:15:33,759
at 64 gigabytes

00:15:30,000 --> 00:15:36,320
and some installations can cross this

00:15:33,759 --> 00:15:37,519
and there is no way you can have a

00:15:36,320 --> 00:15:40,399
larger index size

00:15:37,519 --> 00:15:43,199
than 64gb right now one of the solution

00:15:40,399 --> 00:15:46,079
is to move to 64 bit postings

00:15:43,199 --> 00:15:46,720
but it comes with it the problems in its

00:15:46,079 --> 00:15:49,759
own

00:15:46,720 --> 00:15:51,360
uh one of the problem is space

00:15:49,759 --> 00:15:52,800
on the disk like if you are just

00:15:51,360 --> 00:15:54,240
doubling the number of bits that is

00:15:52,800 --> 00:15:55,199
stored for posting it's going to take a

00:15:54,240 --> 00:15:57,519
lot of

00:15:55,199 --> 00:15:59,120
space on disk and not just that when you

00:15:57,519 --> 00:16:01,839
are storing a lot of data on disk

00:15:59,120 --> 00:16:02,800
fetching it is also slower and one of

00:16:01,839 --> 00:16:06,240
the thing

00:16:02,800 --> 00:16:09,839
prometheus aims is query performance

00:16:06,240 --> 00:16:10,959
so when the move to 64 bit postings come

00:16:09,839 --> 00:16:12,800
you have to take care of both

00:16:10,959 --> 00:16:16,079
performance and the space

00:16:12,800 --> 00:16:18,320
so we tried uh and tested

00:16:16,079 --> 00:16:19,279
lots of different uh compressions that

00:16:18,320 --> 00:16:22,560
we can do on this

00:16:19,279 --> 00:16:26,560
uh 64 bit postings

00:16:22,560 --> 00:16:28,560
and after lots of testing we

00:16:26,560 --> 00:16:31,360
decided to go with prefix compression

00:16:28,560 --> 00:16:34,320
where you store a common prefix

00:16:31,360 --> 00:16:34,800
and a list of suffixes and call it a

00:16:34,320 --> 00:16:38,240
bucket

00:16:34,800 --> 00:16:41,600
in 64-bit postings we chose 48 bits

00:16:38,240 --> 00:16:44,320
of prefix and 16 bits of the suffix

00:16:41,600 --> 00:16:46,480
which gives near equal performance of

00:16:44,320 --> 00:16:48,639
32-bit postings

00:16:46,480 --> 00:16:50,639
and they actually take less space than

00:16:48,639 --> 00:16:53,680
what we're taking right now

00:16:50,639 --> 00:16:54,880
so the pr just needs a little more

00:16:53,680 --> 00:16:56,959
review

00:16:54,880 --> 00:16:58,560
and because there have not been many

00:16:56,959 --> 00:17:01,440
requests for this feature

00:16:58,560 --> 00:17:05,520
it has kind of gotten stale but after

00:17:01,440 --> 00:17:08,480
some review it should be good to go in

00:17:05,520 --> 00:17:10,240
so let's end with what were the features

00:17:08,480 --> 00:17:11,839
that were added for downstream users

00:17:10,240 --> 00:17:12,799
which is not actually accessible in the

00:17:11,839 --> 00:17:16,240
prometheus

00:17:12,799 --> 00:17:17,600
binary itself so there's just one

00:17:16,240 --> 00:17:18,400
feature that i'm going to talk about

00:17:17,600 --> 00:17:21,120
which is

00:17:18,400 --> 00:17:22,000
call backs for series lifecycle so these

00:17:21,120 --> 00:17:25,600
are the set of

00:17:22,000 --> 00:17:27,919
three uh methods that you can pass into

00:17:25,600 --> 00:17:29,039
tsdb when you are creating an instance

00:17:27,919 --> 00:17:30,880
of it

00:17:29,039 --> 00:17:32,720
with which you can create how many

00:17:30,880 --> 00:17:34,960
series that

00:17:32,720 --> 00:17:36,320
you want to be created so there are

00:17:34,960 --> 00:17:39,840
methods like pre-creation

00:17:36,320 --> 00:17:43,280
post creation post deletion

00:17:39,840 --> 00:17:46,799
in pre-creation you can keep

00:17:43,280 --> 00:17:49,039
account of series or any

00:17:46,799 --> 00:17:51,039
metric that you want to keep a track of

00:17:49,039 --> 00:17:52,640
and you can return an error if you don't

00:17:51,039 --> 00:17:53,280
want that particular series to be

00:17:52,640 --> 00:17:56,320
created

00:17:53,280 --> 00:17:59,600
for example cortex already uses it and

00:17:56,320 --> 00:18:00,320
has a per user limit so it keeps track

00:17:59,600 --> 00:18:03,120
of

00:18:00,320 --> 00:18:04,160
a series per user and it has a per

00:18:03,120 --> 00:18:06,320
metric limit

00:18:04,160 --> 00:18:07,200
for example for a given metric name how

00:18:06,320 --> 00:18:10,000
many series

00:18:07,200 --> 00:18:10,799
you want to create so this can be done

00:18:10,000 --> 00:18:12,080
using three

00:18:10,799 --> 00:18:14,160
these three callbacks so if you are a

00:18:12,080 --> 00:18:17,679
consumer of tstp package

00:18:14,160 --> 00:18:21,600
you can write your own custom methods

00:18:17,679 --> 00:18:25,440
to take care of the series creation

00:18:21,600 --> 00:18:28,400
yep and that's about it that i had

00:18:25,440 --> 00:18:29,280
and thank you for joining my talk i

00:18:28,400 --> 00:18:33,440
think you can

00:18:29,280 --> 00:18:35,840
now ask questions uh in the live chat

00:18:33,440 --> 00:18:37,039
or if there's something else please

00:18:35,840 --> 00:18:39,280
reach out to me

00:18:37,039 --> 00:18:40,559
in case you have any questions to ask

00:18:39,280 --> 00:18:43,600
later

00:18:40,559 --> 00:18:45,600
this is my twitter handle

00:18:43,600 --> 00:18:46,960
underscore codesum so you can just dm me

00:18:45,600 --> 00:18:49,360
anytime

00:18:46,960 --> 00:18:49,360
thank you

00:18:50,480 --> 00:18:56,480
so the first question is with a map will

00:18:53,679 --> 00:18:58,000
it lead to spikes in read latency

00:18:56,480 --> 00:19:00,960
because the os might have decided to

00:18:58,000 --> 00:19:04,080
flush the page to the disk etc

00:19:00,960 --> 00:19:06,080
so in practice even when we are running

00:19:04,080 --> 00:19:07,200
benchmarks on the problems we haven't

00:19:06,080 --> 00:19:10,480
seen any

00:19:07,200 --> 00:19:12,240
latencies as such running the queries is

00:19:10,480 --> 00:19:14,000
similar to how you run queries on the

00:19:12,240 --> 00:19:17,360
persistent block where the chunks are

00:19:14,000 --> 00:19:19,280
mapped but this is all assuming fast

00:19:17,360 --> 00:19:22,840
ssds and

00:19:19,280 --> 00:19:24,880
in practice you won't see any latencies

00:19:22,840 --> 00:19:26,640
really

00:19:24,880 --> 00:19:28,400
and the next question is are there any

00:19:26,640 --> 00:19:32,160
plans to improve histograms

00:19:28,400 --> 00:19:35,760
in dsdb i think yes we have uh

00:19:32,160 --> 00:19:38,080
beyond who is actively working on uh

00:19:35,760 --> 00:19:39,360
improving histograms on these so yes

00:19:38,080 --> 00:19:42,640
there are plans

00:19:39,360 --> 00:19:42,640
and the work is ongoing

00:19:42,799 --> 00:19:47,600
have you considered alternative junk

00:19:45,200 --> 00:19:50,240
encoding in dhtp yes that's also in one

00:19:47,600 --> 00:19:52,480
of the roadmaps or the plans

00:19:50,240 --> 00:19:53,600
to consider alternative chunk encoding

00:19:52,480 --> 00:19:56,400
based on that

00:19:53,600 --> 00:19:57,039
data that you see so the answer to that

00:19:56,400 --> 00:20:00,400
question is

00:19:57,039 --> 00:20:02,960
yes we have uh we are considering and

00:20:00,400 --> 00:20:05,679
that might be something that might uh

00:20:02,960 --> 00:20:21,840
qualify from methods to release 3.0

00:20:05,679 --> 00:20:21,840
who knows

00:20:23,039 --> 00:20:26,720
uh there was an interesting discussion

00:20:24,400 --> 00:20:31,600
going on on on the slack channel

00:20:26,720 --> 00:20:31,600
about mmap and syscalls

00:20:32,240 --> 00:20:35,840
do you want to talk about that at all

00:20:37,200 --> 00:20:40,960
it's similar to the first question that

00:20:39,679 --> 00:20:42,400
was asked whether

00:20:40,960 --> 00:20:44,400
there will be spike in the read

00:20:42,400 --> 00:20:47,760
latencies because of

00:20:44,400 --> 00:20:49,840
disk uh right and read collating

00:20:47,760 --> 00:20:51,760
so the answer is same in practice we

00:20:49,840 --> 00:20:54,400
haven't seen any issues like that

00:20:51,760 --> 00:20:55,679
and for assuming we are running this on

00:20:54,400 --> 00:20:57,679
a fast ssd

00:20:55,679 --> 00:20:58,960
there won't be any issues or performance

00:20:57,679 --> 00:21:03,360
impact

00:20:58,960 --> 00:21:05,520
with mapping

00:21:03,360 --> 00:21:06,880
yes looks like we have one more question

00:21:05,520 --> 00:21:11,200
in the q a

00:21:06,880 --> 00:21:14,400
um a block is cut

00:21:11,200 --> 00:21:17,039
on disk every two hours so we

00:21:14,400 --> 00:21:17,760
don't truncate right hand long exactly

00:21:17,039 --> 00:21:21,679
with the

00:21:17,760 --> 00:21:24,320
time in mind uh

00:21:21,679 --> 00:21:25,679
and you know previously it was six hours

00:21:24,320 --> 00:21:28,080
of right ahead log

00:21:25,679 --> 00:21:28,880
uh the basic logic what we follow is if

00:21:28,080 --> 00:21:31,600
there are x

00:21:28,880 --> 00:21:31,919
segments of right hand log on the disk

00:21:31,600 --> 00:21:33,840
we

00:21:31,919 --> 00:21:35,440
truncate two thirds of those segments

00:21:33,840 --> 00:21:38,000
and we don't actually take

00:21:35,440 --> 00:21:38,960
care of time and also three hours

00:21:38,000 --> 00:21:41,120
because

00:21:38,960 --> 00:21:43,440
uh the data that's there in the head

00:21:41,120 --> 00:21:46,400
block is up to three hours and not

00:21:43,440 --> 00:21:47,360
two hours because whenever the data in

00:21:46,400 --> 00:21:50,640
the head touches

00:21:47,360 --> 00:21:52,480
three hours we cut the old two hours of

00:21:50,640 --> 00:21:54,159
data into a block and there is still one

00:21:52,480 --> 00:21:56,640
hour of data in the head

00:21:54,159 --> 00:21:57,520
so looking in that perspective because

00:21:56,640 --> 00:21:59,760
it helps stores

00:21:57,520 --> 00:22:01,039
data for up to three hours the writer's

00:21:59,760 --> 00:22:13,840
log needs to be

00:22:01,039 --> 00:22:13,840
there for three hours

00:22:19,360 --> 00:22:24,480
uh any plants are following values two

00:22:22,080 --> 00:22:27,039
bytes of arbitrary lengths

00:22:24,480 --> 00:22:28,000
it would open up so many interesting

00:22:27,039 --> 00:22:30,960
challenges

00:22:28,000 --> 00:22:32,880
for performance i don't think that's

00:22:30,960 --> 00:22:34,400
actually in the road map and

00:22:32,880 --> 00:22:36,400
not something we are considering right

00:22:34,400 --> 00:22:41,840
now it would be a fixed

00:22:36,400 --> 00:22:41,840
value length

00:22:47,280 --> 00:22:51,200
so the next question is about is there

00:22:48,880 --> 00:22:55,120
any regression between 2.15

00:22:51,200 --> 00:22:57,679
to 2.16 release where

00:22:55,120 --> 00:22:58,640
the users saw 40 increases in bytes per

00:22:57,679 --> 00:23:00,880
sample

00:22:58,640 --> 00:23:02,320
so i can't really answer this question

00:23:00,880 --> 00:23:03,039
right now i would have to look at the

00:23:02,320 --> 00:23:05,120
release

00:23:03,039 --> 00:23:06,400
so maybe this discussion can be taken

00:23:05,120 --> 00:23:09,760
offline i think there is

00:23:06,400 --> 00:23:13,520
an issue open already talking about this

00:23:09,760 --> 00:23:16,960
but there is no regression

00:23:13,520 --> 00:23:23,840
known regression but yep this discussion

00:23:16,960 --> 00:23:23,840
has to be taken offline i guess

00:23:26,720 --> 00:23:33,440
other plans to write ts tv rewrite jc

00:23:30,159 --> 00:23:36,799
from scratch i don't think so

00:23:33,440 --> 00:23:39,360
like it took a very long time to get the

00:23:36,799 --> 00:23:41,760
tsp in prometheus 2.0 right with lots of

00:23:39,360 --> 00:23:43,600
races and lots of issues initially

00:23:41,760 --> 00:23:44,880
so i don't think writing it from the

00:23:43,600 --> 00:23:51,840
scratch

00:23:44,880 --> 00:23:51,840
is in the plans

00:24:02,840 --> 00:24:05,840
yes

00:24:12,480 --> 00:24:18,880
all right um actually i i had one um

00:24:16,880 --> 00:24:20,720
have we put any thought uh have you put

00:24:18,880 --> 00:24:21,760
any have you put any thought into

00:24:20,720 --> 00:24:24,880
supporting

00:24:21,760 --> 00:24:27,360
uh unt64 as an additional data type

00:24:24,880 --> 00:24:30,000
uh in addition to the float 64 that we

00:24:27,360 --> 00:24:34,240
currently support

00:24:30,000 --> 00:24:37,279
uh i haven't thought in that direction

00:24:34,240 --> 00:24:39,200
at all changing the data type of the

00:24:37,279 --> 00:24:41,279
value that we store

00:24:39,200 --> 00:24:42,960
but it would be interesting to see

00:24:41,279 --> 00:24:45,039
possibilities with that in terms of

00:24:42,960 --> 00:24:47,440
compression or any other thing

00:24:45,039 --> 00:24:48,559
but before that may we want to consider

00:24:47,440 --> 00:24:51,200
different types of

00:24:48,559 --> 00:24:54,640
chunk and coding and maybe later we can

00:24:51,200 --> 00:24:58,000
jump to different type of data types

00:24:54,640 --> 00:24:59,600
we have a question in the group chat

00:24:58,000 --> 00:25:01,760
yeah i mentioned in one of the slack

00:24:59,600 --> 00:25:03,840
discussion in cncf that

00:25:01,760 --> 00:25:06,000
ts3 is not element to use with the right

00:25:03,840 --> 00:25:09,600
ahead log

00:25:06,000 --> 00:25:12,640
and the question is why uh why

00:25:09,600 --> 00:25:15,919
i think uh as far as i have understood

00:25:12,640 --> 00:25:17,919
working with the tsdb uh we assume

00:25:15,919 --> 00:25:20,080
prometheus will

00:25:17,919 --> 00:25:22,080
use the right head lock and the

00:25:20,080 --> 00:25:24,960
implementation inside the tsb

00:25:22,080 --> 00:25:26,159
code is based on the assumption that

00:25:24,960 --> 00:25:28,080
there will be right headlock when you

00:25:26,159 --> 00:25:31,200
are running this in a scene setup

00:25:28,080 --> 00:25:33,679
or in prometheus and the case of

00:25:31,200 --> 00:25:34,880
no right headlock is only for writing

00:25:33,679 --> 00:25:37,279
unit tests

00:25:34,880 --> 00:25:40,080
so that's the reason the dsdb should be

00:25:37,279 --> 00:25:43,200
used with the right headlock because

00:25:40,080 --> 00:25:44,720
not much has been uh checked or tested

00:25:43,200 --> 00:25:46,080
when there is no right headlock

00:25:44,720 --> 00:25:48,320
but i think you can still run with right

00:25:46,080 --> 00:25:51,039
dialogue but not in a prometheus binary

00:25:48,320 --> 00:25:51,039
it's not allowed

00:25:54,640 --> 00:25:58,080
we have another question in the q a

00:25:59,840 --> 00:26:03,440
any plans to introduce more caching in

00:26:02,320 --> 00:26:07,440
program core

00:26:03,440 --> 00:26:10,799
similar to cortex query front end

00:26:07,440 --> 00:26:13,840
i don't think any discussion around

00:26:10,799 --> 00:26:16,880
the caching has been done in

00:26:13,840 --> 00:26:20,400
for prometheus but i think

00:26:16,880 --> 00:26:22,960
if it's feasible we might

00:26:20,400 --> 00:26:23,840
introduce caching or i think cortex

00:26:22,960 --> 00:26:26,000
query front end

00:26:23,840 --> 00:26:28,559
itself is very easy to use you can just

00:26:26,000 --> 00:26:32,159
place it in front of the prometheus

00:26:28,559 --> 00:26:34,559
and have the caching layer so

00:26:32,159 --> 00:26:36,720
i guess the answer is no because you

00:26:34,559 --> 00:26:38,400
have a solution already but if it

00:26:36,720 --> 00:26:40,400
looks very interesting it might be

00:26:38,400 --> 00:26:44,880
merged into computers but

00:26:40,400 --> 00:26:44,880
in near-term future i guess that's a no

00:26:46,559 --> 00:26:49,600
yeah there's there's several different

00:26:48,000 --> 00:26:52,960
caching front ends including

00:26:49,600 --> 00:26:55,760
you know the cortex query and then

00:26:52,960 --> 00:26:58,000
trickster is also a good caching front

00:26:55,760 --> 00:26:58,000
end

00:26:58,400 --> 00:27:07,840
all right thanks a lot ganesha

00:27:10,960 --> 00:27:13,039

YouTube URL: https://www.youtube.com/watch?v=K7R2xw4s-5o


