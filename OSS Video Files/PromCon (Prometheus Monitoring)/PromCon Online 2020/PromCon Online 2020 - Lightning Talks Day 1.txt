Title: PromCon Online 2020 - Lightning Talks Day 1
Publication date: 2020-07-23
Playlist: PromCon Online 2020
Description: 
	
Captions: 
	00:00:01,770 --> 00:00:04,940
[Music]

00:00:14,480 --> 00:00:18,880
hello everyone

00:00:16,400 --> 00:00:22,880
today i would like to introduce metrocad

00:00:18,880 --> 00:00:25,680
it's a viewer for prometheus exporters

00:00:22,880 --> 00:00:26,560
first let me briefly introduce myself my

00:00:25,680 --> 00:00:28,160
name is lava

00:00:26,560 --> 00:00:30,560
and today i'm talking with you from

00:00:28,160 --> 00:00:32,640
tokyo japan

00:00:30,560 --> 00:00:34,160
i work in line corporation we are making

00:00:32,640 --> 00:00:37,200
the most popular

00:00:34,160 --> 00:00:39,280
messaging application in japan

00:00:37,200 --> 00:00:42,079
and i do use primitives and grafana on

00:00:39,280 --> 00:00:42,079
daily basis

00:00:42,399 --> 00:00:46,000
let me start from introduction of some

00:00:44,800 --> 00:00:48,000
problem i faced

00:00:46,000 --> 00:00:50,399
i was developing batch processing

00:00:48,000 --> 00:00:52,239
application

00:00:50,399 --> 00:00:54,239
it was typical batch processing that

00:00:52,239 --> 00:00:56,640
runs for several minutes

00:00:54,239 --> 00:00:58,000
and it had some problem on it to solve

00:00:56,640 --> 00:00:59,840
this problem i wanted to

00:00:58,000 --> 00:01:01,120
look at the internal state of this batch

00:00:59,840 --> 00:01:04,720
processing

00:01:01,120 --> 00:01:05,920
and how it changes in time luckily for

00:01:04,720 --> 00:01:08,720
me i already had

00:01:05,920 --> 00:01:12,159
this matrix that i needed for debugging

00:01:08,720 --> 00:01:13,920
exposed as a prometheus exporter

00:01:12,159 --> 00:01:16,240
but how can i look at them in local

00:01:13,920 --> 00:01:19,520
environment

00:01:16,240 --> 00:01:20,640
i came up with three approaches first i

00:01:19,520 --> 00:01:23,439
could deploy this

00:01:20,640 --> 00:01:24,880
batch application somewhere and use

00:01:23,439 --> 00:01:28,560
external permit use

00:01:24,880 --> 00:01:30,799
to scrape metrics for me

00:01:28,560 --> 00:01:32,560
this is a little bit troublesome since i

00:01:30,799 --> 00:01:33,840
would have to deploy it every time i

00:01:32,560 --> 00:01:37,439
make a little change

00:01:33,840 --> 00:01:40,320
to the application second approach is to

00:01:37,439 --> 00:01:42,240
run my primitives locally for example in

00:01:40,320 --> 00:01:43,840
docker and configure it

00:01:42,240 --> 00:01:46,320
to scrape metrics from my local

00:01:43,840 --> 00:01:50,479
application that would require

00:01:46,320 --> 00:01:53,200
of setting up primitives locally and

00:01:50,479 --> 00:01:54,320
making sure it works correctly which

00:01:53,200 --> 00:01:58,240
might not be

00:01:54,320 --> 00:01:58,719
a trivial task the third approach is to

00:01:58,240 --> 00:02:02,079
scrape

00:01:58,719 --> 00:02:04,079
and visualize metrics manually

00:02:02,079 --> 00:02:05,920
that's actually pretty easy to do since

00:02:04,079 --> 00:02:09,520
primitives is pull based system

00:02:05,920 --> 00:02:11,760
and metricat can help us here

00:02:09,520 --> 00:02:13,599
so please meet metricad it's an

00:02:11,760 --> 00:02:18,080
application that scripts and

00:02:13,599 --> 00:02:21,520
visualizes metrics it can work with both

00:02:18,080 --> 00:02:23,760
local and remote exporters it can

00:02:21,520 --> 00:02:26,720
scrape metrics at any interval and it

00:02:23,760 --> 00:02:29,599
has pretty simple ui

00:02:26,720 --> 00:02:29,599
let's look at ui

00:02:30,160 --> 00:02:34,239
this is connection window and you can

00:02:32,959 --> 00:02:37,200
enter

00:02:34,239 --> 00:02:38,319
the exporter url here can be remote or

00:02:37,200 --> 00:02:41,840
local one

00:02:38,319 --> 00:02:44,959
you can enter request interval

00:02:41,840 --> 00:02:47,920
at which the scraping will happen then

00:02:44,959 --> 00:02:50,160
you click start recording

00:02:47,920 --> 00:02:52,480
after you click start recording you will

00:02:50,160 --> 00:02:54,720
see this screen

00:02:52,480 --> 00:02:57,680
this is main recording screen here you

00:02:54,720 --> 00:03:00,400
have the list of metrics on the left

00:02:57,680 --> 00:03:03,200
some stats on the bottom the start stop

00:03:00,400 --> 00:03:05,200
button on the bottom right

00:03:03,200 --> 00:03:06,959
and in the main area you have some

00:03:05,200 --> 00:03:08,159
metric information such as name

00:03:06,959 --> 00:03:13,280
description

00:03:08,159 --> 00:03:18,879
type and data for the selected period

00:03:13,280 --> 00:03:20,640
it it will update in real time

00:03:18,879 --> 00:03:23,760
metrocad can be useful for local

00:03:20,640 --> 00:03:23,760
development and debugging

00:03:24,080 --> 00:03:29,519
for example if you develop exporter

00:03:27,680 --> 00:03:31,360
or if you develop applications that

00:03:29,519 --> 00:03:34,319
supports instrumentation

00:03:31,360 --> 00:03:36,080
or if you develop some or debug some

00:03:34,319 --> 00:03:38,959
functionality

00:03:36,080 --> 00:03:41,519
that exposes metrics through existing

00:03:38,959 --> 00:03:41,519
exporter

00:03:42,640 --> 00:03:49,840
also you may have

00:03:47,280 --> 00:03:52,720
make take advantage of uh high

00:03:49,840 --> 00:03:55,439
resolution metrics

00:03:52,720 --> 00:03:56,000
the normal primitives is that you have

00:03:55,439 --> 00:03:59,840
running

00:03:56,000 --> 00:04:00,480
in your beta or production environment

00:03:59,840 --> 00:04:04,000
might have

00:04:00,480 --> 00:04:07,120
pretty high interval let's say

00:04:04,000 --> 00:04:10,560
10 seconds or 15 seconds but

00:04:07,120 --> 00:04:11,280
for metric at you can set few seconds

00:04:10,560 --> 00:04:14,080
interval

00:04:11,280 --> 00:04:14,799
and have much higher resolution of

00:04:14,080 --> 00:04:16,959
metrics

00:04:14,799 --> 00:04:19,120
and have much better information to look

00:04:16,959 --> 00:04:19,120
at

00:04:19,199 --> 00:04:25,520
and finally you can export your metrics

00:04:23,360 --> 00:04:28,800
to file and share it with your

00:04:25,520 --> 00:04:30,720
colleagues analyze these external tools

00:04:28,800 --> 00:04:32,960
you can visualize it as little as you

00:04:30,720 --> 00:04:32,960
like

00:04:34,320 --> 00:04:38,800
and finally i would like you to go and

00:04:37,360 --> 00:04:40,840
try metricat

00:04:38,800 --> 00:04:42,800
so you can download it from

00:04:40,840 --> 00:04:45,120
metriccat.dev

00:04:42,800 --> 00:04:46,639
it's only available for mac os for at

00:04:45,120 --> 00:04:49,600
the moment

00:04:46,639 --> 00:04:51,199
the windows version is coming soon also

00:04:49,600 --> 00:04:52,880
i update this application quite

00:04:51,199 --> 00:04:54,880
frequently so please

00:04:52,880 --> 00:04:56,880
if you're interested subscribe for

00:04:54,880 --> 00:05:00,160
updates you can do that

00:04:56,880 --> 00:05:02,720
in the on the download page by

00:05:00,160 --> 00:05:03,919
clicking subscribe button and entering

00:05:02,720 --> 00:05:07,120
your email

00:05:03,919 --> 00:05:11,680
or you can use twitter

00:05:07,120 --> 00:05:14,479
it's metric.dev please follow that

00:05:11,680 --> 00:05:17,680
so thank you for your attention and have

00:05:14,479 --> 00:05:17,680
a nice conference day

00:05:18,080 --> 00:05:24,240
so our next liking talk is from jurgen

00:05:21,199 --> 00:05:24,960
estelzoffer about fortifying progressive

00:05:24,240 --> 00:05:28,479
delivery

00:05:24,960 --> 00:05:30,080
with quality gates hi everyone

00:05:28,479 --> 00:05:31,840
my name is jurgen and today i want to

00:05:30,080 --> 00:05:33,680
talk a little bit about fortifying

00:05:31,840 --> 00:05:35,280
progressive delivery with automatic

00:05:33,680 --> 00:05:38,560
quality gates based on

00:05:35,280 --> 00:05:40,639
data from prometheus

00:05:38,560 --> 00:05:42,000
so what is the actual problem we want to

00:05:40,639 --> 00:05:45,199
solve uh

00:05:42,000 --> 00:05:47,280
and it all comes to your cict systems

00:05:45,199 --> 00:05:48,800
where you have manual approval steps

00:05:47,280 --> 00:05:50,720
where you actually want to approve

00:05:48,800 --> 00:05:52,000
some builds before they reach your

00:05:50,720 --> 00:05:54,320
production environment

00:05:52,000 --> 00:05:55,919
and what is the problem here that's

00:05:54,320 --> 00:05:58,400
basically that this manual approval

00:05:55,919 --> 00:06:00,319
takes quite a lot of effort and time

00:05:58,400 --> 00:06:02,639
so first you have to indicate or you

00:06:00,319 --> 00:06:04,160
have to decide if this is if

00:06:02,639 --> 00:06:05,919
something's going wrong if it's really a

00:06:04,160 --> 00:06:07,280
real problem it's the trend that you are

00:06:05,919 --> 00:06:10,080
seeing is enough

00:06:07,280 --> 00:06:11,680
data uh you have to identify which field

00:06:10,080 --> 00:06:12,639
is better was the previous run better or

00:06:11,680 --> 00:06:14,720
was the actual

00:06:12,639 --> 00:06:16,639
the current run better you have to

00:06:14,720 --> 00:06:19,039
manually compare this based

00:06:16,639 --> 00:06:20,160
on some dashboards based on some

00:06:19,039 --> 00:06:23,520
spreadsheets

00:06:20,160 --> 00:06:26,639
so it's a lot of manual comparison and

00:06:23,520 --> 00:06:29,360
the base problem might be that actually

00:06:26,639 --> 00:06:31,360
it's difficult to gather all this data

00:06:29,360 --> 00:06:31,759
from the monitoring solutions underneath

00:06:31,360 --> 00:06:34,400
that

00:06:31,759 --> 00:06:36,319
have all this data and you have to make

00:06:34,400 --> 00:06:39,360
sure that you are providing

00:06:36,319 --> 00:06:42,560
your decisions based on the right

00:06:39,360 --> 00:06:45,520
data so all this manual approval process

00:06:42,560 --> 00:06:46,479
can take quite a long time so how can we

00:06:45,520 --> 00:06:48,400
solve this

00:06:46,479 --> 00:06:49,599
and i want to give you a little bit of

00:06:48,400 --> 00:06:51,680
an idea how we

00:06:49,599 --> 00:06:53,039
resolve this within an open source

00:06:51,680 --> 00:06:55,440
project called captain

00:06:53,039 --> 00:06:57,280
so captain can integrate in your ci see

00:06:55,440 --> 00:06:57,919
this system uh in this case we've just

00:06:57,280 --> 00:07:00,319
sketched it

00:06:57,919 --> 00:07:01,440
with jenkins but you can use captain to

00:07:00,319 --> 00:07:04,639
actually trigger

00:07:01,440 --> 00:07:06,800
the tests in your test environments uh

00:07:04,639 --> 00:07:08,000
with chain leader with neoload with any

00:07:06,800 --> 00:07:10,319
other testing tool

00:07:08,000 --> 00:07:11,840
captain is an event-based framework an

00:07:10,319 --> 00:07:13,440
event-based control plane

00:07:11,840 --> 00:07:15,360
so to say they can just trigger

00:07:13,440 --> 00:07:17,280
different tools to trigger

00:07:15,360 --> 00:07:18,880
the tests and after the tests are

00:07:17,280 --> 00:07:20,560
finished captain again will be triggered

00:07:18,880 --> 00:07:22,960
to evaluate these tests

00:07:20,560 --> 00:07:23,680
so captain will do a quarterly gate

00:07:22,960 --> 00:07:26,560
evaluation

00:07:23,680 --> 00:07:28,400
based on the data so you fetch the data

00:07:26,560 --> 00:07:30,720
for the testing time frame

00:07:28,400 --> 00:07:33,360
and you will evaluate the data based on

00:07:30,720 --> 00:07:35,520
service level objectives

00:07:33,360 --> 00:07:37,440
and after the evaluation it will

00:07:35,520 --> 00:07:39,360
generate a total score

00:07:37,440 --> 00:07:40,639
and based on the score you can then

00:07:39,360 --> 00:07:45,360
decide if you want to

00:07:40,639 --> 00:07:47,440
push it to production or if you want to

00:07:45,360 --> 00:07:48,639
just leave it there or either roll it

00:07:47,440 --> 00:07:50,639
back or just

00:07:48,639 --> 00:07:51,919
give an indication to your developers

00:07:50,639 --> 00:07:54,479
that this build was not

00:07:51,919 --> 00:07:55,039
successfully evaluated so instead of

00:07:54,479 --> 00:07:57,520
having

00:07:55,039 --> 00:07:58,639
this manual approval process for 30 or

00:07:57,520 --> 00:08:02,080
60 minutes

00:07:58,639 --> 00:08:04,560
you can really compress this process to

00:08:02,080 --> 00:08:06,319
to one minute that's basically that's

00:08:04,560 --> 00:08:07,919
basically based on two different files

00:08:06,319 --> 00:08:09,919
that are used inside captain

00:08:07,919 --> 00:08:12,000
and this is a service level indicator

00:08:09,919 --> 00:08:13,520
and a service level objective file so

00:08:12,000 --> 00:08:15,919
how does it work

00:08:13,520 --> 00:08:17,199
uh we do have service level indicators

00:08:15,919 --> 00:08:20,479
that's basically

00:08:17,199 --> 00:08:22,879
let's say a library of different metrics

00:08:20,479 --> 00:08:24,639
that are given a name so like it's a key

00:08:22,879 --> 00:08:27,440
value pair like an error rate

00:08:24,639 --> 00:08:30,080
and the corresponding prompt ql how to

00:08:27,440 --> 00:08:32,000
get this error rate for the service

00:08:30,080 --> 00:08:33,760
this can also be parameterized so you

00:08:32,000 --> 00:08:35,519
can reuse it really easily and captain

00:08:33,760 --> 00:08:37,599
comes with a built-in library

00:08:35,519 --> 00:08:39,120
based on the red metrics already so can

00:08:37,599 --> 00:08:40,800
you reuse this

00:08:39,120 --> 00:08:42,640
but you can also come up with your own

00:08:40,800 --> 00:08:44,159
indicators like for example the number

00:08:42,640 --> 00:08:46,080
of database calls

00:08:44,159 --> 00:08:48,560
and then you can use those indicators

00:08:46,080 --> 00:08:50,480
inside of a service level objective file

00:08:48,560 --> 00:08:51,680
and you will just define that for

00:08:50,480 --> 00:08:54,240
example the number

00:08:51,680 --> 00:08:55,040
of database calls is only allowed to

00:08:54,240 --> 00:08:57,680
increase

00:08:55,040 --> 00:08:59,200
by two percent to the previous runs so

00:08:57,680 --> 00:09:00,959
that basically means

00:08:59,200 --> 00:09:04,640
um for a given time frame you're not

00:09:00,959 --> 00:09:06,560
allowed to execute more database calls

00:09:04,640 --> 00:09:08,480
than in the previous runs only two

00:09:06,560 --> 00:09:11,600
percent increase is allowed

00:09:08,480 --> 00:09:13,519
that basically gives you um very uh

00:09:11,600 --> 00:09:14,640
gives you confidence in not having some

00:09:13,519 --> 00:09:17,120
regression

00:09:14,640 --> 00:09:18,720
in your different runs but you can also

00:09:17,120 --> 00:09:20,959
um

00:09:18,720 --> 00:09:22,480
compare it to absolute thresholds like

00:09:20,959 --> 00:09:23,600
it's done here for the error rate that

00:09:22,480 --> 00:09:26,720
has to stay

00:09:23,600 --> 00:09:29,839
lower than one percent so with this

00:09:26,720 --> 00:09:32,320
two files you can now trigger a captain

00:09:29,839 --> 00:09:33,760
quality gate and you can do this either

00:09:32,320 --> 00:09:36,880
with the captain api

00:09:33,760 --> 00:09:38,880
or with the captain cli you will start

00:09:36,880 --> 00:09:41,519
an evaluation by triggering a captain

00:09:38,880 --> 00:09:44,080
quality gate captain will reach out

00:09:41,519 --> 00:09:44,640
to its sli providers that can connect to

00:09:44,080 --> 00:09:46,720
different

00:09:44,640 --> 00:09:48,320
data stores underneath like prometheus

00:09:46,720 --> 00:09:51,040
or dyna trades or new load

00:09:48,320 --> 00:09:51,600
whatever tool they can provide you data

00:09:51,040 --> 00:09:54,800
about

00:09:51,600 --> 00:09:58,160
your uh test runs or the evaluation

00:09:54,800 --> 00:09:59,839
time frame so captain will evaluate this

00:09:58,160 --> 00:10:00,080
data for fetch this data will evaluate

00:09:59,839 --> 00:10:02,079
the

00:10:00,080 --> 00:10:03,519
data we'll score it and we'll come up

00:10:02,079 --> 00:10:05,839
with a total score

00:10:03,519 --> 00:10:07,760
and this might be like a very successful

00:10:05,839 --> 00:10:10,240
build very successful test one

00:10:07,760 --> 00:10:12,079
or it might be not so successful and you

00:10:10,240 --> 00:10:14,959
will get the final result

00:10:12,079 --> 00:10:15,279
pass warning or fail and upon this you

00:10:14,959 --> 00:10:18,240
can

00:10:15,279 --> 00:10:19,360
act in your ci cd systems so you can see

00:10:18,240 --> 00:10:21,839
it's very easy

00:10:19,360 --> 00:10:22,720
but yet very very powerful mechanism to

00:10:21,839 --> 00:10:26,560
include this

00:10:22,720 --> 00:10:30,640
into your ci cd systems and to fortify

00:10:26,560 --> 00:10:32,480
your progressive delivery pipelines

00:10:30,640 --> 00:10:34,079
since it's only a matter of a couple of

00:10:32,480 --> 00:10:34,560
minutes for its evaluation you can do

00:10:34,079 --> 00:10:36,560
this

00:10:34,560 --> 00:10:37,760
for canary releases for blue-green

00:10:36,560 --> 00:10:40,640
deployments

00:10:37,760 --> 00:10:42,160
because there is no actual effort and

00:10:40,640 --> 00:10:45,519
doesn't take

00:10:42,160 --> 00:10:47,519
much time to evaluate the quality case

00:10:45,519 --> 00:10:49,200
so if you think this might be something

00:10:47,519 --> 00:10:49,720
for you please check out our website

00:10:49,200 --> 00:10:52,800
it's

00:10:49,720 --> 00:10:54,560
captain.sh as said it's an open source

00:10:52,800 --> 00:10:56,000
project we can work with different data

00:10:54,560 --> 00:10:58,839
stores um

00:10:56,000 --> 00:11:00,560
we have our source code on github it's

00:10:58,839 --> 00:11:02,560
github.com

00:11:00,560 --> 00:11:04,320
that's our main repository so that's a

00:11:02,560 --> 00:11:06,079
good starting point to take a look at

00:11:04,320 --> 00:11:07,680
if you have any questions please feel

00:11:06,079 --> 00:11:09,920
free to reach out to me i

00:11:07,680 --> 00:11:10,800
provided here some contact information

00:11:09,920 --> 00:11:15,839
and have fun

00:11:10,800 --> 00:11:15,839
at this year's broadcom bye

00:11:16,800 --> 00:11:23,279
okay well thank you

00:11:20,160 --> 00:11:28,800
so next up we have a talk from

00:11:23,279 --> 00:11:30,480
hello everyone my name is

00:11:28,800 --> 00:11:31,839
who is going to be talking about lessons

00:11:30,480 --> 00:11:34,399
from clothing the

00:11:31,839 --> 00:11:36,399
closing the cloud native infrastructure

00:11:34,399 --> 00:11:38,079
loops

00:11:36,399 --> 00:11:41,120
these all start playing as soon as i hit

00:11:38,079 --> 00:11:44,560
them okay

00:11:41,120 --> 00:11:47,120
hello everyone my name is a network

00:11:44,560 --> 00:11:49,600
software engineer working for intel

00:11:47,120 --> 00:11:51,440
my topic today is lessons from closing

00:11:49,600 --> 00:11:53,760
the cloud native nfa infrastructure

00:11:51,440 --> 00:11:53,760
loops

00:11:54,320 --> 00:11:58,880
here's the legal disclaimer

00:11:57,440 --> 00:12:02,240
and thanks to the team that helped in

00:11:58,880 --> 00:12:02,240
making this work happen

00:12:03,120 --> 00:12:07,040
transformation towards network slicing

00:12:05,040 --> 00:12:09,040
across 5g networks

00:12:07,040 --> 00:12:10,880
and zero touch service management

00:12:09,040 --> 00:12:12,000
require the infrastructure to be heavily

00:12:10,880 --> 00:12:13,920
automated

00:12:12,000 --> 00:12:15,760
which is where closed loop automation

00:12:13,920 --> 00:12:18,880
plays a crucial role

00:12:15,760 --> 00:12:21,279
as indicated in figure 1 key operations

00:12:18,880 --> 00:12:24,399
in a closed loop automation based system

00:12:21,279 --> 00:12:27,120
involved in identification measurement

00:12:24,399 --> 00:12:29,519
detection and corrective action across

00:12:27,120 --> 00:12:31,279
various layers of nfe infrastructure

00:12:29,519 --> 00:12:33,200
and virtual infrastructure management

00:12:31,279 --> 00:12:35,360
aka win

00:12:33,200 --> 00:12:37,360
cloud native deployments need to be

00:12:35,360 --> 00:12:39,440
aware of runtime telemetry

00:12:37,360 --> 00:12:41,519
from the underlying infrastructure to

00:12:39,440 --> 00:12:44,480
perform low latency scheduling

00:12:41,519 --> 00:12:45,360
and resource utilization decisions

00:12:44,480 --> 00:12:47,120
extensions

00:12:45,360 --> 00:12:49,839
such as kubernetes telemetry or

00:12:47,120 --> 00:12:52,399
scheduler horizontal port autoscaler

00:12:49,839 --> 00:12:53,519
or custom kubernetes operators can help

00:12:52,399 --> 00:12:55,519
close the loop

00:12:53,519 --> 00:12:57,760
while averaging infrastructure telemetry

00:12:55,519 --> 00:12:59,920
for workload management

00:12:57,760 --> 00:13:00,880
analytics methods can include streaming

00:12:59,920 --> 00:13:02,959
analytics

00:13:00,880 --> 00:13:04,079
or more traditional periodic collection

00:13:02,959 --> 00:13:06,320
methods and

00:13:04,079 --> 00:13:08,639
insights can be generated using either

00:13:06,320 --> 00:13:08,639
method

00:13:09,519 --> 00:13:12,800
the closed-loop resiliency demo

00:13:11,440 --> 00:13:14,560
indicated here

00:13:12,800 --> 00:13:16,639
highlights the importance of runtime

00:13:14,560 --> 00:13:19,040
telemetry and analytics

00:13:16,639 --> 00:13:20,000
within the kubernetes cluster and its

00:13:19,040 --> 00:13:23,200
impact on workload

00:13:20,000 --> 00:13:24,880
scheduling and migration policies as

00:13:23,200 --> 00:13:27,040
described in figure 2

00:13:24,880 --> 00:13:29,360
analytics component leverage runtime

00:13:27,040 --> 00:13:31,839
telemetry from collectee collector

00:13:29,360 --> 00:13:34,079
across the notes to trigger alerts from

00:13:31,839 --> 00:13:36,480
prometheus alert manager

00:13:34,079 --> 00:13:37,760
platform telemetry like last level cache

00:13:36,480 --> 00:13:39,519
occupancy

00:13:37,760 --> 00:13:41,680
cache misses memory bandwidth

00:13:39,519 --> 00:13:44,160
temperature refine speed etc

00:13:41,680 --> 00:13:46,480
are used to calculate hostile indicator

00:13:44,160 --> 00:13:48,800
by the analytics component

00:13:46,480 --> 00:13:49,760
kubernetes telemetry over scheduler can

00:13:48,800 --> 00:13:51,760
then decide

00:13:49,760 --> 00:13:53,279
if workload needs to be migrated from

00:13:51,760 --> 00:13:57,120
node 1 to node 2

00:13:53,279 --> 00:13:57,120
using alert levels from prometheus

00:13:58,079 --> 00:14:02,079
zooming into the analytics component

00:14:00,240 --> 00:14:02,800
figure 3 shows the usage of streaming

00:14:02,079 --> 00:14:05,760
analytics

00:14:02,800 --> 00:14:07,440
via kafka and k-sql to calculate host

00:14:05,760 --> 00:14:09,040
health indicator

00:14:07,440 --> 00:14:11,680
the indicator is transcript by

00:14:09,040 --> 00:14:13,519
prometheus and alert manager generates

00:14:11,680 --> 00:14:16,720
critical or minor alerts

00:14:13,519 --> 00:14:18,240
based on preset conditions kubernetes

00:14:16,720 --> 00:14:20,399
telemetry ever scheduler

00:14:18,240 --> 00:14:22,000
lets the workload continue running in

00:14:20,399 --> 00:14:24,639
case of minor alert

00:14:22,000 --> 00:14:25,680
or can decide to migrate the workload to

00:14:24,639 --> 00:14:29,199
node 2

00:14:25,680 --> 00:14:31,199
in case of critical alert this demo here

00:14:29,199 --> 00:14:33,199
by indicates the importance of

00:14:31,199 --> 00:14:35,760
software and hardware components

00:14:33,199 --> 00:14:36,959
involved in realizing a low-latency

00:14:35,760 --> 00:14:40,639
closed-loop scenario

00:14:36,959 --> 00:14:40,639
for infrastructure resiliency

00:14:42,240 --> 00:14:46,000
with the when the demonstrated concept

00:14:44,399 --> 00:14:48,079
is applied at scale

00:14:46,000 --> 00:14:50,720
there are few important takeaways that

00:14:48,079 --> 00:14:53,839
help with cloud-native telco deployment

00:14:50,720 --> 00:14:56,160
number one platform telemetry beyond the

00:14:53,839 --> 00:14:58,240
usual cpu and memory metrics

00:14:56,160 --> 00:15:00,959
are crucial to minimize downtime and

00:14:58,240 --> 00:15:03,279
maximize service availability

00:15:00,959 --> 00:15:04,560
some of the key metrics include but not

00:15:03,279 --> 00:15:07,440
limited to

00:15:04,560 --> 00:15:08,560
cpu loss level cash misses occupancy

00:15:07,440 --> 00:15:10,880
levels

00:15:08,560 --> 00:15:11,600
memory bandwidth power and energy

00:15:10,880 --> 00:15:14,240
metrics

00:15:11,600 --> 00:15:15,040
network interface metrics corrected

00:15:14,240 --> 00:15:17,440
memory

00:15:15,040 --> 00:15:19,040
errors that indicate memory reliability

00:15:17,440 --> 00:15:21,440
etc

00:15:19,040 --> 00:15:22,880
number two streaming analytics and

00:15:21,440 --> 00:15:25,440
runtime data processing

00:15:22,880 --> 00:15:26,000
closer to workload is important to take

00:15:25,440 --> 00:15:28,160
corrective

00:15:26,000 --> 00:15:29,920
actions for low latency closed loop

00:15:28,160 --> 00:15:32,160
scenarios

00:15:29,920 --> 00:15:33,920
finally integration of prometheus and

00:15:32,160 --> 00:15:35,199
kubernetes telemetry where scheduler

00:15:33,920 --> 00:15:36,639
extensions

00:15:35,199 --> 00:15:41,839
help provide runtime resource

00:15:36,639 --> 00:15:41,839
availability based workload management

00:15:42,639 --> 00:15:46,000
here are some more references to help

00:15:44,240 --> 00:15:48,079
you look into more details

00:15:46,000 --> 00:15:50,720
and i'm happy to follow up offline for

00:15:48,079 --> 00:15:52,560
any questions

00:15:50,720 --> 00:15:55,680
with that thank you very much for your

00:15:52,560 --> 00:15:58,800
attendance today

00:15:55,680 --> 00:15:59,600
thank you next up we have mate who works

00:15:58,800 --> 00:16:01,600
on timescale

00:15:59,600 --> 00:16:03,199
who's going to talk about long tails

00:16:01,600 --> 00:16:04,639
everywhere lessons learned

00:16:03,199 --> 00:16:06,320
optimizing relational schema for

00:16:04,639 --> 00:16:09,440
prometheus data

00:16:06,320 --> 00:16:10,320
my name is matt and i work at time

00:16:09,440 --> 00:16:13,759
schedule

00:16:10,320 --> 00:16:14,639
on integrating time scale as a long-term

00:16:13,759 --> 00:16:18,959
store

00:16:14,639 --> 00:16:21,519
for prometheus data some people may ask

00:16:18,959 --> 00:16:23,199
is it even possible to scale a

00:16:21,519 --> 00:16:25,920
relational database

00:16:23,199 --> 00:16:28,000
to support the high interest rate of

00:16:25,920 --> 00:16:30,800
prometheus data

00:16:28,000 --> 00:16:32,000
well you're right to ask because

00:16:30,800 --> 00:16:35,279
traditionally

00:16:32,000 --> 00:16:38,639
this has not been possible

00:16:35,279 --> 00:16:43,120
but with time scale db you can indeed

00:16:38,639 --> 00:16:46,240
store time series data in postgres

00:16:43,120 --> 00:16:49,680
and support the high interest rates and

00:16:46,240 --> 00:16:53,040
volumes associated with prometheus

00:16:49,680 --> 00:16:55,680
and metric data in fact

00:16:53,040 --> 00:16:58,320
we support hundreds of billions of

00:16:55,680 --> 00:17:02,160
metrics on a single box

00:16:58,320 --> 00:17:05,360
have efficient time series compression

00:17:02,160 --> 00:17:08,240
and this summer we will release

00:17:05,360 --> 00:17:10,559
a horizontally scalable version of the

00:17:08,240 --> 00:17:13,199
database

00:17:10,559 --> 00:17:16,000
and now let me explain why you would

00:17:13,199 --> 00:17:16,000
want to do this

00:17:16,160 --> 00:17:24,000
well an award support for sql analytics

00:17:20,400 --> 00:17:27,600
by adding sql will allow you to perform

00:17:24,000 --> 00:17:32,080
much deeper analytics including

00:17:27,600 --> 00:17:35,600
arbitrary aggregates and olap roll ups

00:17:32,080 --> 00:17:38,720
joins with other data in your system

00:17:35,600 --> 00:17:41,360
and integrations with existing tools

00:17:38,720 --> 00:17:44,080
for machine learning and predictive

00:17:41,360 --> 00:17:47,120
analytics

00:17:44,080 --> 00:17:50,080
so how do we do this

00:17:47,120 --> 00:17:51,280
the first question we had when designing

00:17:50,080 --> 00:17:54,400
the schema

00:17:51,280 --> 00:17:57,760
was whether we should put all metrics

00:17:54,400 --> 00:18:01,039
into one global table or split up

00:17:57,760 --> 00:18:03,840
metrics into separate tables

00:18:01,039 --> 00:18:04,960
to answer this we analyzed the data

00:18:03,840 --> 00:18:08,400
distribution

00:18:04,960 --> 00:18:10,640
of prometheus data and found that the

00:18:08,400 --> 00:18:15,120
causality of metrics

00:18:10,640 --> 00:18:18,039
further loads a long-tailed distribution

00:18:15,120 --> 00:18:19,600
where a few metrics had very high

00:18:18,039 --> 00:18:23,919
cardinality

00:18:19,600 --> 00:18:27,120
but most had low cardinality

00:18:23,919 --> 00:18:31,120
this motivated our decision to split up

00:18:27,120 --> 00:18:34,160
metrics in order to avoid performance

00:18:31,120 --> 00:18:37,360
interference between metrics with

00:18:34,160 --> 00:18:40,960
very different data distribution

00:18:37,360 --> 00:18:44,320
properties this also allows us to

00:18:40,960 --> 00:18:47,520
optimize single metric queries

00:18:44,320 --> 00:18:50,799
which we found to be much more common

00:18:47,520 --> 00:18:53,520
than the multimetric ones

00:18:50,799 --> 00:18:55,600
the second question was how to normalize

00:18:53,520 --> 00:18:58,720
tags

00:18:55,600 --> 00:18:59,679
the issue is that tags are big because

00:18:58,720 --> 00:19:03,600
they contain

00:18:59,679 --> 00:19:06,320
a lot of long repeating strings

00:19:03,600 --> 00:19:08,480
so you don't want to store them in json

00:19:06,320 --> 00:19:11,360
blobs

00:19:08,480 --> 00:19:12,480
we came up with a normalized schema

00:19:11,360 --> 00:19:16,160
where the

00:19:12,480 --> 00:19:19,200
text for the keys and values of labels

00:19:16,160 --> 00:19:19,919
was normalized out into a separate

00:19:19,200 --> 00:19:23,280
labels

00:19:19,919 --> 00:19:26,320
table and the series table

00:19:23,280 --> 00:19:27,039
simply contained an array of foreign

00:19:26,320 --> 00:19:30,400
keys

00:19:27,039 --> 00:19:33,440
into the labels this avoids

00:19:30,400 --> 00:19:37,360
storing repeating text strings

00:19:33,440 --> 00:19:40,799
multiple times but there was a problem

00:19:37,360 --> 00:19:40,799
with this implementation

00:19:41,520 --> 00:19:46,080
in that uh grouping by a particular

00:19:44,559 --> 00:19:49,840
label key

00:19:46,080 --> 00:19:53,039
say namespace became inefficient

00:19:49,840 --> 00:19:53,600
because you have to join every foreign

00:19:53,039 --> 00:19:56,720
key

00:19:53,600 --> 00:19:59,919
in your label array to the label table

00:19:56,720 --> 00:20:01,679
to find the one single label for the

00:19:59,919 --> 00:20:05,679
namespace key

00:20:01,679 --> 00:20:06,559
that's a lot of drawings but we quickly

00:20:05,679 --> 00:20:09,360
realized

00:20:06,559 --> 00:20:10,240
that if you could just map a particular

00:20:09,360 --> 00:20:13,679
index

00:20:10,240 --> 00:20:17,360
in the label array to a label key

00:20:13,679 --> 00:20:20,799
you'd be able to retrieve the identifier

00:20:17,360 --> 00:20:23,840
for a given key thus

00:20:20,799 --> 00:20:27,200
avoiding all the joints to

00:20:23,840 --> 00:20:30,000
the label table this

00:20:27,200 --> 00:20:31,039
also allows you to create easy to use

00:20:30,000 --> 00:20:33,919
views

00:20:31,039 --> 00:20:35,360
for people to query by aliasing

00:20:33,919 --> 00:20:39,679
particular

00:20:35,360 --> 00:20:42,799
label keys to columns in the view

00:20:39,679 --> 00:20:43,360
here are some examples of queries you

00:20:42,799 --> 00:20:46,640
could do

00:20:43,360 --> 00:20:50,240
on that view which you wouldn't be able

00:20:46,640 --> 00:20:54,280
to do in plain prom ql you could take

00:20:50,240 --> 00:20:57,520
aggregates over both time and series

00:20:54,280 --> 00:20:58,559
simultaneously you could calculate the

00:20:57,520 --> 00:21:02,000
roll-ups and

00:20:58,559 --> 00:21:05,600
cubes set permissions on particular

00:21:02,000 --> 00:21:06,640
metrics perform joints with auxiliary

00:21:05,600 --> 00:21:08,799
data

00:21:06,640 --> 00:21:09,679
and use more advanced statistical

00:21:08,799 --> 00:21:13,280
functions

00:21:09,679 --> 00:21:13,919
even writing your own there's much more

00:21:13,280 --> 00:21:16,480
info

00:21:13,919 --> 00:21:18,480
in the design dock and feel free to

00:21:16,480 --> 00:21:20,960
email me with any questions

00:21:18,480 --> 00:21:20,960
thank you

00:21:22,640 --> 00:21:29,440
okay hey thank you so

00:21:25,840 --> 00:21:33,280
our next talk then is from

00:21:29,440 --> 00:21:33,280
i apologize for the pronunciation

00:21:34,640 --> 00:21:39,760
about smoothly upgrading exporters with

00:21:36,960 --> 00:21:39,760
breaking changes

00:21:40,559 --> 00:21:44,720
today we're going to look at how to

00:21:41,840 --> 00:21:48,400
smoothly upgrade your ramita's exporters

00:21:44,720 --> 00:21:49,760
which have breaking changes so this talk

00:21:48,400 --> 00:21:52,400
came up to be because of

00:21:49,760 --> 00:21:54,559
some work that we did in our project

00:21:52,400 --> 00:21:58,159
around the start of this year

00:21:54,559 --> 00:22:00,559
so we were upgrading our node exporters

00:21:58,159 --> 00:22:03,039
and uh while doing this we noticed that

00:22:00,559 --> 00:22:04,799
there were some breaking changes

00:22:03,039 --> 00:22:06,159
and we noticed this only when our

00:22:04,799 --> 00:22:09,280
graphical dashboards

00:22:06,159 --> 00:22:11,039
were broken so uh

00:22:09,280 --> 00:22:13,280
yeah we started checking out what went

00:22:11,039 --> 00:22:16,640
wrong in a graphing dashboard

00:22:13,280 --> 00:22:18,320
and we found out that the obvious thing

00:22:16,640 --> 00:22:19,360
that can go wrong with an exporter which

00:22:18,320 --> 00:22:22,720
is

00:22:19,360 --> 00:22:25,840
the difference of metrics so the older

00:22:22,720 --> 00:22:27,520
exporter version had a different set of

00:22:25,840 --> 00:22:30,640
metrics and the newer exporter

00:22:27,520 --> 00:22:32,000
version had a different set of metals so

00:22:30,640 --> 00:22:35,039
when i say different

00:22:32,000 --> 00:22:38,320
there were some metric name changes

00:22:35,039 --> 00:22:40,320
or some metrics were removed

00:22:38,320 --> 00:22:42,240
and the same was happening for metric

00:22:40,320 --> 00:22:45,600
labels

00:22:42,240 --> 00:22:46,720
so uh one example was no network

00:22:45,600 --> 00:22:49,039
transmit

00:22:46,720 --> 00:22:52,080
bytes was converted to node network

00:22:49,039 --> 00:22:52,080
transfer bytes total

00:22:52,159 --> 00:22:55,440
so what we did was we uh did some manual

00:22:54,960 --> 00:22:58,000
work

00:22:55,440 --> 00:22:59,919
and we also used some javascript code to

00:22:58,000 --> 00:23:03,120
find out the differences

00:22:59,919 --> 00:23:05,840
between the two versions so uh

00:23:03,120 --> 00:23:06,320
and we created a map wherein we said if

00:23:05,840 --> 00:23:10,000
the

00:23:06,320 --> 00:23:12,320
old name old metric name is this

00:23:10,000 --> 00:23:14,400
then the new metric name is this if

00:23:12,320 --> 00:23:17,600
there is a breaking change

00:23:14,400 --> 00:23:20,840
and we did the same for label names

00:23:17,600 --> 00:23:23,760
and with that data what we did was

00:23:20,840 --> 00:23:25,200
we downloaded the grafana dashboard

00:23:23,760 --> 00:23:27,360
jsons

00:23:25,200 --> 00:23:28,720
which represent the grafana dashboards

00:23:27,360 --> 00:23:31,679
and we used

00:23:28,720 --> 00:23:35,039
bash scripts and changed the name the

00:23:31,679 --> 00:23:37,440
metric names and label names in these

00:23:35,039 --> 00:23:38,799
and uh we put back the graphene

00:23:37,440 --> 00:23:42,000
dashboard json

00:23:38,799 --> 00:23:42,320
back in graphene and the scale of this

00:23:42,000 --> 00:23:44,559
uh

00:23:42,320 --> 00:23:47,679
problem that we had was only around some

00:23:44,559 --> 00:23:49,440
10 to 15 gram dashboards that's it

00:23:47,679 --> 00:23:50,960
so we could afford to do some manual

00:23:49,440 --> 00:23:54,720
work and

00:23:50,960 --> 00:23:57,120
also use basic scripts

00:23:54,720 --> 00:23:59,360
but soon we realized that if the scale

00:23:57,120 --> 00:24:01,039
of the problem was bigger

00:23:59,360 --> 00:24:02,559
let's say you have lot of exporters that

00:24:01,039 --> 00:24:06,159
have some breaking changes in your way

00:24:02,559 --> 00:24:08,640
upgraded them and you also have lot of

00:24:06,159 --> 00:24:09,919
grafana dashboards that have that need

00:24:08,640 --> 00:24:12,240
changes

00:24:09,919 --> 00:24:14,080
then you would have to use some better

00:24:12,240 --> 00:24:16,840
tooling

00:24:14,080 --> 00:24:18,159
so you could use something like a golang

00:24:16,840 --> 00:24:19,919
tool uh

00:24:18,159 --> 00:24:22,240
since pramits and grafana are returning

00:24:19,919 --> 00:24:25,520
golan and you can tightly

00:24:22,240 --> 00:24:28,640
uh you know integrate with them and

00:24:25,520 --> 00:24:30,559
use the packages in them and do your

00:24:28,640 --> 00:24:33,120
work in a better manner

00:24:30,559 --> 00:24:34,720
so i'll show a quick demo of one tool i

00:24:33,120 --> 00:24:38,240
built

00:24:34,720 --> 00:24:40,799
so uh i have the

00:24:38,240 --> 00:24:41,679
you know node exporter version 0.15

00:24:40,799 --> 00:24:46,320
metrics

00:24:41,679 --> 00:24:48,799
here and for version 0.18

00:24:46,320 --> 00:24:51,039
and if i run my tool for this it shows

00:24:48,799 --> 00:24:53,600
me the difference

00:24:51,039 --> 00:24:55,120
between the metric names so these are

00:24:53,600 --> 00:24:56,240
the old metric names that are not

00:24:55,120 --> 00:25:00,080
present in the new

00:24:56,240 --> 00:25:03,200
version so this is one such example

00:25:00,080 --> 00:25:07,440
and you can build more tools for

00:25:03,200 --> 00:25:10,640
you know graphina dashboard json changes

00:25:07,440 --> 00:25:13,440
and uh you know

00:25:10,640 --> 00:25:15,440
the same for uh you know to upload

00:25:13,440 --> 00:25:16,960
graphing dashboard json or to download

00:25:15,440 --> 00:25:20,159
it

00:25:16,960 --> 00:25:23,360
and that was my talk uh

00:25:20,159 --> 00:25:23,919
and one thing to notice uh we faced this

00:25:23,360 --> 00:25:26,640
problem

00:25:23,919 --> 00:25:28,640
uh because our graphana dashboards were

00:25:26,640 --> 00:25:32,000
custom built

00:25:28,640 --> 00:25:32,960
and if you get your governor dashboard

00:25:32,000 --> 00:25:35,120
from

00:25:32,960 --> 00:25:37,039
the internet and you upgrade your

00:25:35,120 --> 00:25:38,320
exporter you can also upgrade your

00:25:37,039 --> 00:25:39,600
grafana dashboard by

00:25:38,320 --> 00:25:42,240
getting the newer version from the

00:25:39,600 --> 00:25:42,720
internet so you wouldn't have this

00:25:42,240 --> 00:25:45,760
problem

00:25:42,720 --> 00:25:46,240
and uh another way to solve the problem

00:25:45,760 --> 00:25:49,600
is just

00:25:46,240 --> 00:25:51,600
don't upgrade uh you know but at some

00:25:49,600 --> 00:25:54,799
point you'll have to upgrade yes

00:25:51,600 --> 00:25:58,720
uh i mean you'll have to see the cost

00:25:54,799 --> 00:26:00,640
versus benefit analysis and

00:25:58,720 --> 00:26:02,400
the sooner you upgrade it's better for

00:26:00,640 --> 00:26:05,600
you yeah

00:26:02,400 --> 00:26:06,799
so that was my talk and i'm karupa

00:26:05,600 --> 00:26:09,440
i'm an application developer at

00:26:06,799 --> 00:26:11,039
thoughtworks and you can reach me at

00:26:09,440 --> 00:26:13,520
corporate 7890

00:26:11,039 --> 00:26:14,320
at github and twitter and you can also

00:26:13,520 --> 00:26:17,360
reach me

00:26:14,320 --> 00:26:21,039
in my email at carpes7890

00:26:17,360 --> 00:26:24,720
gmail.com thank you

00:26:21,039 --> 00:26:29,120
okay thank you our next talk

00:26:24,720 --> 00:26:31,440
is on hi everyone my name is david a

00:26:29,120 --> 00:26:32,159
operational sandbox for prometheus by

00:26:31,440 --> 00:26:34,880
david

00:26:32,159 --> 00:26:34,880
grizzanti

00:26:36,720 --> 00:26:39,919
dante and i'm going to talk a little bit

00:26:38,080 --> 00:26:40,880
today about a sandbox project for

00:26:39,919 --> 00:26:42,080
prometheus

00:26:40,880 --> 00:26:44,640
that my team and i have been working on

00:26:42,080 --> 00:26:46,320
a comcast so i work on a team that's

00:26:44,640 --> 00:26:49,440
offering prometheus as a service

00:26:46,320 --> 00:26:50,799
to developers at comcast and we follow a

00:26:49,440 --> 00:26:52,080
pull request model

00:26:50,799 --> 00:26:54,480
for our clients when making

00:26:52,080 --> 00:26:56,480
configuration updates to their

00:26:54,480 --> 00:26:57,840
prometheus configuration in production

00:26:56,480 --> 00:27:01,039
this is a pull request

00:26:57,840 --> 00:27:02,000
to ci to production model and this as

00:27:01,039 --> 00:27:04,080
opposed to

00:27:02,000 --> 00:27:06,240
allowing users to ssh directly into the

00:27:04,080 --> 00:27:07,840
host to make their changes

00:27:06,240 --> 00:27:09,600
and this oftentimes poses some

00:27:07,840 --> 00:27:10,159
configuration challenges with first-time

00:27:09,600 --> 00:27:12,080
users

00:27:10,159 --> 00:27:14,000
and getting their configuration correct

00:27:12,080 --> 00:27:15,520
so a local developer setup doesn't

00:27:14,000 --> 00:27:17,279
you know mimic this environment

00:27:15,520 --> 00:27:19,120
oftentimes because they can't

00:27:17,279 --> 00:27:20,720
reach their desired targets in

00:27:19,120 --> 00:27:22,640
production due to connectivity

00:27:20,720 --> 00:27:24,640
and like i said we don't want anybody

00:27:22,640 --> 00:27:27,039
ssh in directly to test out these

00:27:24,640 --> 00:27:29,120
changes so developers are oftentimes

00:27:27,039 --> 00:27:31,039
looking for more of a rebel experience

00:27:29,120 --> 00:27:32,480
where they can test out changes

00:27:31,039 --> 00:27:34,640
immediately

00:27:32,480 --> 00:27:36,320
and see what the experience is like kind

00:27:34,640 --> 00:27:38,960
of like a readable print loop in a

00:27:36,320 --> 00:27:40,559
programming language environment

00:27:38,960 --> 00:27:42,080
so if you see this example over here to

00:27:40,559 --> 00:27:43,600
the right this is an

00:27:42,080 --> 00:27:45,440
example we often see where people are

00:27:43,600 --> 00:27:49,360
trying to pull

00:27:45,440 --> 00:27:50,960
hosts from aws filter on some tags

00:27:49,360 --> 00:27:53,840
and then pull some metadata back from

00:27:50,960 --> 00:27:55,919
aws to add to their metrics as labels

00:27:53,840 --> 00:27:57,440
and first time users don't often get

00:27:55,919 --> 00:28:00,080
this configuration

00:27:57,440 --> 00:28:02,320
correct so they end up following the

00:28:00,080 --> 00:28:05,600
cyclical process of opening the pr

00:28:02,320 --> 00:28:08,159
pushing the changes having an error and

00:28:05,600 --> 00:28:09,760
it just gets frustrating for them so we

00:28:08,159 --> 00:28:11,919
built this small web app

00:28:09,760 --> 00:28:13,600
called prombox that gives the users an

00:28:11,919 --> 00:28:14,640
area to play in production

00:28:13,600 --> 00:28:16,720
but that's separate from their

00:28:14,640 --> 00:28:19,360
production instance so it's deployed in

00:28:16,720 --> 00:28:21,919
the same aws vpc or network zone

00:28:19,360 --> 00:28:23,600
uses the same prometheus docker image

00:28:21,919 --> 00:28:25,200
plus client configuration

00:28:23,600 --> 00:28:27,600
and it runs alongside prometheus as a

00:28:25,200 --> 00:28:28,799
sidecar so it has access to their shared

00:28:27,600 --> 00:28:32,080
file system

00:28:28,799 --> 00:28:32,080
so let's jump into a quick demo

00:28:33,120 --> 00:28:39,360
so here's the previous prombox ui

00:28:36,799 --> 00:28:40,960
you see that the configuration is shown

00:28:39,360 --> 00:28:42,240
and the users have access to edit this

00:28:40,960 --> 00:28:43,039
so they can scroll look at the whole

00:28:42,240 --> 00:28:45,360
config

00:28:43,039 --> 00:28:46,559
and make changes so if we try out just a

00:28:45,360 --> 00:28:49,679
simple change like

00:28:46,559 --> 00:28:50,559
make the scrape interval 30 seconds and

00:28:49,679 --> 00:28:52,799
then save it

00:28:50,559 --> 00:28:55,840
we can see that after saving it will

00:28:52,799 --> 00:28:58,080
give us a message

00:28:55,840 --> 00:28:59,360
and on the save we actually are running

00:28:58,080 --> 00:29:02,799
prom tool in the background

00:28:59,360 --> 00:29:05,360
and reloading prometheus via the reload

00:29:02,799 --> 00:29:06,720
admin api so if we try to add a new

00:29:05,360 --> 00:29:08,320
scrape job let's say

00:29:06,720 --> 00:29:10,640
but we you know let's say we make a

00:29:08,320 --> 00:29:12,640
mistake and when we save

00:29:10,640 --> 00:29:14,799
we should get an error saying that the

00:29:12,640 --> 00:29:16,640
configuration is bad

00:29:14,799 --> 00:29:17,840
and this kind of gives users that

00:29:16,640 --> 00:29:19,520
reptile experience i was mentioning

00:29:17,840 --> 00:29:21,120
before where they can try out changes

00:29:19,520 --> 00:29:22,480
and see what the errors are

00:29:21,120 --> 00:29:24,720
without having to commit the pull

00:29:22,480 --> 00:29:27,919
request process

00:29:24,720 --> 00:29:29,600
in addition to that we also pull in

00:29:27,919 --> 00:29:31,679
status information from prometheus via

00:29:29,600 --> 00:29:33,039
the api this is similar things you find

00:29:31,679 --> 00:29:35,520
in the prometheus ui

00:29:33,039 --> 00:29:37,039
such as configuration rules targets etc

00:29:35,520 --> 00:29:39,600
but to kind of fill that gap

00:29:37,039 --> 00:29:42,000
we're still giving users uh the

00:29:39,600 --> 00:29:43,760
prometheus ui directly

00:29:42,000 --> 00:29:45,679
via the frame option so this is

00:29:43,760 --> 00:29:47,200
basically prometheus in an iframe

00:29:45,679 --> 00:29:49,200
and it has the same interface you would

00:29:47,200 --> 00:29:50,640
get with prometheus

00:29:49,200 --> 00:29:52,240
and it is the local prometheus that's

00:29:50,640 --> 00:29:53,840
running alongside prombox

00:29:52,240 --> 00:29:55,600
and you can see your targets and

00:29:53,840 --> 00:29:57,360
everything else here so let's say we

00:29:55,600 --> 00:29:59,120
wanted to add a brand new target and see

00:29:57,360 --> 00:29:59,679
how that gets reflected on the targets

00:29:59,120 --> 00:30:03,120
page

00:29:59,679 --> 00:30:04,640
so we can go back to prombox add the new

00:30:03,120 --> 00:30:06,240
target

00:30:04,640 --> 00:30:08,880
save it and what should end up happening

00:30:06,240 --> 00:30:12,480
is we see that new target appear

00:30:08,880 --> 00:30:15,279
and the the benefit of that is you can

00:30:12,480 --> 00:30:16,480
see how the new targets show up and also

00:30:15,279 --> 00:30:18,240
like what labels

00:30:16,480 --> 00:30:19,840
are getting added if you want to do any

00:30:18,240 --> 00:30:21,520
kind of relay building logic

00:30:19,840 --> 00:30:22,960
or things like that and then you also

00:30:21,520 --> 00:30:23,520
have the ability to even see those

00:30:22,960 --> 00:30:24,799
metrics

00:30:23,520 --> 00:30:27,039
since we're showing the prometheus

00:30:24,799 --> 00:30:28,960
interface here so it's a pretty simple

00:30:27,039 --> 00:30:29,760
concept but we have plans to expand this

00:30:28,960 --> 00:30:32,080
further

00:30:29,760 --> 00:30:33,600
by adding easier configuration of adding

00:30:32,080 --> 00:30:34,399
scrape jobs outside of just editing the

00:30:33,600 --> 00:30:36,159
yaml

00:30:34,399 --> 00:30:37,679
adding new recording rules and adding

00:30:36,159 --> 00:30:39,440
new alerting rules

00:30:37,679 --> 00:30:41,760
that's all the time i have for today uh

00:30:39,440 --> 00:30:43,200
thanks and if you have any questions or

00:30:41,760 --> 00:30:45,440
just want a chat feel free to hit me up

00:30:43,200 --> 00:30:50,320
on twitter on my twitter handles in the

00:30:45,440 --> 00:30:53,520
lower left hand corner thanks

00:30:50,320 --> 00:30:56,799
okay thank you david then our

00:30:53,520 --> 00:30:58,000
final lightning talk for today is from

00:30:56,799 --> 00:31:00,640
rob skillington

00:30:58,000 --> 00:31:03,279
on highly available metrics with m3 and

00:31:00,640 --> 00:31:03,279
prometheus

00:31:03,360 --> 00:31:07,200
hi my name is rob i'm here to talk to

00:31:05,200 --> 00:31:09,679
you about highly available metrics with

00:31:07,200 --> 00:31:13,200
m3 and prometheus i'm the creator of m3

00:31:09,679 --> 00:31:15,600
and a openmetrics cncf

00:31:13,200 --> 00:31:16,399
contributor m3 is a remote storage for

00:31:15,600 --> 00:31:17,919
prometheus

00:31:16,399 --> 00:31:19,919
you can use a kubernetes operator to

00:31:17,919 --> 00:31:20,720
deploy it into kubernetes and scale it

00:31:19,919 --> 00:31:22,320
up and down

00:31:20,720 --> 00:31:24,240
or you can use a single docker image for

00:31:22,320 --> 00:31:26,559
tests uh you could store metrics for

00:31:24,240 --> 00:31:27,679
weeks months or years with down sampling

00:31:26,559 --> 00:31:29,360
you can store metrics at different

00:31:27,679 --> 00:31:30,080
retention based on different mapping

00:31:29,360 --> 00:31:33,360
rules so

00:31:30,080 --> 00:31:35,919
optionally keeping some sla stats for

00:31:33,360 --> 00:31:37,760
certain months or years versus your

00:31:35,919 --> 00:31:38,640
whatever else you keep your fire hose

00:31:37,760 --> 00:31:41,120
for

00:31:38,640 --> 00:31:42,720
um allow for scaling high cardinality

00:31:41,120 --> 00:31:44,880
metrics by aggregating across dimensions

00:31:42,720 --> 00:31:46,480
that you want to keep

00:31:44,880 --> 00:31:47,919
at ingestion time rather than solely

00:31:46,480 --> 00:31:50,559
relying on recording rules

00:31:47,919 --> 00:31:50,960
uh which you know always add extra load

00:31:50,559 --> 00:31:52,480
um

00:31:50,960 --> 00:31:54,720
because you're querying them over and

00:31:52,480 --> 00:31:57,039
over again to produce a result

00:31:54,720 --> 00:31:58,880
and uh all entry also allows you to

00:31:57,039 --> 00:32:00,159
scale up storage just by adding nodes

00:31:58,880 --> 00:32:03,440
rebalancing

00:32:00,159 --> 00:32:04,880
the data using peer streaming so

00:32:03,440 --> 00:32:06,880
you know it's really designed for zero

00:32:04,880 --> 00:32:09,200
down time upgrades restarts

00:32:06,880 --> 00:32:11,039
and with standards high replica failures

00:32:09,200 --> 00:32:14,320
you will always be able to read

00:32:11,039 --> 00:32:16,720
and write data no matter what as long as

00:32:14,320 --> 00:32:18,159
two of your three replicas are up so you

00:32:16,720 --> 00:32:21,279
know a single note goes down

00:32:18,159 --> 00:32:23,840
that doesn't impact your visibility

00:32:21,279 --> 00:32:26,159
or insight into the system uh it keeps

00:32:23,840 --> 00:32:28,080
on going and is highly available

00:32:26,159 --> 00:32:29,679
so that's no matter whether you have one

00:32:28,080 --> 00:32:32,000
many or all nodes down

00:32:29,679 --> 00:32:33,200
um and three can also be deployed in a

00:32:32,000 --> 00:32:35,600
multi-region fashion

00:32:33,200 --> 00:32:37,519
and at query time join data across the

00:32:35,600 --> 00:32:40,559
regions if you want to keep your data

00:32:37,519 --> 00:32:42,320
data local to a region uh for to keep

00:32:40,559 --> 00:32:44,960
egress costs down um

00:32:42,320 --> 00:32:46,720
and at uber uh you know we really did

00:32:44,960 --> 00:32:49,039
have to focus on the scalability of this

00:32:46,720 --> 00:32:50,559
it can warehouse and does warehouse more

00:32:49,039 --> 00:32:51,360
than one petabyte of metrics data there

00:32:50,559 --> 00:32:54,399
and stores

00:32:51,360 --> 00:32:55,279
40 million data points per second um and

00:32:54,399 --> 00:32:56,559
so it's really

00:32:55,279 --> 00:32:58,399
again it's really only when you hit that

00:32:56,559 --> 00:33:01,120
second replica down do you

00:32:58,399 --> 00:33:01,760
see an impact um in rats uh you still

00:33:01,120 --> 00:33:05,039
actually

00:33:01,760 --> 00:33:08,080
have visibility in reads but um uh

00:33:05,039 --> 00:33:10,240
that's that's a side point so

00:33:08,080 --> 00:33:12,320
looking at the aggregator and how that's

00:33:10,240 --> 00:33:14,880
reliable and what that lets you do

00:33:12,320 --> 00:33:16,159
uh the aggregator is a streaming metrics

00:33:14,880 --> 00:33:18,640
aggregator which option

00:33:16,159 --> 00:33:19,760
lets you keep all of your metrics uh

00:33:18,640 --> 00:33:22,880
down sampled

00:33:19,760 --> 00:33:23,600
so that allows you to make your metrics

00:33:22,880 --> 00:33:24,960
less dense

00:33:23,600 --> 00:33:27,279
so when you're going to view them over

00:33:24,960 --> 00:33:28,960
months and years

00:33:27,279 --> 00:33:30,320
both the query time is fast and the

00:33:28,960 --> 00:33:32,080
storage cost is

00:33:30,320 --> 00:33:33,840
much cheaper than keeping all the raw

00:33:32,080 --> 00:33:36,399
and aggregated data

00:33:33,840 --> 00:33:37,519
and allows you to also create roll-up

00:33:36,399 --> 00:33:40,960
rules which

00:33:37,519 --> 00:33:42,080
can basically uh squash the cardinality

00:33:40,960 --> 00:33:44,320
of metrics down

00:33:42,080 --> 00:33:45,600
um and move expensive recording rules to

00:33:44,320 --> 00:33:48,480
streaming aggregation

00:33:45,600 --> 00:33:49,600
uh to store less and alert on very high

00:33:48,480 --> 00:33:52,720
cardinality metrics

00:33:49,600 --> 00:33:56,720
cheaply and it does this reliably by

00:33:52,720 --> 00:33:57,760
uh using etcd to transparently elect a

00:33:56,720 --> 00:34:00,320
leader

00:33:57,760 --> 00:34:01,120
across two availability zones or two

00:34:00,320 --> 00:34:03,600
replicas

00:34:01,120 --> 00:34:04,720
and essentially when it performs a

00:34:03,600 --> 00:34:07,120
failover it will

00:34:04,720 --> 00:34:08,480
make sure anything that wasn't flushed

00:34:07,120 --> 00:34:10,800
before

00:34:08,480 --> 00:34:12,399
an aggregator went down is flushed by

00:34:10,800 --> 00:34:15,440
its uh follower which

00:34:12,399 --> 00:34:17,200
takes over as leader so diving into a

00:34:15,440 --> 00:34:20,399
specific example of

00:34:17,200 --> 00:34:23,200
a high cardinality http latency metric

00:34:20,399 --> 00:34:25,760
here we're going to track basically the

00:34:23,200 --> 00:34:29,119
status codes and latency

00:34:25,760 --> 00:34:30,240
of 50 hp routes across 100 pods per

00:34:29,119 --> 00:34:32,320
region

00:34:30,240 --> 00:34:34,320
the bucket tag so say you have 30

00:34:32,320 --> 00:34:35,839
buckets status code tag

00:34:34,320 --> 00:34:38,480
say we have 10 status codes we're

00:34:35,839 --> 00:34:40,320
tracking 50 http routes 4 active server

00:34:38,480 --> 00:34:44,079
versions and 4 regions we

00:34:40,320 --> 00:34:44,800
run in if you add all these and multiply

00:34:44,079 --> 00:34:47,119
these

00:34:44,800 --> 00:34:48,399
unique values together we get 24 million

00:34:47,119 --> 00:34:50,240
unique series

00:34:48,399 --> 00:34:51,440
essentially using an aggregator roll up

00:34:50,240 --> 00:34:53,520
we can

00:34:51,440 --> 00:34:56,079
aggregate this in a streaming way at

00:34:53,520 --> 00:34:57,680
ingestion time down to 240 000 time

00:34:56,079 --> 00:35:00,480
series if we don't care about

00:34:57,680 --> 00:35:03,040
the pod label uh when we go to you know

00:35:00,480 --> 00:35:05,040
query or alert off these um

00:35:03,040 --> 00:35:07,040
to get a high thousand to get a high ten

00:35:05,040 --> 00:35:08,960
thousand foot few into these metrics

00:35:07,040 --> 00:35:10,160
um and you can still slice and dice by

00:35:08,960 --> 00:35:13,520
git region

00:35:10,160 --> 00:35:15,680
route um status and all that um and so

00:35:13,520 --> 00:35:17,680
you know uh so that number going for 24

00:35:15,680 --> 00:35:19,359
million to 240 000 series if you're

00:35:17,680 --> 00:35:20,079
looking at a single http route you only

00:35:19,359 --> 00:35:22,160
need to touch

00:35:20,079 --> 00:35:23,200
like 5 000 time series now so it's much

00:35:22,160 --> 00:35:24,880
much faster

00:35:23,200 --> 00:35:26,720
so breaking down how that actually works

00:35:24,880 --> 00:35:28,560
inside m3 basically

00:35:26,720 --> 00:35:30,560
uh we formed this roll-off rule on the

00:35:28,560 --> 00:35:33,440
right which is valid yaml for

00:35:30,560 --> 00:35:35,839
a roller pull inside n3 we basically

00:35:33,440 --> 00:35:36,560
take the increase of these original time

00:35:35,839 --> 00:35:39,680
series that

00:35:36,560 --> 00:35:43,200
are per pod um then essentially

00:35:39,680 --> 00:35:44,400
we sum the deltas that we took on perpod

00:35:43,200 --> 00:35:46,560
by uh

00:35:44,400 --> 00:35:47,839
the you know dimensions we actually care

00:35:46,560 --> 00:35:49,839
about ellie getshar

00:35:47,839 --> 00:35:52,000
route status current region uh then we

00:35:49,839 --> 00:35:54,560
get a resulting delta time series

00:35:52,000 --> 00:35:55,280
then essentially we do an a running

00:35:54,560 --> 00:35:58,320
cumulative

00:35:55,280 --> 00:36:00,400
sum on these to get a monotonic counter

00:35:58,320 --> 00:36:01,440
based on the dimensions that we still uh

00:36:00,400 --> 00:36:03,280
care about

00:36:01,440 --> 00:36:05,119
um and then essentially you know the

00:36:03,280 --> 00:36:06,560
summary here is this allows you to

00:36:05,119 --> 00:36:08,000
convert millions of times series to

00:36:06,560 --> 00:36:09,680
thousands

00:36:08,000 --> 00:36:11,520
uh and it allows for very high

00:36:09,680 --> 00:36:15,440
cardinality metrics

00:36:11,520 --> 00:36:17,599
to impact your tsdb by performing

00:36:15,440 --> 00:36:18,800
constant query load um the way that

00:36:17,599 --> 00:36:20,640
recording rules do

00:36:18,800 --> 00:36:22,000
recording rules are still super valuable

00:36:20,640 --> 00:36:24,079
it's just that for some of these high

00:36:22,000 --> 00:36:25,760
cardinality workloads it's definitely

00:36:24,079 --> 00:36:26,960
can be done cheaper this way it

00:36:25,760 --> 00:36:27,520
optionally lets you drop on interesting

00:36:26,960 --> 00:36:29,680
labels

00:36:27,520 --> 00:36:31,040
um and keeps the ability to apply

00:36:29,680 --> 00:36:33,359
further transforms

00:36:31,040 --> 00:36:34,400
uh thank you to all the m3 contributors

00:36:33,359 --> 00:36:37,839
if you want to learn more here are some

00:36:34,400 --> 00:36:37,839
links and thank you for listening to my

00:36:38,839 --> 00:36:41,839
talk

00:36:44,640 --> 00:36:49,839
[Music]

00:36:47,760 --> 00:36:49,839

YouTube URL: https://www.youtube.com/watch?v=F0y4L9S8mQU


